{"used_features": ["def feature(text: str) -> float:\n    'Contraction ratio: fraction of tokens that are contractions (contain apostrophes like \"don\\'t\", \"I\\'m\", \"it\\'s\")'\n    import re\n    if not text:\n        return 0.0\n    # count contraction patterns with apostrophe\n    contr = re.findall(r\"\\b\\w+'[a-z]{1,4}\\b\", text.lower())\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(len(contr))\n    return len(contr) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    \"Smoothed ratio of contraction-like apostrophes to possessive \\\"'s\\\" occurrences\"\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    poss = len(re.findall(r\"\\b\\w+'s\\b\", lower))\n    # contractions: n't, 're, 've, 'll, 'm, 'd (exclude 's)\n    contr = len(re.findall(r\"\\b\\w+(?:n't|'re|'ve|'ll|'m|'d)\\b\", lower))\n    # smoothing to avoid division by zero\n    return float(contr + 1) / float(poss + 1)\n\n", "def feature(text: str) -> float:\n    'Fraction of long words (length > 7) as a marker of lexical sophistication'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 7)\n    return float(long_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words \u2014 high variability can indicate dialog or informal style'\n    import re, math\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    lengths = [len(re.findall(r'\\b[\\w\\']+\\b', s)) for s in sents]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n", "def feature(text: str) -> float:\n    'Parentheses density: fraction of characters that are parentheses (common in academic/case documents with citations)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    return float(paren_count) / float(total_chars)\n\n\n", "def feature(text: str) -> float:\n    'Contraction ratio: fraction of tokens that are contractions (contain apostrophes like \"don\\'t\", \"I\\'m\", \"it\\'s\")'\n    import re\n    if not text:\n        return 0.0\n    # count contraction patterns with apostrophe\n    contr = re.findall(r\"\\b\\w+'[a-z]{1,4}\\b\", text.lower())\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(len(contr))\n    return len(contr) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (lexical diversity): unique word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words \u2014 high variability can indicate dialog or informal style'\n    import re, math\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    lengths = [len(re.findall(r'\\b[\\w\\']+\\b', s)) for s in sents]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n", "def feature(text: str) -> float:\n    'Density of newline characters (newlines per character) to detect line-oriented formats'\n    if not text:\n        return 0.0\n    newlines = text.count('\\n')\n    return float(newlines) / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending with -ed (simple past/participial marker density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z'-]+\", text.lower())\n    if not tokens:\n        return 0.0\n    ed_count = sum(1 for t in tokens if t.endswith('ed') and len(t) > 3)\n    return float(ed_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters present in the text'\n    if not text:\n        return 0.0\n    puncts = {c for c in text if not c.isalnum() and not c.isspace()}\n    return float(len(puncts))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words \u2014 high variability can indicate dialog or informal style'\n    import re, math\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    lengths = [len(re.findall(r'\\b[\\w\\']+\\b', s)) for s in sents]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like simple past-tense verbs (end with \"ed\"), as a loose proxy for narrative past tense usage'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count_ed = sum(1 for t in tokens if len(t) > 3 and t.endswith('ed'))\n    return count_ed / len(tokens)\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique lowercase word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"[A-Za-z']+\", text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of long words (length > 7) as a marker of lexical sophistication'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 7)\n    return float(long_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (approx.), using split on .!? to capture narrative short sentences vs long formal sentences'\n    if not text:\n        return 0.0\n    import re\n    # split into sentences by punctuation, ignore empty fragments\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(s.split()) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts)) / float(len(word_counts))\n\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), robust to missing sentence punctuation'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_words = len(words)\n    # Count non-empty sentence-like segments\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sentences = len(sentences) if sentences else 1\n    return total_words / float(num_sentences)\n\n", "def feature(text: str) -> float:\n    'Parentheses density: fraction of characters that are parentheses (common in academic/case documents with citations)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    return float(paren_count) / float(total_chars)\n\n\n", "def feature(text: str) -> float:\n    \"Fraction of tokens that look like simple past-tense verbs (tokens ending with 'ed')\"\n    import re\n    try:\n        tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n        if not tokens:\n            return 0.0\n        ed_count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ed'))\n        return float(ed_count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Density of explicit line breaks: number of newline characters normalized by text length'\n    if not text:\n        return 0.0\n    # Normalize by length to avoid division by zero\n    return float(text.count('\\n')) / float(len(text) + 1)\n\n", "def feature(text: str) -> float:\n    'Punctuation-to-word ratio: total punctuation characters divided by word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    num_words = len(words)\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if num_words == 0:\n        return float(punct_count)\n    return punct_count / float(num_words)\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that are short (<= 5 words) \u2014 captures poetic or clipped/dialogue style'\n    import re\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    nonempty = [ln for ln in lines if ln.strip() != '']\n    if not nonempty:\n        return 0.0\n    short_lines = 0\n    for ln in nonempty:\n        wc = len(re.findall(r'\\w+', ln))\n        if 0 < wc <= 5:\n            short_lines += 1\n    return short_lines / len(nonempty)\n\n", "def feature(text: str) -> float:\n    'Normalized count of 4-digit years (1000-2099) appearing in the text'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(?:1[0-9]{3}|20[0-9]{2})\\b', text)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(years)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Stopword ratio: fraction of tokens that are common English stopwords (function-word density)'\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you',\n        'do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one',\n        'all','would','there','their','what','so','up','out','if','about','who','get','which','go','me'\n    }\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of possessive apostrophe tokens (\\'s or \u2019s) to total apostrophe occurrences (possessive vs contraction tendency)'\n    import re\n    if not text:\n        return 0.0\n    # count possessive forms 's and \u2019s (lowercase or uppercase)\n    possessive = len(re.findall(r\"(?:'s|\u2019s)\\b\", text, flags=re.IGNORECASE))\n    total_apost = text.count(\"'\") + text.count('\\u2019') + text.count('\\u2018')\n    if total_apost == 0:\n        return 0.0\n    return float(possessive) / float(total_apost)\n", "def feature(text: str) -> float:\n    'Past-tense-looking word ratio: fraction of words ending in \"ed\" (simple proxy for past-tense narration)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    past = sum(1 for w in words if len(w) > 2 and w.endswith('ed'))\n    return past / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Variance of sentence lengths (words per sentence) \u2014 narratives often have higher variance than formal prose'\n    import re, math\n    if not text:\n        return 0.0\n    # split on sentence terminators but keep non-empty\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sentences) <= 1:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences if re.findall(r'\\w+', s)]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique words divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (length >= 7), a proxy for formality and vocabulary complexity'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 7)\n    return float(long_words) / len(words)\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation count'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    unique = len(set(puncts))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count), 0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = max(1, len(sentences))\n    return float(comma_count) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Past-tense-looking word ratio: fraction of words ending in \"ed\" (simple proxy for past-tense narration)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    past = sum(1 for w in words if len(w) > 2 and w.endswith('ed'))\n    return past / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    total = len(words)\n    if total == 0:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(total)\n", "def feature(text: str) -> float:\n    'Fraction of alphabetic tokens ending in \"ed\" (simple proxy for past-tense verb usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    words = max(1, len(tokens))\n    ed_count = 0\n    for t in tokens:\n        tl = t.lower()\n        if len(tl) > 3 and tl.endswith('ed'):\n            ed_count += 1\n    return float(ed_count) / words\n\n", "def feature(text: str) -> float:\n    'Proportion of text lines that begin with a dialogue marker (quote, dash, or quote-like characters)'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    count = 0\n    for l in lines:\n        stripped = l.lstrip()\n        if not stripped:\n            continue\n        first = stripped[0]\n        if first in {'\"', \"'\", '\u201c', '\u201d', '\u2014', '-', '\u2013'}:\n            count += 1\n    return float(count) / float(len(lines))\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a pronoun (I, he, she, they, we, it, you)'\n    import re\n    if not text:\n        return 0.0\n    # Split on sentence terminators; keep non-empty trimmed segments\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    pronouns = {'i', 'he', 'she', 'they', 'we', 'it', 'you'}\n    starts = 0\n    for s in sents:\n        w = re.findall(r'\\b\\w+\\b', s)\n        if not w:\n            continue\n        if w[0].lower() in pronouns:\n            starts += 1\n    return float(starts) / float(len(sents))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of quotation marks that are curly/typographic quotes (e.g., \u2018 \u2019 \u201c \u201d) vs plain ASCII quotes, indicating published/edited text'\n    import re\n    if not text:\n        return 0.0\n    # count curly quote characters and total quote-like characters\n    curly = sum(text.count(ch) for ch in ('\u2018','\u2019','\u201a','\u201c','\u201d','\u201e'))\n    plain = sum(text.count(ch) for ch in (\"'\", '\"'))\n    total = curly + plain\n    if total == 0:\n        return 0.0\n    return float(curly) / total\n\n", "def feature(text: str) -> float:\n    'Parenthesis/bracket density: fraction of characters that are parentheses or square brackets'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    count = sum(1 for c in text if c in '()[]{}')\n    return float(count) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Unique word ratio: number of unique words divided by total words (low values indicate repetition, might reveal certain generation patterns)'\n    if not text:\n        return 0.0\n    words = [w.strip(\".,;:\\\"'()[]{}\").lower() for w in text.split() if w.strip(\".,;:\\\"'()[]{}\")]\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n", "def feature(text: str) -> float:\n    'Diversity of punctuation: number of distinct punctuation characters divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of long words (length > 7) as a marker of lexical sophistication'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 7)\n    return float(long_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of a small set of formal/academic transition words (therefore, however, thus, furthermore, moreover) to total words'\n    import re\n    if not text:\n        return 0.0\n    transitions = {'therefore','however','thus','moreover','furthermore','consequently','additionally','hence','nevertheless','notwithstanding'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in transitions)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ed\" (simple past-tense heuristic)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[A-Za-z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ed_count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ed'))\n    return float(ed_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of simple past auxiliaries to present auxiliaries (was/were/had/did vs is/are/am/have/do) to hint at tense usage'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    past_set = {'was', 'were', 'had', 'did'}\n    present_set = {'is', 'are', 'am', 'have', 'do', 'does'}\n    past = sum(1 for w in words if w in past_set)\n    present = sum(1 for w in words if w in present_set)\n    # add small smoothing to avoid division by zero\n    return float(past + 0.5) / float(present + 0.5)\n\n", "def feature(text: str) -> float:\n    'Diversity of apostrophe-bearing tokens: unique apostrophe tokens divided by total apostrophe tokens (higher means many different contractions/possessives)'\n    import re\n    if not text:\n        return 0.0\n    apos_tokens = re.findall(r\"\\b[\\w']*'[^\\s]+\\b\", text)\n    total = len(apos_tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(t.lower() for t in apos_tokens))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens appearing inside double-quoted spans to total tokens (approximate dialogue proportion)'\n    import re\n    if not text:\n        return 0.0\n    total_tokens = len(re.findall(r'\\b\\w+\\b', text))\n    if total_tokens == 0:\n        return 0.0\n    parts = re.split(r'\"', text)\n    inside_tokens = 0\n    # odd-indexed parts are inside quotes when quotes are paired\n    for i in range(1, len(parts), 2):\n        inside_tokens += len(re.findall(r'\\b\\w+\\b', parts[i]))\n    return float(inside_tokens) / float(total_tokens)\n\n\n", "def feature(text: str) -> float:\n    'Normalized difference between exclamation/question density inside quotes vs outside (inside - outside)'\n    import re\n    if not text:\n        return 0.0\n    # find double-quoted spans and rest\n    quote_spans = re.findall(r'\"([^\"]*)\"', text)\n    inside_chars = sum(len(s) for s in quote_spans)\n    inside_marks = sum(s.count('!') + s.count('?') for s in quote_spans)\n    outside_text = re.sub(r'\"[^\"]*\"', ' ', text)\n    outside_chars = len(outside_text)\n    outside_marks = outside_text.count('!') + outside_text.count('?')\n    # normalize densities; handle zero-length regions\n    inside_density = (inside_marks / inside_chars) if inside_chars > 0 else 0.0\n    outside_density = (outside_marks / outside_chars) if outside_chars > 0 else 0.0\n    return float(inside_density - outside_density)\n\n", "def feature(text: str) -> float:\n    'Proportion of text lines that begin with a dialogue marker (quote, dash, or quote-like characters)'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    count = 0\n    for l in lines:\n        stripped = l.lstrip()\n        if not stripped:\n            continue\n        first = stripped[0]\n        if first in {'\"', \"'\", '\u201c', '\u201d', '\u2014', '-', '\u2013'}:\n            count += 1\n    return float(count) / float(len(lines))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (small anchored list) \u2014 can indicate function-word heavy prose vs descriptive'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {'the', 'and', 'of', 'to', 'a', 'in', 'that', 'is', 'was', 'it', 'for', 'on', 'with', 'as', 'by', 'an', 'be'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words \u2014 high variability can indicate dialog or informal style'\n    import re, math\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    lengths = [len(re.findall(r'\\b[\\w\\']+\\b', s)) for s in sents]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n", "def feature(text: str) -> float:\n    'Density of capitalized words that look like names or proper nouns (not sentence-start \"I\")'\n    try:\n        words = re.findall(r'\\b[A-Za-z][a-z]+\\b', text)\n    except Exception:\n        words = []\n    try:\n        total_words = re.findall(r'\\w+', text)\n    except Exception:\n        total_words = []\n    if not total_words:\n        return 0.0\n    # count capitalized words (first letter uppercase) excluding single-letter \"I\"\n    cap_count = sum(1 for w in words if w[0].isupper() and w.lower() != 'i')\n    return float(cap_count) / float(len(total_words))\n\n", "def feature(text: str) -> float:\n    'Balance between past-tense markers and present-tense markers: (past-present)/(past+present) in [-1,1]'\n    if not text:\n        return 0.0\n    try:\n        words = re.findall(r'\\w+', text.lower())\n    except Exception:\n        words = []\n    if not words:\n        return 0.0\n    past_markers = {'was', 'were', 'had', 'did'}\n    present_markers = {'is', 'am', 'are', 'have', 'has', 'do', 'does'}\n    past_ed = sum(1 for w in words if w.endswith('ed') and len(w) > 2)\n    past_count = past_ed + sum(1 for w in words if w in past_markers)\n    present_ing = sum(1 for w in words if w.endswith('ing') and len(w) > 3)\n    present_count = present_ing + sum(1 for w in words if w in present_markers)\n    denom = past_count + present_count\n    if denom == 0:\n        return 0.0\n    return float(past_count - present_count) / float(denom)\n", "def feature(text: str) -> float:\n    'Fraction of long words (length >= 8 characters) as a proxy for vocabulary complexity'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 8)\n    return float(long_words) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words; returns 0.0 for a single or empty sentence'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+|\\n+' , text) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()] if text.strip() else []\n    lens = []\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        lens.append(len(words))\n    if len(lens) <= 1:\n        return 0.0\n    mean = sum(lens) / float(len(lens))\n    if mean == 0:\n        return 0.0\n    variance = sum((l - mean) ** 2 for l in lens) / float(len(lens))\n    std = math.sqrt(variance)\n    return std / mean\n", "def feature(text: str) -> float:\n    'Function-word ratio: fraction of tokens that are common English function words (small stopword set)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','of','to','a','in','is','it','that','for','on','as','are','with','was','by','be','this','an','or','from','at','which','but','have','has','not','they','their','its','we','can','may','such','will'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stop)\n    return count / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are long (7+ characters) \u2014 higher values suggest more descriptive or formal vocabulary'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) >= 7)\n    return float(long_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (words longer than 7 characters) \u2014 lexical complexity indicator'\n    import re\n    tokens = re.findall(r\"\\b\\w+\\b\", text)\n    if not tokens:\n        return 0.0\n    long_words = sum(1 for t in tokens if len(t) > 7)\n    return long_words / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return unique / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Contraction ratio: fraction of tokens that are contractions (contain apostrophes like \"don\\'t\", \"I\\'m\", \"it\\'s\")'\n    import re\n    if not text:\n        return 0.0\n    # count contraction patterns with apostrophe\n    contr = re.findall(r\"\\b\\w+'[a-z]{1,4}\\b\", text.lower())\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(len(contr))\n    return len(contr) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (sentences split on .!?; fallback to 1 if no sentence boundary)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences using punctuation, keep non-empty\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    words = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not words:\n        return 0.0\n    if not sents:\n        return float(len(words))\n    avg = float(len(words)) / float(len(sents))\n    return avg\n\n", "def feature(text: str) -> float:\n    'Paragraph length coefficient of variation: std/mean of paragraph word counts (0 if fewer than 2 paragraphs or no words)'\n    import re, math\n    if not text:\n        return 0.0\n    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n    if len(paragraphs) < 2:\n        return 0.0\n    counts = []\n    for p in paragraphs:\n        words = re.findall(r\"\\b[\\w'\u2019]+\\b\", p)\n        counts.append(len(words))\n    # filter zero-length paragraphs\n    counts = [c for c in counts if c > 0]\n    if len(counts) < 2:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    variance = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(variance)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (length >= 7), a proxy for formality and vocabulary complexity'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 7)\n    return float(long_words) / len(words)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of line lengths in words \u2014 captures poetic/line-oriented structure'\n    import re, math\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    counts = []\n    for L in lines:\n        if L.strip():\n            counts.append(len(re.findall(r'\\w+', L)))\n    if not counts:\n        return 0.0\n    mean = sum(counts) / float(len(counts))\n    if mean == 0:\n        return 0.0\n    variance = sum((c - mean) ** 2 for c in counts) / float(len(counts))\n    std = math.sqrt(variance)\n    return float(std) / float(mean)\n", "def feature(text: str) -> float:\n    'Unique word ratio: number of unique words divided by total words (low values indicate repetition, might reveal certain generation patterns)'\n    if not text:\n        return 0.0\n    words = [w.strip(\".,;:\\\"'()[]{}\").lower() for w in text.split() if w.strip(\".,;:\\\"'()[]{}\")]\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n", "def feature(text: str) -> float:\n    'Ratio of curly (smart) apostrophes/quotes (\u2019 \u2018 \u201c \u201d) to all apostrophe/quote characters'\n    if not text:\n        return 0.0\n    curly = sum(text.count(ch) for ch in ['\u2019', '\u2018', '\u201c', '\u201d'])\n    straight = sum(text.count(ch) for ch in [\"'\", '\"'])\n    total = curly + straight\n    if total == 0:\n        return 0.0\n    return float(curly) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that begin with a quotation mark or a dash (dialogue/line-oriented style)'\n    if not text:\n        return 0.0\n    try:\n        lines = [ln for ln in text.splitlines() if ln.strip()]\n        if not lines:\n            return 0.0\n        starts = 0\n        for ln in lines:\n            s = ln.lstrip()\n            if s.startswith('\"') or s.startswith(\"'\") or s.startswith('-') or s.startswith('\u2014') or s.startswith('\u2013'):\n                starts += 1\n        return float(starts) / len(lines)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Passive-voice proxy: matches of \"was/were/is/are/been/being/has/had\" followed within 0-3 words by an -ed token, normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    # count sentences to normalize\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    # rough passive pattern\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|being|be|has|had|have)\\b(?:\\s+\\w+){0,3}\\s+\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    return float(len(matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of long words (length > 7) as a marker of lexical sophistication'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 7)\n    return float(long_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proportion of characters that lie inside parentheses (measures parenthetical/planned content like \"(50 words)\")'\n    import re\n    if not text:\n        return 0.0\n    parens = re.findall(r'\\([^)]*\\)', text)\n    if not parens:\n        return 0.0\n    inside_chars = sum(len(p) - 2 for p in parens)  # exclude parentheses chars themselves\n    total_chars = len(text)\n    return float(inside_chars) / total_chars if total_chars > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Density of adjacent capitalized-word bigrams (e.g., \"Jeremiah Smith\", \"American colonies\") \u2014 heuristic for named entities'\n    import re\n    if not text:\n        return 0.0\n    matches = list(re.finditer(r'\\b\\w+\\b', text))\n    if len(matches) < 2:\n        return 0.0\n    bigram_count = 0\n    for i in range(len(matches) - 1):\n        w1 = matches[i].group(0)\n        w2 = matches[i+1].group(0)\n        # require both start with uppercase letter and look like proper names (not all-caps)\n        if w1 and w2 and w1[0].isupper() and w2[0].isupper() and not w1.isupper() and not w2.isupper():\n            # ensure the first of the pair isn't clearly at a sentence start\n            start = matches[i].start()\n            j = start - 1\n            while j >= 0 and text[j].isspace():\n                j -= 1\n            if j >= 0 and text[j] in '.!?':\n                # likely sentence start -> still could be names, but deprioritize\n                continue\n            bigram_count += 1\n    total_words = len(matches)\n    return float(bigram_count) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: occurrences of forms like \"was|were|is|are|been|being\" followed by an -ed word per word'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r'\\b(?:was|were|is|are|been|being|be)\\s+[A-Za-z0-9\\-]+ed\\b', text, flags=re.IGNORECASE)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of em-dash/long-dash usage per word (\u2014, \u2013, or --), normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    word_count = max(1, len(re.findall(r'\\w+', text)))\n    dash_count = text.count('\u2014') + text.count('\u2013') + text.count('--')\n    return dash_count / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Unique word ratio: number of unique words divided by total words (low values indicate repetition, might reveal certain generation patterns)'\n    if not text:\n        return 0.0\n    words = [w.strip(\".,;:\\\"'()[]{}\").lower() for w in text.split() if w.strip(\".,;:\\\"'()[]{}\")]\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n", "def feature(text: str) -> float:\n    'Immediate repeated-word rate: fraction of adjacent word pairs that are identical (e.g., \"the the\")'\n    import re\n    words = re.findall(r\"\\b\\w+\\b\", (text or '').lower())\n    if not words:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    return repeats / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Sentence-end emphasis ratio: proportion of sentences ending in \"!\" or \"?\" (exclamation/question emphasis)'\n    if not text:\n        return 0.0\n    import re\n    # find sentence endings with their final punctuation\n    ends = re.findall(r'([^\\S\\r\\n]*)([^.!?]*)([.!?])', text)\n    if not ends:\n        return 0.0\n    total = len(ends)\n    emph = sum(1 for (_,_,p) in ends if p in ('!','?'))\n    return float(emph) / float(total)\n", "def feature(text: str) -> float:\n    'Density of newline characters (newlines per character) to detect line-oriented formats'\n    if not text:\n        return 0.0\n    newlines = text.count('\\n')\n    return float(newlines) / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Punctuation characters per word (overall punctuation density normalized by word count)'\n    import re\n    if not text:\n        return 0.0\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words = re.findall(r'\\w+', text)\n    return float(punct_count) / float(len(words) + 1)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens appearing inside double-quoted spans to total tokens (approximate dialogue proportion)'\n    import re\n    if not text:\n        return 0.0\n    total_tokens = len(re.findall(r'\\b\\w+\\b', text))\n    if total_tokens == 0:\n        return 0.0\n    parts = re.split(r'\"', text)\n    inside_tokens = 0\n    # odd-indexed parts are inside quotes when quotes are paired\n    for i in range(1, len(parts), 2):\n        inside_tokens += len(re.findall(r'\\b\\w+\\b', parts[i]))\n    return float(inside_tokens) / float(total_tokens)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord(c) > 127) \u2014 picks up typographic quotes, em-dashes, and some copy-paste artifacts'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / total\n", "def feature(text: str) -> float:\n    'Discourse marker density: frequency of common discourse markers (however, moreover, therefore, etc.) per word'\n    import re\n    if not text:\n        return 0.0\n    markers = re.findall(r'\\b(?:however|moreover|therefore|furthermore|additionally|consequently|nevertheless|thus|meanwhile|alternatively)\\b', text, re.I)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(markers)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of violent/visceral lexical items (small curated lexicon) per token, useful for gore/violence signal'\n    import re\n    if not text:\n        return 0.0\n    lexicon = ['kill', 'killed', 'murder', 'axe', 'axe', 'head', 'decap', 'decapit', 'blood', 'bleed', 'stab', 'knife', 'shot', 'rifle', 'thud', 'gore', 'cut', 'swing', 'chop']\n    tokens = re.findall(r\"[A-Za-z0-9']+\", text.lower())\n    if not tokens:\n        return 0.0\n    cnt = 0\n    for t in tokens:\n        for root in lexicon:\n            if root in t:\n                cnt += 1\n                break\n    return float(cnt) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Normalized standard deviation of paragraph lengths (words per paragraph): stddev / (mean+1) to avoid blowup'\n    import re\n    if not text:\n        return 0.0\n    # paragraphs separated by empty lines\n    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n    if not paragraphs:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', p)) for p in paragraphs]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = var ** 0.5\n    return float(std) / (mean + 1.0)\n\n", "def feature(text: str) -> float:\n    'Density of common markdown/formatting symbols (*, #, `_`, `) relative to text length'\n    if not text:\n        return 0.0\n    symbols = set('*#_`>')\n    count = sum(1 for c in text if c in symbols)\n    return count / max(1, len(text))\n\n", "def feature(text: str) -> float:\n    'Ratio of titlecase tokens that appear NOT at the start of sentences (proxy for proper-name density)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence fragments then tokens to avoid counting sentence-initial capitals\n    sentences = re.split(r'[.!?]+', text)\n    total_tokens = 0\n    title_non_initial = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        parts = s.split()\n        if len(parts) <= 1:\n            continue\n        # skip the first token as sentence-initial\n        for tok in parts[1:]:\n            total_tokens += 1\n            # consider tokens with internal alphabetic characters and titlecase form\n            if tok.istitle() and any(c.isalpha() for c in tok):\n                title_non_initial += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(title_non_initial) / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of capitalized tokens that are likely proper nouns (capitalized not at sentence start)'\n    import re\n    if not text:\n        return 0.0\n    raw_tokens = re.findall(r\"\\b[^\\s]+\\b\", text)\n    if not raw_tokens:\n        return 0.0\n    proper_like = 0\n    capitalized_total = 0\n    sentence_boundaries = set()\n    for m in re.finditer(r'[.!?]+', text):\n        # mark index after punctuation as potential sentence start\n        sentence_boundaries.add(m.end())\n    for i, tok in enumerate(raw_tokens):\n        stripped = tok.strip('()[]{}\"\\'\u201c\u201d')  # remove surrounding punctuation\n        if not stripped:\n            continue\n        if stripped[0].isupper():\n            capitalized_total += 1\n            # compute approximate start index of token\n            try:\n                start_idx = text.index(tok)\n            except ValueError:\n                start_idx = None\n            # if not at very start and not immediately after sentence punctuation, count as proper-like\n            if start_idx is None:\n                proper_like += 1\n            else:\n                if start_idx != 0 and start_idx not in sentence_boundaries:\n                    proper_like += 1\n    if capitalized_total == 0:\n        return 0.0\n    return float(proper_like) / capitalized_total\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are surrounded by or contain double-quote characters, approximating dialog density'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    dq = 0\n    for t in tokens:\n        if '\"' in t or t.startswith('\"') or t.endswith('\"'):\n            dq += 1\n    return float(dq) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences whose first alphabetic character is lowercase (informal/lax capitalization)'\n    import re\n    if not text:\n        return 0.0\n    # Split on sentence terminators or newlines to get candidates\n    parts = re.split(r'(?<=[.!?])\\s+|\\n+', text)\n    starts = 0\n    total = 0\n    for p in parts:\n        s = p.strip()\n        if not s:\n            continue\n        m = re.search(r'[A-Za-z]', s)\n        if not m:\n            continue\n        total += 1\n        if m.group(0).islower():\n            starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of interior words (not the first word of a sentence) that start with an uppercase letter (proper nouns / title-casing irregularity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'([.!?]+\\s*)', text)\n    # reconstruct sentences to ensure splitting keeps content\n    chunks = []\n    temp = ''\n    for part in sentences:\n        temp += part\n        if re.search(r'[.!?]$', part.strip()):\n            chunks.append(temp.strip())\n            temp = ''\n    if temp:\n        chunks.append(temp.strip())\n    interior_capital = 0\n    interior_total = 0\n    for s in chunks:\n        words = re.findall(r'\\b\\w+\\b', s)\n        for w in words[1:]:\n            interior_total += 1\n            if w and w[0].isupper():\n                interior_capital += 1\n    if interior_total == 0:\n        return 0.0\n    return float(interior_capital) / float(interior_total)\n\n", "def feature(text: str) -> float:\n    'Stopword ratio: fraction of tokens that are common English stopwords (function-word density)'\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you',\n        'do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one',\n        'all','would','there','their','what','so','up','out','if','about','who','get','which','go','me'\n    }\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of common English stopwords (heuristic set) among tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','of','and','to','a','in','that','is','for','on','with','as','by','an','be','are','was','were','this','these','those','it','its','from','at'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    stopcount = sum(1 for w in words if w in stopwords)\n    return stopcount / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of colons and semicolons per character (formal/complex punctuation usage)'\n    count = text.count(':') + text.count(';')\n    denom = max(1, len(text))\n    return float(count) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Function-word ratio: fraction of tokens that are common English function words (small stopword set)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','of','to','a','in','is','it','that','for','on','as','are','with','was','by','be','this','an','or','from','at','which','but','have','has','not','they','their','its','we','can','may','such','will'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stop)\n    return count / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Simple passive-voice indicator: occurrences of forms like \"was/ were/ is/ are\" followed by an -ed/-en token, normalized by sentences'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r'\\b(?:was|were|is|are|been|being|be)\\b\\s+\\w+(?:ed|en)\\b', text.lower())\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    denom = float(len(sentences)) if sentences else 1.0\n    return float(len(matches)) / denom\n\n", "def feature(text: str) -> float:\n    'Character-level Shannon entropy (bits) normalized by log2(unique_chars+1), 0 for empty text'\n    import math, collections\n    if not text:\n        return 0.0\n    counts = collections.Counter(text)\n    total = float(len(text))\n    entropy = 0.0\n    for cnt in counts.values():\n        p = cnt / total\n        entropy -= p * math.log2(p)\n    # normalize by maximum possible log2(len(unique_chars)+1) to keep in [0,1]\n    unique = len(counts)\n    if unique <= 1:\n        return 0.0\n    norm = math.log2(unique)\n    if norm <= 0:\n        return 0.0\n    return float(entropy) / float(norm)\n", "def feature(text: str) -> float:\n    'Density of spaced hyphen/dash occurrences (\" - \", em-dash, en-dash) per sentence'\n    import re\n    if not text:\n        return 0.0\n    occurrences = len(re.findall(r'(?:\\s-\\s|\\u2014|\\u2013)', text))\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return occurrences / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Ratio of past-perfect constructions (\"had\" + past participle-like token ending in -ed) to tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    matches = re.findall(r'\\bhad\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of words that appear only once in the text'\n    import re\n    words = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not words:\n        return 0.0\n    from collections import Counter\n    cnt = Counter(words)\n    hapax = sum(1 for w, c in cnt.items() if c == 1)\n    return hapax / len(words)\n\n", "def feature(text: str) -> float:\n    'Proportion of titlecase words (words starting with an uppercase followed by lowercase letters) among all tokens'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    titlecase = sum(1 for t in tokens if len(t) > 1 and t[0].isupper() and t[1:].islower())\n    return titlecase / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Character-level Shannon entropy (bits) normalized by log2(unique_chars+1), 0 for empty text'\n    import math, collections\n    if not text:\n        return 0.0\n    counts = collections.Counter(text)\n    total = float(len(text))\n    entropy = 0.0\n    for cnt in counts.values():\n        p = cnt / total\n        entropy -= p * math.log2(p)\n    # normalize by maximum possible log2(len(unique_chars)+1) to keep in [0,1]\n    unique = len(counts)\n    if unique <= 1:\n        return 0.0\n    norm = math.log2(unique)\n    if norm <= 0:\n        return 0.0\n    return float(entropy) / float(norm)\n", "def feature(text: str) -> float:\n    'Average number of commas per (detected) sentence: comma_count / (sentence_count+epsilon) to estimate clause density'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(commas) / float(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending with -ed (simple past/participial marker density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z'-]+\", text.lower())\n    if not tokens:\n        return 0.0\n    ed_count = sum(1 for t in tokens if t.endswith('ed') and len(t) > 3)\n    return float(ed_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    \"Smoothed ratio of contraction-like apostrophes to possessive \\\"'s\\\" occurrences\"\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    poss = len(re.findall(r\"\\b\\w+'s\\b\", lower))\n    # contractions: n't, 're, 've, 'll, 'm, 'd (exclude 's)\n    contr = len(re.findall(r\"\\b\\w+(?:n't|'re|'ve|'ll|'m|'d)\\b\", lower))\n    # smoothing to avoid division by zero\n    return float(contr + 1) / float(poss + 1)\n\n", "def feature(text: str) -> float:\n    'Diversity of contractions: unique contraction forms divided by total contraction tokens (higher = varied contractions)'\n    import re\n    if not text:\n        return 0.0\n    # capture typical contractions like don't, I\\'m, he\\'ll, we\\'ve etc.\n    contractions = re.findall(r\"\\b[a-zA-Z]+\\'[a-zA-Z]{1,4}\\b\", text)\n    total = len(contractions)\n    if total == 0:\n        return 0.0\n    unique = len(set([c.lower() for c in contractions]))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Normalized lexical entropy of word frequency distribution (0..1), approximates lexical variety vs repetition'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    cnt = Counter(tokens)\n    N = float(len(tokens))\n    probs = [v / N for v in cnt.values()]\n    # entropy\n    H = -sum(p * math.log(p + 1e-12) for p in probs)\n    V = len(cnt)\n    if V <= 1:\n        return 0.0\n    # normalize by log(V)\n    return float(H / (math.log(V) + 1e-12))\n\n", "def feature(text: str) -> float:\n    'Ratio (first-person tokens +1) / (third-person tokens +1) to capture narrative perspective bias'\n    import re\n    if not text:\n        return 1.0\n    tokens = re.findall(r'\\w+', text.lower())\n    first = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    third = {'he', 'she', 'they', 'him', 'her', 'them', 'his', 'hers', 'their', 'theirs'}\n    fcount = sum(1 for t in tokens if t in first)\n    tcount = sum(1 for t in tokens if t in third)\n    return float(fcount + 1) / float(tcount + 1)\n\n", "def feature(text: str) -> float:\n    'Loose passive-voice indicator: count of auxiliary verbs followed by an -ed participle (is/was/were + Xed) normalized by words'\n    import re\n    if not text:\n        return 0.0\n    text_l = text.lower()\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|has|have|had)\\b\\s+\\w+ed\\b', text_l)\n    words = re.findall(r'\\w+', text_l)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of short, capitalized header-like lines (e.g., \"Background\" on its own line)'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    count = 0\n    for l in lines:\n        s = l.strip()\n        parts = s.split()\n        if 1 <= len(parts) <= 3 and parts[0][:1].isupper() and all(p.isalpha() for p in parts):\n            # treat single- or short-word title-like lines as headers\n            count += 1\n    return float(count) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Citation-like token density: occurrences of bracket citations [1], parenthetical author-year citations (Smith, 2020), or \"et al.\" per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    bracket_cites = len(re.findall(r'\\[\\s*\\d+\\s*\\]', text))\n    author_year = len(re.findall(r'\\([A-Z][A-Za-z\\-\\']{1,20},\\s*\\d{4}\\)', text))\n    et_al = len(re.findall(r'\\bet\\s+al\\.?', text, re.IGNORECASE))\n    total = bracket_cites + author_year + et_al\n    return float(total) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Estimated clauses per sentence: (coordinating/subordinating conjunctions + semicolons + half the commas) per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj_pattern = r'\\b(?:and|but|because|although|while|since|when|which|that|however|so|therefore)\\b'\n    conj_count = len(re.findall(conj_pattern, text, flags=re.IGNORECASE))\n    semicolons = text.count(';')\n    commas = text.count(',')\n    # sentence count robust split\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    n_sent = max(1, len(sentences))\n    clause_estimate = conj_count + semicolons + 0.5 * commas\n    return float(clause_estimate) / float(n_sent)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain double quotes (dialogue sentence ratio)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    dialogue_sent = sum(1 for s in sentences if '\"' in s)\n    return float(dialogue_sent) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Paragraph break density: number of double newlines (\\\\n\\\\n) per 100 words'\n    import re\n    if not text:\n        return 0.0\n    words = max(1, len(re.findall(r'\\w+', text)))\n    pbreaks = text.count('\\n\\n')\n    return float(pbreaks) / float(words) * 100.0\n\n", "def feature(text: str) -> float:\n    'Average word length (characters) \u2014 longer average can signal more formal or descriptive prose'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    total_len = sum(len(w) for w in words)\n    return float(total_len) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of double-quote style dialogue markers per sentence (counts \" and smart-quotes per sentence)'\n    import re\n    if not text:\n        return 0.0\n    quote_chars = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    # sentence count (fallback)\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(quote_chars) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation characters (0 if none)'\n    import string\n    punct_chars = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    if not punct_chars:\n        return 0.0\n    distinct = set(punct_chars)\n    return float(len(distinct)) / float(len(punct_chars))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens matching a small sensory vocabulary (see, hear, feel, smell, taste, glow, etc.)'\n    import re\n    sensory = {'see','saw','seen','look','looked','look','hear','heard','listen','feel','felt','smell','smelled','taste',\n               'touch','glow','glowing','bright','dark','shimmer','shimmering','whisper','hiss','silent','silence',\n               'shine','shining','shadow','shadowed','glitter'}\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Approximate passive/past constructions: occurrences of \"was|were <word>ed\" per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    matches = re.findall(r'\\b(?:was|were)\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    return len(matches) / len(words)\n\n", "def feature(text: str) -> float:\n    'Normalized density of multi-word capitalized sequences (e.g., \"New York\", \"Lord Voldemort\") per sentence'\n    import re\n    if not text:\n        return 0.0\n    # find sequences of two or more consecutive Titlecase words\n    matches = re.findall(r'\\b(?:[A-Z][a-z]+(?:\\s+|$)){2,}', text)\n    sents = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    scount = max(1, len(sents))\n    return float(len(matches)) / float(scount)\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (length >= 7), a proxy for formality and vocabulary complexity'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 7)\n    return float(long_words) / len(words)\n\n", "def feature(text: str) -> float:\n    'Pronoun variety: distinct pronouns used divided by total pronoun occurrences (higher = more varied pronoun use)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','you','your','yours','he','him','his','she','her','hers','they','them','their','theirs','it','its','who','whom','whose'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    found = []\n    for w in tokens:\n        lw = w.lower().strip(\"'-\")\n        if lw in pronouns:\n            found.append(lw)\n    if not found:\n        return 0.0\n    distinct = len(set(found))\n    return float(distinct) / len(found)\n\n", "def feature(text: str) -> float:\n    'Ratio of modal verbs (can/could/will/would/shall/should/may/might/must) to all tokens \u2014 signals speculative or advisory tone'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of all \\\"\\'s\\\" occurrences that attach to a capitalized word (proxy for proper-noun possessives like \\\"John\\'s\\\")'\n    import re\n    if not text:\n        return 0.0\n    poss = re.findall(r\"\\b([A-Za-z]\\w*)'s\\b\", text)\n    total_s = len(re.findall(r\"\\b\\w+'s\\b\", text))\n    if total_s == 0:\n        return 0.0\n    cap_count = sum(1 for w in poss if w and w[0].isupper())\n    return float(cap_count) / float(total_s)\n\n", "def feature(text: str) -> float:\n    'Rate of ellipses (\\'...\\', unicode ellipsis) per token \u2014 indicates trailing thoughts/truncation or dramatic pauses'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    ellipsis_count = text.count('...') + text.count('\u2026')\n    return float(ellipsis_count) / float(max(1, len(tokens)))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a subordinating or clause-introducing conjunction (because, although, since, while, if, when, though, unless, after, before, as, once)'\n    import re\n    subs = {'because','although','since','while','if','when','though','unless','after','before','as','once'}\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [sent.strip() for sent in re.split(r'(?<=[.!?])\\s+', s) if sent.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for sent in sentences:\n        # remove leading non-word characters like quotes or dashes\n        sent_clean = re.sub(r'^[^\\w]+', '', sent).lower()\n        first_word_match = re.match(r'(\\w+)', sent_clean)\n        if first_word_match and first_word_match.group(1) in subs:\n            starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Normalized difference between exclamation/question density inside quotes vs outside (inside - outside)'\n    import re\n    if not text:\n        return 0.0\n    # find double-quoted spans and rest\n    quote_spans = re.findall(r'\"([^\"]*)\"', text)\n    inside_chars = sum(len(s) for s in quote_spans)\n    inside_marks = sum(s.count('!') + s.count('?') for s in quote_spans)\n    outside_text = re.sub(r'\"[^\"]*\"', ' ', text)\n    outside_chars = len(outside_text)\n    outside_marks = outside_text.count('!') + outside_text.count('?')\n    # normalize densities; handle zero-length regions\n    inside_density = (inside_marks / inside_chars) if inside_chars > 0 else 0.0\n    outside_density = (outside_marks / outside_chars) if outside_chars > 0 else 0.0\n    return float(inside_density - outside_density)\n\n", "def feature(text: str) -> float:\n    'Density of hyphen and dash characters (-, \u2014, \u2013) per word, capturing compound/descriptive style'\n    import re\n    if not text:\n        return 0.0\n    hyphen_count = text.count('-') + text.count('\\u2014') + text.count('\\u2013')\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(hyphen_count) / float(words)\n\n", "def feature(text: str) -> float:\n    'Density of first-person pronouns (I, me, my, we, our, etc.) relative to token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours',\"i'm\",\"i've\",\"i'd\",\"i'll\"}\n    count = 0\n    for w in tokens:\n        if w.lower() in first_person:\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain hyphens or dashes (hyphenated compounds are common in some academic styles)'\n    import re\n    if not text:\n        return 0.0\n    # consider ASCII hyphen and en/em dashes\n    tokens = re.findall(r\"\\b[\\w\\-\\\u2013\\\u2014]+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t or '\u2013' in t or '\u2014' in t)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain hyphens (compound/technical terms or ranges)'\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphens = sum(1 for t in tokens if '-' in t and any(ch.isalnum() for ch in t))\n    return float(hyphens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of numeric tokens (tokens containing digits) to capture dates, addresses, years'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_tokens) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Max run length of consecutive sentences that start with the same first 3-word phrase, normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences and extract normalized starts\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    m = len(sentences)\n    if m == 0:\n        return 0.0\n    starts = []\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s.lower())\n        start = ' '.join(words[:3])\n        starts.append(start)\n    max_run = 1\n    cur_run = 1\n    for i in range(1, len(starts)):\n        if starts[i] and starts[i] == starts[i-1]:\n            cur_run += 1\n            if cur_run > max_run:\n                max_run = cur_run\n        else:\n            cur_run = 1\n    return float(max_run) / m\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count; sentences fallback to 1 if none)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # split on sentence end punctuation; fallback to whole text if none\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    return comma_count / sent_count\n\n", "def feature(text: str) -> float:\n    'Normalized difference between exclamation/question density inside quotes vs outside (inside - outside)'\n    import re\n    if not text:\n        return 0.0\n    # find double-quoted spans and rest\n    quote_spans = re.findall(r'\"([^\"]*)\"', text)\n    inside_chars = sum(len(s) for s in quote_spans)\n    inside_marks = sum(s.count('!') + s.count('?') for s in quote_spans)\n    outside_text = re.sub(r'\"[^\"]*\"', ' ', text)\n    outside_chars = len(outside_text)\n    outside_marks = outside_text.count('!') + outside_text.count('?')\n    # normalize densities; handle zero-length regions\n    inside_density = (inside_marks / inside_chars) if inside_chars > 0 else 0.0\n    outside_density = (outside_marks / outside_chars) if outside_chars > 0 else 0.0\n    return float(inside_density - outside_density)\n\n", "def feature(text: str) -> float:\n    'Density of long punctuation sequences like ellipses or double-dashes (count per character)'\n    if not text:\n        return 0.0\n    long_seqs = text.count('...') + text.count('--') + text.count('\u2014')\n    return long_seqs / max(1, len(text))\n\n", "def feature(text: str) -> float:\n    'Density of adjacent capitalized-word bigrams (e.g., \"Jeremiah Smith\", \"American colonies\") \u2014 heuristic for named entities'\n    import re\n    if not text:\n        return 0.0\n    matches = list(re.finditer(r'\\b\\w+\\b', text))\n    if len(matches) < 2:\n        return 0.0\n    bigram_count = 0\n    for i in range(len(matches) - 1):\n        w1 = matches[i].group(0)\n        w2 = matches[i+1].group(0)\n        # require both start with uppercase letter and look like proper names (not all-caps)\n        if w1 and w2 and w1[0].isupper() and w2[0].isupper() and not w1.isupper() and not w2.isupper():\n            # ensure the first of the pair isn't clearly at a sentence start\n            start = matches[i].start()\n            j = start - 1\n            while j >= 0 and text[j].isspace():\n                j -= 1\n            if j >= 0 and text[j] in '.!?':\n                # likely sentence start -> still could be names, but deprioritize\n                continue\n            bigram_count += 1\n    total_words = len(matches)\n    return float(bigram_count) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Fraction of interior words (not the first word of a sentence) that start with an uppercase letter (proper nouns / title-casing irregularity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'([.!?]+\\s*)', text)\n    # reconstruct sentences to ensure splitting keeps content\n    chunks = []\n    temp = ''\n    for part in sentences:\n        temp += part\n        if re.search(r'[.!?]$', part.strip()):\n            chunks.append(temp.strip())\n            temp = ''\n    if temp:\n        chunks.append(temp.strip())\n    interior_capital = 0\n    interior_total = 0\n    for s in chunks:\n        words = re.findall(r'\\b\\w+\\b', s)\n        for w in words[1:]:\n            interior_total += 1\n            if w and w[0].isupper():\n                interior_capital += 1\n    if interior_total == 0:\n        return 0.0\n    return float(interior_capital) / float(interior_total)\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice score: number of \"be\" + past-participles patterns per sentence'\n    import re\n    if not text:\n        return 0.0\n    # Look for patterns like \"was created\", \"has been transformed\", \"is discussed\"\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|being|has been|have been|had been|will be|would be|should be)\\b\\s+\\w+(?:ed|en)\\b', re.I)\n    matches = pattern.findall(text)\n    sentences = max(1.0, float(text.count('.') + text.count('!') + text.count('?')))\n    return float(len(matches)) / sentences\n\n", "def feature(text: str) -> float:\n    'Diversity of punctuation: number of distinct punctuation characters divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of long words (length > 7) as a marker of lexical sophistication'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 7)\n    return float(long_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proportion of repeated adjacent word bigrams (1 - unique_bigrams/total_bigrams), 0 if no bigrams'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    total = max(0, len(words) - 1)\n    if total == 0:\n        return 0.0\n    bigrams = [' '.join((words[i], words[i+1])) for i in range(len(words)-1)]\n    unique = len(set(bigrams))\n    return 1.0 - (unique / float(total))\n\n", "def feature(text: str) -> float:\n    'Parenthetical density: count of parentheses characters per token (captures citations or aside notes)'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    paren_count = text.count('(') + text.count(')')\n    denom = max(1, len(words))\n    return float(paren_count) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Comma density per sentence: average number of commas per sentence (comma_count / sentence_count)'\n    import re\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    commas = text.count(',')\n    return float(commas) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are capitalized but not the first word of their sentence (proxy for named-entity or mid-sentence capitalization)'\n    if not text:\n        return 0.0\n    import re\n    # Split into sentences heuristically\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    total_words = 0\n    mid_capitalized = 0\n    for sent in sentences:\n        sent_words = re.findall(r\"\\b[\\w'-]+\\b\", sent)\n        if not sent_words:\n            continue\n        total_words += len(sent_words)\n        # skip first word of sentence\n        for w in sent_words[1:]:\n            if w[0].isupper() and not w.isupper():  # ignore full acronyms\n                mid_capitalized += 1\n    if total_words == 0:\n        return 0.0\n    return float(mid_capitalized) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (length >= 7), a proxy for formality and vocabulary complexity'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 7)\n    return float(long_words) / len(words)\n\n", "def feature(text: str) -> float:\n    \"Smoothed ratio of contraction-like apostrophes to possessive \\\"'s\\\" occurrences\"\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    poss = len(re.findall(r\"\\b\\w+'s\\b\", lower))\n    # contractions: n't, 're, 've, 'll, 'm, 'd (exclude 's)\n    contr = len(re.findall(r\"\\b\\w+(?:n't|'re|'ve|'ll|'m|'d)\\b\", lower))\n    # smoothing to avoid division by zero\n    return float(contr + 1) / float(poss + 1)\n\n", "def feature(text: str) -> float:\n    'Density of explicit line breaks: number of newline characters normalized by text length'\n    if not text:\n        return 0.0\n    # Normalize by length to avoid division by zero\n    return float(text.count('\\n')) / float(len(text) + 1)\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (words longer than 7 characters) \u2014 lexical complexity indicator'\n    import re\n    tokens = re.findall(r\"\\b\\w+\\b\", text)\n    if not tokens:\n        return 0.0\n    long_words = sum(1 for t in tokens if len(t) > 7)\n    return long_words / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of capitalized words that look like names or proper nouns (not sentence-start \"I\")'\n    try:\n        words = re.findall(r'\\b[A-Za-z][a-z]+\\b', text)\n    except Exception:\n        words = []\n    try:\n        total_words = re.findall(r'\\w+', text)\n    except Exception:\n        total_words = []\n    if not total_words:\n        return 0.0\n    # count capitalized words (first letter uppercase) excluding single-letter \"I\"\n    cap_count = sum(1 for w in words if w[0].isupper() and w.lower() != 'i')\n    return float(cap_count) / float(len(total_words))\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\" or longer) per token (indicates trailing/reflective style)'\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    tokens = re.findall(r'\\w+|\\S', text)\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    return float(ellipses) / float(total)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of paragraph word counts (paragraphs split on blank lines)'\n    import re, math\n    if not text:\n        return 0.0\n    paras = [p for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n    counts = [len(re.findall(r'\\w+', p)) for p in paras if re.findall(r'\\w+', p)]\n    if len(counts) < 2:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Density of common dialogue-reporting verbs (said, asked, replied, shouted, whispered, etc.) per word'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    reporting_verbs = r'\\b(said|asked|replied|muttered|whispered|shouted|exclaimed|answered|sighed|laughed|grinned|smiled)\\b'\n    matches = re.findall(reporting_verbs, text, flags=re.IGNORECASE)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Diversity of contraction forms: unique contraction tokens divided by total contraction-like tokens (higher = more varied contraction usage)'\n    import re\n    if not text:\n        return 0.0\n    contractions = re.findall(r\"\\b\\w+['\u2019]\\w+\\b\", text)\n    total = len(contractions)\n    if total == 0:\n        return 0.0\n    unique = len(set(c.lower() for c in contractions))\n    return unique / float(total)\n\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count), 0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = max(1, len(sentences))\n    return float(comma_count) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Fraction of long words (length > 7) as a marker of lexical sophistication'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 7)\n    return float(long_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are second-person pronouns (you, your, yours) indicating promotional/personal tone'\n    if not text:\n        return 0.0\n    words = [w.strip('.,;:\"\\'()[]') .lower() for w in text.split()]\n    if not words:\n        return 0.0\n    second = sum(1 for w in words if w in {'you', 'your', 'yours', 'yourselves', 'yourself'})\n    return second / len(words)\n\n", "def feature(text: str) -> float:\n    'Average commas per sentence (commas indicate clause complexity), normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    sentence_splits = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    # if no sentence punctuation, treat as one sentence\n    if not sentence_splits:\n        sentence_count = 1\n        comma_count = text.count(',')\n    else:\n        sentence_count = len(sentence_splits)\n        comma_count = sum(s.count(',') for s in sentence_splits)\n    return float(comma_count) / float(sentence_count) if sentence_count else 0.0\n\n", "def feature(text: str) -> float:\n    'Density of adjacent capitalized-word bigrams (e.g., \"Jeremiah Smith\", \"American colonies\") \u2014 heuristic for named entities'\n    import re\n    if not text:\n        return 0.0\n    matches = list(re.finditer(r'\\b\\w+\\b', text))\n    if len(matches) < 2:\n        return 0.0\n    bigram_count = 0\n    for i in range(len(matches) - 1):\n        w1 = matches[i].group(0)\n        w2 = matches[i+1].group(0)\n        # require both start with uppercase letter and look like proper names (not all-caps)\n        if w1 and w2 and w1[0].isupper() and w2[0].isupper() and not w1.isupper() and not w2.isupper():\n            # ensure the first of the pair isn't clearly at a sentence start\n            start = matches[i].start()\n            j = start - 1\n            while j >= 0 and text[j].isspace():\n                j -= 1\n            if j >= 0 and text[j] in '.!?':\n                # likely sentence start -> still could be names, but deprioritize\n                continue\n            bigram_count += 1\n    total_words = len(matches)\n    return float(bigram_count) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens containing any digit (years, quantities, case-study numbers), useful for formal/business texts'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(digit_tokens) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Comma density per sentence: average number of commas in each sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback to punctuation density if no clear sentences\n        return float(text.count(',')) if text else 0.0\n    return float(text.count(',')) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Comma density per sentence: average number of commas in each sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback to punctuation density if no clear sentences\n        return float(text.count(',')) if text else 0.0\n    return float(text.count(',')) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Lexical density measured as fraction of content words (non-function words) among tokens'\n    import re\n    if not text:\n        return 0.0\n    function_words = {'the','a','an','of','and','to','in','for','on','with','as','by','is','are','was','were','be','that','which','who','whom','this','these','those','it','its','their','they','them','at','from','or','but','if','than','so','then','such'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    function_count = sum(1 for t in tokens if t in function_words)\n    content_count = len(tokens) - function_count\n    return float(content_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters present in the text'\n    if not text:\n        return 0.0\n    puncts = {c for c in text if not c.isalnum() and not c.isspace()}\n    return float(len(puncts))\n\n", "def feature(text: str) -> float:\n    'Ratio of short words (<=3 letters) to long words (>7 letters) approximated as short_count / (1 + long_count)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    short_count = sum(1 for t in tokens if len(t) <= 3)\n    long_count = sum(1 for t in tokens if len(t) > 7)\n    return float(short_count) / (1.0 + float(long_count))\n\n", "def feature(text: str) -> float:\n    'Heading-style fraction: fraction of non-empty lines that are all-uppercase or end with a colon (likely headers)'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_header(ln):\n        s = ln.strip()\n        # treat as header if ends with ':' or is uppercase with at least one letter\n        return s.endswith(':') or (any(ch.isalpha() for ch in s) and s == s.upper())\n    headers = sum(1 for ln in lines if is_header(ln))\n    return headers / len(lines)\n", "def feature(text: str) -> float:\n    'Proportion of lines that look like headings (short line starting with capitalized word)'\n    import re\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    heading_pattern = re.compile(r'^[A-Z][A-Za-z0-9 \\-]{0,60}\\s*$')\n    heading_count = sum(1 for l in lines if heading_pattern.match(l.strip()))\n    return float(heading_count) / max(1.0, len(lines))\n\n", "def feature(text: str) -> float:\n    'Parenthesis density: number of parentheses characters \"(\" or \")\" per word (indicator of citations/parentheticals)'\n    import re\n    if not text:\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(paren_count) / float(words)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Function-word density: fraction of common function words present among all tokens (proxy for grammatical/stopword usage)'\n    import re\n    if not text:\n        return 0.0\n    stopset = {'the','and','of','to','a','in','that','it','is','was','for','as','with','on','at','by','an','be','this','which','or','from','but','not','are','his','her','their','they','he','she','you','i','we','my','your','our'}\n    words = re.findall(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?\", text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in stopset)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Hapax legomena rate: fraction of word types that occur exactly once (lowercased)'\n    import re\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\b\\w+\\b', text)]\n    if not words:\n        return 0.0\n    from collections import Counter\n    freqs = Counter(words)\n    hapaxes = sum(1 for v in freqs.values() if v == 1)\n    types = len(freqs)\n    return float(hapaxes) / max(types, 1)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of line lengths in words \u2014 captures poetic/line-oriented structure'\n    import re, math\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    counts = []\n    for L in lines:\n        if L.strip():\n            counts.append(len(re.findall(r'\\w+', L)))\n    if not counts:\n        return 0.0\n    mean = sum(counts) / float(len(counts))\n    if mean == 0:\n        return 0.0\n    variance = sum((c - mean) ** 2 for c in counts) / float(len(counts))\n    std = math.sqrt(variance)\n    return float(std) / float(mean)\n", "def feature(text: str) -> float:\n    'Density of em-dash/long-dash usage per word (\u2014, \u2013, or --), normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    word_count = max(1, len(re.findall(r'\\w+', text)))\n    dash_count = text.count('\u2014') + text.count('\u2013') + text.count('--')\n    return dash_count / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Smoothed ratio of common irregular past-tense verbs to total tokens (captures non-\"ed\" past usage)'\n    import re\n    if not text:\n        return 0.0\n    irregulars = {'was','were','had','saw','went','came','got','made','became','began','brought',\n                  'broke','chose','did','drank','drove','flew','found','gave','kept','knew','left',\n                  'lost','put','read','ran','said','sent','slept','spoke','took','told','thought',\n                  'won','felt','built','fell','held','heard','lost','paid','rode','rose','shot','shut'}\n    tokens = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in irregulars)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that look like internal-abbreviations with embedded dots (e.g., \"S.S.\", \"U.S.\")'\n    if not text:\n        return 0.0\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if re.search(r'[A-Za-z]\\.[A-Za-z]', t):\n            # avoid counting pure ellipses or trailing punctuation-only tokens\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Frequency of ellipses per sentence (counts \"...\" and single-character ellipsis \"\u2026\")'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...') + text.count('\u2026')\n    # approximate sentence count by splitting on sentence-ending punctuation; ensure at least 1\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    return float(ellipses) / max(1, len(sentences))\n\n", "def feature(text: str) -> float:\n    'Average number of words per quoted segment (mean quoted-turn length)'\n    import re\n    if not text:\n        return 0.0\n    quotes = re.findall(r'\"([^\"]+)\"', text)\n    if not quotes:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', q)) for q in quotes]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Number of distinct punctuation characters used (variety of punctuation marks)'\n    if not text:\n        return 0.0\n    puncts = {c for c in text if not c.isalnum() and not c.isspace()}\n    return float(len(puncts))\n", "def feature(text: str) -> float:\n    \"Smoothed ratio of contraction-like apostrophes to possessive \\\"'s\\\" occurrences\"\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    poss = len(re.findall(r\"\\b\\w+'s\\b\", lower))\n    # contractions: n't, 're, 've, 'll, 'm, 'd (exclude 's)\n    contr = len(re.findall(r\"\\b\\w+(?:n't|'re|'ve|'ll|'m|'d)\\b\", lower))\n    # smoothing to avoid division by zero\n    return float(contr + 1) / float(poss + 1)\n\n", "def feature(text: str) -> float:\n    'Ratio of common English stopwords to total tokens (a proxy for function-word density and formal prose)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[^\\d\\W_][\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    stopwords = {'the','is','and','of','to','a','in','that','it','for','with','as','on','are','was','by','an','be','this','which','or','from','at','their','these','they','has','have','had'}\n    sw_count = sum(1 for t in tokens if t in stopwords)\n    return float(sw_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common stopwords (the, and, of, to, a, in, is, that)'\n    import re\n    stopwords = {'the','and','of','to','a','in','is','that','it','for','on','with','as','are','was','be','by','an','this'}\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of long words (length > 7) as a marker of lexical sophistication'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 7)\n    return float(long_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Citation-year ratio: fraction of tokens that look like 4-digit years (1000-2099), common in scholarly text'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z0-9'-]+\", text)\n    if not words:\n        return 0.0\n    years = re.findall(r'\\b(?:1[0-9]{3}|20[0-9]{2})\\b', text)\n    return float(len(years)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of 4-digit year mentions (1900-2099) per word, common in academic citations'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = text.split()\n    if not words:\n        return 0.0\n    years = re.findall(r'\\b(?:19|20)\\d{2}\\b', text)\n    return float(len(years)) / len(words)\n\n", "def feature(text: str) -> float:\n    'Non-ASCII punctuation density: fraction of characters that are punctuation and non-ASCII (captures smart quotes, em-dashes, ellipses)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    count = 0\n    for c in text:\n        if not c.isalnum() and not c.isspace() and ord(c) > 127:\n            count += 1\n    return float(count) / total_chars\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the second-person pronoun \"you\" (robust to quotes/capitalization)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence-like chunks using punctuation or newlines\n    parts = [p.strip() for p in re.split(r'[.!?]+\\s+|\\n+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    starts_you = 0\n    for p in parts:\n        if re.match(r'^[\\'\"\\(\\[]*\\s*you\\b', p, re.I):\n            starts_you += 1\n    return float(starts_you) / len(parts)\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are inside single or double quotes (quoted content density)'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    # find double-quoted and single-quoted content (simple pairwise)\n    dq = re.findall(r'\"([^\"]+)\"', text)\n    sq = re.findall(r\"'([^']+)'\", text)\n    quoted_chars = sum(len(m) for m in dq) + sum(len(m) for m in sq)\n    return float(quoted_chars) / float(total_len) if total_len > 0 else 0.0\n\n\n", "def feature(text: str) -> float:\n    'Ratio of modal verbs (can/could/may/might/must/should/would/will/shall) to total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total = len(words)\n    if total == 0:\n        return 0.0\n    modals = re.findall(r'\\b(?:can|could|may|might|must|should|would|will|shall)\\b', text, flags=re.I)\n    return float(len(modals)) / float(total)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), robust to missing sentence punctuation'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_words = len(words)\n    # Count non-empty sentence-like segments\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sentences = len(sentences) if sentences else 1\n    return total_words / float(num_sentences)\n\n", "def feature(text: str) -> float:\n    \"Indicator for explicit translation tags like '[translated]' (1.0 if present, else 0.0)\"\n    if not text:\n        return 0.0\n    return 1.0 if '[translated]' in text.lower() else 0.0\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation symbols used divided by total punctuation occurrences (0-1)'\n    import string\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Binary-ish score: detects title-like first non-empty line (short, title-case words) to indicate headings'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    first = lines[0].strip()\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", first)\n    if not (1 <= len(words) <= 6):\n        return 0.0\n    # Check many words start with uppercase letter\n    upper_count = sum(1 for w in words if w[0].isupper())\n    if upper_count >= max(1, int(0.6 * len(words))):\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are unusually long (>12 characters) \u2014 may indicate complex vocabulary or concatenated tokens typical in some generated text'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) > 12)\n    return float(long_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with -ly (heuristic adverb density)'\n    import re, string\n    if not text:\n        return 0.0\n    raw = re.findall(r\"\\S+\", text)\n    cleaned = []\n    for tok in raw:\n        tok = tok.strip(string.punctuation).lower()\n        if tok:\n            cleaned.append(tok)\n    if not cleaned:\n        return 0.0\n    ly_count = sum(1 for w in cleaned if w.endswith('ly'))\n    return float(ly_count) / len(cleaned)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas density reflecting clause complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_total = text.count(',')\n    return float(comma_total) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\w+['\u2019]?\\w*|\\w+\", text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i', \"i'm\", 'im', \"i've\", \"i'd\", 'me', 'my', 'mine', 'we', \"we're\", 'us', 'our', 'ours'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<=4 words) \u2014 often dialogue fragments'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence terminators but keep fragments\n    sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    short_count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 4 and len(words) > 0:\n            short_count += 1\n    return float(short_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Contraction ratio: fraction of tokens that are contractions (contain apostrophes like \"don\\'t\", \"I\\'m\", \"it\\'s\")'\n    import re\n    if not text:\n        return 0.0\n    # count contraction patterns with apostrophe\n    contr = re.findall(r\"\\b\\w+'[a-z]{1,4}\\b\", text.lower())\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(len(contr))\n    return len(contr) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of vocabulary types that occur only once (an indicator of lexical richness vs repetition)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    c = Counter(tokens)\n    vocab = len(c)\n    if vocab == 0:\n        return 0.0\n    hapax = sum(1 for k,v in c.items() if v == 1)\n    return float(hapax) / float(vocab)\n", "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (std / mean) \u2014 captures variability in lexical shape'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = float(sum(lengths)) / len(lengths)\n    if mean == 0.0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are the second-person pronoun \"you\" (case-insensitive)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    you_count = sum(1 for t in tokens if t == 'you')\n    return you_count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (may, might, could, would, should, must, can)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'may', 'might', 'could', 'would', 'should', 'must', 'can', 'shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proxy for passive voice: fraction of \"be\" auxiliaries followed by likely past participles (be + .*ed or *en)'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    # Match common 'be' forms followed by a token that ends with ed or en (approximate past participle)\n    matches = re.findall(r'\\b(?:is|are|was|were|am|be|been|being)\\s+\\w+(?:ed|en)\\b', lowered)\n    # Normalise by token count to avoid bias with very long/short texts\n    tokens = re.findall(r'\\b\\w+\\b', lowered)\n    denom = max(1, len(tokens))\n    return float(len(matches)) / denom\n\n", "def feature(text: str) -> float:\n    'Density of discourse markers (however, therefore, moreover, thus, consequently, furthermore) per sentence'\n    import re\n    if not text:\n        return 0.0\n    markers = ['however', 'therefore', 'moreover', 'furthermore', 'thus', 'consequently', 'in addition', 'on the other hand']\n    lower = text.lower()\n    matches = 0\n    for m in markers:\n        # count whole-word occurrences; for multiword markers allow a simple substring search anchored by word boundaries\n        matches += len(re.findall(r'\\b' + re.escape(m) + r'\\b', lower))\n    # sentence count fallback\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(matches) / float(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain quotation marks (dialogue indicator)'\n    try:\n        import re\n        # split into sentence-like segments\n        sentences = re.split(r'(?<=[.!?])\\s+', text)\n        if not sentences or (len(sentences) == 1 and sentences[0].strip() == ''):\n            return 0.0\n        quote_chars = re.compile(r'[\"\\u201c\\u201d]')\n        qcount = sum(1 for s in sentences if quote_chars.search(s))\n        return float(qcount) / float(len(sentences))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ly\" (common adverb suffix) \u2014 may capture stylistic adverb use'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ly'))\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a gerund/participle word (first token ends with \"ing\")'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    count = 0\n    for s in sents:\n        parts = s.split()\n        if parts:\n            first = re.sub(r'^[^\\w]+|[^\\w]+$', '', parts[0])  # strip non-word chars around\n            if first.lower().endswith('ing') and len(first) > 3:\n                count += 1\n    return float(count) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Density of mid-sentence capitalized words (heuristic proper-name density) = caps not immediately after .!?'\n    import re\n    if not text:\n        return 0.0\n    # find capitalized words like \"John\" or \"Mars\"\n    caps = list(re.finditer(r'\\b[A-Z][a-z]+\\b', text))\n    if not caps:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_tokens = len(words) if words else 1\n    count = 0\n    for m in caps:\n        start = m.start()\n        # look back to find last non-space character before this word\n        i = start - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        if i < 0:\n            # likely start of text -> treat as sentence start -> skip\n            continue\n        if text[i] in '.!?':\n            # likely sentence start -> skip\n            continue\n        count += 1\n    return float(count) / total_tokens\n\n\n", "def feature(text: str) -> float:\n    'Progressive -ing token fraction: fraction of tokens ending with \"ing\" (approx. continuous actions or gerunds)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing'))\n    return ing_count / n\n\n", "def feature(text: str) -> float:\n    'Adverb density: fraction of tokens that end with \"ly\" (heuristic for adverb use)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain any numeric digit (numbers, years, measurements)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Em-dash or double-dash frequency normalized by word count (heuristic for stylistic punctuation like \"\u2014\" or \"--\")'\n    import re\n    if not text:\n        return 0.0\n    dash_count = text.count('\u2014') + text.count('--')\n    words = re.findall(r'\\w+', text)\n    return float(dash_count) / max(1.0, len(words))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences normalized by token count'\n    import re\n    ellipses = text.count('...')\n    tokens = re.findall(r'\\w+', text)\n    return ellipses / max(1, len(tokens))\n\n", "def feature(text: str) -> float:\n    'Citation year density: fraction of tokens that are 4-digit years in the 1900-2099 range'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(?:19|20)\\d{2}\\b', text)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(len(years)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated and contain at least one capitalized component (e.g., Shapiro-Stiglitz)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[\\w-]+\\b', text)\n    if not tokens:\n        return 0.0\n    hyphen_caps = 0\n    for t in tokens:\n        if '-' in t:\n            parts = [p for p in t.split('-') if p]\n            if any(p[0].isupper() for p in parts if p):\n                hyphen_caps += 1\n    return float(hyphen_caps) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one hyphen (detects age ranges, compound adjectives, e.g., \"73-year-old\")'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens containing digits (numeric token ratio), catches years and enumerations'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    numeric = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(numeric) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text)\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t.lower() in first_person)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause density and complex sentence structure)'\n    import re\n    if not text:\n        return 0.0\n    # Rough sentence split on .!? and newlines\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback: treat whole text as one sentence\n        sentences = [text.strip()]\n    comma_count = text.count(',')\n    return float(comma_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Comma density per sentence: average number of commas per sentence (proxy for clause-complexity)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # count non-empty sentence fragments\n    sents = [s for s in re.split(r'[.!?]', text) if s.strip()]\n    num_sents = len(sents) if len(sents) > 0 else 1\n    return float(comma_count) / float(num_sents)\n\n", "def feature(text: str) -> float:\n    'Heading density: fraction of lines that look like short title/headings (one-4 words, title-case)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    heading_count = 0\n    for ln in lines:\n        words = ln.split()\n        if 1 <= len(words) <= 4:\n            # consider title-case or a single word starting with uppercase\n            if all(w[0].isupper() for w in words if w):\n                heading_count += 1\n    return heading_count / len(lines)\n\n", "def feature(text: str) -> float:\n    'Density of common academic/Latin abbreviations (e.g., \"e.g.\", \"i.e.\", \"et al.\") per token'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    patterns = ['e.g.', 'eg,', 'i.e.', 'ie,', 'et al', 'cf.', 'viz.', 'ibid', 'op. cit.']\n    count = 0\n    for p in patterns:\n        count += lower.count(p)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Median sentence length in words (robust to outliers)'\n    import re, math\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+|\\n+', s)\n    lens = []\n    for sent in sentences:\n        sent = sent.strip()\n        if not sent:\n            continue\n        words = re.findall(r'\\w+', sent)\n        lens.append(len(words))\n    if not lens:\n        return 0.0\n    lens.sort()\n    n = len(lens)\n    if n % 2 == 1:\n        return float(lens[n//2])\n    else:\n        return (lens[n//2 - 1] + lens[n//2]) / 2.0\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentence_count), sentences min 1'\n    if not text:\n        return 0.0\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    comma_count = text.count(',')\n    return float(comma_count) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common prepositions (of, in, on, with, by, for, to, from, about, as, at)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    preps = {'of', 'in', 'on', 'with', 'by', 'for', 'to', 'from', 'about', 'as', 'at', 'into', 'through', 'between', 'among', 'over', 'against', 'during', 'without', 'within', 'toward', 'towards'}\n    count = sum(1 for t in tokens if t in preps)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of mid-sentence capitalized tokens (likely proper nouns) excluding sentence-initial capitals'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    total = 0\n    for m in re.finditer(r'\\b[A-Z][a-z]+\\b', text):\n        start = m.start()\n        # look back one char to see if this is sentence start\n        if start == 0:\n            continue\n        prev_char = text[start-1]\n        # treat as mid-sentence if previous char is not sentence terminator or newline\n        if prev_char not in '.!? \\n\\r\\t':\n            count += 1\n        total += 1\n    # normalize by token count to be comparable across lengths\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total tokens (lexical diversity)'\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / n\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, us, our) as a proxy for personal vs. impersonal tone'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / len(words)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hedging/modal/qualifying words common in academic writing (may, might, could, suggest, indicate, likely, possibly, appears, tends)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    hedges = {'may','might','could','can','suggest','suggests','suggested','indicate','indicates','indicated','likely','possibly','potentially','appear','appears','appeared','tend','tends','tended','suggesting','indicating'}\n    count = sum(1 for w in words if w in hedges)\n    return float(count) / float(len(words))\n\n\n", "def feature(text: str) -> float:\n    'Average count of colons and semicolons per sentence (proxy for clause-complex punctuation use)'\n    import re\n    if not text:\n        return 0.0\n    colons_semis = text.count(':') + text.count(';')\n    # sentence count fallback to at least 1\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(colons_semis) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are extremely long (length > 12 characters) \u2014 catches technical names and compound terms'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) > 12)\n    return float(long_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a digit or look like numeric measurements (percent, mg, ml, cm) \u2014 numeric density'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    num_patterns = re.compile(r'\\d|%|mg\\b|ml\\b|cm\\b|km\\b|mm\\b|kg\\b|g\\b|\u00b5g\\b|ml\\)|\\b\\(\\d')\n    count = 0\n    for t in tokens:\n        if num_patterns.search(t.lower()):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated (contain a hyphen), normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    word_tokens = re.findall(r'\\w+', text)\n    denom = max(1, len(word_tokens))\n    hyphen_count = sum(1 for t in tokens if '-' in t and any(ch.isalpha() for ch in t))\n    return float(hyphen_count) / denom\n\n", "def feature(text: str) -> float:\n    'Average number of transitional/adversative adverbs (however, moreover, therefore, etc.) per sentence'\n    import re\n    if not text:\n        return 0.0\n    trans = {'however','moreover','furthermore','therefore','consequently','nevertheless','additionally','subsequently','meanwhile','thus','hence'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    trans_count = sum(1 for t in tokens if t in trans)\n    return float(trans_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like headings (short lines with title-cased words or that end with a question/exclamation)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    heading_count = 0\n    for ln in lines:\n        words = ln.split()\n        if not words:\n            continue\n        # candidate heading: short and many title-cased initials, or ends with ?/!\n        if len(words) <= 8:\n            title_initials = sum(1 for w in words if w[0].isupper())\n            if title_initials >= max(1, len(words) // 2):\n                heading_count += 1\n                continue\n        if ln.endswith('?') or ln.endswith('!'):\n            heading_count += 1\n    return float(heading_count) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Proportion of bigram tokens that are repeated beyond their first occurrence (repeat-density of bigrams)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[a-zA-Z0-9]+\\b', text.lower())\n    if len(words) < 2:\n        return 0.0\n    bigrams = ['%s %s' % (words[i], words[i+1]) for i in range(len(words)-1)]\n    freq = Counter(bigrams)\n    total = len(bigrams)\n    repeated_tokens = sum((f - 1) for f in freq.values() if f > 1)\n    return float(repeated_tokens) / float(total) if total > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that look \"definition-like\" (linking verb such as \"is/are/can be/are used to/is defined as\" followed by \"that\" or a defining construction)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    pattern = re.compile(r'\\b(?:is|are|can be|are used to|serves to|serve to|refers to|is defined as|are defined as|acts as|can serve as)\\b.*\\bthat\\b', flags=re.I)\n    count = 0\n    for s in sentences:\n        if pattern.search(s):\n            count += 1\n    return float(count) / float(len(sentences)) if sentences else 0.0\n\n", "def feature(text: str) -> float:\n    'Density of transition/adverbial connectors (however, moreover, therefore, furthermore, additionally, in addition) per token'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    transitions = {'however','moreover','therefore','furthermore','additionally','consequently','nevertheless','nonetheless','in addition','thus','hence'}\n    count = 0\n    for t in tokens:\n        if t in transitions:\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of Latinate/technical suffixes (words ending with -ize/-ise/-ic/-al/-ary/-ous/-ive) as a proxy for formal/academic tone'\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w[\\w'-]*\", text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ize','ise','ic','al','ary','ous','ive','ent','ant')\n    count = 0\n    for t in tokens:\n        for suf in suffixes:\n            if len(t) > len(suf) + 1 and t.endswith(suf):\n                count += 1\n                break\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with common discourse markers (However, Moreover, In addition, Therefore, When, While, Although, Because, Thus, Meanwhile)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # Split into sentence-like chunks\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    markers = {'however','moreover','furthermore','therefore','additionally','consequently','meanwhile','thus','hence','in','when','while','although','because','there','nonetheless','nevertheless'}\n    count = 0\n    for s in sents:\n        m = re.match(r\"^\\s*['\\\"\\(\\[]*([A-Za-z\\-]+)\", s)\n        if m:\n            first = m.group(1).lower()\n            if first in markers:\n                count += 1\n    return float(count) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hedging/epistemic modal words (may, might, could, seem, appear, suggest, possibly, likely)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    hedges = {'may', 'might', 'could', 'would', 'seem', 'seems', 'seemed', 'appear', 'appears', 'appeared', 'suggest', 'suggests', 'suggested', 'possibly', 'perhaps', 'likely', 'probable', 'probabilistic'}\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in -ly (a proxy for adverb/adverbial style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 3)\n    return float(ly_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of tokens that occur only once (lexical variety proxy)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    counts = Counter(tokens)\n    hapaxes = sum(1 for c in counts.values() if c == 1)\n    return float(hapaxes) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of repeated adjacent bigrams (duplicate bigrams / total bigrams), proxy for formulaic repetition'\n    import re\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    total = len(bigrams)\n    unique = len(set(bigrams))\n    repeats = total - unique\n    return float(repeats) / total\n\n", "def feature(text: str) -> float:\n    'Density of explicit quotation marks (double quotes + single quotes used as quotation marks, excluding apostrophes inside words), normalized by token count.'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    token_count = max(1, len(words))\n    double_q = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    # Count single quotes that are likely quotation marks (not embedded apostrophes inside words)\n    single_q_matches = re.findall(r\"(?<![A-Za-z0-9])'|'(?![A-Za-z0-9])\", text)\n    single_q = len(single_q_matches)\n    total_q = double_q + single_q\n    return float(total_q) / token_count\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words; high values indicate variable sentence sizing'\n    import re, math\n    if not text:\n        return 0.0\n    # Split on sentence end punctuation sequences\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', p)) for p in parts]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (can, could, may, might, should, would, must) indicating hedging or instruction'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'should', 'would', 'must', 'shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Discourse-marker density: frequency of common academic/connective phrases (however, therefore, moreover, on the other hand) per sentence'\n    import re\n    if not text:\n        return 0.0\n    lc = text.lower()\n    markers = ['however', 'therefore', 'moreover', 'furthermore', 'in addition', 'on the other hand', 'conversely', 'nevertheless', 'as a result', 'in contrast']\n    count = 0\n    for m in markers:\n        # count word/phrase occurrences\n        count += len(re.findall(r'\\b' + re.escape(m) + r'\\b', lc))\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(count) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, we, me, us, my, our, mine, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','we','me','us','my','our','mine','ours','myself','ourselves'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of 4-digit year-like numbers (1000-2100) per sentence to capture historical/academic references'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(1[0-9]{3}|20[0-9]{2}|2100)\\b', text)\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(years)) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), 0 if no sentence delimiters'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence end punctuation, but keep robust for no punctuation\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback: use entire text as one sentence if there are words\n        words = re.findall(r'\\w+', text)\n        return float(len(words)) if words else 0.0\n    words_per_sent = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not words_per_sent:\n        return 0.0\n    return float(sum(words_per_sent)) / float(len(words_per_sent))\n\n", "def feature(text: str) -> float:\n    'Comma density: number of commas per word'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    num_words = len(words)\n    if num_words == 0:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count) / float(num_words)\n\n", "def feature(text: str) -> float:\n    'Density of double-quote characters (indicator of direct speech/dialogue) per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_words = max(1, len(words))\n    quote_chars = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    return float(quote_chars) / total_words\n\n", "def feature(text: str) -> float:\n    'Proportion of words that are adverbs ending with \"ly\" (indicative of descriptive/flowery prose)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if w.endswith('ly') and len(w) > 3)\n    return float(ly_count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Normalized count of ellipses (\"...\") occurrences per 100 words (captures poetic/hesitant style)'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    # word count robustly\n    import re\n    words = re.findall(r\"\\w+(?:'\\w+)?\", text)\n    wc = max(1, len(words))\n    return float(ellipses) / float(wc) * 100.0\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the word \"as\" (case-insensitive)'\n    import re\n    if not text or text.strip() == '':\n        return 0.0\n    # Split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts_with_as = 0\n    for s in sentences:\n        first_word_match = re.match(r\"^\\s*([A-Za-z']+)\", s)\n        if first_word_match and first_word_match.group(1).lower() == 'as':\n            starts_with_as += 1\n    return float(starts_with_as) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Repeated trigram fraction: proportion of word-trigrams that occur more than once (0 if none)'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 3:\n        return 0.0\n    trigrams = [' '.join(words[i:i+3]) for i in range(len(words)-2)]\n    from collections import Counter\n    c = Counter(trigrams)\n    repeated_count = sum(count for trigram, count in c.items() if count > 1)\n    # measure repeated trigram occurrences relative to total trigrams\n    return float(repeated_count) / float(len(trigrams))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total words (lexical diversity)'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    uniq = len(set(words))\n    return float(uniq) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of modal verbs (can/could/may/might/must/shall/should/will/would) among tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Estimated fraction of sentences showing a passive-voice-like pattern (was/were/... + past-participle ending in -ed)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    passive_pattern = re.compile(r'\\b(?:was|were|is|are|been|be|had been|has been|have been|was being|were being)\\s+\\w+ed\\b', re.I)\n    passive_count = sum(1 for s in sentences if passive_pattern.search(s))\n    return float(passive_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Density of the auxiliary \"had\" (common in past-perfect narrative) as tokens per word'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    return words.count('had') / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of the definite article \"the\" per token (may capture generic/over-descriptive phrasing)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    the_count = sum(1 for w in words if w == 'the')\n    return float(the_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Lexical diversity: number of unique lowercased word tokens divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / len(words)\n\n", "def feature(text: str) -> float:\n    'Density of subordinating conjunctions (because, although, while, since, unless, etc.) per sentence'\n    import re\n    if not text:\n        return 0.0\n    conj_pattern = r'\\b(because|although|though|while|since|whereas|unless|after|before|provided|if|when|whenever)\\b'\n    matches = re.findall(conj_pattern, text, flags=re.IGNORECASE)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that end with \"ing\" (proxy for progressive aspect / gerunds)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) >= 4 and t.endswith('ing'))\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are sensory words (see, look, hear, feel, smell, taste, touch and variants)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    senses = {'see','saw','seen','look','looked','gaze','gazed','watch','watched',\n              'hear','heard','listen','listened','feel','felt','touch','touched',\n              'smell','smelt','smelled','taste','tasted','observe','observed','perceive','perceived'}\n    count = sum(1 for w in words if w in senses)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain any digit (captures explicit durations, numbers, or measurements)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the definite article \"the\" (case-insensitive)'\n    sentences = re.split(r'[.!?]+', text)\n    sent_tokens = []\n    for s in sentences:\n        s = s.strip()\n        if s:\n            tokens = re.findall(r'\\w+', s, flags=re.UNICODE)\n            sent_tokens.append(tokens)\n    if not sent_tokens:\n        return 0.0\n    starts = 0\n    for tokens in sent_tokens:\n        if tokens and tokens[0].lower() == 'the':\n            starts += 1\n    return float(starts) / float(len(sent_tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that look like short title lines (1-6 words, each word title-cased), e.g. \"The Colors Fade\"'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip() != '']\n    if not lines:\n        return 0.0\n    title_like = 0\n    for ln in lines:\n        tokens = re.findall(r'\\b\\w+\\b', ln)\n        if 1 <= len(tokens) <= 6:\n            alpha_tokens = [t for t in tokens if any(c.isalpha() for c in t)]\n            if alpha_tokens and all(t[0].isupper() for t in alpha_tokens):\n                title_like += 1\n    return float(title_like) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a common preposition (in, on, at, during, within, under, above, across, into, among, between, along, beneath, inside, outside, over)'\n    import re\n    preps = {'in','on','at','during','within','under','above','across','into','among','between','along','beneath','inside','outside','over'}\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s and s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.match(r'\\s*([A-Za-z]+)', s)\n        if m:\n            if m.group(1).lower() in preps:\n                count += 1\n    return float(count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are third-person pronouns (he/she/they and variants) as a narrative-person indicator'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    third = {'he','she','they','him','her','them','his','their','hers','theirs'}\n    count = sum(1 for t in tokens if t in third)\n    return float(count) / total\n\n", "def feature(text: str) -> float:\n    'Average number of colon characters (:) per sentence, 0.0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = len(sentences)\n    colons = text.count(':')\n    if num_sent == 0:\n        return 0.0\n    return float(colons) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique lowercased word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"\\b[\\w'-]+\\b\", text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return unique / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that end with \"ing\" (present participles / progressive aspect)'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    ing_count = 0\n    for t in tokens:\n        tl = t.lower()\n        if len(tl) > 3 and tl.endswith('ing'):\n            ing_count += 1\n    return float(ing_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    words = re.findall(r'\\w+', text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    first_person = re.findall(r'\\b(?:i|me|my|mine|we|us|our|ours)\\b', text, flags=re.IGNORECASE)\n    return float(len(first_person)) / float(wc)\n\n", "def feature(text: str) -> float:\n    'Fraction of modal verbs (can/could/may/might/shall/should/will/would/must/ought) among tokens'\n    import re\n    modals = {'can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would', 'must', 'ought'}\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in modals)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of relative/connecting pronouns (which, that, who, whom, whose) per token - hints at explanatory/relative-clause style'\n    import re\n    tokens = re.findall(r\"[A-Za-z]+\", text)\n    if not tokens:\n        return 0.0\n    rel = {'which', 'that', 'who', 'whom', 'whose'}\n    count = sum(1 for t in tokens if t.lower() in rel)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of typographic (non-ASCII) quotation/apostrophe/dash characters per word (curly quotes, em-dash, ellipsis, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    nwords = max(1, len(tokens))\n    special_chars = ['\\u2018', '\\u2019', '\\u201c', '\\u201d', '\\u2013', '\\u2014', '\\u2026']\n    count = sum(text.count(ch) for ch in special_chars)\n    return count / float(nwords)\n\n", "def feature(text: str) -> float:\n    'Average length in words of quoted segments (0.0 if no quoted segments)'\n    import re\n    if not text:\n        return 0.0\n    # capture content between common quote characters\n    segments = re.findall(r'[\"\u201c\u201d](.*?)[\"\u201c\u201d]', text, flags=re.DOTALL)\n    if not segments:\n        # try single quotes as fallback\n        segments = re.findall(r\"[\u2018\u2019'](.*?)[\u2018\u2019']\", text, flags=re.DOTALL)\n    if not segments:\n        return 0.0\n    word_counts = []\n    for seg in segments:\n        words = re.findall(r\"\\b[\\w']+\\b\", seg)\n        word_counts.append(len(words))\n    return float(sum(word_counts)) / float(len(word_counts))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ly\" (proxy for adverb/adjectival style) per word'\n    try:\n        import re\n        tokens = re.findall(r'\\w+', text.lower())\n        if not tokens:\n            return 0.0\n        ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n        return float(ly_count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Ratio of explicit past auxiliary verbs (was/were) to combined past+present auxiliaries (was/were vs is/are); 0 if none'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    past = len(re.findall(r'\\bwas\\b', lower)) + len(re.findall(r'\\bwere\\b', lower))\n    present = len(re.findall(r'\\bis\\b', lower)) + len(re.findall(r'\\bare\\b', lower))\n    denom = past + present\n    if denom == 0:\n        return 0.0\n    return float(past) / denom\n\n", "def feature(text: str) -> float:\n    'Average number of quoted/dialogue segments per sentence (0 if no sentences)'\n    import re\n    if not text:\n        return 0.0\n    # find quoted segments using straight and curly double quotes\n    quoted = re.findall(r'[\u201c\"\u201d](.+?)[\u201c\"\u201d]', text, flags=re.DOTALL)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    return float(len(quoted)) / float(len(sentences))\n\n\n", "def feature(text: str) -> float:\n    'Density of blank/empty lines (number of blank lines divided by total lines)'\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    blank = sum(1 for ln in lines if not ln.strip())\n    return float(blank) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Fraction of hapax legomena (tokens that occur exactly once) among all word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    freqs = Counter(tokens)\n    hapaxes = sum(1 for v in freqs.values() if v == 1)\n    return float(hapaxes) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common speech/dialogue verbs (said, asked, shouted, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    speech_verbs = {'said','asked','replied','whispered','muttered','shouted','exclaimed','cried',\n                    'sighed','yelled','answered','answered','beckoned','laughed','snapped','observed'}\n    count = sum(1 for t in tokens if t in speech_verbs)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that match a short list of common irregular past-tense verbs (approx. irregular past usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    irregular_past = {'was','were','had','went','came','said','took','made','got','saw','knew','thought','told','left','felt','kept','found','gave','began','brought','ran','fell','flew','chose','spoke','slept','stood','wrote','read','shook','rode','built','brought','hid','held'}\n    count = sum(1 for t in tokens if t in irregular_past)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per word (captures trailing/pausing stylistic markers)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = max(1, len(words))\n    ellipses = text.count('...')\n    return float(ellipses) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (words per sentence); 0 for <2 sentences'\n    import re, math\n    if not text:\n        return 0.0\n    # split on sentence-ending punctuation, keep non-empty\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sentences) < 2:\n        return 0.0\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like time expressions (e.g., \"5:01\", \"12:30\", tokens containing \":\" or standalone AM/PM)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    time_re = re.compile(r'\\b\\d{1,2}:\\d{2}\\b')\n    ampm_re = re.compile(r'(?i)\\b(?:am|pm)\\b')\n    count = 0\n    for t in tokens:\n        if time_re.search(t) or ampm_re.search(t) or (':' in t and any(ch.isdigit() for ch in t)):\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: count of sequences of 3+ dots normalized by number of sentences (0 if none)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = re.findall(r'\\.{3,}', text)\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(ellipses)) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that are adverbs ending in \"-ly\" (proxy for descriptive/flowery style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text, flags=re.IGNORECASE)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 2 and t.lower().endswith('ly'))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens ending with \"ing\" (proxy for progressive/descriptive phrasing)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Normalized count of explicit simile markers (e.g., \"like a\", \"as if\", \"as though\", \"as ... as\") per token'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    patterns = [r'\\blike\\s+(a|an|the)\\b', r'\\bas if\\b', r'\\bas though\\b', r'\\bas\\s+\\w+\\s+as\\b']\n    tokens = re.findall(r'\\b\\w+\\b', lower)\n    if not tokens:\n        return 0.0\n    matches = 0\n    for p in patterns:\n        matches += len(re.findall(p, lower))\n    return float(matches) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of explicit dialogue markers: count of quotation marks and em-dashes per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    num_sent = max(1, len(sentences))\n    quote_marks = text.count('\"') + text.count('\u201c') + text.count('\u201d') + text.count(\"''\")\n    emdash_marks = text.count('\u2014') + text.count('--') + text.count('\u2013')\n    markers = quote_marks + emdash_marks\n    return float(markers) / num_sent\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a subordinating conjunction (as, when, while, although, because, since, if, though, after, before)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences using punctuation\n    raw_sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in raw_sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    starters = {'as', 'when', 'while', 'although', 'because', 'since', 'if', 'though', 'after', 'before', 'once', 'unless'}\n    count = 0\n    for s in sentences:\n        m = re.match(r\"\\s*([A-Za-z']+)\", s)\n        if m and m.group(1).lower() in starters:\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that belong to a small narrative genre vocabulary (sci-fi/horror words like \"galaxy\",\"ship\",\"alien\",\"void\",\"corpse\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    genre_words = {'galaxy','ship','alien','void','captain','orbit','planet','surface','readings','gelatinous',\n                   'mortician','corpse','slumber','woke','body','glowing','game','void','orb','sci','science','engine',\n                   'ship','star','world','alien','creature','spaceship','colony','astronaut','synthetic','android','cyber'}\n    count = sum(1 for t in tokens if t in genre_words)\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") measured as ellipsis occurrences per sentence (0 if no sentences)'\n    try:\n        import re\n        if not text:\n            return 0.0\n        ellipses = len(re.findall(r'\\.\\.\\.+', text))\n        # count sentences as occurrences of .!? (fallback minimum 1)\n        sentences = max(1, len(re.findall(r'[.!?]', text)))\n        return float(ellipses) / sentences\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like a short title (short line, initial caps on words)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    title_lines = 0\n    for ln in lines:\n        words = ln.split()\n        if 1 <= len(words) <= 6:\n            all_initial_caps = True\n            for w in words:\n                cleaned = re.sub(r'^[^A-Za-z]+|[^A-Za-z]+$', '', w)\n                if not cleaned:\n                    all_initial_caps = False\n                    break\n                if not cleaned[0].isupper():\n                    all_initial_caps = False\n                    break\n            if all_initial_caps:\n                title_lines += 1\n    return float(title_lines) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are numeric or contain digits (numeric density)'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num) / max(1, len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average sentence length in tokens (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # sentences split at punctuation followed by whitespace\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip())\n    token_re = re.compile(r'\\w+')\n    lengths = []\n    for s in sentences:\n        tokens = token_re.findall(s)\n        if tokens:\n            lengths.append(len(tokens))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Density of short-to-medium acronyms (all-caps tokens of length 2\u20136) as a fraction of word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    acr = re.findall(r'\\b[A-Z]{2,6}\\b', text)\n    return float(len(acr)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of parenthetical years/citation-style years (e.g., (2001)) per sentence, proxy for academic citations'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[.!?]', text)\n    num_sent = max(1, len(sentences))\n    # detect 4-digit years in parentheses (1900-2099)\n    year_paren_matches = re.findall(r'\\(\\s*(?:19|20)\\d{2}\\b[^\\)]*\\)', text)\n    return float(len(year_paren_matches)) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Density of tokens containing technical symbols (hyphens, slashes, percent, degree, micro, \u00b1 etc.)'\n    s = text or ''\n    toks = s.split()\n    if not toks:\n        return 0.0\n    symbols = set('-/%%\u00b5\u03bc\u00b0\u00b1\u2013\u2014')\n    # count tokens that include any of these special technical characters\n    cnt = 0\n    for t in toks:\n        if any((ch in symbols) for ch in t):\n            cnt += 1\n    result = float(cnt) / float(len(toks))\n    return result\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas indicate clause/subordinate density)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    # Count sentence separators (., !, ?). If none, treat as one sentence.\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(commas) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Estimate of passive-voice constructions: occurrences of be-forms followed by an -ed token per sentence'\n    import re\n    if not text:\n        return 0.0\n    be_forms = r'\\b(?:is|are|was|were|be|been|being|am)\\b'\n    # look for patterns like 'is produced', 'were observed', etc.\n    matches = re.findall(be_forms + r'\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    # normalize by number of sentences (approx)\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(matches)) / sentences\n\n", "def feature(text: str) -> float:\n    'Ratio of detected Latin binomial-like phrases (Capitalized lowercase + lowercase, e.g., \"Homo sapiens\") to token count'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = len(words)\n    if word_count == 0:\n        return 0.0\n    matches = re.findall(r'\\b[A-Z][a-z]{2,}\\s+[a-z]{2,}\\b', text)\n    # each match covers two words; normalize by token count\n    return float(len(matches) * 2) / word_count\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are measurement units or numeric tokens immediately followed by units (ml, mg, \u00b0C, %, etc.)'\n    import re\n    if not text:\n        return 0.0\n    units = {'ml','l','mg','g','kg','\u00b5l','ul','mm','cm','m','mol','mM','M','kda','kb','bp','kbp','%','\u00b0c','\u00b0f'}\n    # count explicit unit tokens\n    tokens = re.findall(r\"\\b[\\w%\u00b0\u00b5\u03bc]+(?:\\b|$)\", text.lower())\n    if not tokens:\n        return 0.0\n    unit_count = 0\n    # detect patterns like '10ml' or '10 ml' and standalone unit tokens\n    unit_pattern = re.compile(r'\\d+(?:[\\.,]\\d+)?\\s*(%|\u00b0c|\u00b0f|ml|l|mg|g|kg|\u00b5l|ul|mm|cm|m|mol|mmol|mM|M|kda|kb|bp|kbp)\\b', re.I)\n    for m in unit_pattern.finditer(text):\n        unit_count += 1\n    for t in tokens:\n        if t.strip('.,;:()[]') in units:\n            unit_count += 1\n    # normalize by token count\n    return float(unit_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (indicative of complex, academic clause-joining)'\n    if not text:\n        return 0.0\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences <= 0:\n        sentences = 1\n    return float(text.count(';')) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (captures clause density / syntactic complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    comma_count = text.count(',')\n    if not sentences:\n        # if no clear sentence split, normalize by word count instead\n        words = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text)\n        return float(comma_count) / max(1.0, len(words))\n    return float(comma_count) / max(1.0, len(sentences))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total tokens (lexical diversity)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Newline density: fraction of characters that are newline characters (indicates paragraph/section structure)'\n    if not text:\n        return 0.0\n    return text.count('\\n') / float(len(text))\n\n", "def feature(text: str) -> float:\n    'Density of time expressions like \"7:00\", \"07:30 am\" per token (captures logs/diaries)'\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    times = re.findall(r'\\b\\d{1,2}:\\d{2}\\b(?:\\s?(?:am|pm))?|\\b\\d{1,2}\\s?(?:am|pm)\\b', text, flags=re.I)\n    return float(len(times)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Frequency of common author-year citation patterns (e.g., \"(Smith, 2019)\" or \"Smith et al., 2019\") per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    patterns = [\n        r'\\([A-Za-z][A-Za-z0-9\\-\\s\\.]+,\\s*\\d{4}\\)',   # (Lastname, 2019) or (Lastname et al., 2019)\n        r'\\b[A-Z][A-Za-z\\-]+ et al\\.,?\\s*\\d{4}\\b',   # Lastname et al., 2019\n        r'\\[\\s*[A-Za-z][A-Za-z\\-\\s\\.]*\\d{4}\\s*\\]'    # [Lastname 2019] etc.\n    ]\n    combined = '|'.join('(?:%s)' % p for p in patterns)\n    matches = re.findall(combined, text)\n    return float(len(matches)) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that contain any digit (years, chapter numbers, statistics) to total tokens'\n    tokens = text.split()\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count) / float(n)\n\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per sentence (captures clause-complexity style)'\n    import re\n    semis = text.count(';')\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(semis) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of short title-like lines: lines of 1-6 words where each word starts with an uppercase letter (heuristic for headings)'\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_title_like(line):\n        if any(line.endswith(p) for p in '.:!?'):\n            return False\n        words = line.split()\n        if not (1 <= len(words) <= 6):\n            return False\n        for w in words:\n            if not w[0].isupper():\n                return False\n        return True\n    title_lines = sum(1 for ln in lines if is_title_like(ln))\n    return float(title_lines) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are capitalized but not sentence-initial (captures named-entity mid-sentence density)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    mid_caps = 0\n    total = 0\n    for s in sentences:\n        words = re.findall(r\"\\b[\\w\u2019'-]+\\b\", s, re.UNICODE)\n        if not words:\n            continue\n        for w in words[1:]:\n            total += 1\n            # exclude lone \"I\"\n            if len(w) > 1 and w[0].isupper():\n                mid_caps += 1\n    if total == 0:\n        return 0.0\n    return float(mid_caps) / total\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: auxiliaries (was/were/is/are/been/being) followed by a past-participle-like token (/ed|en|wn|t endings)'\n    import re\n    if not text:\n        return 0.0\n    # look for patterns like \"was created\", \"were established\", \"is driven\", \"has been cited\"\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|has|had|have)(?:\\s+been)?\\s+[A-Za-z-]+(?:ed|en|wn|t)\\b', re.I)\n    matches = pattern.findall(text)\n    words = re.findall(r'\\b\\w+\\b', text)\n    return float(len(matches)) / len(words) if words else 0.0\n\n", "def feature(text: str) -> float:\n    'Semicolon density: semicolons per 1000 characters (captures clause-linking complexity)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    semis = text.count(';')\n    # scale to per-1000 characters for numeric stability across lengths\n    return float(semis) / float(total_chars) * 1000.0\n\n", "def feature(text: str) -> float:\n    'Ratio of words that look like nominalizations (common formal suffixes like -tion, -ment, -ity, -ness)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    suffixes = ('tion', 'sion', 'ment', 'ity', 'ness', 'ence', 'ance', 'alism', 'ization')\n    count = 0\n    for w in words:\n        for s in suffixes:\n            if w.endswith(s) and len(w) > len(s) + 2:\n                count += 1\n                break\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that look like short Title-Case headings (e.g., \"Case Summary\")'\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_title_case_line(ln: str) -> bool:\n        parts = ln.split()\n        if len(parts) < 1 or len(parts) > 6:\n            return False\n        # require most words to start with uppercase and have at least one lowercase letter after\n        good = 0\n        for p in parts:\n            if len(p) >= 2 and p[0].isupper() and any(ch.islower() for ch in p[1:]):\n                good += 1\n        return good >= max(1, len(parts) - 1)\n    count = sum(1 for ln in lines if is_title_case_line(ln))\n    return float(count) / float(len(lines))\n\n\n", "def feature(text: str) -> float:\n    'Comma density: fraction of characters that are commas (complex, clause-heavy prose)'\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count) / max(1.0, float(len(text)))\n\n", "def feature(text: str) -> float:\n    'Hapax ratio: fraction of word types that occur only once (vocabulary richness indicator)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\b\\w+\\b', text)]\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    hapaxes = sum(1 for w, c in freqs.items() if c == 1)\n    return float(hapaxes) / len(words)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (distinct word types divided by total tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    return float(len(set(tokens))) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Semicolon density: fraction of characters that are semicolons'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    semis = text.count(';')\n    return float(semis) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (words per sentence) to capture structural variability'\n    if not text:\n        return 0.0\n    import re, math\n    sentences = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        counts.append(len(words))\n    if len(counts) <= 1:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / (len(counts) - 1)\n    return float(math.sqrt(var))\n\n", "def feature(text: str) -> float:\n    'Proportion of adverbs approximated by tokens ending with \"ly\" (case-insensitive)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[^\\s]+\\b\", text)\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if w.lower().endswith('ly'))\n    return float(ly_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice indicator: proportion of \"be\" forms followed by past-participle-like tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    token_count = max(1, len(tokens))\n    # look for common \"be + past-participle\" patterns (e.g., \"was produced\", \"is shown\", \"were written\", \"has been done\")\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|am)\\s+\\w+(?:ed|en)\\b', text, flags=re.IGNORECASE)\n    # also include \"has been <verb>\" or \"have been <verb>\"\n    matches += re.findall(r'\\b(?:has|have|had)\\s+been\\s+\\w+(?:ed|en)\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / token_count\n\n", "def feature(text: str) -> float:\n    'Density of internal capitalized tokens (probable proper-nouns) excluding sentence-initial tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    total_tokens = 0\n    proper_tokens = 0\n    for s in sentences:\n        tokens = [t for t in re.findall(r'\\S+', s)]\n        if not tokens:\n            continue\n        # ignore first token of the sentence (may be capitalized by sentence start)\n        for t in tokens[1:]:\n            cleaned = t.strip('\"\\',:;()[]{}')\n            if not cleaned:\n                continue\n            total_tokens += 1\n            # consider as proper noun if starts with uppercase letter and has at least one lowercase letter following\n            if cleaned[0].isupper() and any(c.islower() for c in cleaned[1:]):\n                proper_tokens += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(proper_tokens) / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of hedge/modality words (may, might, could, appears, suggests, likely, etc.)'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','can','would','should','possibly','likely',\n              'suggests','suggest','appears','appear','seems','seem','tends','tend','probable','probably'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (typographic quotes, em-dash, accented chars)'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / total if total else 0.0\n\n", "def feature(text: str) -> float:\n    'Ratio of punctuation tokens that are preceded by a space (e.g., \"word .\"), which flags odd spacing'\n    import re\n    if not text:\n        return 0.0\n    spaced_before = len(re.findall(r'\\s[.,;:!?]', text))\n    total_punct = len(re.findall(r'[.,;:!?]', text))\n    if total_punct == 0:\n        return 0.0\n    return float(spaced_before) / float(total_punct)\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, mine, we, us, our, ours) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = re.findall(r'\\b(?:i|me|my|mine|we|us|our|ours)\\b', text, flags=re.IGNORECASE)\n    return float(len(first_person)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Colon density: fraction of characters that are colons (\":\"), common in formal lists/clauses'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float(text.count(':')) / total_chars\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating/discourse word (And, But, So, Also, However) \u2014 signals conversational or rhetorical structure'\n    import re\n    if not text:\n        return 0.0\n    parts = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not parts:\n        return 0.0\n    starts = 0\n    for s in parts:\n        m = re.match(r'^\\W*([A-Za-z\\'\u2019]+)', s)\n        if m:\n            w = m.group(1).lower()\n            if w in {'and', 'but', 'so', 'also', 'however', 'thus', 'then', 'therefore', 'meanwhile'}:\n                starts += 1\n    return float(starts) / float(len(parts))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence-ending punctuation that are question marks or exclamation points'\n    import re\n    if not text:\n        return 0.0\n    ends = re.findall(r'[.!?]', text)\n    if not ends:\n        return 0.0\n    q_e = sum(1 for c in ends if c in ('?', '!'))\n    return float(q_e) / len(ends)\n\n", "def feature(text: str) -> float:\n    'Density of quotation characters (double and curly quotes) as fraction of characters'\n    if not text:\n        return 0.0\n    quote_chars = ['\"', '\u201c', '\u201d', '\u00ab', '\u00bb']\n    count = sum(text.count(q) for q in quote_chars)\n    return float(count) / max(1.0, len(text))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that appear to be possessives (ending with \\'s, \u2019s, or s\\')'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\S+\", text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        tl = t.lower().rstrip('.,;:!?')\n        if tl.endswith(\"'s\") or tl.endswith(\"\u2019s\") or tl.endswith(\"s'\"):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of the definite article \"the\" to total tokens (lowercased)'\n    if not text:\n        return 0.0\n    toks = text.split()\n    if not toks:\n        return 0.0\n    the_count = sum(1 for t in toks if t.lower().strip(\".,;:!?\\\"'()[]{}\") == 'the')\n    return the_count / len(toks)\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: distinct punctuation characters divided by total punctuation characters'\n    import re\n    if not text:\n        return 0.0\n    punct = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not punct:\n        return 0.0\n    distinct = len(set(punct))\n    total = len(punct)\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen (compound adjective/noun usage indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9\u2019'-]+\", text)\n    if not tokens:\n        return 0.0\n    hyphenated = sum(1 for t in tokens if '-' in t)\n    return float(hyphenated) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that are uppercase acronyms (2+ consecutive ASCII uppercase letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    acr = 0\n    for t in tokens:\n        if len(t) >= 2 and re.fullmatch(r'[A-Z]{2,}', t):\n            acr += 1\n    return float(acr) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of ampersand tokens (&) to total word tokens (captures \"X & Y\" citation style)'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    amp_count = text.count('&')\n    return float(amp_count) / float(max(1, len(words)))\n\n", "def feature(text: str) -> float:\n    'Normalized variance of sentence lengths (variance / (mean+1)) to capture sentence-length variability'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(var) / (mean + 1.0)\n\n", "def feature(text: str) -> float:\n    'Proportion of word tokens that are -ing gerunds/continuous verbs (heuristic: tokens ending with \"ing\")'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return float(ing_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of capitalized tokens appearing mid-sentence (proxy for proper nouns / named entities)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences, then tokens per sentence\n    sentences = [s.strip() for s in re.split(r'([.!?])', text)]\n    # build real sentence chunks (join fragments)\n    chunks = []\n    cur = ''\n    for part in sentences:\n        if not part:\n            continue\n        cur += part\n        if re.search(r'[.!?]\\s*$', part):\n            chunks.append(cur.strip())\n            cur = ''\n        elif len(part) > 0 and part[-1] in '.!?':\n            chunks.append(cur.strip())\n            cur = ''\n    if cur:\n        chunks.append(cur.strip())\n    if not chunks:\n        chunks = [text]\n    mid_caps = 0\n    total_tokens = 0\n    for s in chunks:\n        tokens = re.findall(r'\\b\\w+\\b', s)\n        for i, t in enumerate(tokens):\n            total_tokens += 1\n            if i == 0:\n                continue\n            # capitalized but not all-caps (to avoid acronyms)\n            if t[0].isupper() and not t.isupper():\n                mid_caps += 1\n    if total_tokens == 0:\n        return 0.0\n    return mid_caps / total_tokens\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters used divided by total characters (captures stylistic richness)'\n    import re\n    if not text:\n        return 0.0\n    punct_chars = set('. , ; : ! ? - \u2014 ( ) [ ] { } \" \\' / \\\\ @ # $ % ^ & * _ ` ~ < > = +'.split())\n    # flatten to single-char set\n    punct_chars = set(''.join(punct_chars))\n    found = set(c for c in text if c in punct_chars)\n    # normalize by length to avoid bias from long texts\n    denom = max(1, len(text))\n    return float(len(found)) / denom\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that look like adverbs (ending in \"ly\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and any(c.isalpha() for c in t))\n    return float(ly_count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (ASCII \"...\" or unicode \"\u2026\") as a fraction of characters, capturing excerpting or trailing ellipses'\n    if not text:\n        return 0.0\n    ellipses = text.count('...') + text.count('\u2026')\n    return float(ellipses) / max(1, len(text))\n\n", "def feature(text: str) -> float:\n    'Proportion of apostrophe uses that look like possessives (e.g., owner\\'s or goods\u2019 ) among all apostrophe-like characters'\n    if not text:\n        return 0.0\n    # curly and straight apostrophes\n    apost_count = text.count(\"'\") + text.count(\"\u2019\")\n    if apost_count == 0:\n        return 0.0\n    possessive_patterns = re.findall(r\"\\b\\w+(?:'|\u2019)\\s?s\\b\", text)  # captures patterns like word's (with optional stray space)\n    # also capture plural possessive like words' or words\u2019 (word + s + apostrophe)\n    possessive_patterns += re.findall(r\"\\b\\w+s(?:'|\u2019)\\b\", text)\n    possessive_count = len(possessive_patterns)\n    return float(possessive_count) / apost_count\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice score: occurrences of a \"be\" form followed by an -ed token per sentence'\n    import re\n    if not text:\n        return 0.0\n    # look for common \"be\" auxiliaries followed by a past-participial-looking word\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|wasn\\'t|weren\\'t|has been|have been|had been)\\b\\s+\\b\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    # normalize by number of sentences (to be robust across lengths)\n    sent_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(matches) / sent_count)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that appear to be past-tense verbs (heuristic: alphabetic tokens ending with \"ed\" and length>3)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    past_ed = sum(1 for t in tokens if t.endswith('ed') and len(t) > 3)\n    return past_ed / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain a quoted span that looks like a title (starts with a capitalized word and is short)'\n    import re\n    if not text:\n        return 0.0\n    # extract sentences\n    sentences = re.findall(r'[^.!?]+[.!?]?', text, flags=re.S)\n    if not sentences:\n        return 0.0\n    # find all quoted spans and mark sentences that contain a likely title\n    quoted_spans = []\n    for m in re.finditer(r'[\u201c\"\\'\\u2018\\u201C](.+?)[\u201d\"\\'\\u2019\\u201D]', text, flags=re.S):\n        inner = m.group(1).strip()\n        # limit length and word count to likely title length\n        words = re.findall(r'\\w+', inner)\n        if words and len(inner) <= 60 and 1 <= len(words) <= 6 and words[0][0].isupper():\n            # record span indices\n            quoted_spans.append((m.start(), m.end(), inner))\n    if not quoted_spans:\n        return 0.0\n    # assign each sentence if it contains any title-like quoted span\n    sentence_contains = 0\n    for s in sentences:\n        s_start = text.find(s)\n        if s_start == -1:\n            continue\n        s_end = s_start + len(s)\n        for (qs, qe, _) in quoted_spans:\n            if qs >= s_start and qe <= s_end:\n                sentence_contains += 1\n                break\n    return float(sentence_contains) / max(1.0, float(len(sentences)))\n\n", "def feature(text: str) -> float:\n    'Ratio of hedging/epistemic verbs (seem, appear, suggest, indicate variants) to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    hedges = {'seem','seems','seemed','appear','appears','appeared','suggest','suggests','suggested','indicate','indicates','indicated','imply','implies','implied'}\n    count = sum(1 for w in words if w in hedges)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Proportion of non-ASCII characters in the text (typographic punctuation, dashes, ellipses, accented letters)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / float(total_chars)\n\n"], "all_features": ["def feature(text: str) -> float:\n    'Contraction ratio: fraction of tokens that are contractions (contain apostrophes like \"don\\'t\", \"I\\'m\", \"it\\'s\")'\n    import re\n    if not text:\n        return 0.0\n    # count contraction patterns with apostrophe\n    contr = re.findall(r\"\\b\\w+'[a-z]{1,4}\\b\", text.lower())\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(len(contr))\n    return len(contr) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word count divided by total word count) as a measure of lexical diversity'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return unique / float(len(tokens))\n", "def feature(text: str) -> float:\n    \"Fraction of tokens that end with a trailing apostrophe (e.g., 'darlin'')\"\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    trailing_chars = (\"'\", \"\u2019\")\n    count = 0\n    for t in tokens:\n        core = t.rstrip('.,;:!?\")]}')  # strip common trailing punctuation\n        if core.endswith(trailing_chars):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    \"Smoothed ratio of contraction-like apostrophes to possessive \\\"'s\\\" occurrences\"\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    poss = len(re.findall(r\"\\b\\w+'s\\b\", lower))\n    # contractions: n't, 're, 've, 'll, 'm, 'd (exclude 's)\n    contr = len(re.findall(r\"\\b\\w+(?:n't|'re|'ve|'ll|'m|'d)\\b\", lower))\n    # smoothing to avoid division by zero\n    return float(contr + 1) / float(poss + 1)\n\n", "def feature(text: str) -> float:\n    \"Fraction of detected contractions that occur inside double-quoted spans\"\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    # contraction regex excluding possessive 's\n    contr_re = re.compile(r\"\\b\\w+(?:n't|'re|'ve|'ll|'m|'d)\\b\")\n    matches = list(contr_re.finditer(lower))\n    if not matches:\n        return 0.0\n    # compute double-quote ranges\n    quote_ranges = []\n    for m in re.finditer(r'\"', text):\n        quote_ranges.append(m.start())\n    ranges = []\n    for i in range(0, len(quote_ranges)-1, 2):\n        ranges.append((quote_ranges[i], quote_ranges[i+1]))\n    inside = 0\n    if not ranges:\n        return 0.0\n    for m in matches:\n        pos = m.start()\n        for a, b in ranges:\n            if a < pos < b:\n                inside += 1\n                break\n    return float(inside) / float(len(matches))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<=4 words) \u2014 often dialogue fragments'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence terminators but keep fragments\n    sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    short_count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 4 and len(words) > 0:\n            short_count += 1\n    return float(short_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Distinct punctuation characters divided by total punctuation count (punctuation variety)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Normalized difference between exclamation/question density inside quotes vs outside (inside - outside)'\n    import re\n    if not text:\n        return 0.0\n    # find double-quoted spans and rest\n    quote_spans = re.findall(r'\"([^\"]*)\"', text)\n    inside_chars = sum(len(s) for s in quote_spans)\n    inside_marks = sum(s.count('!') + s.count('?') for s in quote_spans)\n    outside_text = re.sub(r'\"[^\"]*\"', ' ', text)\n    outside_chars = len(outside_text)\n    outside_marks = outside_text.count('!') + outside_text.count('?')\n    # normalize densities; handle zero-length regions\n    inside_density = (inside_marks / inside_chars) if inside_chars > 0 else 0.0\n    outside_density = (outside_marks / outside_chars) if outside_chars > 0 else 0.0\n    return float(inside_density - outside_density)\n\n", "def feature(text: str) -> float:\n    'Fraction of quoted sentences that include a second-person address (you/your) \u2014 dialogue directness'\n    import re\n    if not text:\n        return 0.0\n    quoted_spans = re.findall(r'\"([^\"]*)\"', text)\n    if not quoted_spans:\n        return 0.0\n    quoted_sentences = []\n    for span in quoted_spans:\n        parts = re.split(r'[.!?]+', span)\n        for p in parts:\n            p = p.strip()\n            if p:\n                quoted_sentences.append(p.lower())\n    if not quoted_sentences:\n        return 0.0\n    direct = 0\n    for s in quoted_sentences:\n        if re.search(r'\\b(you|your|yours)\\b', s):\n            direct += 1\n    return float(direct) / float(len(quoted_sentences))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    total = len(words)\n    if total == 0:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(total)\n", "def feature(text: str) -> float:\n    'Ratio of capitalized tokens that appear NOT at the start of a sentence (possible names/proper nouns mid-sentence)'\n    import re\n    if not text:\n        return 0.0\n    tokens = []\n    for m in re.finditer(r'\\b\\w+\\b', text):\n        tokens.append((m.group(0), m.start()))\n    if not tokens:\n        return 0.0\n    mid_caps = 0\n    for tok, start in tokens:\n        if not tok[0].isupper():\n            continue\n        # find previous non-space character\n        i = start - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        if i < 0:\n            # start of text -> treat as sentence start\n            continue\n        if text[i] in '.!?':\n            # preceded by sentence terminator -> sentence start\n            continue\n        # otherwise it's a capitalized token not at sentence start\n        mid_caps += 1\n    return float(mid_caps) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of long words (length > 7) as a marker of lexical sophistication'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 7)\n    return float(long_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Variance of sentence lengths (words per sentence) \u2014 narratives often have higher variance than formal prose'\n    import re, math\n    if not text:\n        return 0.0\n    # split on sentence terminators but keep non-empty\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sentences) <= 1:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences if re.findall(r'\\w+', s)]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Density of adjacent capitalized-word bigrams (e.g., \"Jeremiah Smith\", \"American colonies\") \u2014 heuristic for named entities'\n    import re\n    if not text:\n        return 0.0\n    matches = list(re.finditer(r'\\b\\w+\\b', text))\n    if len(matches) < 2:\n        return 0.0\n    bigram_count = 0\n    for i in range(len(matches) - 1):\n        w1 = matches[i].group(0)\n        w2 = matches[i+1].group(0)\n        # require both start with uppercase letter and look like proper names (not all-caps)\n        if w1 and w2 and w1[0].isupper() and w2[0].isupper() and not w1.isupper() and not w2.isupper():\n            # ensure the first of the pair isn't clearly at a sentence start\n            start = matches[i].start()\n            j = start - 1\n            while j >= 0 and text[j].isspace():\n                j -= 1\n            if j >= 0 and text[j] in '.!?':\n                # likely sentence start -> still could be names, but deprioritize\n                continue\n            bigram_count += 1\n    total_words = len(matches)\n    return float(bigram_count) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Normalized count of emphatic punctuation (! and ellipses ...) per token'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    exclam = text.count('!')\n    ellipses = text.count('...')\n    ques = text.count('?')\n    # weight ellipses higher (stylized narrative)\n    score = float(exclam) + 1.5 * float(ellipses) + 0.5 * float(ques)\n    return score / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of all \\\"\\'s\\\" occurrences that attach to a capitalized word (proxy for proper-noun possessives like \\\"John\\'s\\\")'\n    import re\n    if not text:\n        return 0.0\n    poss = re.findall(r\"\\b([A-Za-z]\\w*)'s\\b\", text)\n    total_s = len(re.findall(r\"\\b\\w+'s\\b\", text))\n    if total_s == 0:\n        return 0.0\n    cap_count = sum(1 for w in poss if w and w[0].isupper())\n    return float(cap_count) / float(total_s)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are colloquial/slang contractions (gonna, wanna, ain\\'t, imma, gotta, lemme, etc.)'\n    import re\n    if not text:\n        return 0.0\n    colloquial = {'gonna','wanna','ain\\'t','imma','gotta','lemme','ya','y\\'all','yall','hafta','kinda','sorta'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in colloquial)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of double-quote style dialogue markers per sentence (counts \" and smart-quotes per sentence)'\n    import re\n    if not text:\n        return 0.0\n    quote_chars = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    # sentence count (fallback)\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(quote_chars) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \\\"...\\\" occurrences normalized by number of tokens'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    tokens = re.findall(r'\\b[\\w\\'-]+\\b', text)\n    if not tokens:\n        return float(ellipses)\n    return float(ellipses) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of quoted segments (double quotes only) that are short (less than 6 words) \u2014 indicates short dialogue/exclamations'\n    import re\n    if not text:\n        return 0.0\n    segments = re.findall(r'[\"\u201c\u201d](.+?)[\"\u201c\u201d]', text, flags=re.S)\n    if not segments:\n        return 0.0\n    short_count = 0\n    for s in segments:\n        words = re.findall(r'\\b[\\w\\']+\\b', s)\n        if len(words) < 6:\n            short_count += 1\n    return float(short_count) / float(len(segments))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words \u2014 high variability can indicate dialog or informal style'\n    import re, math\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    lengths = [len(re.findall(r'\\b[\\w\\']+\\b', s)) for s in sents]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n", "def feature(text: str) -> float:\n    'Diversity of apostrophe-bearing tokens: unique apostrophe tokens divided by total apostrophe tokens (higher means many different contractions/possessives)'\n    import re\n    if not text:\n        return 0.0\n    apos_tokens = re.findall(r\"\\b[\\w']*'[^\\s]+\\b\", text)\n    total = len(apos_tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(t.lower() for t in apos_tokens))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of capitalized words that appear not at sentence start (proxy for internal proper nouns like \"John\", \"Obama\", etc.)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'([.!?])', text) if s and not re.match(r'[.!?]', s)]\n    # fallback simpler split if above fails\n    if not sentences:\n        sentences = [text]\n    total_words = 0\n    internal_caps = 0\n    for sent in sentences:\n        words = re.findall(r'\\b[A-Za-z][\\w-]*\\b', sent)\n        if not words:\n            continue\n        # count words after the first in this sentence\n        for w in words[1:]:\n            total_words += 1\n            if w[0].isupper():\n                internal_caps += 1\n    if total_words == 0:\n        return 0.0\n    return float(internal_caps) / float(total_words)\n", "def feature(text: str) -> float:\n    'Ratio of tokens containing any digit (years, quantities, case-study numbers), useful for formal/business texts'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(digit_tokens) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (approx.), using split on .!? to capture narrative short sentences vs long formal sentences'\n    if not text:\n        return 0.0\n    import re\n    # split into sentences by punctuation, ignore empty fragments\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(s.split()) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts)) / float(len(word_counts))\n\n\n", "def feature(text: str) -> float:\n    'Proportion of first-person pronouns (I, me, my, we, us, our) indicating personal narrative style'\n    if not text:\n        return 0.0\n    words = [w.strip(\"()[]{}.,;:\\\"'\").lower() for w in text.split()]\n    if not words:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / float(len(words))\n\n\n", "def feature(text: str) -> float:\n    'Normalized ellipsis frequency: count of \"...\" divided by number of sentences (captures narrative trailing-off)'\n    if not text:\n        return 0.0\n    ellipsis_count = text.count('...')\n    # approximate sentence count\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)  # avoid division by zero\n    return float(ellipsis_count) / float(sentence_count)\n\n\n", "def feature(text: str) -> float:\n    'Parentheses density: fraction of characters that are parentheses (common in academic/case documents with citations)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    return float(paren_count) / float(total_chars)\n\n\n", "def feature(text: str) -> float:\n    'Ratio of modal/hedging verbs (may, might, could, would, should, must, can) to words, signalling formal/analytical tone'\n    if not text:\n        return 0.0\n    modals = {'may', 'might', 'could', 'would', 'should', 'must', 'can'}\n    words = [w.strip(\".,;:\\\"'()[]{}\").lower() for w in text.split()]\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in modals)\n    return float(count) / float(len(words))\n\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation count (0 if none)'\n    if not text:\n        return 0.0\n    punct_chars = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(punct_chars)\n    if total == 0:\n        return 0.0\n    distinct = len(set(punct_chars))\n    return float(distinct) / float(total)\n\n\n", "def feature(text: str) -> float:\n    'Unique word ratio: number of unique words divided by total words (low values indicate repetition, might reveal certain generation patterns)'\n    if not text:\n        return 0.0\n    words = [w.strip(\".,;:\\\"'()[]{}\").lower() for w in text.split() if w.strip(\".,;:\\\"'()[]{}\")]\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n", "def feature(text: str) -> float:\n    'Density of em-dash/long-dash usage per word (\u2014, \u2013, or --), normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    word_count = max(1, len(re.findall(r'\\w+', text)))\n    dash_count = text.count('\u2014') + text.count('\u2013') + text.count('--')\n    return dash_count / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, mine, we, us, our, ours, I\\'m, I\\'ve) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'\u2019]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours',\"i'm\",\"i\u2019m\",\"i've\",\"i\u2019ve\",\"i'd\",\"i\u2019d\"}\n    count = sum(1 for t in tokens if t in first_person)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<=4 words) \u2014 often dialogue or punchy narration'\n    import re\n    if not text:\n        return 0.0\n    parts = [p.strip() for p in re.split(r'(?<=[\\.\\!\\?])\\s+|\\n+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    short = 0\n    for p in parts:\n        words = re.findall(r'\\w+', p)\n        if len(words) <= 4:\n            short += 1\n    return short / float(len(parts))\n\n", "def feature(text: str) -> float:\n    'Binary-ish score: detects title-like first non-empty line (short, title-case words) to indicate headings'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    first = lines[0].strip()\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", first)\n    if not (1 <= len(words) <= 6):\n        return 0.0\n    # Check many words start with uppercase letter\n    upper_count = sum(1 for w in words if w[0].isupper())\n    if upper_count >= max(1, int(0.6 * len(words))):\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") and trailing dots per word \u2014 indicates trailing thoughts or truncation'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.\\.\\.+', text))\n    word_count = max(1, len(re.findall(r'\\w+', text)))\n    return ellipses / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Average number of dialog turns per sentence: lines starting with quotes or dashes normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    # Count sentences\n    sentences = [s for s in re.split(r'(?<=[\\.\\!\\?])\\s+|\\n+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    turns = 0\n    for s in sentences:\n        if re.match(r'^\\s*[\"\u201c\u201d\\']', s) or re.match(r'^\\s*[-\\\u2013\\\u2014]\\s*\\w', s):\n            turns += 1\n        # also lines that have a short quoted span inside\n        elif re.search(r'[\"\u201c\u201d].+[\"\u201c\u201d]', s):\n            turns += 1\n    return turns / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (comma density indicates clause complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[\\.\\!\\?])\\s+|\\n+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_counts = [s.count(',') for s in sentences]\n    return sum(comma_counts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences containing parentheses or brackets (parenthetical asides frequency)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[\\.\\!\\?])\\s+|\\n+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    paren_count = sum(1 for s in sentences if '(' in s or ')' in s or '[' in s or ']' in s or '{' in s or '}' in s)\n    return paren_count / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter (measures sentence-start capitalization errors)'\n    import re\n    s = text.strip()\n    if not s:\n        return 0.0\n    # Split into sentence-like chunks using punctuation or newlines as boundaries\n    sentences = re.split(r'(?<=[.!?])\\s+|\\n+', s)\n    if not sentences:\n        return 0.0\n    starts_lower = 0\n    total = 0\n    for sent in sentences:\n        if not sent.strip():\n            continue\n        m = re.search(r'[A-Za-z]', sent)\n        if not m:\n            total += 1\n            continue\n        total += 1\n        first_alpha = sent[m.start()]\n        if first_alpha.islower():\n            starts_lower += 1\n    if total == 0:\n        return 0.0\n    return float(starts_lower) / float(total)\n\n", "def feature(text: str) -> float:\n    'Density of mid-sentence capitalized words (proper-noun-like tokens not at sentence start)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence-like chunks\n    sentences = re.split(r'(?<=[.!?])\\s+|\\n+', text)\n    total_words = 0\n    mid_capitals = 0\n    for sent in sentences:\n        ws = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", sent)\n        if not ws:\n            continue\n        for i, w in enumerate(ws):\n            total_words += 1\n            if i > 0 and w and w[0].isupper() and w.lower() != 'i':\n                mid_capitals += 1\n    if total_words == 0:\n        return 0.0\n    return float(mid_capitals) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Punctuation characters per word (overall punctuation density normalized by word count)'\n    import re\n    if not text:\n        return 0.0\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words = re.findall(r'\\w+', text)\n    return float(punct_count) / float(len(words) + 1)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (clause-heavy vs. terse style)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # Count sentences using sentence-ending punctuation; if none, treat whole text as one sentence\n    sentence_end_count = len(re.findall(r'[.!?]', text))\n    sentences = sentence_end_count if sentence_end_count > 0 else 1\n    return float(comma_count) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Average word length (characters) \u2014 longer average can signal more formal or descriptive prose'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    total_len = sum(len(w) for w in words)\n    return float(total_len) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (lexical diversity): unique word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Number of distinct punctuation characters used (variety of punctuation marks)'\n    if not text:\n        return 0.0\n    puncts = {c for c in text if not c.isalnum() and not c.isspace()}\n    return float(len(puncts))\n", "def feature(text: str) -> float:\n    'Frequency of ellipses (\"...\") normalized by number of words'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(ellipses) / float(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is lowercase (indicative of informal or loose punctuation)'\n    import re\n    if not text:\n        return 0.0\n    # Split on sentence terminators, keep non-empty\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lower_count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m:\n            ch = m.group(0)\n            if ch.islower():\n                lower_count += 1\n    return float(lower_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Density of hyphen and dash characters (-, \u2014, \u2013) per word, capturing compound/descriptive style'\n    import re\n    if not text:\n        return 0.0\n    hyphen_count = text.count('-') + text.count('\\u2014') + text.count('\\u2013')\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(hyphen_count) / float(words)\n\n", "def feature(text: str) -> float:\n    'Proportion of short words (length <= 3) among all words \u2014 measures simplicity or contraction-heavy style'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    short = sum(1 for w in words if len(w) <= 3)\n    return float(short) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of capitalized words appearing mid-sentence (likely proper nouns) to total words'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    words_total = 0\n    mid_caps = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if not words:\n            continue\n        words_total += len(words)\n        # count capitalized words excluding the first word of the sentence\n        for w in words[1:]:\n            if w and w[0].isupper():\n                mid_caps += 1\n    if words_total == 0:\n        return 0.0\n    return float(mid_caps) / float(words_total)\n\n", "def feature(text: str) -> float:\n    'Fraction of repeated punctuation sequences (e.g., \"!!\", \"??\", \"...\") per word'\n    import re\n    if not text:\n        return 0.0\n    repeats = re.findall(r'([^\\w\\s])\\1{1,}', text)\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(len(repeats)) / float(words)\n\n", "def feature(text: str) -> float:\n    'Smoothed ratio of very short sentences (<=3 words) to very long sentences (>=20 words)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    long = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lw = len(words)\n        if lw <= 3:\n            short += 1\n        if lw >= 20:\n            long += 1\n    # Smoothed ratio to avoid division by zero\n    return float(short + 0.5) / float(long + 0.5)\n\n", "def feature(text: str) -> float:\n    'Proportion of text lines that begin with a dialogue marker (quote, dash, or quote-like characters)'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    count = 0\n    for l in lines:\n        stripped = l.lstrip()\n        if not stripped:\n            continue\n        first = stripped[0]\n        if first in {'\"', \"'\", '\u201c', '\u201d', '\u2014', '-', '\u2013'}:\n            count += 1\n    return float(count) / float(len(lines))\n", "def feature(text: str) -> float:\n    'Density of newline characters (newlines per character) to detect line-oriented formats'\n    if not text:\n        return 0.0\n    newlines = text.count('\\n')\n    return float(newlines) / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Average line length in characters (short average lines indicate poetry/lists/poster style)'\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    total_len = sum(len(l) for l in lines)\n    return float(total_len) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences whose first alphabetic character is lowercase (informal/lax capitalization)'\n    import re\n    if not text:\n        return 0.0\n    # Split on sentence terminators or newlines to get candidates\n    parts = re.split(r'(?<=[.!?])\\s+|\\n+', text)\n    starts = 0\n    total = 0\n    for p in parts:\n        s = p.strip()\n        if not s:\n            continue\n        m = re.search(r'[A-Za-z]', s)\n        if not m:\n            continue\n        total += 1\n        if m.group(0).islower():\n            starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts) / float(total)\n\n", "def feature(text: str) -> float:\n    'Ellipsis frequency per 100 words (counts of \"...\" \u2014 often used in informal/human texts)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    words = len(re.findall(r'\\b\\w+\\b', text))\n    return float(ellipses) * 100.0 / float(max(1, words))\n\n", "def feature(text: str) -> float:\n    'Colon frequency normalized by word count (lists, inventory entries, and labels often use colons)'\n    import re\n    if not text:\n        return 0.0\n    colons = text.count(':')\n    words = len(re.findall(r'\\b\\w+\\b', text))\n    return float(colons) / float(max(1, words))\n\n", "def feature(text: str) -> float:\n    'Ratio of standalone lowercase \"i\" tokens to total words (informal casing of pronoun \"I\")'\n    import re\n    if not text:\n        return 0.0\n    lowercase_i = len(re.findall(r'\\bi\\b', text))\n    words = len(re.findall(r'\\b\\w+\\b', text))\n    return float(lowercase_i) / float(max(1, words))\n\n", "def feature(text: str) -> float:\n    'Proportion of non-empty lines that are majority-uppercase (useful to detect posters and headings)'\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    caps_lines = 0\n    counted = 0\n    for line in lines:\n        letters = [c for c in line if c.isalpha()]\n        if not letters:\n            continue\n        counted += 1\n        up = sum(1 for c in letters if c.isupper())\n        if up / float(len(letters)) >= 0.6:\n            caps_lines += 1\n    if counted == 0:\n        return 0.0\n    return float(caps_lines) / float(counted)\n\n", "def feature(text: str) -> float:\n    'Punctuation variety ratio: number of distinct punctuation characters divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    total = len(puncts)\n    return float(distinct) / float(total)\n", "def feature(text: str) -> float:\n    'Ratio of titlecase tokens that appear NOT at the start of sentences (proxy for proper-name density)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence fragments then tokens to avoid counting sentence-initial capitals\n    sentences = re.split(r'[.!?]+', text)\n    total_tokens = 0\n    title_non_initial = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        parts = s.split()\n        if len(parts) <= 1:\n            continue\n        # skip the first token as sentence-initial\n        for tok in parts[1:]:\n            total_tokens += 1\n            # consider tokens with internal alphabetic characters and titlecase form\n            if tok.istitle() and any(c.isalpha() for c in tok):\n                title_non_initial += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(title_non_initial) / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Density of quoted material: number of quotation marks (straight or curly) per word'\n    import re\n    if not text:\n        return 0.0\n    quote_chars = text.count('\"') + text.count('\u201c') + text.count('\u201d') + text.count('\u2018') + text.count('\u2019')\n    words = len(re.findall(r'\\w+', text))\n    if words == 0:\n        return 0.0\n    return float(quote_chars) / float(words)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending with -ly (heuristic adverb density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z'-]+\", text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n    return float(ly_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending with -ed (simple past/participial marker density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z'-]+\", text.lower())\n    if not tokens:\n        return 0.0\n    ed_count = sum(1 for t in tokens if t.endswith('ed') and len(t) > 3)\n    return float(ed_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique lowercase word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"[A-Za-z']+\", text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating/subordinating conjunction (and, but, so, then, because, when, however)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    conj = {'and', 'but', 'so', 'then', 'because', 'when', 'however', 'although', 'though', 'while'}\n    starts = 0\n    for s in sentences:\n        words = re.findall(r\"[A-Za-z']+\", s)\n        if not words:\n            continue\n        first = words[0].lower()\n        if first in conj:\n            starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ratio of ellipses (\"...\") occurrences to sentence count'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences == 0:\n        return float(ellipses)\n    return float(ellipses) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Maximum unigram frequency divided by total tokens (measures repetition / focal word dominance)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"[A-Za-z']+\", text)]\n    if not tokens:\n        return 0.0\n    freqs = Counter(tokens)\n    maxfreq = max(freqs.values())\n    return float(maxfreq) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Average sentence length in words (sentences split on .!?), returns 0 for no sentences'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return sum(word_counts) / len(word_counts)\n\n", "def feature(text: str) -> float:\n    'Normalized sentence length variability: stddev(sentence word counts) / mean (0 if not defined)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences if re.findall(r'\\w+', s)]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters present in the text'\n    if not text:\n        return 0.0\n    puncts = {c for c in text if not c.isalnum() and not c.isspace()}\n    return float(len(puncts))\n\n", "def feature(text: str) -> float:\n    'List marker ratio: fraction of non-empty lines that start with a bullet, dash, or numbered marker'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    markers = 0\n    for ln in lines:\n        if re.match(r'^\\s*([-*\u2022]|(\\d+[\\.\\)]))\\s+', ln):\n            markers += 1\n    return markers / len(lines)\n\n", "def feature(text: str) -> float:\n    'Function-word ratio: fraction of tokens that are common English function words (small stopword set)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','of','to','a','in','is','it','that','for','on','as','are','with','was','by','be','this','an','or','from','at','which','but','have','has','not','they','their','its','we','can','may','such','will'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stop)\n    return count / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Bigram repetition rate: fraction of bigrams that are repeats (1 - unique_bigrams/total_bigrams)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    total = len(bigrams)\n    unique = len(set(bigrams))\n    if total == 0:\n        return 0.0\n    return 1.0 - (unique / total)\n\n", "def feature(text: str) -> float:\n    'Heading-style fraction: fraction of non-empty lines that are all-uppercase or end with a colon (likely headers)'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_header(ln):\n        s = ln.strip()\n        # treat as header if ends with ':' or is uppercase with at least one letter\n        return s.endswith(':') or (any(ch.isalpha() for ch in s) and s == s.upper())\n    headers = sum(1 for ln in lines if is_header(ln))\n    return headers / len(lines)\n", "def feature(text: str) -> float:\n    'Proportion of sentence-like segments that begin with a quotation mark (dialogue starts)'\n    import re\n    if not text:\n        return 0.0\n    # Consider sentence starts (after .!? or start of text) and line starts as potential dialogue starts\n    sentence_starts = re.findall(r'(?:^|[\\.!\\?]\\s+)[\\'\"]?', text)\n    starts_total = max(1, len(sentence_starts))\n    # Count occurrences where a sentence or line start is immediately a double quote or single quote\n    dialogue_starts = len(re.findall(r'(?:^|[\\.!\\?]\\s+)[\\'\"]\\s*\\w', text))\n    return float(dialogue_starts) / float(starts_total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours', \"i'm\", \"i'd\", \"i've\", \"we're\", \"we've\", \"we'd\"}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of titlecase words not appearing immediately after sentence-ending punctuation (proxy for in-sentence proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[A-Za-z][A-Za-z]+\\b', text)\n    total_words = max(1, len(re.findall(r'\\b\\w+\\b', text)))\n    count = 0\n    for m in re.finditer(r'\\b[A-Z][a-z]+\\b', text):\n        start = m.start()\n        # if at very start, consider it sentence start -> skip\n        if start == 0:\n            continue\n        # find previous non-space character\n        prev_idx = start - 1\n        while prev_idx >= 0 and text[prev_idx].isspace():\n            prev_idx -= 1\n        prev_char = text[prev_idx] if prev_idx >= 0 else ''\n        if prev_char not in '.!?':\n            count += 1\n    return float(count) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Shannon entropy estimate of punctuation character distribution (higher => more varied punctuation)'\n    import math\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    from collections import Counter\n    ctr = Counter(puncts)\n    total = float(len(puncts))\n    entropy = 0.0\n    for v in ctr.values():\n        p = v / total\n        entropy -= p * math.log(p, 2)\n    return float(entropy)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique tokens / total tokens) as a measure of lexical diversity'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of quotation occurrences that are followed by a lowercase letter (indicating mid-sentence quotations/continuations)'\n    import re\n    if not text:\n        return 0.0\n    quote_follow_lower = len(re.findall(r'\"\\s*[a-z]', text))\n    total_quotes = text.count('\"')\n    denom = max(1, total_quotes)\n    return float(quote_follow_lower) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Relative skew of sentence length: absolute(mean - median) / mean (captures asymmetry of sentence-length distribution)'\n    import re\n    if not text:\n        return 0.0\n    sents = re.split(r'[.!?]+', text)\n    # compute sentence lengths in words, ignoring empty fragments\n    lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in sents if len(re.findall(r'\\b\\w+\\b', s)) > 0]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    lengths_sorted = sorted(lengths)\n    n = len(lengths_sorted)\n    if n % 2 == 1:\n        median = lengths_sorted[n//2]\n    else:\n        median = (lengths_sorted[n//2 - 1] + lengths_sorted[n//2]) / 2.0\n    if mean == 0:\n        return 0.0\n    return abs(mean - median) / float(mean)\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens ending in \"ing\" (gerund/continuous form density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing'))\n    return float(ing_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count'\n    import re\n    words = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not words:\n        return 0.0\n    uniq = len(set(words))\n    return uniq / len(words)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of words that appear only once in the text'\n    import re\n    words = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not words:\n        return 0.0\n    from collections import Counter\n    cnt = Counter(words)\n    hapax = sum(1 for w, c in cnt.items() if c == 1)\n    return hapax / len(words)\n\n", "def feature(text: str) -> float:\n    'Density of long punctuation sequences like ellipses or double-dashes (count per character)'\n    if not text:\n        return 0.0\n    long_seqs = text.count('...') + text.count('--') + text.count('\u2014')\n    return long_seqs / max(1, len(text))\n\n", "def feature(text: str) -> float:\n    'Ratio of apostrophes used as possessive (\\'s or \\'S) among all apostrophes'\n    if not text:\n        return 0.0\n    total_apos = text.count(\"'\")\n    if total_apos == 0:\n        return 0.0\n    poss = text.count(\"'s\") + text.count(\"'S\")\n    return poss / total_apos\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that end with an ellipsis or with multiple terminal dots (incomplete/trailing thoughts)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.findall(r'[^.!?]*[.!?]*', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    ellip_end = 0\n    total = 0\n    for s in sentences:\n        s_strip = s.strip()\n        if not s_strip:\n            continue\n        total += 1\n        if s_strip.endswith('...') or re.search(r'\\.{2,}\\Z', s_strip):\n            ellip_end += 1\n    if total == 0:\n        return 0.0\n    return ellip_end / total\n\n", "def feature(text: str) -> float:\n    'Shannon entropy (base 2) over a small set of common function words to capture distributional regularity'\n    import re, math\n    if not text:\n        return 0.0\n    func = ['the','a','an','and','or','but','of','in','on','to','was','were','is','it','that','as','for','with','by']\n    words = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not words:\n        return 0.0\n    from collections import Counter\n    cnt = Counter(words)\n    total = sum(cnt[w] for w in func)\n    if total == 0:\n        return 0.0\n    entropy = 0.0\n    for w in func:\n        c = cnt.get(w, 0)\n        if c > 0:\n            p = c / total\n            entropy -= p * math.log2(p)\n    return entropy\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like simple past-tense verbs (end with \"ed\"), as a loose proxy for narrative past tense usage'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count_ed = sum(1 for t in tokens if len(t) > 3 and t.endswith('ed'))\n    return count_ed / len(tokens)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a run of the same character three or more times (e.g., sooo, hellooo) \u2014 a sign of informal emphasis or typos'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1', re.IGNORECASE)\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total word tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of ellipses (\"...\") per sentence (ellipses count divided by sentence count) \u2014 indicates trailing thoughts or informal pacing'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    # approximate sentence count by punctuation\n    sentence_end_count = text.count('.') + text.count('!') + text.count('?')\n    # if ellipses are present, they contribute to sentence_end_count multiple times; still use sentence_end_count as denominator but ensure non-zero\n    denom = sentence_end_count if sentence_end_count > 0 else 1\n    return float(ellipses) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences whose first alphanumeric character is lowercase (sentence-initial lowercase), capturing informal typos or stylistic choices'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence-like pieces\n    pieces = re.split(r'(?<=[.!?])\\s+', text)\n    if not pieces:\n        return 0.0\n    lower_start = 0\n    valid_sentences = 0\n    for p in pieces:\n        s = p.strip()\n        if not s:\n            continue\n        # find first alphanumeric character\n        m = re.search(r'[A-Za-z0-9]', s)\n        if not m:\n            continue\n        valid_sentences += 1\n        ch = s[m.start()]\n        if ch.isalpha() and ch.islower():\n            lower_start += 1\n    return float(lower_start) / float(valid_sentences) if valid_sentences > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are unusually long (>12 characters) \u2014 may indicate complex vocabulary or concatenated tokens typical in some generated text'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) > 12)\n    return float(long_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of adverbs (tokens ending with \"ly\") as a fraction of total tokens \u2014 stylistic marker (formal vs. conversational)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n    return float(ly_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of lines that look like short section headings (single or few words, titlecase)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    heading_like = 0\n    for ln in lines:\n        words = re.findall(r'\\w+[-\\w]*', ln)\n        if 1 <= len(words) <= 5:\n            # check many tokens are Titlecase or ALLCAPS\n            title_tokens = sum(1 for w in words if w[0].isupper())\n            if title_tokens >= 1:\n                # avoid counting normal sentence lines with trailing punctuation\n                if not ln.endswith(('.', '?', '!', ':')):\n                    heading_like += 1\n    return heading_like / len(lines)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence starts that begin with common discourse/connective markers (e.g., however, moreover, therefore)'\n    import re\n    if not text:\n        return 0.0\n    markers = {'however', 'moreover', 'therefore', 'furthermore', 'additionally', 'also', 'thus', 'consequently',\n               'meanwhile', 'notably', 'importantly', 'particularly', 'in addition', 'on the other hand', 'finally',\n               'firstly', 'secondly', 'in conclusion', 'in the modern'}\n    # find sentence starts\n    starts = []\n    parts = re.split(r'([.!?])', text)  # keep separators\n    buffer = ''\n    for i in range(0, len(parts), 2):\n        s = parts[i].strip()\n        if s:\n            starts.append(s)\n    if not starts:\n        return 0.0\n    count = 0\n    for s in starts:\n        # take first up to 4 tokens\n        first = ' '.join(re.findall(r\"\\w+'?\\w*|-+\", s)[:4]).lower()\n        for m in markers:\n            if first.startswith(m):\n                count += 1\n                break\n    return count / len(starts)\n\n", "def feature(text: str) -> float:\n    'Ratio of common English stopwords to all word tokens (measure of function-word density)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at',\n        'this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there',\n        'their','what','so','up','out','if','about','who','get','which','go','me','when','make','can','like','time',\n        'no','just','him','know','take','people','into','year','your','good','some','could','them','see','other','than',\n        'then','now','look','only','come','its','over','think','also','back','after','use','two','how','our','work',\n        'first','well','way','even','new','want','because','any','these','give','day','most','us'\n    }\n    words = re.findall(r\"\\w+['-]?\\w*\", text.lower())\n    if not words:\n        return 0.0\n    sw_count = sum(1 for w in words if w in stopwords)\n    return sw_count / len(words)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r\"\\w+['-]?\\w*\", text)]\n    if not words:\n        return 0.0\n    uniq = len(set(words))\n    return uniq / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are internal capitalized words (capitalized not being the first token of a sentence) as a proxy for named entities'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences, then look at tokens after the first token of each sentence\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    total_tokens = 0\n    cap_tokens = 0\n    for s in sentences:\n        tokens = re.findall(r'\\b[\\w\\'-]+\\b', s)\n        if not tokens:\n            continue\n        for tok in tokens[1:]:\n            total_tokens += 1\n            if tok[0].isupper():\n                cap_tokens += 1\n    if total_tokens == 0:\n        return 0.0\n    return cap_tokens / total_tokens\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that contain a hyphen (hyphenation frequency)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[\\w-]+\\b', text)\n    if not tokens:\n        return 0.0\n    hyphenated = sum(1 for t in tokens if '-' in t and any(ch.isalpha() for ch in t))\n    return hyphenated / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proxy for passive/nominalized constructions: normalized count of auxiliary+been/was/were occurrences per sentence'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    patterns = [' was ', ' were ', ' has been ', ' have been ', ' is being ', ' been ']\n    count = 0\n    for p in patterns:\n        count += lower.count(p)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    denom = max(1, len(sentences))\n    return count / denom\n\n", "def feature(text: str) -> float:\n    'Average number of characters before the first comma in each sentence (measures pre-comma clause length/complexity)'\n    import re, statistics\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = []\n    for s in sentences:\n        idx = s.find(',')\n        if idx == -1:\n            lengths.append(len(s))\n        else:\n            lengths.append(max(0, idx))\n    try:\n        return statistics.mean(lengths)\n    except Exception:\n        return 0.0\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that are capitalized internally (possible named entities), excluding the very first token'\n    if not text:\n        return 0.0\n    words = text.split()\n    if len(words) <= 1:\n        return 0.0\n    cap_count = 0\n    total = 0\n    for i, w in enumerate(words):\n        # strip surrounding punctuation\n        core = w.strip('()[]{}.,;:\"\\'\u201c\u201d\u2018\u2019`')\n        if not core:\n            continue\n        total += 1\n        if i == 0:\n            continue\n        if core[0].isupper() and not core.isupper():  # avoid all-caps acronyms\n            cap_count += 1\n    return cap_count / total if total > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Normalized frequency of \"by <CapitalizedName>\" patterns (e.g., book/author attributions)'\n    import re\n    if not text:\n        return 0.0\n    tokens = len(re.findall(r'\\w+', text))\n    if tokens == 0:\n        return 0.0\n    matches = re.findall(r'\\bby\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?\\b', text)\n    return len(matches) / tokens\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are second-person pronouns (you, your, yours) indicating promotional/personal tone'\n    if not text:\n        return 0.0\n    words = [w.strip('.,;:\"\\'()[]') .lower() for w in text.split()]\n    if not words:\n        return 0.0\n    second = sum(1 for w in words if w in {'you', 'your', 'yours', 'yourselves', 'yourself'})\n    return second / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens matching a small marketing adjective lexicon (perfect, stylish, durable, premium, high-end, ideal, versatile)'\n    if not text:\n        return 0.0\n    lex = {'perfect','stylish','durable','premium','ideal','versatile','high-end','highend','luxury','affordable','sleek'}\n    words = [w.strip('.,;:\"\\'()[]').lower() for w in text.split()]\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if w in lex:\n            count += 1\n        else:\n            # also check for substring matches for hyphenated forms\n            for key in lex:\n                if key in w and len(key) >= 4:\n                    count += 1\n                    break\n    return count / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen (compound words, e.g., post-World, high-end)'\n    if not text:\n        return 0.0\n    tokens = [t for t in text.split() if t.strip()]\n    if not tokens:\n        return 0.0\n    hy = sum(1 for t in tokens if '-' in t)\n    return hy / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas as proxy for clause complexity); 0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    # approximate sentence splitting by punctuation\n    sentences = re.split(r'[.!?]+', text)\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_counts = [s.count(',') for s in sentences]\n    return sum(comma_counts) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of short all-uppercase tokens (length 1-4) that may indicate Roman numerals or acronyms (e.g., II, UK)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    short_upper = 0\n    for t in tokens:\n        core = t.strip('.,;:\"\\'()[]')\n        if 1 <= len(core) <= 4 and core.isalpha() and core.upper() == core:\n            short_upper += 1\n    return short_upper / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), robust to missing sentence punctuation'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_words = len(words)\n    # Count non-empty sentence-like segments\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sentences = len(sentences) if sentences else 1\n    return total_words / float(num_sentences)\n\n", "def feature(text: str) -> float:\n    'Proportion of words that look like adverbs (ending with \"ly\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    advs = sum(1 for w in words if len(w) > 2 and w.endswith('ly'))\n    return advs / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Past-tense-looking word ratio: fraction of words ending in \"ed\" (simple proxy for past-tense narration)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    past = sum(1 for w in words if len(w) > 2 and w.endswith('ed'))\n    return past / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Quote density: number of double-quote characters per sentence (captures dialogue or quoted citations)'\n    import re\n    if not text:\n        return 0.0\n    quote_count = text.count('\"')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sentences = len(sentences) if sentences else 1\n    return quote_count / float(num_sentences)\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation characters'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return distinct / float(total)\n\n", "def feature(text: str) -> float:\n    'Sensory word density: fraction of words that match a small lexicon of sensory/descriptive terms'\n    import re\n    if not text:\n        return 0.0\n    sensory = {'warm','cold','bright','dark','soft','loud','quiet','sweet','bitter','faint','bleak','sharp','rough','smooth','fragrant','sour','hot','cool','heavy','light','wet','dry'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in sensory)\n    return count / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Punctuation-to-word ratio: total punctuation characters divided by word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    num_words = len(words)\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if num_words == 0:\n        return float(punct_count)\n    return punct_count / float(num_words)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are all-uppercase acronyms (2-6 letters), e.g., ESRF, USA'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[A-Z]{2,6}\\b', text)\n    total_tokens = max(1, len(re.findall(r'\\w+', text)))\n    return float(len(tokens)) / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Density of common Latin/abbreviation phrases like \"e.g.\", \"i.e.\", \"et al.\", \"cf.\" (counts per word)'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r'\\b(?:e\\.g\\.|i\\.e\\.|et al\\.?|cf\\.)\\b', text, flags=re.IGNORECASE)\n    total_words = max(1, len(re.findall(r'\\w+', text)))\n    return float(len(matches)) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice indicator: fraction of auxiliary verbs followed by a past-participle-looking token (ends with ed/en)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    aux = {'was','were','is','are','been','be','being','has','have','had','hasn','haven','hadn'}\n    count = 0\n    checks = 0\n    for i, w in enumerate(words[:-1]):\n        lw = w.lower()\n        # handle simple auxiliaries (ignore contractions)\n        if lw in aux or lw in ('has','have','had','was','were','is','are','been','being'):\n            checks += 1\n            nextw = words[i+1].lower()\n            if nextw.endswith(('ed','en')) or nextw in ('born','seen','made','taken'):\n                count += 1\n    return float(count) / float(checks) if checks > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are square brackets or braces (citation/annotation bracket density)'\n    if not text:\n        return 0.0\n    total_chars = max(1, len(text))\n    bracket_chars = text.count('[') + text.count(']') + text.count('{') + text.count('}')\n    return float(bracket_chars) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Lexical diversity: ratio of unique word tokens to total word tokens'\n    import re\n    words = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for syntactic complexity and clause density)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'[.!?]+', text)\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        # treat whole text as one sentence\n        return float(text.count(','))\n    comma_counts = [s.count(',') for s in sentences]\n    return float(sum(comma_counts)) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of mid-sentence words that are title-cased (Uppercase initial) \u2014 indicates proper nouns and named entities'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences to avoid counting sentence-initial capitalized words\n    sentences = re.split(r'[.!?]+\\s*', text)\n    total_mid = 0\n    cap_mid = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 1:\n            continue\n        # skip first word of sentence\n        for w in words[1:]:\n            total_mid += 1\n            if w[0].isupper():\n                cap_mid += 1\n    return float(cap_mid) / float(total_mid) if total_mid > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Density of hyphenated tokens (tokens containing -), as fraction of total word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyph = sum(1 for t in tokens if '-' in t and any(ch.isalpha() for ch in t))\n    return float(hyph) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Ratio of non-ASCII characters to total characters (captures accented letters like \u00f1)'\n    try:\n        if not text:\n            return 0.0\n        total = len(text)\n        non_ascii = sum(1 for c in text if ord(c) > 127)\n        return float(non_ascii) / float(total)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that lie inside double-quoted spans (dialogue density)'\n    import re\n    try:\n        if not text:\n            return 0.0\n        matches = re.findall(r'\"([^\"]+)\"', text)\n        inside_len = sum(len(m) for m in matches)\n        return float(inside_len) / float(len(text))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, mine, we, us, our, ours) to total tokens'\n    import re\n    try:\n        tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n        if not tokens:\n            return 0.0\n        first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n        count = sum(1 for t in tokens if t in first_person)\n        return float(count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Number of ellipses (\"...\") per sentence (ellipses density normalized by sentence count)'\n    try:\n        if not text:\n            return 0.0\n        ellipses = text.count('...')\n        sentence_count = text.count('.') + text.count('!') + text.count('?')\n        if sentence_count <= 0:\n            # treat the whole text as one sentence if none found\n            sentence_count = 1\n        return float(ellipses) / float(sentence_count)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of word lengths; returns 0.0 if insufficient data'\n    import re, math\n    try:\n        words = re.findall(r\"\\b\\w+\\b\", text)\n        if not words:\n            return 0.0\n        lengths = [len(w) for w in words]\n        mean = sum(lengths) / len(lengths)\n        if mean == 0:\n            return 0.0\n        variance = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n        std = math.sqrt(variance)\n        return float(std) / float(mean)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    \"Fraction of tokens that look like simple past-tense verbs (tokens ending with 'ed')\"\n    import re\n    try:\n        tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n        if not tokens:\n            return 0.0\n        ed_count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ed'))\n        return float(ed_count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    \"Proportion of adverb-like tokens ending with 'ly' (naive adverb density)\"\n    import re\n    try:\n        tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n        if not tokens:\n            return 0.0\n        ly_count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ly'))\n        return float(ly_count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Lexical concentration: frequency of the most common token divided by total tokens (high -> repetitive)'\n    import re, collections\n    try:\n        tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n        if not tokens:\n            return 0.0\n        freq = collections.Counter(tokens)\n        most_common = freq.most_common(1)[0][1]\n        return float(most_common) / float(len(tokens))\n    except Exception:\n        return 0.0\n", "def feature(text: str) -> float:\n    'Density of explicit line breaks: number of newline characters normalized by text length'\n    if not text:\n        return 0.0\n    # Normalize by length to avoid division by zero\n    return float(text.count('\\n')) / float(len(text) + 1)\n\n", "def feature(text: str) -> float:\n    'Median sentence length in words (returns median word count per sentence)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[^.!?\u2026\\n]+(?:[.!?\u2026]+|$)', text, flags=re.M)\n    lens = []\n    for s in sentences:\n        words = re.findall(r\"\\w[\\w']*\", s)\n        if words:\n            lens.append(len(words))\n    if not lens:\n        return 0.0\n    lens.sort()\n    n = len(lens)\n    if n % 2 == 1:\n        return float(lens[n // 2])\n    else:\n        return float((lens[n//2 - 1] + lens[n//2]) / 2.0)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are first-person pronouns (I, me, my, mine, we, our, us)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w[\\w']*\", text)\n    if not tokens:\n        return 0.0\n    pronouns = re.findall(r'\\b(?:i|me|my|mine|we|our|us)\\b', text, flags=re.I)\n    return float(len(pronouns)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of adjacent sentences that start with the same first word (repetition of sentence-start)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[^.!?\u2026\\n]+(?:[.!?\u2026]+|$)', text, flags=re.M)\n    first_words = []\n    for s in sentences:\n        words = re.findall(r\"\\w[\\w']*\", s)\n        if words:\n            first_words.append(words[0].lower())\n    if len(first_words) < 2:\n        return 0.0\n    same_adj = sum(1 for i in range(1, len(first_words)) if first_words[i] == first_words[i-1])\n    return float(same_adj) / float(len(first_words) - 1)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that end with an ellipsis (...) or unicode ellipsis (\u2026)'\n    import re\n    if not text:\n        return 0.0\n    # Find sentence-like chunks including trailing punctuation\n    sentences = re.findall(r'[^.!?\u2026\\n]+(?:[.!?\u2026]+|$)', text, flags=re.M)\n    if not sentences:\n        return 0.0\n    ell_count = 0\n    for s in sentences:\n        s_strip = s.rstrip()\n        if s_strip.endswith('...') or s_strip.endswith('\u2026'):\n            ell_count += 1\n    return float(ell_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ratio of sentences that consist of exactly one word'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[^.!?\u2026\\n]+(?:[.!?\u2026]+|$)', text, flags=re.M)\n    if not sentences:\n        return 0.0\n    single = 0\n    total = 0\n    for s in sentences:\n        words = re.findall(r\"\\w[\\w']*\", s)\n        if words:\n            total += 1\n            if len(words) == 1:\n                single += 1\n    if total == 0:\n        return 0.0\n    return float(single) / float(total)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are third-person plural pronouns (they, them, their, theirs)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w[\\w']*\", text)\n    if not tokens:\n        return 0.0\n    matches = re.findall(r'\\b(?:they|them|their|theirs)\\b', text, flags=re.I)\n    return float(len(matches)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of internally capitalized words (capitalized words not at sentence start) indicating proper-noun-like usage'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[^.!?\u2026\\n]+(?:[.!?\u2026]+|$)', text, flags=re.M)\n    internal_caps = 0\n    internal_count = 0\n    for s in sentences:\n        words = re.findall(r\"\\w[\\w']*\", s)\n        for i, w in enumerate(words):\n            if i == 0:\n                # sentence-initial capital words are not counted as internal\n                continue\n            internal_count += 1\n            # consider a capitalized token with at least one lowercase letter following\n            if len(w) > 0 and w[0].isupper():\n                # ensure it's not all-caps acronyms\n                if any(ch.islower() for ch in w[1:]):\n                    internal_caps += 1\n    if internal_count == 0:\n        return 0.0\n    return float(internal_caps) / float(internal_count)\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per sentence (ellipses count divided by number of sentences)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    ellipses = text.count('...')\n    # count sentences via terminal punctuation clusters; fallback to 1 if none\n    sentence_matches = re.findall(r'[.!?]+', text)\n    sentence_count = max(1, len(sentence_matches))\n    return ellipses / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of words ending in \"-ly\" (approximate adverb density)'\n    import re\n    words = re.findall(r\"\\b\\w+\\b\", (text or '').lower())\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if len(w) >= 3 and w.endswith('ly'))\n    return ly_count / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of capitalized words that appear mid-sentence (proxy for proper-noun / named-entity density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # Split into sentences and then words to identify first-word capitalization\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    total_words = 0\n    midcap = 0\n    for s in sentences:\n        words = re.findall(r\"\\b[\\w'-]+\\b\", s)\n        if not words:\n            continue\n        for i, w in enumerate(words):\n            # count words that start with uppercase letter and are not the first word of sentence\n            if i > 0 and w and w[0].isupper():\n                # avoid counting all-caps tokens (ACRONYM) as proper nouns by requiring some lowercase\n                if any(c.islower() for c in w):\n                    midcap += 1\n            total_words += 1\n    if total_words == 0:\n        return 0.0\n    return midcap / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas indicate clause complexity)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentence_segs = re.split(r'(?<=[.!?])\\s+', text.strip())\n    # fallback to 1 sentence if split yields empty\n    sentence_count = max(1, len([s for s in sentence_segs if s.strip() != '']))\n    comma_count = text.count(',')\n    return comma_count / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Immediate repeated-word rate: fraction of adjacent word pairs that are identical (e.g., \"the the\")'\n    import re\n    words = re.findall(r\"\\b\\w+\\b\", (text or '').lower())\n    if not words:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    return repeats / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Parenthetical usage density: number of opening parentheses \"(\" per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    paren_count = text.count('(')\n    sentence_matches = re.findall(r'[.!?]+', text)\n    sentence_count = max(1, len(sentence_matches))\n    return paren_count / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are sensory verbs/adjectives (see, hear, smell, taste, touch, feel and variants)'\n    import re\n    sensory = {\n        'see','saw','seen','look','looked','watch','watched','glimpse',\n        'hear','heard','listen','listened','sound','sounds',\n        'smell','smelled','scent','stink','aroma',\n        'taste','tasted','flavor','flavour','savory','sweet',\n        'touch','touched','feel','felt','felt','brush'\n    }\n    words = re.findall(r\"\\b\\w+\\b\", (text or '').lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in sensory)\n    return count / float(len(words))\n", "def feature(text: str) -> float:\n    'Ratio of asterisk emphasis markers (\"*\") to word count'\n    import re\n    if not text:\n        return 0.0\n    asterisks = text.count('*')\n    words = len(re.findall(r'\\w+', text))\n    return asterisks / max(1, words)\n\n", "def feature(text: str) -> float:\n    'Binary indicator (1.0/0.0) of horizontal separator lines like \"____\" or \"----\"'\n    import re\n    if not text:\n        return 0.0\n    # look for runs of underscores or hyphens of length >= 3 on a line (often used in poems or manual separators)\n    if re.search(r'(^|\\n)[\\s]*[_-]{3,}[\\s]*(\\n|$)', text):\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    \"Fraction of standalone lowercase 'i' tokens (informal first-person lowercase) among words\"\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text)\n    if not tokens:\n        return 0.0\n    lower_i = sum(1 for t in tokens if t == 'i')\n    return lower_i / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that are short (<= 5 words) \u2014 captures poetic or clipped/dialogue style'\n    import re\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    nonempty = [ln for ln in lines if ln.strip() != '']\n    if not nonempty:\n        return 0.0\n    short_lines = 0\n    for ln in nonempty:\n        wc = len(re.findall(r'\\w+', ln))\n        if 0 < wc <= 5:\n            short_lines += 1\n    return short_lines / len(nonempty)\n\n", "def feature(text: str) -> float:\n    'Proportion of capitalized words that occur mid-sentence (likely proper nouns) relative to all words'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences conservatively\n    sentences = re.split(r'(?<=[\\.!?])\\s+', text)\n    mid_cap = 0\n    total_words = 0\n    for sent in sentences:\n        words = re.findall(r\"\\b\\w+\\b\", sent)\n        for i, w in enumerate(words):\n            total_words += 1\n            if i > 0 and w and w[0].isupper():\n                mid_cap += 1\n    if total_words == 0:\n        return 0.0\n    return mid_cap / total_words\n\n", "def feature(text: str) -> float:\n    'Variety of contraction forms: distinct contraction tokens divided by total contraction tokens (0 if none)'\n    import re\n    if not text:\n        return 0.0\n    contractions = re.findall(r\"\\b\\w*'\\w+\\b\", text)\n    total = len(contractions)\n    if total == 0:\n        return 0.0\n    distinct = len(set(contractions))\n    return distinct / total\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<= 5 words) \u2014 contrasts clipped, fragmentary prose vs. longer narrative sentences'\n    import re\n    if not text:\n        return 0.0\n    # split by sentence terminators; keep fragments that look like sentences\n    fragments = re.split(r'[.!?]+', text)\n    sentence_words = [len(re.findall(r'\\w+', frag)) for frag in fragments if frag.strip() != '']\n    if not sentence_words:\n        return 0.0\n    short_sentences = sum(1 for w in sentence_words if 0 < w <= 5)\n    return short_sentences / len(sentence_words)\n", "def feature(text: str) -> float:\n    'Normalized count of 4-digit years (1000-2099) appearing in the text'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(?:1[0-9]{3}|20[0-9]{2})\\b', text)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(years)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of parenthesis characters \"(\" or \")\" to number of words'\n    import re\n    if not text:\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(paren_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Frequency of academic citation patterns (e.g., \"et al.\", \"[1]\", \"(Smith et al., 2020)\") per word'\n    import re\n    if not text:\n        return 0.0\n    patterns = 0\n    patterns += len(re.findall(r'\\bet al\\.\\b', text, flags=re.IGNORECASE))\n    patterns += len(re.findall(r'\\[\\s*\\d+(?:,\\s*\\d+)*\\s*\\]', text))\n    patterns += len(re.findall(r'\\([A-Za-z]+ et al[.,]?\\s*\\d{0,4}\\)', text))\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(patterns) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: occurrences of forms like \"was|were|is|are|been|being\" followed by an -ed word per word'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r'\\b(?:was|were|is|are|been|being|be)\\s+[A-Za-z0-9\\-]+ed\\b', text, flags=re.IGNORECASE)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proportion of capitalized words that are not obvious sentence starts (possible named entities or acronyms)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[A-Za-z][A-Za-z\\'\u2019\\-]+\\b', text)\n    if not words:\n        return 0.0\n    # find capitalized words via regex and check preceding char not sentence terminator\n    caps = 0\n    for m in re.finditer(r'\\b[A-Z][a-zA-Z\\'\u2019\\-]{1,}\\b', text):\n        i = m.start()\n        # find previous non-space char\n        j = i - 1\n        while j >= 0 and text[j].isspace():\n            j -= 1\n        prev_char = text[j] if j >= 0 else ''\n        if prev_char not in '.!?;\\n' and i != 0:\n            caps += 1\n    return float(caps) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Punctuation dominance: 1 - (count of most common punctuation / total punctuation); high means diverse punctuation'\n    import collections\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    counts = collections.Counter(puncts)\n    most_common = counts.most_common(1)[0][1]\n    total = len(puncts)\n    dominance = float(most_common) / float(total)\n    return 1.0 - dominance\n\n", "def feature(text: str) -> float:\n    'Proportion of parenthetical acronyms like \"(EIA)\" per word, useful for texts that expand acronyms'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r'\\(\\s*[A-Z]{2,}\\s*\\)', text)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Character-level Shannon entropy (bits) normalized by log2(unique_chars+1), 0 for empty text'\n    import math, collections\n    if not text:\n        return 0.0\n    counts = collections.Counter(text)\n    total = float(len(text))\n    entropy = 0.0\n    for cnt in counts.values():\n        p = cnt / total\n        entropy -= p * math.log2(p)\n    # normalize by maximum possible log2(len(unique_chars)+1) to keep in [0,1]\n    unique = len(counts)\n    if unique <= 1:\n        return 0.0\n    norm = math.log2(unique)\n    if norm <= 0:\n        return 0.0\n    return float(entropy) / float(norm)\n", "def feature(text: str) -> float:\n    'Density of scientific-style abbreviations or tokens containing digits (e.g., CO2, DNA, H2O)'\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    # all-caps abbreviations of length >=2 and tokens containing digits\n    caps_abbrev = re.findall(r'\\b[A-Z]{2,}\\b', text)\n    digit_tokens = re.findall(r'\\b\\w*\\d\\w*\\b', text)\n    # count unique occurrences found in the word list (allow duplicates as occurrences)\n    count = len(caps_abbrev) + len(digit_tokens)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    if not text:\n        return 0.0\n    # split into sentences by ., !, ?\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences]\n    # guard against zero-length sentences\n    valid = [l for l in lengths if l > 0]\n    if not valid:\n        return 0.0\n    return float(sum(valid)) / len(valid)\n\n", "def feature(text: str) -> float:\n    'Ratio of modal verbs (can, could, may, might, must, shall, should, will, would) to total tokens'\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice indicator: number of passive-like \"be + past-participle\" matches per sentence'\n    if not text:\n        return 0.0\n    # simple pattern: be-form followed by word ending in -ed (covers many passives)\n    matches = re.findall(r'\\b(am|is|are|was|were|be|been|being)\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    denom = max(1, len(sentences))\n    return float(len(matches)) / denom\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens (lexical diversity)'\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    unique = set(tokens)\n    return float(len(unique)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Frequency of the phrase \"one of the\" normalized by total tokens (captures formulaic/templated phrasing)'\n    if not text:\n        return 0.0\n    s = text.lower()\n    phrase_count = len(re.findall(r'\\bone of the\\b', s))\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(phrase_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Stopword ratio: fraction of tokens that are common English stopwords (function-word density)'\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you',\n        'do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one',\n        'all','would','there','their','what','so','up','out','if','about','who','get','which','go','me'\n    }\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Punctuation variety density: unique punctuation characters used divided by total punctuation occurrences'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    unique_count = len(set(puncts))\n    total_count = len(puncts)\n    return float(unique_count) / total_count\n", "def feature(text: str) -> float:\n    'Fraction of short, capitalized header-like lines (e.g., \"Background\" on its own line)'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    count = 0\n    for l in lines:\n        s = l.strip()\n        parts = s.split()\n        if 1 <= len(parts) <= 3 and parts[0][:1].isupper() and all(p.isalpha() for p in parts):\n            # treat single- or short-word title-like lines as headers\n            count += 1\n    return float(count) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Density of inner-sentence capitalized words (proxy for proper-noun / named-entity density)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences to avoid counting sentence-initial capitals\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    total_words = 0\n    proper_like = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        for i, w in enumerate(words):\n            total_words += 1\n            if i > 0 and w[0].isupper() and not w.isupper() and len(w) > 1:\n                # not counting single-letter 'I' or acronyms\n                proper_like += 1\n    if total_words == 0:\n        return 0.0\n    return float(proper_like) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Average number of double-quote characters per sentence (dialogue / quoted material density)'\n    import re\n    if not text:\n        return 0.0\n    # count double quote characters including curly variants\n    quote_count = text.count('\"') + text.count('\\u201c') + text.count('\\u201d')\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(quote_count) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain digits (dates, years, numeric references)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(digit_tokens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis occurrence density: count of \"...\" normalized by character length'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    length = max(1, len(text))\n    return float(ellipses) / float(length)\n\n", "def feature(text: str) -> float:\n    'Average words per sentence (sentence length as a measure of syntactic complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences]\n    if not counts:\n        return 0.0\n    return float(sum(counts)) / float(len(counts))\n\n", "def feature(text: str) -> float:\n    'Density of colons and semicolons as a fraction of characters (punctuation style / academic tone)'\n    if not text:\n        return 0.0\n    count = text.count(':') + text.count(';')\n    return float(count) / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Ratio of possessive apostrophe tokens (\\'s or \u2019s) to total apostrophe occurrences (possessive vs contraction tendency)'\n    import re\n    if not text:\n        return 0.0\n    # count possessive forms 's and \u2019s (lowercase or uppercase)\n    possessive = len(re.findall(r\"(?:'s|\u2019s)\\b\", text, flags=re.IGNORECASE))\n    total_apost = text.count(\"'\") + text.count('\\u2019') + text.count('\\u2018')\n    if total_apost == 0:\n        return 0.0\n    return float(possessive) / float(total_apost)\n", "def feature(text: str) -> float:\n    'Quotation density: number of quotation mark characters (standard + smart) per sentence (quotes/sentences)'\n    import re\n    if not text:\n        return 0.0\n    quote_chars_count = text.count('\"') + text.count('\u201c') + text.count('\u201d') + text.count(\"'\") - text.count(\"\u2019\")*0  # single quotes are noisy; still count double/smart separately below\n    # Count double quotes and smart double quotes only for more reliable dialogue indicator\n    double_quotes = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(double_quotes) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Header-like short title lines ratio: fraction of non-empty lines that look like short title headings (1-4 Titlecase words)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_title_like(line):\n        words = line.split()\n        if not (1 <= len(words) <= 4):\n            return False\n        for w in words:\n            # consider a word Titlecase if first char upper and rest lower (allow short acronyms length 1)\n            if not (w[0].isupper() and (w[1:].islower() or len(w) == 1)):\n                return False\n        return True\n    title_like = sum(1 for ln in lines if is_title_like(ln))\n    return float(title_like) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: occurrences of \"...\" or single-character ellipsis per 100 words (scaled by words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    wcount = max(1, len(words))\n    ellipses = text.count('...') + text.count('\u2026')\n    # scale down to fraction per word\n    return float(ellipses) / float(wcount)\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio: fraction of tokens that are first-person pronouns or contractions (I, me, my, we, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    # match standalone pronouns and common contractions (i'm, i've, i'll, i'd)\n    pattern = re.compile(r\"\\b(i|me|my|mine|we|us|our|ours)\\b\", re.IGNORECASE)\n    pattern_contr = re.compile(r\"\\bi['\u2019](m|ve|ll|d)\\b\", re.IGNORECASE)\n    pron_count = len(pattern.findall(lowered)) + len(pattern_contr.findall(lowered))\n    words = re.findall(r'\\w+', text)\n    wcount = max(1, len(words))\n    return float(pron_count) / float(wcount)\n\n", "def feature(text: str) -> float:\n    'Proper-noun density (approx): number of Titlecase tokens beyond one per sentence divided by word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    wcount = max(1, len(words))\n    # Titlecase words with at least 3 letters (to avoid single-letter sentence starts)\n    title_tokens = re.findall(r'\\b[A-Z][a-z]{2,}\\b', text)\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    # assume one Titlecase per sentence is from sentence-begin; subtract that baseline\n    extra_titles = max(0, len(title_tokens) - sentence_count)\n    return float(extra_titles) / float(wcount)\n\n", "def feature(text: str) -> float:\n    'Short-sentence fraction: fraction of sentences with three or fewer words (indicates terse/punchy style)'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence punctuation to get rough sentences\n    parts = re.split(r'[.!?]+', text)\n    sentences = [p.strip() for p in parts if p.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        wc = len(re.findall(r'\\w+', s))\n        if wc <= 3:\n            short += 1\n    return float(short) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Modal verb ratio: fraction of words that are modal verbs (should/would/could/shall/will/must/may/might)'\n    import re\n    if not text:\n        return 0.0\n    modals = re.findall(r'\\b(should|would|could|shall|will|must|may|might)\\b', text, flags=re.IGNORECASE)\n    words = re.findall(r'\\w+', text)\n    wcount = max(1, len(words))\n    return float(len(modals)) / float(wcount)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain quotation marks (single or double), indicating dialogue or quoted text'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences heuristically\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_cnt = sum(1 for s in sentences if '\"' in s or \"'\" in s or '\u201c' in s or '\u201d' in s or '\u2018' in s or '\u2019' in s)\n    return float(quote_cnt) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Binary-ish indicator (0.0/1.0) for a \"Title by Name\" pattern near the start (e.g., \"The X by Author\")'\n    import re\n    if not text or len(text) < 4:\n        return 0.0\n    # look within the first 120 characters for patterns like \"Title by Name\"\n    window = text[:200]\n    # match \"by\" followed by a capitalized name (one or two tokens)\n    m = re.search(r'\\bby\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?\\b', window)\n    return 1.0 if m else 0.0\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count), 0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = max(1, len(sentences))\n    return float(comma_count) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Ellipsis (\"...\") occurrences normalized by word count, capturing trailing/thoughtful style'\n    import re\n    if not text:\n        return 0.0\n    ell = text.count('...')\n    words = re.findall(r'\\w+', text)\n    return float(ell) / float(len(words) + 1)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word forms / total tokens) as a simple lexical diversity measure'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    unique = set(tokens)\n    return float(len(unique)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of simple past auxiliaries to present auxiliaries (was/were/had/did vs is/are/am/have/do) to hint at tense usage'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    past_set = {'was', 'were', 'had', 'did'}\n    present_set = {'is', 'are', 'am', 'have', 'do', 'does'}\n    past = sum(1 for w in words if w in past_set)\n    present = sum(1 for w in words if w in present_set)\n    # add small smoothing to avoid division by zero\n    return float(past + 0.5) / float(present + 0.5)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a first-person pronoun (I, my, we, our), capturing first-person narrative openings'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.match(r'\\b(\\w+)\\b', s)\n        if m and m.group(1).lower() in {'i', 'my', 'we', 'our'}:\n            count += 1\n    return float(count) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique words divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Adverb ratio: fraction of word tokens ending in -ly (approximate adverb usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    advs = sum(1 for t in tokens if len(t) > 3 and t.endswith('ly'))\n    return float(advs) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Passive voice score: count of common passive-like patterns per sentence (approximate)'\n    import re\n    if not text:\n        return 0.0\n    patterns = re.compile(r'\\b(?:is|was|were|are|be|been|being|has been|have been|had been|will be|would be|should be|can be)\\s+\\w+ed\\b', re.I)\n    matches = patterns.findall(text)\n    # Normalize by number of sentences to get a per-sentence tendency\n    sentence_count = max(1, len(re.findall(r'[^.!?]+[.!?]?', text)))\n    return float(len(matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Heading-like start indicator: 1.0 if text begins with a capitalized heading followed by a colon (e.g., \"Introduction:\"), else 0.0'\n    import re\n    if not text:\n        return 0.0\n    # Check start of text or after a newline\n    if re.search(r'^(?:\\s*)[A-Z][\\w\\s]{0,60}:', text):\n        return 1.0\n    if re.search(r'\\n\\s*[A-Z][\\w\\s]{0,60}:', text):\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Proper noun ratio (mid-sentence titlecased tokens): proportion of titlecased words that are not sentence-initial'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    total_words = 0\n    pn_count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        for i, w in enumerate(words):\n            total_words += 1\n            # consider titlecase and not the first token of a sentence\n            if i != 0 and w[0].isupper() and not w.isupper():\n                pn_count += 1\n    if total_words == 0:\n        return 0.0\n    return float(pn_count) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Discourse marker density: frequency of common discourse markers (however, moreover, therefore, etc.) per word'\n    import re\n    if not text:\n        return 0.0\n    markers = re.findall(r'\\b(?:however|moreover|therefore|furthermore|additionally|consequently|nevertheless|thus|meanwhile|alternatively)\\b', text, re.I)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(markers)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Comma density: average number of commas per sentence (proxy for clause complexity)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = max(1, len(re.findall(r'[^.!?]+[.!?]?', text)))\n    return float(commas) / float(sentences)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a discourse/transition word (however, moreover, first, finally, etc.)'\n    import re\n    if not text:\n        return 0.0\n    transitions = ['however','moreover','furthermore','additionally','first','second','finally',\n                   'in addition','therefore','consequently','for example','for instance','overall',\n                   'meanwhile','similarly','nevertheless','on the other hand','in contrast','importantly']\n    # split sentences robustly\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        low = s.lower()\n        for t in transitions:\n            if low.startswith(t + ' ') or low.startswith(t + ',') or low == t:\n                count += 1\n                break\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count (lexical diversity)'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / len(words)\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (length >= 7), a proxy for formality and vocabulary complexity'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 7)\n    return float(long_words) / len(words)\n\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (semicolon usage signals complex sentence structuring)'\n    import re\n    if not text:\n        return 0.0\n    semis = text.count(';')\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(semis) / sentences\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like headings (short line, title-case or ALL CAPS, no terminal punctuation)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    heading_like = 0\n    for l in lines:\n        s = l.strip()\n        if len(s) > 1 and len(s) <= 120 and not re.search(r'[.!?]', s):\n            words = s.split()\n            if not words:\n                continue\n            # count words starting with uppercase or fully uppercase\n            capcount = sum(1 for w in words if w[0].isupper())\n            allcapcount = sum(1 for w in words if w.isalpha() and w.isupper())\n            if capcount >= max(1, len(words) // 2) or allcapcount >= 1:\n                heading_like += 1\n    return float(heading_like) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Passive-voice indicator density: matches of \"was/were/is/are ... -ed\" plus occurrences of \" by \" per sentence'\n    import re\n    if not text:\n        return 0.0\n    # sentences count\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    passive_ed = len(re.findall(r'\\b(?:was|were|is|are|been|being|be|wasn\\'t|weren\\'t)\\s+\\w+ed\\b', text, flags=re.IGNORECASE))\n    by_phrases = len(re.findall(r'\\bby\\s+[A-Za-z0-9_\\-]+\\b', text, flags=re.IGNORECASE))\n    return float(passive_ed + by_phrases) / sentences\n\n", "def feature(text: str) -> float:\n    'Average clause length in words (split clauses by commas and semicolons), useful to detect multi-clause sentences'\n    import re\n    if not text:\n        return 0.0\n    clauses = [c.strip() for c in re.split(r'[;,]\\s*', text) if c.strip()]\n    if not clauses:\n        return 0.0\n    words_per_clause = [len(re.findall(r'\\w+', c)) for c in clauses]\n    if not words_per_clause:\n        return 0.0\n    return float(sum(words_per_clause)) / len(words_per_clause)\n\n", "def feature(text: str) -> float:\n    'Normalized sentence-length range: (max_len - min_len) / (max_len + min_len + 1e-6)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lens = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lens:\n        return 0.0\n    mx = max(lens)\n    mn = min(lens)\n    return float(mx - mn) / (mx + mn + 1e-6)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences (approx) that contain an ellipsis \"...\"'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(ellipses) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation count'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    unique = len(set(puncts))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Density of sensory/imagery words from a small lexicon (sight/sound/touch/smell/taste terms)'\n    import re\n    if not text:\n        return 0.0\n    lex = {'piercing','flicker','flickering','humming','buzzed','buzz','light','dark','bright',\n           'sound','smell','scent','taste','see','saw','heard','hear','feel','felt','cold','warm',\n           'soft','loud','silent','shimmer','glow','shine','glare','gloom','sharp','sharpness'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in lex)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentence_count)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio: fraction of tokens that are first-person pronouns (I/me/my/we/us/etc.)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','myself','ourselves'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in pronouns)\n    return float(count) / float(len(words))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, us, our) \u2014 signals personal narration'\n    if not text or not text.strip():\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    tokens = [t.strip(\".,;:!?()\\\"'\").lower() for t in text.split()]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word tokens / total word tokens), robust to punctuation and case'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation occurrences'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(len(puncts))\n\n", "def feature(text: str) -> float:\n    'Normalized lexical entropy of word frequency distribution (0..1), approximates lexical variety vs repetition'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    cnt = Counter(tokens)\n    N = float(len(tokens))\n    probs = [v / N for v in cnt.values()]\n    # entropy\n    H = -sum(p * math.log(p + 1e-12) for p in probs)\n    V = len(cnt)\n    if V <= 1:\n        return 0.0\n    # normalize by log(V)\n    return float(H / (math.log(V) + 1e-12))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that end with exclamation or ellipsis (...): often higher in expressive fiction'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # Split into sentence-like blocks while preserving punctuation\n    # Use a simple heuristic split on sentence terminators\n    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [p for p in parts if p.strip()]\n    if not sentences:\n        return 0.0\n    special_count = 0\n    for s in sentences:\n        s_strip = s.rstrip()\n        if s_strip.endswith('!') or s_strip.endswith('...') or s_strip.endswith('\u2026'):\n            special_count += 1\n    return float(special_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Density of multi-word titlecase sequences (>=2 consecutive Titlecase words) per token \u2014 signals formal titles or headings'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    seq_count = 0\n    i = 0\n    while i < len(words):\n        # check titlecase: first letter uppercase and rest lowercase (simple heuristic)\n        if words[i][0].isupper() and (words[i][1:].islower() if len(words[i]) > 1 else True):\n            j = i + 1\n            while j < len(words) and words[j][0].isupper() and (words[j][1:].islower() if len(words[j]) > 1 else True):\n                j += 1\n            length = j - i\n            if length >= 2:\n                seq_count += 1\n            i = j\n        else:\n            i += 1\n    return float(seq_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (small anchored list) \u2014 can indicate function-word heavy prose vs descriptive'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {'the', 'and', 'of', 'to', 'a', 'in', 'that', 'is', 'was', 'it', 'for', 'on', 'with', 'as', 'by', 'an', 'be'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are adverbs ending in -ly (captures descriptive/adverbial style)'\n    if not text or not text.strip():\n        return 0.0\n    words = text.split()\n    total = max(1, len(words))\n    ly_count = sum(1 for w in words if w.lower().endswith('ly') and len(w) > 2)\n    return float(ly_count) / float(total)\n\n", "def feature(text: str) -> float:\n    'Density of internal Titlecase words (proper-noun-like words not at sentence start) per token'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    total = max(1, len(tokens))\n    proper_count = 0\n    for m in re.finditer(r'\\b[A-Z][a-z]{1,}\\b', text):\n        start = m.start()\n        # find previous non-space character\n        prev_idx = start - 1\n        while prev_idx >= 0 and text[prev_idx].isspace():\n            prev_idx -= 1\n        # if at very start or previous non-space is sentence-ending punctuation, treat as sentence start -> skip\n        if prev_idx < 0 or text[prev_idx] in '.!?':\n            continue\n        proper_count += 1\n    return float(proper_count) / float(total)\n\n", "def feature(text: str) -> float:\n    'Ratio of third-person pronouns to total tokens (he/she/they/his/her/their etc.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    pronouns = r'\\b(?:he|she|they|him|her|them|his|hers|their|theirs|himself|herself|themselves)\\b'\n    matches = re.findall(pronouns, text, flags=re.IGNORECASE)\n    tokens = re.findall(r'\\w+', text)\n    total = max(1, len(tokens))\n    return float(len(matches)) / float(total)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word forms / total word tokens) as a measure of lexical diversity'\n    import re\n    tokens = re.findall(r'\\w+', text.lower()) if text else []\n    total = max(1, len(tokens))\n    unique = len(set(tokens))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Estimated clauses per sentence: (coordinating/subordinating conjunctions + semicolons + half the commas) per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj_pattern = r'\\b(?:and|but|because|although|while|since|when|which|that|however|so|therefore)\\b'\n    conj_count = len(re.findall(conj_pattern, text, flags=re.IGNORECASE))\n    semicolons = text.count(';')\n    commas = text.count(',')\n    # sentence count robust split\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    n_sent = max(1, len(sentences))\n    clause_estimate = conj_count + semicolons + 0.5 * commas\n    return float(clause_estimate) / float(n_sent)\n\n", "def feature(text: str) -> float:\n    'Sentence-length variability: standard deviation of sentence lengths divided by mean length (0 if not applicable)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    raw_sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    lengths = [len(re.findall(r'\\w+', s)) for s in raw_sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Parenthetical density: number of parentheses characters per 100 tokens (captures asides and explanatory inserts)'\n    if not text or not text.strip():\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    tokens = len(text.split())\n    tokens = max(1, tokens)\n    # scale to per-100 tokens for easier interpretability\n    return float(paren_count) * 100.0 / float(tokens)\n\n", "def feature(text: str) -> float:\n    'Story-trope phrase frequency per sentence (counts occurrences of common narrative tropes like \"once upon a time\", \"long ago\", \"after the fall\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    patterns = [r'once upon a time', r'long ago', r'in a small town', r'after the fall', r'after the', r'the ancient', r'at the entrance']\n    lower = text.lower()\n    trope_count = sum(1 for p in patterns if re.search(p, lower))\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    n_sent = max(1, len(sentences))\n    return float(trope_count) / float(n_sent)\n", "def feature(text: str) -> float:\n    'Ellipsis frequency: count of \"...\" occurrences per 1000 words (scaled) to handle short texts'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = max(1, len(words))\n    # scale to per-1000 words to keep values in a useful numeric range\n    return float(ellipses) * 1000.0 / float(word_count)\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio (I, me, my, mine, we, us, our, ours) among tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','myself','ourselves'}\n    count = sum(1 for t in tokens if t.lower() in pronouns)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Dialogue-tag verb ratio: fraction of tokens that are common dialogue verbs like \"said\", \"asked\", \"replied\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    tag_verbs = {'said','asked','replied','whispered','murmured','shouted','yelled','cried','responded','added','snapped','said.'}\n    count = sum(1 for t in tokens if t.lower().strip(\".,;:!?\") in tag_verbs)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique token count divided by total token count (lexical richness)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"\\b[\\w']+\\b\", text)]\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Adverb ratio: fraction of tokens ending in \"ly\" (common adverb marker)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 2 and t.lower().endswith('ly'))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Comma density: average number of commas per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    comma_count = text.count(',')\n    return float(comma_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Gerund-to-past ratio: (count of tokens ending in \"ing\") / (count of tokens ending in \"ed\"), smoothed to avoid division by zero'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    ing = sum(1 for t in tokens if len(t) > 3 and t.lower().endswith('ing'))\n    ed = sum(1 for t in tokens if len(t) > 2 and t.lower().endswith('ed'))\n    # Laplace smoothing (add 0.5) to keep ratio stable for small counts\n    return float(ing + 0.5) / float(ed + 0.5)\n", "def feature(text: str) -> float:\n    'Normalized density of em-dashes (\u2014) per word to detect stylistic interruptions'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    words = max(1, len(tokens))\n    em_count = text.count('\u2014')\n    return float(em_count) / words\n\n", "def feature(text: str) -> float:\n    'Normalized count of ellipses (\"...\" or \"\u2026\") per word (often used in trailing or suspenseful text)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    words = max(1, len(tokens))\n    ellipses = len(re.findall(r'\\.{3}|\u2026', text))\n    return float(ellipses) / words\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns/mentions (I, me, my, we, us, our) to all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    words = max(1, len(tokens))\n    first_set = {'i','me','my','mine','we','us','our','ours','i\\'m','im','ive','i\\'ve'}\n    count = 0\n    for t in tokens:\n        tl = t.lower()\n        if tl in first_set:\n            count += 1\n    return float(count) / words\n\n", "def feature(text: str) -> float:\n    'Ratio of second-person pronouns (you, your, yours) to all tokens (useful for imperative/2nd-person narratives)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    words = max(1, len(tokens))\n    second_set = {'you','your','yours','youre','you\\'re','youve','you\\'ve'}\n    count = 0\n    for t in tokens:\n        if t.lower() in second_set:\n            count += 1\n    return float(count) / words\n\n", "def feature(text: str) -> float:\n    'Fraction of alphabetic tokens ending in \"ed\" (simple proxy for past-tense verb usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    words = max(1, len(tokens))\n    ed_count = 0\n    for t in tokens:\n        tl = t.lower()\n        if len(tl) > 3 and tl.endswith('ed'):\n            ed_count += 1\n    return float(ed_count) / words\n\n", "def feature(text: str) -> float:\n    'Proportion of characters that lie inside parentheses (total chars inside (...) divided by total chars)'\n    import re\n    if not text:\n        return 0.0\n    total_chars = max(1, len(text))\n    inside = 0\n    for m in re.findall(r'\\((.*?)\\)', text, flags=re.DOTALL):\n        inside += len(m)\n    return float(inside) / total_chars\n\n", "def feature(text: str) -> float:\n    'Ratio of Titlecase-like words (First letter uppercase, rest lowercase, len>1) to tokens (proxy for proper-noun density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    total = max(1, len(tokens))\n    count = 0\n    for t in tokens:\n        if len(t) > 1 and t[0].isupper() and t[1:].islower():\n            count += 1\n    return float(count) / total\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: distinct punctuation characters divided by total punctuation occurrences (0 if none)'\n    import re\n    if not text:\n        return 0.0\n    puncts = re.findall(r'[^\\w\\s]', text)\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    total = len(puncts)\n    return float(distinct) / total\n", "def feature(text: str) -> float:\n    'Density of common dialogue-reporting verbs (said, asked, replied, shouted, whispered, etc.) per word'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    reporting_verbs = r'\\b(said|asked|replied|muttered|whispered|shouted|exclaimed|answered|sighed|laughed|grinned|smiled)\\b'\n    matches = re.findall(reporting_verbs, text, flags=re.IGNORECASE)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Rate of repeated punctuation sequences (ellipses or multi-exclamation/question) per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    repeats = re.findall(r'(\\.\\.\\.|!{2,}|\\?{2,}|!\\?)', text)\n    return float(len(repeats)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated (contain - or \u2014), capturing nicknames or stylized names like \"Tobias-14\" or \"Safi-boy\"'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphenated = sum(1 for t in tokens if '-' in t or '\u2014' in t)\n    return float(hyphenated) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a first-person pronoun (I, I\\'m, I\\'ve, we) \u2014 indicates narrator presence or direct speech'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    pattern = re.compile(r'^[\\'\"]?\\s*(I\\b|I\\'m\\b|I\\'ve\\b|we\\b|we\\'re\\b)', flags=re.IGNORECASE)\n    count = sum(1 for s in sentences if pattern.search(s))\n    return float(count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Normalized entropy of a small set of common function words (and,the,of,to,is,it,that) \u2014 measures distributional variety of function words'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    func_words = ['and', 'the', 'of', 'to', 'is', 'it', 'that', 'a', 'in', 'for']\n    words = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not words:\n        return 0.0\n    total = len(words)\n    freqs = []\n    for fw in func_words:\n        c = words.count(fw)\n        if c > 0:\n            freqs.append(c / total)\n    if not freqs:\n        return 0.0\n    entropy = -sum(p * math.log(p + 1e-12) for p in freqs)\n    # normalize by log(k) where k = number of considered function words that appear\n    k = len(freqs)\n    if k <= 1:\n        return 0.0\n    normalized = entropy / math.log(k)\n    return float(normalized)\n", "def feature(text: str) -> float:\n    'Density of capitalized words that do NOT appear to start a sentence (proxy for proper-noun usage)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    total = len(words)\n    if total == 0:\n        return 0.0\n    cap_count = 0\n    for m in re.finditer(r'\\b[A-Z][a-zA-Z]+\\b', text):\n        pos = m.start()\n        # find last non-space character before this word\n        i = pos - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        # if there is punctuation that likely ends a sentence immediately before, consider it sentence-start -> skip\n        if i >= 0 and text[i] in '.!?':\n            continue\n        # also skip if at very start (treat start-of-text as sentence-start)\n        if pos == 0:\n            continue\n        cap_count += 1\n    return float(cap_count) / float(total)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are inside single or double quotes (quoted content density)'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    # find double-quoted and single-quoted content (simple pairwise)\n    dq = re.findall(r'\"([^\"]+)\"', text)\n    sq = re.findall(r\"'([^']+)'\", text)\n    quoted_chars = sum(len(m) for m in dq) + sum(len(m) for m in sq)\n    return float(quoted_chars) / float(total_len) if total_len > 0 else 0.0\n\n\n", "def feature(text: str) -> float:\n    'Ratio of modal verbs (can/could/will/would/shall/should/may/might/must) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    modals = {'can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must'}\n    count = sum(1 for w in words if w in modals)\n    return float(count) / float(len(words))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a pronoun (I, he, she, they, we, it, you)'\n    import re\n    if not text:\n        return 0.0\n    # Split on sentence terminators; keep non-empty trimmed segments\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    pronouns = {'i', 'he', 'she', 'they', 'we', 'it', 'you'}\n    starts = 0\n    for s in sents:\n        w = re.findall(r'\\b\\w+\\b', s)\n        if not w:\n            continue\n        if w[0].lower() in pronouns:\n            starts += 1\n    return float(starts) / float(len(sents))\n\n\n", "def feature(text: str) -> float:\n    'Parenthetical character density: fraction of characters that are parentheses ( or )'\n    if not text:\n        return 0.0\n    paren_chars = text.count('(') + text.count(')')\n    total = len(text)\n    return float(paren_chars) / float(total) if total > 0 else 0.0\n\n\n", "def feature(text: str) -> float:\n    'Proxy for passive voice: occurrences of \"be\" forms followed by an -ed word per sentence'\n    import re\n    if not text:\n        return 0.0\n    # count simple patterns like \"is/are/was/were/been/am/being + wordED\"\n    matches = re.findall(r'\\b(?:am|is|are|was|were|be|been|being)\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    sents = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sents))\n    return float(len(matches)) / float(sent_count)\n\n\n", "def feature(text: str) -> float:\n    'Ellipsis frequency: number of \"...\" occurrences per sentence'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sents = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sents))\n    return float(ellipses) / float(sent_count)\n", "def feature(text: str) -> float:\n    'Fraction of first-person pronouns (I, we, me, my, our, us, mine, ourselves) indicating personal vs. impersonal tone'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','myself','we','us','our','ours','ourselves'}\n    count = sum(1 for t in tokens if t.strip(\"'-\") in first_person)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (marker of complex, formal sentence structure)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    semis = text.count(';')\n    return float(semis) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses (contain http/www/@) indicating copied web/metadata or informal content'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        low = t.lower()\n        if 'http' in low or 'www.' in low or '@' in t:\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain hyphens or dashes (hyphenated compounds are common in some academic styles)'\n    import re\n    if not text:\n        return 0.0\n    # consider ASCII hyphen and en/em dashes\n    tokens = re.findall(r\"\\b[\\w\\-\\\u2013\\\u2014]+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t or '\u2013' in t or '\u2014' in t)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of quotation marks that are curly/typographic quotes (e.g., \u2018 \u2019 \u201c \u201d) vs plain ASCII quotes, indicating published/edited text'\n    import re\n    if not text:\n        return 0.0\n    # count curly quote characters and total quote-like characters\n    curly = sum(text.count(ch) for ch in ('\u2018','\u2019','\u201a','\u201c','\u201d','\u201e'))\n    plain = sum(text.count(ch) for ch in (\"'\", '\"'))\n    total = curly + plain\n    if total == 0:\n        return 0.0\n    return float(curly) / total\n\n", "def feature(text: str) -> float:\n    'Indicator-like score (0 or 1) for presence of a leading or standalone rhetorical question/heading'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    t = text.strip()\n    # If first non-empty line is a question or there exists a short line that ends with a question mark\n    lines = [ln for ln in t.splitlines() if ln.strip()]\n    if lines:\n        first = lines[0].strip()\n        if first.endswith('?'):\n            return 1.0\n    # look for short-question lines (common in human rhetorical headings)\n    if re.search(r'(?m)^\\s{0,10}.{1,200}\\?\\s*$', text):\n        return 1.0\n    # also treat a question near the start as signal\n    if '?' in text[:120]:\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice score: passive construction matches per sentence (approximate)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # approximate sentences\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    n_sent = len(sentences)\n    if n_sent == 0:\n        return 0.0\n    # find passive-like patterns: (is/are/was/were/has/have/had) (been )? <past-participle>\n    pattern = re.compile(r'\\b(?:is|are|was|were|be|been|being|has|have|had)\\s+(?:been\\s+)?[A-Za-z-]+(?:ed|en|n)\\b', re.I)\n    matches = pattern.findall(text)\n    return float(len(matches)) / float(n_sent)\n\n", "def feature(text: str) -> float:\n    'Density of common academic/transition phrases (e.g., \"in order to\", \"according to\") as fraction of tokens'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    phrases = ['in order to','according to','it is essential','it is important','as a result','in contrast','on the other hand','as such','for example','moreover','furthermore','therefore']\n    count = 0\n    for p in phrases:\n        # count non-overlapping occurrences\n        count += len(re.findall(re.escape(p), lower))\n    tokens = lower.split()\n    return float(count) / max(1, len(tokens))\n\n", "def feature(text: str) -> float:\n    'Numeric token density: fraction of tokens containing digits, percent signs, or currency symbols'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    num = 0\n    for t in tokens:\n        if any(ch.isdigit() for ch in t) or '%' in t or '$' in t:\n            num += 1\n    return float(num) / max(1, len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation that appears as repeated punctuation sequences (e.g., \"...\", \"!!\")'\n    import re\n    if not text:\n        return 0.0\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    repeats = len(re.findall(r'([^\\w\\s])\\1{1,}', text))\n    # return fraction of repeated-punct sequences weighted by total punctuation\n    return float(repeats) / float(total_punct)\n\n", "def feature(text: str) -> float:\n    'Parenthesis/bracket density: fraction of characters that are parentheses or square brackets'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    count = sum(1 for c in text if c in '()[]{}')\n    return float(count) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Fraction of word-tokens that are titlecase (start with an uppercase then lowercase) excluding the very first token (helps detect proper nouns and headings)'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    title_count = 0\n    total = 0\n    for i, w in enumerate(words):\n        # exclude the very first token to reduce sentence-start bias\n        if i == 0:\n            continue\n        total += 1\n        if w.istitle():\n            title_count += 1\n    if total == 0:\n        return 0.0\n    return float(title_count) / float(total)\n\n", "def feature(text: str) -> float:\n    'Average number of word tokens per sentence (words / sentences) \u2014 long values indicate denser, more complex sentences'\n    import re\n    words = re.findall(r'\\w+', text)\n    num_words = len(words)\n    # count sentence-ending punctuation groups as sentence boundaries\n    sentences = re.findall(r'[.!?]+', text)\n    num_sent = len(sentences)\n    if num_sent == 0:\n        # treat whole text as single sentence if none found\n        num_sent = 1\n    return float(num_words) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word forms divided by total word tokens), a measure of lexical diversity'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentences) \u2014 higher values often indicate more nested clauses'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentences = re.findall(r'[.!?]+', text)\n    num_sent = len(sentences)\n    if num_sent == 0:\n        num_sent = 1\n    return float(comma_count) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: count of \"...\" sequences normalized by total characters (captures trailing/informal ellipses)'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    return float(ellipses) / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio: fraction of tokens that are first-person pronouns (I, we, me, us, my, our, mine, ours)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i', 'we', 'me', 'us', 'my', 'our', 'mine', 'ours'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in pronouns)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Punctuation variety ratio: number of distinct punctuation characters divided by total punctuation characters (measures diversity of punctuation use)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(len(puncts))\n\n", "def feature(text: str) -> float:\n    'Acronym density: fraction of tokens that are all-caps acronyms of length >= 2 (captures items like TED, SWOT)'\n    import re\n    words = re.findall(r'\\b[A-Z]{2,}\\b', text)\n    # total token count measured by word-like tokens\n    total_tokens = len(re.findall(r'\\w+', text))\n    if total_tokens == 0:\n        return 0.0\n    return float(len(words)) / float(total_tokens)\n", "def feature(text: str) -> float:\n    'Average sentence length measured in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    # Split on sentence end punctuation (one or more) to avoid empty fragments\n    raw_sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in raw_sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts)) / len(word_counts)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths (words per sentence)'\n    import re, math\n    if not text:\n        return 0.0\n    raw_sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in raw_sentences if s.strip()]\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences if len(re.findall(r'\\w+', s)) > 0]\n    if len(lengths) < 2:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / (len(lengths) - 1)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (clause density proxy)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # Determine number of sentences (fallback to 1 if none)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = len(sentences) if sentences else 1\n    return float(comma_count) / num_sent\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that look like short title/headings (short, title-cased, may indicate sections)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    title_like = 0\n    for ln in lines:\n        words = re.findall(r\"[A-Za-z]+['-]?[A-Za-z]*\", ln.strip())\n        if 0 < len(words) <= 6:\n            # count as title-like if a majority of words are titlecased (First letter uppercase, rest lowercase)\n            titlecased = sum(1 for w in words if w[0].isupper() and (len(w) == 1 or w[1:].islower()))\n            if titlecased >= max(1, len(words) // 2):\n                title_like += 1\n    return float(title_like) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that begin with a list marker or numbering (e.g., \"1.\", \"2)\", \"-\", \"*\")'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    pattern = re.compile(r'^\\s*(?:\\d+[\\.\\)]|[-\\*\\u2022])\\s+', re.UNICODE)\n    matches = sum(1 for ln in lines if pattern.search(ln))\n    return float(matches) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Diversity of punctuation: number of distinct punctuation characters divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (contain a \"?\" character) as a proxy for interrogative style'\n    import re\n    if not text:\n        return 0.0\n    # Count sentences by splitting on .!? sequences; preserve ones that include '?'\n    raw_sentences = re.split(r'(?<=[.!?])\\s+', text)\n    sentences = [s for s in raw_sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if '?' in s)\n    return float(q_count) / len(sentences)\n", "def feature(text: str) -> float:\n    'Ratio of digit characters to total characters (numeric density)'\n    if not text:\n        return 0.0\n    digits = sum(1 for c in text if c.isdigit())\n    return digits / float(len(text))\n\n", "def feature(text: str) -> float:\n    'Estimated passive-voice occurrences per sentence using simple be + past-participle pattern'\n    import re\n    if not text:\n        return 0.0\n    # simple matches like \"is done\", \"was created\", \"were seen\", \"being written\", \"been eaten\"\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|has been|have been|had been)\\b\\s+\\w+(?:ed|en)\\b', text, flags=re.IGNORECASE)\n    sentence_count = max(1.0, len(re.findall(r'[.!?]', text)))\n    return float(len(matches)) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Density of common English stopwords (heuristic set) among tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','of','and','to','a','in','that','is','for','on','with','as','by','an','be','are','was','were','this','these','those','it','its','from','at'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    stopcount = sum(1 for w in words if w in stopwords)\n    return stopcount / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Count of ellipses (\"...\") normalized by number of sentences'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentence_count = max(1.0, len(re.findall(r'[.!?]', text)))\n    return float(ellipses) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Variance of sentence lengths measured in words (0 if single or no sentence)'\n    import re, math\n    if not text:\n        return 0.0\n    # split on sentence terminators\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sents) <= 1:\n        return 0.0\n    counts = [len(re.findall(r'\\b\\w+\\b', s)) for s in sents]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / float(len(counts))\n    var = sum((c - mean) ** 2 for c in counts) / float(len(counts))\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Ratio of repeated word bigrams to total bigrams (measures phrase repetition)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if len(words) < 2:\n        return 0.0\n    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n    total = len(bigrams)\n    unique = len(set(bigrams))\n    repeats = total - unique\n    return repeats / float(total)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (comma usage intensity)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentence_count = max(1.0, len(re.findall(r'[.!?]', text)))\n    return commas / sentence_count\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), robust to missing sentence punctuation'\n    import re\n    if not text:\n        return 0.0\n    words = text.split()\n    if not words:\n        return 0.0\n    # approximate sentences by splitting on sentence-ending punctuation\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback: treat whole text as one sentence\n        return float(len(words))\n    lengths = [len(s.split()) for s in sentences]\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (words per sentence) \u2014 measures variability of sentence structure'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(s.split()) for s in sentences]\n    if len(lengths) == 1:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain hyphens or dashes (-, \u2013 , \u2014), common in formal/compound terms'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    dash_chars = set(['-', '\u2013', '\u2014'])\n    dash_tokens = sum(1 for t in tokens if any(d in t for d in dash_chars))\n    return float(dash_tokens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens containing digits (years, measurements, enumerations) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_tokens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of titlecase words (Word or Proper nouns starting with uppercase followed by lowercase) to total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    titlecase = sum(1 for w in words if len(w) > 1 and w[0].isupper() and w[1:].islower())\n    return float(titlecase) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of a small set of formal/academic transition words (therefore, however, thus, furthermore, moreover) to total words'\n    import re\n    if not text:\n        return 0.0\n    transitions = {'therefore','however','thus','moreover','furthermore','consequently','additionally','hence','nevertheless','notwithstanding'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in transitions)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Diversity of bracket types used: fraction of distinct bracket groups present among (), [], {}, <> (0.0\u20131.0)'\n    if not text:\n        return 0.0\n    types = 0\n    if '(' in text or ')' in text:\n        types += 1\n    if '[' in text or ']' in text:\n        types += 1\n    if '{' in text or '}' in text:\n        types += 1\n    if '<' in text or '>' in text:\n        types += 1\n    return float(types) / 4.0\n\n", "def feature(text: str) -> float:\n    'Ratio of acronyms (consecutive 2+ uppercase letters or dotted acronyms like U.S.A.) to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w[\\w.]*\\b', text)\n    if not tokens:\n        return 0.0\n    # match bare acronyms like \"AI\" or \"NASA\" and dotted acronyms like \"U.S.\"\n    acr_pattern1 = re.compile(r'^[A-Z]{2,}$')\n    acr_pattern2 = re.compile(r'^(?:[A-Z]\\.){2,}[A-Z]?\\.?$')\n    acr_count = 0\n    for t in tokens:\n        if acr_pattern1.match(t) or acr_pattern2.match(t):\n            acr_count += 1\n    return float(acr_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Normalized count of ellipses (either \"...\" or \"\u2026\") per sentence'\n    import re\n    if not text:\n        return 0.0\n    ellipsis_count = text.count('...') + text.count('\u2026')\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    denom = max(1, len(sentences))\n    return float(ellipsis_count) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<= 3 words) indicating clipped or emphatic phrasing'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = s.split()\n        if len(words) <= 3:\n            short += 1\n    return float(short) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns to total words (indicates first-person narration)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours',\"i'm\",\"i've\",\"i'll\",\"i'd\",\"we're\",\"we've\",\"we'll\",\"we'd\"}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ed\" (simple past-tense heuristic)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[A-Za-z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ed_count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ed'))\n    return float(ed_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences containing parenthetical or dash interruptions (\"(\", \")\", \"\u2014\", \"--\")'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    interrupt_count = 0\n    for s in sentences:\n        if '(' in s or ')' in s or '\u2014' in s or '--' in s or ' - ' in s:\n            interrupt_count += 1\n    return float(interrupt_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (std / mean) \u2014 captures variability in lexical shape'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = float(sum(lengths)) / len(lengths)\n    if mean == 0.0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n", "def feature(text: str) -> float:\n    'Ratio of sensory-related verbs (see/hear/taste/smell/feel/look/listen/watch/gaze forms) to total words'\n    import re\n    if not text:\n        return 0.0\n    sensory = {'see','saw','seen','look','looked','looked','look','looking','watch','watched','watching',\n               'hear','heard','listen','listened','listen','listening','taste','tasted','smell','smelled',\n               'feel','felt','feelings','felt','gaze','gazed','gazing','glance','glanced'}\n    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of lines that look like list items (start with number+., -, or *)'\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    list_starts = 0\n    for line in lines:\n        if re.match(r'^\\s*\\d+\\.', line) or re.match(r'^\\s*[-*]\\s+', line):\n            list_starts += 1\n    return float(list_starts) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per 100 words to capture trailing/thoughty style'\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = max(1, len(words))\n    ellipses = text.count('...')\n    return float(ellipses) * 100.0 / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Average number of double-quote characters per sentence (captures quoted speech/dialogue)'\n    if not text:\n        return 0.0\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    quote_count = text.count('\"')\n    return float(quote_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain a semicolon or colon (proxy for clause complexity)'\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    complex_sentences = sum(1 for s in sentences if (';' in s) or (':' in s))\n    return float(complex_sentences) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total words) as a simple lexical diversity measure'\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of mid-sentence capitalized words (probable proper nouns) to total words \u2014 finds capitals not immediately after sentence end'\n    if not text:\n        return 0.0\n    total_words = max(1, len(re.findall(r'\\b\\w+\\b', text)))\n    mid_caps = 0\n    for m in re.finditer(r'\\b[A-Z][a-z]+\\b', text):\n        # find last non-space char before match\n        i = m.start() - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        if i >= 0 and text[i] in '.!?':\n            continue\n        # if at start or previous non-space char isn't sentence end, count as mid-sentence capital\n        mid_caps += 1\n    return float(mid_caps) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question or exclamation mark (captures emotive/interactive tone)'\n    if not text:\n        return 0.0\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    qex = text.count('?') + text.count('!')\n    return float(qex) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Density of em-dashes (\u2014) or double hyphens (--) per 100 words as a feature of stylistic punctuation'\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = max(1, len(words))\n    dashes = text.count('\u2014') + text.count('--')\n    return float(dashes) * 100.0 / float(word_count)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text)\n    if not tokens:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t.lower() in pronouns)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that end with \"ly\" (proxy for adverb/adjectival style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text)\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if len(t) > 2 and t.lower().endswith('ly'))\n    return float(ly_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ellipsis frequency: number of \"...\" occurrences per sentence (smoothed)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.\\.\\.', text))\n    # sentence count: punctuation-based fallback\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(ellipses) / sentences\n\n", "def feature(text: str) -> float:\n    'Ratio of interior capitalized words (likely proper nouns) excluding sentence-initial capitals'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences to identify sentence-initial words\n    sent_seps = re.split(r'(?<=[.!?])\\s+', text.strip())\n    interior_capitals = 0\n    interior_words = 0\n    for s in sent_seps:\n        words = re.findall(r\"\\b[A-Za-z][a-zA-Z']*\\b\", s)\n        if not words:\n            continue\n        for i, w in enumerate(words):\n            # skip the first word of a sentence\n            if i == 0:\n                continue\n            interior_words += 1\n            if w[0].isupper():\n                interior_capitals += 1\n    # fallback denominator: total word count minus sentence starts\n    denom = interior_words if interior_words > 0 else max(1, len(re.findall(r\"\\b[A-Za-z][a-zA-Z']*\\b\", text)) - len(sent_seps))\n    return float(interior_capitals) / denom if denom > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Proportion of short sentences (<=5 words) as a measure of punchy/informal style'\n    import re\n    if not text:\n        return 0.0\n    # naive sentence split\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r\"[A-Za-z']+\", s)\n        if len(words) <= 5:\n            short += 1\n    return float(short) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Strong expletive density: fraction of tokens that match a small list of swear/expletive words'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text)\n    if not tokens:\n        return 0.0\n    swears = {'shit','fuck','damn','crap','bitch','bastard','hell'}\n    count = sum(1 for t in tokens if t.lower() in swears)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain double quotes (dialogue sentence ratio)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    dialogue_sent = sum(1 for s in sentences if '\"' in s)\n    return float(dialogue_sent) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (can/could/will/would/shall/should/may/might/must)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text)\n    if not tokens:\n        return 0.0\n    modals = {'can','could','will','would','shall','should','may','might','must'}\n    count = sum(1 for t in tokens if t.lower() in modals)\n    return float(count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Proportion of words that are capitalized but not the first word of their sentence (mid-sentence proper noun density)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    mid_caps = 0\n    total = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        for i, w in enumerate(words):\n            if i == 0:\n                continue\n            total += 1\n            # check original substring for capitalization to avoid lowercasing issues\n            if w and w[0].isupper():\n                mid_caps += 1\n    if total == 0:\n        return 0.0\n    return float(mid_caps) / float(total)\n\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens appearing inside double-quoted spans to total tokens (approximate dialogue proportion)'\n    import re\n    if not text:\n        return 0.0\n    total_tokens = len(re.findall(r'\\b\\w+\\b', text))\n    if total_tokens == 0:\n        return 0.0\n    parts = re.split(r'\"', text)\n    inside_tokens = 0\n    # odd-indexed parts are inside quotes when quotes are paired\n    for i in range(1, len(parts), 2):\n        inside_tokens += len(re.findall(r'\\b\\w+\\b', parts[i]))\n    return float(inside_tokens) / float(total_tokens)\n\n\n", "def feature(text: str) -> float:\n    'Density of common dialogue tag verbs (said, asked, replied, whispered, muttered, shouted, cried, answered) per sentence'\n    import re\n    if not text:\n        return 0.0\n    tags = re.findall(r'\\b(?:said|asked|replied|whispered|muttered|shouted|cried|answered)\\b', text, flags=re.IGNORECASE)\n    sent_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(tags)) / float(sent_count)\n\n\n", "def feature(text: str) -> float:\n    'Average count of ellipses (\"...\") per sentence (ellipses density)'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(ellipses) / float(sent_count)\n\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (comma density as a proxy for clause complexity)'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(commas) / float(sent_count)\n\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are long words (8 or more characters) indicating lexical density'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) >= 8)\n    return float(long_count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Ratio of the longest sentence length (in words) to the mean sentence length (captures extreme sentence length vs average)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lens = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences]\n    if not lens:\n        return 0.0\n    mean_len = sum(lens) / float(len(lens))\n    if mean_len == 0:\n        return 0.0\n    longest = max(lens)\n    return float(longest) / float(mean_len)\n\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain parentheses (e.g., editorial notes or aside as in writing-prompt metadata)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = sum(1 for s in sentences if '(' in s or ')' in s)\n    return float(count) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Fraction of standalone lowercase \"i\" tokens (exact token \"i\") \u2014 often a human informal indicator vs. well-capitalized \"I\"'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    lower_i = sum(1 for w in words if w == 'i')\n    return float(lower_i) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of unique word types to total tokens (lexical diversity measure)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of common short function words (approximate stopword density) using a small built-in stopword set'\n    import re\n    if not text:\n        return 0.0\n    stopset = {'the','and','a','to','of','in','is','was','it','he','she','they','i','we','you','that','for','on','as','with','at','by','an','be','this','which','or','from','his','her','my','our','its'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopset)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Rate at which sentences (or start of text) begin with a lowercase letter \u2014 may indicate informal or inconsistent punctuation/capitalization'\n    import re\n    if not text:\n        return 0.0\n    # occurrences where a sentence-ending punctuation is followed by lowercase\n    mid = len(re.findall(r'[.!?]\\s+[a-z]', text))\n    start = 1 if re.search(r'^\\s*[a-z]', text) else 0\n    sent_count = text.count('.') + text.count('!') + text.count('?')\n    denom = max(1, sent_count + 1)  # +1 to allow for start-of-text\n    return float(mid + start) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count) \u2014 proxy for clause density and sentence complexity'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(commas) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Proportion of words matching a small set of literary/analytical terms (poem, poet, analysis, theme, etc.) \u2014 captures formal literary analysis style'\n    import re\n    if not text:\n        return 0.0\n    terms = {'poem','poet','poetry','theme','themes','analysis','analyses','stanza','meter','rhyme','verse','sonnet','imagery','technique','employs'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in terms)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that contain an internal uppercase letter or are mixed-case beyond the first character \u2014 may indicate named entities, acronyms, or formatting artifacts'\n    if not text:\n        return 0.0\n    words = text.split()\n    cleaned = [w.strip('''\"\u201c\u201d'()[]{}.,;:!?''') for w in words if w.strip('''\"\u201c\u201d'()[]{}.,;:!?''') != '']\n    if not cleaned:\n        return 0.0\n    count = 0\n    for w in cleaned:\n        if len(w) > 1 and any(c.isupper() for c in w[1:]):\n            count += 1\n    return float(count) / float(len(cleaned))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per sentence, capturing trailing thoughts and informal truncation'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(ellipses) / float(sentences)\n", "def feature(text: str) -> float:\n    'Diversity of punctuation used: distinct punctuation chars divided by typical punctuation set size'\n    import re\n    if not text:\n        return 0.0\n    # find all punctuation-like characters (not alnum or whitespace)\n    puncts = set(re.findall(r'([^\\w\\s])', text))\n    # Typical ASCII punctuation set size approximation\n    typical_punct_count = 32.0\n    return min(1.0, len(puncts) / typical_punct_count)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for syntactic complexity / descriptive prose)'\n    import re\n    if not text:\n        return 0.0\n    parts = [s for s in re.split(r'(?<=[.!?])\\s+|\\n+', text) if s.strip()]\n    sentences = max(1, len(parts))\n    comma_count = text.count(',')\n    return float(comma_count) / sentences\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per sentence (often used in informal/dialogue or trailing thoughts)'\n    import re\n    if not text:\n        return 0.0\n    parts = [s for s in re.split(r'(?<=[.!?])\\s+|\\n+', text) if s.strip()]\n    sentences = max(1, len(parts))\n    ellipses = text.count('...')\n    return float(ellipses) / sentences\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, mine, we, us, our) to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours','myself','ourselves'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that are common profanities (simple slang/profanity indicator)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    profane = {'fuck','fucking','fucks','shit','damn','bastard','ass','bitch','crap','hell'}\n    count = sum(1 for w in words if w in profane)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain an uppercase letter beyond the first character (mid-word capitals / name styling)'\n    import re\n    if not text:\n        return 0.0\n    raw_tokens = re.findall(r'\\S+', text)\n    if not raw_tokens:\n        return 0.0\n    count = 0\n    for t in raw_tokens:\n        if len(t) > 1 and any(ch.isupper() for ch in t[1:]):\n            count += 1\n    return float(count) / len(raw_tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<= 3 words) \u2014 often indicates dialog beats or terse lines'\n    import re\n    if not text:\n        return 0.0\n    parts = [s.strip() for s in re.split(r'(?<=[.!?])\\s+|\\n+', text) if s.strip()]\n    if not parts:\n        return 0.0\n    short = 0\n    for s in parts:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if len(words) <= 3:\n            short += 1\n    return float(short) / len(parts)\n\n", "def feature(text: str) -> float:\n    'Density of adverbial -ly words (tokens ending with \"ly\") as a proxy for descriptive/adverb-rich prose'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) > 3 and w.endswith('ly'))\n    return float(count) / len(words)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that start with an uppercase letter but are not the very first token (proxy for mid-sentence proper nouns / invented names)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w\u2019']+\\b\", text)\n    if len(tokens) <= 1:\n        return 0.0\n    count = 0\n    for i, t in enumerate(tokens):\n        if i == 0:\n            continue\n        if t and t[0].isupper():\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ly\" (adverb density), which often correlates with descriptive/narrative style'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w\u2019']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ly'))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per sentence (captures trailing suspense or informal style)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.\\.\\.+', text))\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(ellipses) / sentences\n\n", "def feature(text: str) -> float:\n    'Fraction of first-person pronouns (I, me, my, we, us, our) to detect personal narration'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w\u2019']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that repeat later within a 5-token window (local repetition rate), indicating emphasis or simple phrasing'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w\u2019']+\\b\", text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    repeat_count = 0\n    for i, t in enumerate(tokens):\n        window = tokens[i+1:i+6]\n        if t in window:\n            repeat_count += 1\n    return float(repeat_count) / n\n\n", "def feature(text: str) -> float:\n    'Fraction of modal verbs (may, might, could, would, should, must, can) as a marker of hedging or speculative style'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w\u2019']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'may', 'might', 'could', 'would', 'should', 'must', 'can'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Standard deviation of word lengths (population std) \u2014 captures variability in lexical choices'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w\u2019']+\\b\", text)\n    lengths = [len(t) for t in tokens if len(t) > 0]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord(c) > 127) \u2014 picks up typographic quotes, em-dashes, and some copy-paste artifacts'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / total\n", "def feature(text: str) -> float:\n    'Fraction of all personal-pronoun occurrences that are first-person (first/(first+third))'\n    try:\n        words = re.findall(r'\\w+', text.lower())\n    except Exception:\n        words = []\n    first_set = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    third_set = {'he', 'she', 'they', 'him', 'her', 'them', 'his', 'hers', 'their', 'theirs', 'it', 'its'}\n    first = sum(1 for w in words if w in first_set)\n    third = sum(1 for w in words if w in third_set)\n    denom = first + third\n    if denom == 0:\n        return 0.0\n    return float(first) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Density of explicit quotation marks (double-quote characters) per token \u2014 indicates dialogue'\n    if not text:\n        return 0.0\n    try:\n        words = re.findall(r'\\w+', text)\n    except Exception:\n        words = []\n    quotes = len(re.findall(r'[\"\u201c\u201d]', text))\n    denom = max(1, len(words))\n    return float(quotes) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Density of modal verbs (could, would, should, might, may, must, can, will) per token'\n    if not text:\n        return 0.0\n    try:\n        words = re.findall(r'\\w+', text.lower())\n    except Exception:\n        words = []\n    modals = {'could', 'would', 'should', 'might', 'may', 'must', 'can', 'will', 'shall'}\n    modal_count = sum(1 for w in words if w in modals)\n    denom = max(1, len(words))\n    return float(modal_count) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Proportion of word-tokens that are adverbs ending in -ly (adverbial density)'\n    if not text:\n        return 0.0\n    try:\n        words = re.findall(r\"\\w+\", text.lower())\n    except Exception:\n        words = []\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if w.endswith('ly') and len(w) > 2)\n    return float(ly_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause density/complexity)'\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # approximate sentence count by punctuation . ! ?\n    sent_count = text.count('.') + text.count('!') + text.count('?')\n    if sent_count == 0:\n        # if no sentence punctuation, treat whole text as one sentence\n        sent_count = 1\n    return float(comma_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Density of capitalized words that look like names or proper nouns (not sentence-start \"I\")'\n    try:\n        words = re.findall(r'\\b[A-Za-z][a-z]+\\b', text)\n    except Exception:\n        words = []\n    try:\n        total_words = re.findall(r'\\w+', text)\n    except Exception:\n        total_words = []\n    if not total_words:\n        return 0.0\n    # count capitalized words (first letter uppercase) excluding single-letter \"I\"\n    cap_count = sum(1 for w in words if w[0].isupper() and w.lower() != 'i')\n    return float(cap_count) / float(len(total_words))\n\n", "def feature(text: str) -> float:\n    'Density of sensory/action words (see, look, hear, feel, smell, taste, listen, watch, touch) per token'\n    if not text:\n        return 0.0\n    try:\n        words = re.findall(r'\\w+', text.lower())\n    except Exception:\n        words = []\n    if not words:\n        return 0.0\n    lex = {'see', 'saw', 'look', 'looked', 'looked', 'seen', 'hear', 'heard', 'listen', 'listened',\n           'feel', 'felt', 'touch', 'touched', 'smell', 'smelled', 'taste', 'tasted', 'watch', 'watched', 'gaze', 'glance', 'glanced'}\n    sens_count = sum(1 for w in words if w in lex)\n    return float(sens_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Balance between past-tense markers and present-tense markers: (past-present)/(past+present) in [-1,1]'\n    if not text:\n        return 0.0\n    try:\n        words = re.findall(r'\\w+', text.lower())\n    except Exception:\n        words = []\n    if not words:\n        return 0.0\n    past_markers = {'was', 'were', 'had', 'did'}\n    present_markers = {'is', 'am', 'are', 'have', 'has', 'do', 'does'}\n    past_ed = sum(1 for w in words if w.endswith('ed') and len(w) > 2)\n    past_count = past_ed + sum(1 for w in words if w in past_markers)\n    present_ing = sum(1 for w in words if w.endswith('ing') and len(w) > 3)\n    present_count = present_ing + sum(1 for w in words if w in present_markers)\n    denom = past_count + present_count\n    if denom == 0:\n        return 0.0\n    return float(past_count - present_count) / float(denom)\n", "def feature(text: str) -> float:\n    'Normalized count of Markdown emphasis markers (double asterisks or double underscores) to detect metadata/formatting like \"**October 7, 2046**\"'\n    if not text:\n        return 0.0\n    double_aster = text.count('**')\n    double_under = text.count('__')\n    # normalize by text length to keep value bounded and comparable across lengths\n    denom = max(1.0, len(text))\n    return (double_aster + double_under) / denom\n\n", "def feature(text: str) -> float:\n    'Binary indicator (0.0 or 1.0) for presence of a date-like header (e.g., \"October 7, 2046\" or ISO yyyy-mm-dd) within the first 120 characters'\n    import re\n    if not text:\n        return 0.0\n    head = text[:120]\n    date_patterns = [\n        r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s*\\d{4}\\b',\n        r'\\b\\d{4}-\\d{2}-\\d{2}\\b',\n        r'\\b\\d{1,2}\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}\\b',\n        r'\\b\\d{4}\\b'\n    ]\n    for p in date_patterns:\n        if re.search(p, head, flags=re.IGNORECASE):\n            return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain direct-speech quotes (double quotes \u201c or \") to measure dialogue density'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences (approximate)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', '\u201f', '\u201a')  # include some quote variants\n    quoted = sum(1 for s in sentences if any(q in s for q in quote_chars))\n    return quoted / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with first-person pronouns (I, I\\'m, we, we\\'re etc.), indicating first-person narration frequency'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    fp_forms = {\"i\", \"i'm\", \"i\u2019ve\", \"i've\", \"we\", \"we're\", \"we've\", \"we'll\", \"we'd\", \"weve\", \"were\"}\n    count = 0\n    for s in sentences:\n        m = re.search(r\"[A-Za-z']+\", s)\n        if not m:\n            continue\n        first = m.group(0).lower()\n        if first in fp_forms or first.startswith(\"i'\"):\n            count += 1\n    return count / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Adverb density: fraction of word tokens that end with \"ly\" (approximate adverb/adjectival usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text)\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.lower().endswith('ly'))\n    return ly_count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per sentence (signals trailing thoughts, dramatic pauses, or truncation)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    denom = max(1.0, len(sentences))\n    return ellipses / denom\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are capitalized in medial positions (Titlecase tokens not at sentence start) \u2014 signals named entities or headings'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    medial_tokens = []\n    for s in sentences:\n        toks = re.findall(r\"[A-Za-z']+\", s)\n        if len(toks) <= 1:\n            continue\n        # exclude the first token of the sentence\n        medial_tokens.extend(toks[1:])\n    if not medial_tokens:\n        return 0.0\n    title_like = sum(1 for t in medial_tokens if t[0].isupper() and t[1:].islower())\n    # normalize by total token count to be comparable across texts\n    total_tokens = max(1, len(re.findall(r\"[A-Za-z']+\", text)))\n    return title_like / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Max token repetition ratio: frequency of the most common token divided by total tokens (measures repetition/redundancy)'\n    import re, collections\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"[A-Za-z0-9']+\", text) if t]\n    if not tokens:\n        return 0.0\n    counter = collections.Counter(tokens)\n    most_common = counter.most_common(1)[0][1]\n    return most_common / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (captures sentence-internal complexity)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sent_count = text.count('.') + text.count('!') + text.count('?')\n    sent_count = sent_count if sent_count > 0 else 1\n    return float(commas) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, we, our, us, mine) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+'\\w+|\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'our', 'us', 'ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of past-perfect constructions (\"had\" + past participle-like token ending in -ed) to tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    matches = re.findall(r'\\bhad\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending with -ing (progressive/gerund density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    ing_count = sum(1 for w in words if len(w) > 3 and w.endswith('ing'))\n    return float(ing_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of adverbs approximated by tokens ending in -ly'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if w.endswith('ly') and len(w) > 2)\n    return float(ly_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Passive-voice indicator: matches of (was|were|is|are|be|been|being) + past-participle-like (-ed) per sentence'\n    import re\n    if not text:\n        return 0.0\n    sent_segs = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sent_segs:\n        sent_segs = [text]\n    matches = 0\n    for s in sent_segs:\n        matches += len(re.findall(r'\\b(?:was|were|is|are|be|been|being)\\s+\\w+ed\\b', s, flags=re.IGNORECASE))\n    return float(matches) / float(len(sent_segs)) if sent_segs else 0.0\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of punctuation counts per sentence (captures punctuation variability)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        sentences = [text]\n    counts = []\n    for s in sentences:\n        counts.append(sum(1 for c in s if not c.isalnum() and not c.isspace()))\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n", "def feature(text: str) -> float:\n    'Ratio of adverbs approximated by words ending in \"ly\" (case-insensitive) to total word count'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    ly = sum(1 for w in words if w.lower().endswith('ly'))\n    return float(ly) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (falls back to commas per word if no clear sentence punctuation)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences by punctuation or newlines\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+|\\n+' , text) if s.strip()]\n    commas = text.count(',')\n    if sentences:\n        return float(commas) / float(len(sentences))\n    # fallback to words\n    words = re.findall(r'\\b\\w+\\b', text)\n    return float(commas) / float(max(1, len(words)))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word types (lowercased) divided by total word tokens'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of long words (length >= 8 characters) as a proxy for vocabulary complexity'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 8)\n    return float(long_words) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proportion of titlecase tokens that occur NOT at the start of sentences (counts interior capitalized tokens)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences (preserve newlines as separators too)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+|\\n+' , text) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()] if text.strip() else []\n    # collect first words to exclude\n    first_words = set()\n    for s in sentences:\n        m = re.search(r'\\b\\w+\\b', s)\n        if m:\n            first_words.add(m.group(0))\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    interior_titlecase = 0\n    for t in tokens:\n        if t in first_words:\n            continue\n        if t[0].isupper() and t[1:].islower():\n            interior_titlecase += 1\n    return float(interior_titlecase) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of passive-like patterns (was/were/is/are/been + word ending in \"ed\") per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+|\\n+' , text) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()] if text.strip() else []\n    matches = re.findall(r'\\b(?:was|were|is|are|has been|have been|had been|been)\\s+\\w+ed\\b', text, flags=re.I)\n    return float(len(matches)) / float(max(1, len(sentences)))\n\n", "def feature(text: str) -> float:\n    'Proportion of punctuation that belongs to multi-character punctuation clusters (e.g., \"...\", \"\u2014?\", \"?!\")'\n    import re\n    if not text:\n        return 0.0\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    clusters = re.findall(r'[^\\w\\s]{2,}', text)\n    return float(len(clusters)) / float(total_punct)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words; returns 0.0 for a single or empty sentence'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+|\\n+' , text) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()] if text.strip() else []\n    lens = []\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        lens.append(len(words))\n    if len(lens) <= 1:\n        return 0.0\n    mean = sum(lens) / float(len(lens))\n    if mean == 0:\n        return 0.0\n    variance = sum((l - mean) ** 2 for l in lens) / float(len(lens))\n    std = math.sqrt(variance)\n    return std / mean\n", "def feature(text: str) -> float:\n    'Ratio of tokens that are present-participles (end in \"ing\"), a proxy for ongoing-action style'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9']+\", text.lower())\n    if not tokens:\n        return 0.0\n    cnt = 0\n    for t in tokens:\n        if len(t) > 3 and t.endswith('ing'):\n            cnt += 1\n    return float(cnt) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short fragments (fewer than 4 words)'\n    import re\n    if not text:\n        return 0.0\n    # Split on terminal punctuation; keep fragments. If none found, treat whole text as one sentence.\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()] if text.strip() else []\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r\"[A-Za-z0-9']+\", s)\n        if len(words) < 4:\n            short += 1\n    return float(short) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Density of double/curly quotation marks per sentence (indicator of dialogue or quoted speech)'\n    import re\n    if not text:\n        return 0.0\n    quote_count = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    # treat sentences as split by terminal punctuation; ensure at least one to avoid div by zero\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    return float(quote_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Indicator (0.0/1.0) for presence of a Reddit-style username (/u/username), common in user-generated posts'\n    import re\n    if not text:\n        return 0.0\n    # case-insensitive search for /u/username or u/username at beginning\n    if re.search(r'(?i)(?:^|[^A-Za-z0-9])/?u/[_A-Za-z0-9-]+', text):\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Normalized count of ellipses or long dot sequences (\"..\" or \"...\") per sentence'\n    import re\n    if not text:\n        return 0.0\n    dot_seqs = re.findall(r'\\.{2,}', text)\n    seq_count = len(dot_seqs)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    return float(seq_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Density of violent/visceral lexical items (small curated lexicon) per token, useful for gore/violence signal'\n    import re\n    if not text:\n        return 0.0\n    lexicon = ['kill', 'killed', 'murder', 'axe', 'axe', 'head', 'decap', 'decapit', 'blood', 'bleed', 'stab', 'knife', 'shot', 'rifle', 'thud', 'gore', 'cut', 'swing', 'chop']\n    tokens = re.findall(r\"[A-Za-z0-9']+\", text.lower())\n    if not tokens:\n        return 0.0\n    cnt = 0\n    for t in tokens:\n        for root in lexicon:\n            if root in t:\n                cnt += 1\n                break\n    return float(cnt) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique tokens divided by total tokens) as a measure of lexical diversity (0..1)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9']+\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation that appears in sequences of two or more (e.g., \"--\", \"?!\", \"..\", \"...\"), captures stylized punctuation'\n    import re\n    if not text:\n        return 0.0\n    seqs = re.findall(r'[^A-Za-z0-9\\s]{2,}', text)\n    seq_count = len(seqs)\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    return float(seq_count) / float(total_punct)\n", "def feature(text: str) -> float:\n    'Proportion of characters that lie inside parentheses (measures parenthetical/planned content like \"(50 words)\")'\n    import re\n    if not text:\n        return 0.0\n    parens = re.findall(r'\\([^)]*\\)', text)\n    if not parens:\n        return 0.0\n    inside_chars = sum(len(p) - 2 for p in parens)  # exclude parentheses chars themselves\n    total_chars = len(text)\n    return float(inside_chars) / total_chars if total_chars > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, we, my, our, us, me, mine, ours) to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+'?\\w*\\b\", text.lower())\n    if not words:\n        return 0.0\n    fp = re.findall(r'\\b(?:i|we|me|us|my|our|mine|ours)\\b', text.lower())\n    return float(len(fp)) / len(words)\n\n", "def feature(text: str) -> float:\n    'Approximate fraction of sentences containing passive-voice-like constructions (is/are/was + past-participle proxy)'\n    import re, math\n    if not text:\n        return 0.0\n    # split into rough sentences\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    passive_pattern = re.compile(r'\\b(?:is|are|was|were|be|been|being|has been|have been|had been)\\b\\s+\\w+(?:ed|en|n)\\b', re.IGNORECASE)\n    passive_count = sum(1 for s in sents if passive_pattern.search(s))\n    return float(passive_count) / len(sents)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens (vocabulary richness)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / len(words)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words (higher = more variability)'\n    import re, math\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in sents]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    variance = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(variance)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Binary-ish indicator (0.0 or 1.0) for presence of explicit word-count or planning tokens like \"(50 words)\" or \"150 words\"'\n    import re\n    if not text:\n        return 0.0\n    if re.search(r'\\(\\s*\\d+\\s*words\\s*\\)', text, re.IGNORECASE) or re.search(r'\\b\\d+\\s+words\\b', text, re.IGNORECASE):\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Ratio of all-caps acronyms (2-6 letters) to total word tokens (captures academic abbreviations like EU, USA)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    acronyms = re.findall(r'\\b[A-Z]{2,6}\\b', text)\n    return float(len(acronyms)) / len(words)\n\n", "def feature(text: str) -> float:\n    'Average number of colons \":\" per sentence (high when sections or label-like constructions are present)'\n    import re\n    if not text:\n        return 0.0\n    sent_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    colon_count = text.count(':')\n    return float(colon_count) / sent_count\n", "def feature(text: str) -> float:\n    'Proportion of newline characters relative to total characters (captures poems/linebreak-heavy text)'\n    if not text:\n        return 0.0\n    newlines = text.count('\\n')\n    return float(newlines) / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are long (7+ characters) \u2014 higher values suggest more descriptive or formal vocabulary'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) >= 7)\n    return float(long_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens containing at least two uppercase letters (acronyms or SHOUTED tokens) to total tokens'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    def upper_count(tok):\n        return sum(1 for c in tok if c.isupper())\n    acr_count = sum(1 for t in tokens if upper_count(t) >= 2)\n    return float(acr_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation symbols used divided by total punctuation occurrences (0-1)'\n    import string\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence) using simple terminal-punctuation sentence splitting'\n    import re\n    words = len(re.findall(r'\\w+', text))\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(words) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Ratio of common past-tense auxiliaries (\"was\",\"were\",\"had\",\"did\",\"been\") to total tokens (simple past-tense signal)'\n    import re\n    past_aux = {'was', 'were', 'had', 'did', 'been'}\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in past_aux)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of line lengths in words \u2014 captures poetic/line-oriented structure'\n    import re, math\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    counts = []\n    for L in lines:\n        if L.strip():\n            counts.append(len(re.findall(r'\\w+', L)))\n    if not counts:\n        return 0.0\n    mean = sum(counts) / float(len(counts))\n    if mean == 0:\n        return 0.0\n    variance = sum((c - mean) ** 2 for c in counts) / float(len(counts))\n    std = math.sqrt(variance)\n    return float(std) / float(mean)\n", "def feature(text: str) -> float:\n    'Proportion of tokens that end with -ly (approximate adverb density)'\n    import re\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly'))\n    return ly_count / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Comma density: average number of commas per sentence (commas / max(1, sentences))'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # approximate sentence count\n    sent_count = len([s for s in re.split(r'[.!?]+', text) if s.strip()])\n    sent_count = max(1, sent_count)\n    return comma_count / sent_count\n\n", "def feature(text: str) -> float:\n    'Normalized density of long dashes (em-dash or en-dash or double hyphen) per sentence'\n    import re\n    if not text:\n        return 0.0\n    dash_count = text.count('\u2014') + text.count('\u2013') + text.count('--')\n    sent_count = len([s for s in re.split(r'[.!?]+', text) if s.strip()])\n    sent_count = max(1, sent_count)\n    return dash_count / sent_count\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of ellipses (\"...\" or longer) normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    ellipses = re.findall(r'\\.{3,}', text)\n    sent_count = len([s for s in re.split(r'[.!?]+', text) if s.strip()])\n    sent_count = max(1, sent_count)\n    return len(ellipses) / sent_count\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (words longer than 7 characters) \u2014 lexical complexity indicator'\n    import re\n    tokens = re.findall(r\"\\b\\w+\\b\", text)\n    if not tokens:\n        return 0.0\n    long_words = sum(1 for t in tokens if len(t) > 7)\n    return long_words / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of capitalized words occurring mid-sentence (approximate proper-noun density)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    mid_caps = 0\n    mid_total = 0\n    for s in sents:\n        words = re.findall(r\"\\b\\w+\\b\", s)\n        if not words:\n            continue\n        for w in words[1:]:  # exclude first word of sentence\n            mid_total += 1\n            if w[0].isupper():\n                mid_caps += 1\n    if mid_total == 0:\n        return 0.0\n    return mid_caps / mid_total\n\n", "def feature(text: str) -> float:\n    'Fraction of words that occur inside parentheses (word-level parenthetical density)'\n    import re\n    tokens = re.findall(r\"\\b\\w+\\b\", text)\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    parens = re.findall(r'\\((.*?)\\)', text, re.DOTALL)\n    inside_words = 0\n    for p in parens:\n        inside_words += len(re.findall(r\"\\b\\w+\\b\", p))\n    return inside_words / total\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return unique / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are long (7 or more characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 7)\n    return long_count / len(words)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens containing any digit (numeric token prevalence)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if re.search(r'\\d', t))\n    return num_tokens / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Approximate passive/past constructions: occurrences of \"was|were <word>ed\" per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    matches = re.findall(r'\\b(?:was|were)\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    return len(matches) / len(words)\n\n", "def feature(text: str) -> float:\n    'Density of double-quoted spans: number of double-quote pairs normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    quotes = text.count('\"')\n    # count pairs (each pair uses two quotes)\n    pairs = quotes // 2\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return pairs / sentences\n\n", "def feature(text: str) -> float:\n    'Fraction of sensory/descriptive verbs (saw, heard, smelled, felt, tasted, noticed, watched)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    sensory = {'saw', 'heard', 'smelled', 'smelt', 'felt', 'tasted', 'noticed', 'watched', 'sensed', 'observed'}\n    count = sum(1 for t in tokens if t in sensory)\n    return count / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of titlecase tokens that appear not at sentence starts (internal capitalization ratio)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    total_tokens = 0\n    internal_titlecase = 0\n    for s in sentences:\n        toks = s.split()\n        if not toks:\n            continue\n        for i, tok in enumerate(toks):\n            total_tokens += 1\n            if i == 0:\n                continue\n            # consider a token titlecase if it starts with uppercase letter and has lowercase after\n            if len(tok) > 1 and tok[0].isupper() and any(c.islower() for c in tok[1:]):\n                internal_titlecase += 1\n    if total_tokens == 0:\n        return 0.0\n    return internal_titlecase / total_tokens\n\n", "def feature(text: str) -> float:\n    'Indicator (0.0 or 1.0) whether the text ends with an ellipsis (truncated narrative marker)'\n    if not text:\n        return 0.0\n    return 1.0 if text.strip().endswith('...') else 0.0\n", "def feature(text: str) -> float:\n    'Ratio of tokens that are first-person pronouns or first-person contractions (I, me, my, we, our, I\\'m, I\\'ve, etc.)'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    tokens = re.findall(r\"\\b[\\w']+\\b\", lower)\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours',\"i'm\",\"im\",\"i've\",\"ive\",\"i'd\",\"i'll\",\"we're\",\"weve\",\"we'll\"}\n    count = sum(1 for t in tokens if t in first_person)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of common conversational filler words (just, like, actually, basically, literally, you, know, etc.) as fraction of tokens'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    tokens = re.findall(r\"\\b[\\w']+\\b\", lower)\n    if not tokens:\n        return 0.0\n    fillers = {'just','like','actually','basically','literally','really','you','know','sort','kinda','i mean','honestly'}\n    # count tokens that are filler (single-token matches)\n    count = sum(1 for t in tokens if t in fillers)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (useful to detect accented characters or Unicode usage like \"\u00f1\")'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return non_ascii / float(total)\n\n", "def feature(text: str) -> float:\n    'Ratio short_to_long_sentences = (# sentences with <=3 words) / (1 + # sentences with >=12 words); smooths to avoid divide-by-zero'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    long = 0\n    for s in sentences:\n        words = re.findall(r\"\\b[\\w']+\\b\", s)\n        lw = len(words)\n        if lw <= 3:\n            short += 1\n        if lw >= 12:\n            long += 1\n    return short / float(1 + long)\n\n", "def feature(text: str) -> float:\n    'Normalized count of repeated-punctuation sequences (e.g., \"...\", \"!!\", \"??\") per 100 characters'\n    import re\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    # find runs of the same punctuation of length >= 2\n    runs = re.findall(r'([!?.]{2,}|\\.{2,})', text)\n    # include any punctuation character repeated (like \"!!!\" or \"??\")\n    count = len(runs)\n    return (count / float(total_chars)) * 100.0\n\n", "def feature(text: str) -> float:\n    'Fraction of word-trigrams where all three tokens are common stopwords (indicative of formulaic/functional phrasing)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','of','to','a','in','is','it','that','i','you','for','on','with','as','was','are','be','this','but','not','so','have','had'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if len(tokens) < 3:\n        return 0.0\n    total_trigrams = len(tokens) - 2\n    count = 0\n    for i in range(total_trigrams):\n        if tokens[i] in stopwords and tokens[i+1] in stopwords and tokens[i+2] in stopwords:\n            count += 1\n    return count / float(total_trigrams)\n\n", "def feature(text: str) -> float:\n    'Ratio of commas to periods (commas divided by 1+periods) to detect heavy clause usage versus sentence boundaries'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    periods = text.count('.')\n    return commas / float(1 + periods)\n\n", "def feature(text: str) -> float:\n    'Approximate adverb density: fraction of tokens ending with \"ly\" (simple proxy for adverb/adjective use)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n    return ly_count / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Density of common profanity (counts of common swear words divided by token count)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    swear_list = {'fuck', 'fucking', 'shit', 'damn', 'goddamn', 'piss', 'bastard', 'crap'}\n    count = sum(1 for t in tokens if t in swear_list)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that appear within or adjacent to ellipses (\"...\") occurrences'\n    import re\n    if not text:\n        return 0.0\n    ell_count = text.count('...')\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    # normalize by token count so long texts are comparable\n    return float(ell_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total word tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (sentences split on .!?; fallback to 1 if no sentence boundary)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences using punctuation, keep non-empty\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    words = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not words:\n        return 0.0\n    if not sents:\n        return float(len(words))\n    avg = float(len(words)) / float(len(sents))\n    return avg\n\n", "def feature(text: str) -> float:\n    'Ratio of double-quote characters (\") to token count, indicating direct quoting or reported speech'\n    import re\n    if not text:\n        return 0.0\n    quote_count = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    return float(quote_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of sensory verbs (see/hear/smell/taste/look/watch/notice/stare/gaze and common inflections) per token'\n    import re\n    if not text:\n        return 0.0\n    pattern = r'\\b(?:see|saw|seeing|heard|hear|hearing|smell|smelled|smelling|taste|tasted|tasting|look|looked|looking|watch|watched|watching|notice|noticed|noticing|stare|staring|stared|gaze|gazed|gazing)\\b'\n    matches = re.findall(pattern, text.lower())\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    return float(len(matches)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of hyphenated or double-dash tokens (contains \"-\" or \"--\") among all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\S+\", text)  # coarse tokens to capture punctuation-connected tokens\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters used divided by total punctuation occurrences'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    unique = len(set(puncts))\n    total = len(puncts)\n    return float(unique) / float(total)\n", "def feature(text: str) -> float:\n    'Second-person pronoun ratio: fraction of tokens that are second-person pronouns (you, your, yours, you\\'re, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'\u2019]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    second = {'you', \"you're\", 'youre', 'your', 'yours', \"y'all\", 'ya'}\n    count = sum(1 for t in tokens if t in second)\n    return count / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count; sentences fallback to 1 if none)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # split on sentence end punctuation; fallback to whole text if none\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    return comma_count / sent_count\n\n", "def feature(text: str) -> float:\n    'Sentence-start pronoun ratio: fraction of sentences whose first token is a pronoun'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        # fallback: treat whole text as one sentence\n        sentences = [text.strip()]\n    pronouns = {'i','you','he','she','it','they','we','me','him','her','us','them','my','your','their','our'}\n    starts = 0\n    total = 0\n    for s in sentences:\n        m = re.search(r\"\\b[\\w'\u2019]+\\b\", s.lower() or '')\n        if m:\n            total += 1\n            if m.group(0) in pronouns:\n                starts += 1\n    if total == 0:\n        return 0.0\n    return starts / total\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation count (0 if none)'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    if not puncts:\n        return 0.0\n    unique_types = len(set(puncts))\n    total = len(puncts)\n    return unique_types / total\n\n", "def feature(text: str) -> float:\n    'Consecutive duplicate token ratio: fraction of adjacent token pairs that are identical (measures stuttering/repetition)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'\u2019]+\\b\", text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    dup = sum(1 for a, b in zip(tokens, tokens[1:]) if a == b)\n    # normalize by number of adjacent pairs\n    pairs = len(tokens) - 1\n    return dup / pairs\n\n", "def feature(text: str) -> float:\n    'Paragraph length coefficient of variation: std/mean of paragraph word counts (0 if fewer than 2 paragraphs or no words)'\n    import re, math\n    if not text:\n        return 0.0\n    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n    if len(paragraphs) < 2:\n        return 0.0\n    counts = []\n    for p in paragraphs:\n        words = re.findall(r\"\\b[\\w'\u2019]+\\b\", p)\n        counts.append(len(words))\n    # filter zero-length paragraphs\n    counts = [c for c in counts if c > 0]\n    if len(counts) < 2:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    variance = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(variance)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'First-person sentence ratio: fraction of sentences that contain a first-person pronoun token (\"i\" or \"we\" etc.)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    first_pronouns = {'i','we','me','us','my','mine','our','ours'}\n    hit = 0\n    total = 0\n    for s in sentences:\n        tokens = re.findall(r\"\\b[\\w'\u2019]+\\b\", s.lower())\n        if not tokens:\n            continue\n        total += 1\n        if any(t in first_pronouns for t in tokens):\n            hit += 1\n    if total == 0:\n        return 0.0\n    return hit / total\n", "def feature(text: str) -> float:\n    'Normalized frequency of storytelling/genre-opening phrases like \"once upon\" or \"faraway\"'\n    import re\n    if not text:\n        return 0.0\n    low = text.lower()\n    phrases = ['once upon', 'faraway', 'far away', 'childhood memory', 'once upon a time']\n    occ = sum(low.count(p) for p in phrases)\n    words = re.findall(r'\\w+', low)\n    return float(occ) / float(max(1, len(words)))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: count of \"...\" sequences normalized by word count (captures trailing or dramatic fragments)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    ellipses = text.count('...')\n    return float(ellipses) / float(max(1, len(words)))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total words (lexical variety)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proportion of short lines (<6 words) among non-empty lines (detects poetry/line-based structure)'\n    import re\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    nonempty = [ln for ln in lines if re.search(r'\\w', ln)]\n    if not nonempty:\n        return 0.0\n    short = 0\n    for ln in nonempty:\n        words = re.findall(r'\\w+', ln)\n        if 0 < len(words) < 6:\n            short += 1\n    return float(short) / float(len(nonempty))\n\n", "def feature(text: str) -> float:\n    'Normalized frequency of simple past auxiliaries \"was\"/\"were\" per word (proxy for past-tense narrative framing)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    past_aux = len(re.findall(r'\\bwas\\b|\\bwere\\b', text.lower()))\n    return float(past_aux) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Parenthetical punctuation density: count of \"(\" or \")\" normalized by word count (indicates asides, editorial tone)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    parens = text.count('(') + text.count(')')\n    return float(parens) / float(len(words))\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I/me/my/we/us) to total token count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    n = len(words)\n    if n == 0:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / n\n\n", "def feature(text: str) -> float:\n    'Density of numeric/list markers like \"1.\", \"1)\", \"Rule #1\", \"#1\" normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    n = len(words)\n    if n == 0:\n        return 0.0\n    # patterns: digits followed by .:) or a hash-number, or 'rule' followed by #digits\n    markers = re.findall(r'\\b\\d+[:\\.\\)]|\\#\\d+|\\b[rR]ule\\s*#?\\d+', text)\n    return float(len(markers)) / n\n\n", "def feature(text: str) -> float:\n    'Ellipsis (\"...\") occurrences per token, capturing trailing/intentional pauses'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    n = len(words)\n    if n == 0:\n        return 0.0\n    ellipses = text.count('...')\n    # Also catch spaced ellipsis like '. . .' (approx)\n    ellipses += len(re.findall(r'\\.\\s*\\.\\s*\\.', text)) - text.count('...')\n    return float(ellipses) / n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are adverbs ending in -ly (simple adverb density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    n = len(words)\n    if n == 0:\n        return 0.0\n    count = sum(1 for w in words if len(w) > 2 and w.endswith('ly'))\n    return float(count) / n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in -ing (present participle / gerund density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    n = len(words)\n    if n == 0:\n        return 0.0\n    count = sum(1 for w in words if len(w) > 3 and w.endswith('ing'))\n    return float(count) / n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<= 3 words), capturing staccato/dialogue-like style'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences on punctuation; keep non-empty\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    m = len(sentences)\n    if m == 0:\n        return 0.0\n    short_count = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if len(words) <= 3:\n            short_count += 1\n    return float(short_count) / m\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are titlecase words (Initial capital then lowercase), indicating headings/labels'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    titlecase_count = sum(1 for t in tokens if len(t) >= 2 and t[0].isupper() and t[1:].islower())\n    return float(titlecase_count) / n\n\n", "def feature(text: str) -> float:\n    'Max run length of consecutive sentences that start with the same first 3-word phrase, normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences and extract normalized starts\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    m = len(sentences)\n    if m == 0:\n        return 0.0\n    starts = []\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s.lower())\n        start = ' '.join(words[:3])\n        starts.append(start)\n    max_run = 1\n    cur_run = 1\n    for i in range(1, len(starts)):\n        if starts[i] and starts[i] == starts[i-1]:\n            cur_run += 1\n            if cur_run > max_run:\n                max_run = cur_run\n        else:\n            cur_run = 1\n    return float(max_run) / m\n", "def feature(text: str) -> float:\n    'Lexical diversity: ratio of unique word types to total tokens (lowercased)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Sentence length coefficient of variation (std / mean) using token counts per sentence'\n    import re, math\n    if not text:\n        return 0.0\n    # split on sentence-ending punctuation (keep robustness to multiple punctuation)\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    variance = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(variance)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # count sentences by punctuation; fallback to 1 to avoid division by zero\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are ellipses occurrences (\"...\") relative to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    ellipsis_count = text.count('...')\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(ellipsis_count)\n    return float(ellipsis_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of words that appear inside parentheses (words_in_parentheses / total_words)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    inside = 0\n    # find all parenthetical contents and count words inside\n    for content in re.findall(r'\\((.*?)\\)', text, re.S):\n        inside += len(re.findall(r'\\w+', content))\n    return float(inside) / float(total)\n\n", "def feature(text: str) -> float:\n    'Relative imbalance of parentheses: absolute difference between \"(\" and \")\" counts normalized by total parentheses'\n    if not text:\n        return 0.0\n    left = text.count('(')\n    right = text.count(')')\n    total = left + right\n    if total == 0:\n        return 0.0\n    return float(abs(left - right)) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen (hyphenated words) \u2014 captures compound constructions'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphenated = sum(1 for t in tokens if '-' in t)\n    return float(hyphenated) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Tail of long-word distribution: fraction of words longer than 12 characters'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 12)\n    return float(long_count) / float(len(words))\n", "def feature(text: str) -> float:\n    'Ratio of curly (smart) apostrophes/quotes (\u2019 \u2018 \u201c \u201d) to all apostrophe/quote characters'\n    if not text:\n        return 0.0\n    curly = sum(text.count(ch) for ch in ['\u2019', '\u2018', '\u201c', '\u201d'])\n    straight = sum(text.count(ch) for ch in [\"'\", '\"'])\n    total = curly + straight\n    if total == 0:\n        return 0.0\n    return float(curly) / total\n\n", "def feature(text: str) -> float:\n    'Average sentence length measured in words (words per sentence), robust to missing punctuation'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # Split into sentence-like chunks by punctuation, preserve chunks that have words\n    chunks = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not chunks:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', c)) for c in chunks]\n    # avoid division by zero if somehow all chunks have zero words\n    word_counts = [c for c in word_counts if c > 0]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts)) / len(word_counts)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences exhibiting a simple passive-voice pattern (e.g., \"was observed\", \"is shown\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    passive_re = re.compile(r'\\b(?:is|are|was|were|be|been|being|has been|have been|had been)\\b\\s+[A-Za-z0-9_-]+ed\\b', re.IGNORECASE)\n    passive_count = 0\n    for s in sentences:\n        if passive_re.search(s):\n            passive_count += 1\n    return float(passive_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation characters that are followed by exactly one space (consistency of spacing after punctuation)'\n    import re\n    if not text:\n        return 1.0\n    punct_chars = '.!,;:?'\n    total = 0\n    consistent = 0\n    for i, ch in enumerate(text[:-1]):\n        if ch in punct_chars:\n            total += 1\n            # check next character(s) to see if exactly one space follows (and then a non-space)\n            if text[i+1] == ' ':\n                # check if the following char is not another space (exactly one)\n                if i+2 < len(text):\n                    if text[i+2] != ' ':\n                        consistent += 1\n                else:\n                    # punctuation at end followed by single space (rare) - count as consistent\n                    consistent += 1\n    if total == 0:\n        return 1.0\n    return float(consistent) / total\n\n", "def feature(text: str) -> float:\n    'Ratio of characters inside balanced parentheses pairs to total characters (how much content is parenthetical)'\n    import re\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    # find contents inside parentheses (simple non-nested approach)\n    contents = re.findall(r'\\(([^)]*)\\)', text)\n    inside_len = sum(len(s) for s in contents)\n    return float(inside_len) / total_chars\n\n", "def feature(text: str) -> float:\n    'Density of a small curated technical/discipline wordlist (fraction of tokens matching common academic/scientific/legal words)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    keyword_set = {\n        'protein','cell','axis','signaling','vertebrate','hypothesis','experiment','study','analysis','method',\n        'perception','sensation','limb','surrealism','artwork','dream','legal','liability','damages','vehicle',\n        'shock','theory','evidence','result','data','conclusion','sample','participant'\n    }\n    count = sum(1 for t in tokens if t in keyword_set)\n    return float(count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that begin with a quotation mark or a dash (dialogue/line-oriented style)'\n    if not text:\n        return 0.0\n    try:\n        lines = [ln for ln in text.splitlines() if ln.strip()]\n        if not lines:\n            return 0.0\n        starts = 0\n        for ln in lines:\n            s = ln.lstrip()\n            if s.startswith('\"') or s.startswith(\"'\") or s.startswith('-') or s.startswith('\u2014') or s.startswith('\u2013'):\n                starts += 1\n        return float(starts) / len(lines)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Proportion of the first up-to-5 non-empty lines that look like short title/heading lines (<=6 words and title-cased)'\n    if not text:\n        return 0.0\n    try:\n        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n        if not lines:\n            return 0.0\n        head = lines[:5]\n        matches = 0\n        for ln in head:\n            words = ln.split()\n            if 0 < len(words) <= 6:\n                # consider title-cased if most words start with uppercase\n                cap_count = sum(1 for w in words if len(w)>0 and w[0].isupper())\n                if cap_count >= max(1, len(words)//2):\n                    matches += 1\n        return float(matches) / len(head)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Average number of ellipsis (\"...\") occurrences per sentence (ellipses often in stylized or truncated text)'\n    if not text:\n        return 0.0\n    try:\n        import re\n        ell = len(re.findall(r'\\.\\.\\.|\u2026', text))\n        # approximate sentence count\n        sent_seps = re.findall(r'[.!?]+', text)\n        sent_count = max(1, len(sent_seps))\n        return float(ell) / sent_count\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Ratio of capitalized words that do NOT follow a sentence terminator (likely proper nouns or headings mid-sentence)'\n    if not text:\n        return 0.0\n    try:\n        import re\n        words = list(re.finditer(r'\\b[A-Za-z][A-Za-z\\'-]*\\b', text))\n        if not words:\n            return 0.0\n        cap_mid = 0\n        total = 0\n        for m in words:\n            w = m.group(0)\n            # consider only words with initial capital and at least one lowercase letter after\n            if not (w[0].isupper() and any(ch.islower() for ch in w[1:])):\n                continue\n            # check preceding non-space char to see if it's a sentence boundary\n            prefix = text[:m.start()].rstrip()\n            if not prefix:\n                # start of text => sentence start, skip counting as mid-sentence\n                pass\n            else:\n                last = prefix[-1]\n                if last not in '.!?':\n                    cap_mid += 1\n            total += 1\n        # avoid division by zero: use total words with capitalization considered; if zero, use overall word count\n        if total == 0:\n            # fallback: ratio of any capitalized words to total words\n            all_words = re.findall(r'\\b\\w+\\b', text)\n            if not all_words:\n                return 0.0\n            caps = sum(1 for w in all_words if w[0].isupper() and any(ch.islower() for ch in w[1:]))\n            return float(caps) / len(all_words)\n        return float(cap_mid) / total\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that contain at least one digit (numeric-token density like \"Level 50\")'\n    if not text:\n        return 0.0\n    try:\n        import re\n        tokens = re.findall(r'\\b[\\w-]+\\b', text)\n        if not tokens:\n            return 0.0\n        num = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n        return float(num) / len(tokens)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain at least one comma (measure of clause complexity per sentence)'\n    if not text:\n        return 0.0\n    try:\n        import re\n        # split into sentence-like chunks\n        sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n        if not sents:\n            return 0.0\n        with_comma = sum(1 for s in sents if ',' in s)\n        return float(with_comma) / len(sents)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<=5 words) indicating clipped or punchy sentence style'\n    if not text:\n        return 0.0\n    try:\n        import re\n        sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n        if not sents:\n            return 0.0\n        short = 0\n        for s in sents:\n            words = re.findall(r'\\b\\w+\\b', s)\n            if len(words) <= 5:\n                short += 1\n        return float(short) / len(sents)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of punctuation counts per sentence (captures uneven punctuation usage)'\n    if not text:\n        return 0.0\n    try:\n        import re, math\n        sents = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n        if not sents:\n            return 0.0\n        punct_counts = []\n        for s in sents:\n            pc = sum(1 for c in s if not c.isalnum() and not c.isspace())\n            punct_counts.append(float(pc))\n        if not punct_counts:\n            return 0.0\n        mean = sum(punct_counts) / len(punct_counts)\n        if mean == 0.0:\n            return 0.0\n        var = sum((x - mean) ** 2 for x in punct_counts) / len(punct_counts)\n        std = math.sqrt(var)\n        return std / mean\n    except Exception:\n        return 0.0\n", "def feature(text: str) -> float:\n    'Average sentence length in tokens (words per sentence), 0.0 for empty text'\n    import re\n    if not text:\n        return 0.0\n    # Split on sentence-ending punctuation sequences\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # treat whole text as one sentence if no sentence punctuation\n        tokens = re.findall(r'\\w+', text)\n        return float(len(tokens)) if tokens else 0.0\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid division by zero\n    counts = [c for c in counts if c > 0]\n    if not counts:\n        return 0.0\n    return float(sum(counts)) / float(len(counts))\n\n", "def feature(text: str) -> float:\n    'Passive-voice proxy: matches of \"was/were/is/are/been/being/has/had\" followed within 0-3 words by an -ed token, normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    # count sentences to normalize\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    # rough passive pattern\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|being|be|has|had|have)\\b(?:\\s+\\w+){0,3}\\s+\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    return float(len(matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Average commas per sentence (commas indicate clause complexity), normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    sentence_splits = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    # if no sentence punctuation, treat as one sentence\n    if not sentence_splits:\n        sentence_count = 1\n        comma_count = text.count(',')\n    else:\n        sentence_count = len(sentence_splits)\n        comma_count = sum(s.count(',') for s in sentence_splits)\n    return float(comma_count) / float(sentence_count) if sentence_count else 0.0\n\n", "def feature(text: str) -> float:\n    'Titlecase-internal-word ratio: fraction of titlecase words that occur not immediately after sentence-ending punctuation (proper nouns mid-sentence)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    # find all titlecase words\n    titlecase_positions = []\n    for m in re.finditer(r'\\b[A-Z][a-zA-Z]+\\b', text):\n        titlecase_positions.append(m.start())\n    if not titlecase_positions:\n        return 0.0\n    mid_count = 0\n    for pos in titlecase_positions:\n        # find preceding non-space char\n        prefix = text[:pos].rstrip()\n        if prefix == '':\n            # at very start -> not mid-sentence\n            continue\n        # if prefix ends with sentence end punctuation, treat as sentence start\n        if prefix.endswith(('.', '!', '?')):\n            continue\n        # otherwise count as mid-sentence Titlecase\n        mid_count += 1\n    # normalize by total word count to avoid inflated ratio on small texts\n    return float(mid_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Enumerated-list density: count of enumeration markers like \"1.\", \"2)\", \"(a)\" divided by word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = max(1, len(words))\n    markers = 0\n    markers += len(re.findall(r'\\b\\d+\\.', text))\n    markers += len(re.findall(r'\\b\\d+\\)', text))\n    markers += len(re.findall(r'\\([a-zA-Z]\\)', text))\n    markers += len(re.findall(r'^[\\-\\*]\\s+', text, re.MULTILINE))\n    return float(markers) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Repeated bigram ratio: proportion of bigrams that repeat elsewhere (text redundancy / template-likeness)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = list(zip(tokens, tokens[1:]))\n    total = len(bigrams)\n    unique = len(set(bigrams))\n    repeats = total - unique\n    return float(repeats) / float(total) if total else 0.0\n\n", "def feature(text: str) -> float:\n    'Stopword fraction: proportion of common English function words, a proxy for naturalness and fluency'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','and','a','an','in','on','of','to','for','with','that','is','are','was','were','it','as','by','from',\n        'this','these','those','be','or','which','at','but','not','have','has','had','can','may','will','would','should'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Citation-like density: occurrences of \"et al\", numeric bracket citations like [1], or tokens like \"doi\"/\"pmid\", normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    word_count = max(1, len(words))\n    citations = 0\n    citations += len(re.findall(r'\\bet al\\b', text, re.IGNORECASE))\n    citations += len(re.findall(r'\\[\\s*\\d+\\s*\\]', text))\n    citations += len(re.findall(r'\\bdoi[:/]?', text, re.IGNORECASE))\n    citations += len(re.findall(r'\\bpmid\\b', text, re.IGNORECASE))\n    citations += len(re.findall(r'\\bfig\\.?\\s*\\d+\\b', text, re.IGNORECASE))\n    return float(citations) / float(word_count)\n", "def feature(text: str) -> float:\n    'Fraction of first-person pronouns (I, we, me, our, my, mine, us, ours) among all tokens'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'we', 'me', 'us', 'my', 'our', 'mine', 'ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of modal/hedging verbs (may, might, should, must, could, would, shall) per word'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'may', 'might', 'should', 'must', 'could', 'would', 'shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence); uses .!? to segment sentences'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    word_count = len(tokens)\n    # split on sentence terminators and ignore empty pieces\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return float(word_count)\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # guard: if no words then 0\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n", "def feature(text: str) -> float:\n    'Variance of sentence lengths (population variance); 0.0 if fewer than 2 sentences'\n    import re, math\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences if re.findall(r'\\w+', s)]\n    if not lengths:\n        return 0.0\n    if len(lengths) == 1:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Ratio of common English stopwords to total tokens (simple stopword list)'\n    import re\n    stopwords = {'the','and','of','to','a','in','for','is','that','on','with','as','by','an','be','are','this','it','from','or','at','was','were','which','these','their','has','have'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of colons and semicolons per word (captures formal and list-like punctuation patterns)'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    word_count = len(tokens)\n    if word_count == 0:\n        return 0.0\n    punct_count = text.count(':') + text.count(';')\n    return float(punct_count) / word_count\n\n", "def feature(text: str) -> float:\n    'Proportion of lines that contain numbered/list patterns (e.g., \"1. \", \"a)\"), indicating structured exposition'\n    import re\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    pattern1 = re.compile(r'^\\s*\\d+\\.\\s+')\n    pattern2 = re.compile(r'^\\s*[a-zA-Z]\\)\\s+')\n    pattern3 = re.compile(r'^\\s*[-*]\\s+')  # bullet points\n    matches = 0\n    for l in lines:\n        if pattern1.search(l) or pattern2.search(l) or pattern3.search(l):\n            matches += 1\n    return float(matches) / len(lines)\n", "def feature(text: str) -> float:\n    'Proportion of text lines that look like headings or short title-like lines (e.g., \"Introduction\", \"Figure 1.\")'\n    import re\n    if not text:\n        return 0.0\n    lines = [l.strip() for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    title_like = 0\n    for l in lines:\n        tokens = re.findall(r\"[A-Za-z']+\", l)\n        if not tokens:\n            continue\n        # short lines more likely to be headings\n        if len(tokens) <= 6:\n            caps = sum(1 for t in tokens if t[0].isupper())\n            # require at least one capitalized token and at least half tokens capitalized (approx)\n            if caps >= 1 and caps >= len(tokens) / 2.0:\n                title_like += 1\n            # also treat explicit markers like \"Figure 1.\" or single-word headings\n            elif re.match(r'^(Figure|Table|Introduction|Abstract|Conclusion)\\b', l, re.IGNORECASE) or len(tokens) == 1:\n                title_like += 1\n    return float(title_like) / float(len(lines))\n\n\n", "def feature(text: str) -> float:\n    'Density of short parenthetical phrases (proxy for citations, parenthetical notes like \"(Smith, 2010)\" or \"(n.d.)\") per word'\n    import re\n    if not text:\n        return 0.0\n    parentheticals = re.findall(r'\\([^\\)]{1,120}\\)', text)\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(len(parentheticals)) / float(words)\n\n\n", "def feature(text: str) -> float:\n    'Density of double-quote occurrences (proxy for quoted titles, book names) per word'\n    import re\n    if not text:\n        return 0.0\n    # count several types of quotation marks\n    quote_chars = ['\"', '\u201c', '\u201d', '\u201e', '\u00ab', '\u00bb']\n    quote_count = sum(text.count(q) for q in quote_chars)\n    words = max(1, len(text.split()))\n    return float(quote_count) / float(words)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated (contain - or \u2013 or \u2014), indicating compound technical terms or figure ranges'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t or '\u2013' in t or '\u2014' in t)\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Proxy for passive voice: occurrences of be-forms followed by a past-participle-like token (word ending in -ed) per sentence'\n    import re\n    if not text:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|has been|have been|had been)\\b\\s+\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(matches)) / float(sentence_count)\n\n\n", "def feature(text: str) -> float:\n    'Density of short abbreviations or acronym tokens (e.g., \"e.g.\", \"i.e.\", \"n.d.\", \"U.S.\") per word'\n    import re\n    if not text:\n        return 0.0\n    # dot-abbreviations like e.g., n.d., U.S.\n    dot_abbr = re.findall(r'\\b(?:[A-Za-z]\\.){2,}\\b', text)\n    # common lower-case abbreviations like e.g., i.e., n.d.\n    common = re.findall(r'\\b(?:e\\.g\\.|i\\.e\\.|n\\.d\\.|et al\\.)', text, re.IGNORECASE)\n    # all-caps acronyms of length 2-4\n    allcaps = re.findall(r'\\b[A-Z]{2,4}\\b', text)\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(len(dot_abbr) + len(common) + len(allcaps)) / float(words)\n\n\n", "def feature(text: str) -> float:\n    'Ratio of commas to commas-plus-semicolons (commas / (commas + semicolons + 1)) to capture dense, clause-heavy academic style'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    semis = text.count(';')\n    # +1 in denominator to avoid division by zero; yields value in [0,1)\n    return float(commas) / float(commas + semis + 1.0)\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words (captures variability of sentence length)'\n    import re, math\n    if not text:\n        return 0.0\n    # crude sentence split\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lens = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lens.append(len(words))\n    if not lens:\n        return 0.0\n    mean = sum(lens) / len(lens)\n    if mean <= 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lens) / len(lens)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens containing hyphens, slashes, or dashes (captures technical compound terms like \"open-source\")'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    reps = 0\n    for t in tokens:\n        if any(ch in t for ch in '-/\u2013\u2014'):\n            reps += 1\n    return float(reps) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that start with common academic/section headings (Introduction, Abstract, Conclusion, Methods, References...)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    header_words = r'^(?:Abstract|Introduction|Conclusion|References|Methods|Methodology|Discussion|Results|Findings|Evaluation|Circumstances|Encountered Issues)\\b'\n    count = 0\n    for ln in lines:\n        if re.search(header_words, ln, flags=re.IGNORECASE):\n            count += 1\n    return float(count) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Density of less-common punctuation marks (; : ( ) [ ] { } \u2014) normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    punct_chars = ';:()[]{}\u2014\u2013-'\n    count = sum(1 for c in text if c in punct_chars)\n    return float(count) / float(wc)\n\n", "def feature(text: str) -> float:\n    'Ratio of mean length of the top 10% longest words to the overall mean word length (captures long-word peaks)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    lengths = sorted([len(w) for w in words])\n    mean_all = sum(lengths) / len(lengths)\n    if mean_all == 0:\n        return 0.0\n    top_n = max(1, int(len(lengths) * 0.1))\n    top_mean = sum(lengths[-top_n:]) / top_n\n    return float(top_mean) / float(mean_all)\n\n", "def feature(text: str) -> float:\n    'Proportion of common stopwords present (simple stopword density using a small common list)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','of','in','to','a','is','for','on','that','with','as','by','an','are','be','or','this','it'}\n    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stop)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences containing a colon or semicolon (often present in formal/technical prose and headings)'\n    import re\n    if not text:\n        return 0.0\n    # crude sentence splitting preserving punctuation\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        if ':' in s or ';' in s:\n            count += 1\n    return float(count) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Fraction of sentences containing a simple passive-voice marker pattern (e.g., \"was ...ed\", \"were ...ed\")'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    passive_hits = 0\n    pattern = re.compile(r'\\b(was|were|is|are|been|being|be)\\b[^.?!]{0,60}\\b\\w+ed\\b', re.I)\n    for s in sentences:\n        if pattern.search(s):\n            passive_hits += 1\n    return float(passive_hits) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total tokens (lexical diversity)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Parenthesis density: number of parentheses characters \"(\" or \")\" per word (indicator of citations/parentheticals)'\n    import re\n    if not text:\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(paren_count) / float(words)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (mean words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback: treat whole text as one sentence\n        words = len(re.findall(r'\\w+', text))\n        return float(words)\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated (contain a hyphen between word characters)'\n    import re\n    if not text:\n        return 0.0\n    # tokens by whitespace to preserve punctuation attachments\n    tokens = [t for t in re.findall(r'\\S+', text) if t.strip()]\n    if not tokens:\n        return 0.0\n    hyphenated = 0\n    for t in tokens:\n        # simple check: contains hyphen and has at least one alphabetic character on both sides\n        if '-' in t and re.search(r'[A-Za-z]-[A-Za-z]', t):\n            hyphenated += 1\n    return float(hyphenated) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of common short stopwords (the, of, and, in, to, for, with, on, as) per word'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','of','and','in','to','for','with','on','as','by','is','are','was','were'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count) / float(len(words))\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a quotation mark (dialogue-start ratio)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences by sentence terminators; keep leading/trailing whitespace trimmed\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts_with_quote = sum(1 for s in sentences if s and s[0] == '\"')\n    return float(starts_with_quote) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of word tokens ending with -ing (progressive/gerund density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing'))\n    return float(ing_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") occurrences normalized by number of words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w[\\w']*\\b\", text)\n    word_count = max(1, len(words))\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    return float(ellipses) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (mean words per sentence)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = []\n    for s in sentences:\n        tokens = re.findall(r\"\\b\\w[\\w']*\\b\", s)\n        counts.append(len(tokens))\n    if not counts:\n        return 0.0\n    return float(sum(counts)) / float(len(counts))\n\n", "def feature(text: str) -> float:\n    'Density of repeated punctuation runs (e.g., \"!!\", \"??\", \"---\", \"...\") normalized by total punctuation count'\n    import re\n    if not text:\n        return 0.0\n    runs = re.findall(r'([!?.,;:\\-\\'\"`~_])\\1+', text)\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    return float(len(runs)) / float(total_punct)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word types that occur exactly once (lexical uniqueness measure)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"\\b\\w[\\w']*\\b\", text)]\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    counts = Counter(tokens)\n    hapax = sum(1 for v in counts.values() if v == 1)\n    return float(hapax) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Difference between proportion of -ing tokens and -ed tokens (prop_ing - prop_ed)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ing = sum(1 for t in tokens if t.endswith('ing'))\n    ed = sum(1 for t in tokens if t.endswith('ed'))\n    prop_ing = float(ing) / float(len(tokens))\n    prop_ed = float(ed) / float(len(tokens))\n    return prop_ing - prop_ed\n", "def feature(text: str) -> float:\n    'Fraction of modal verbs (may, might, could, should, would, must, can) as a hedge/possibility indicator'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    modals = {'may','might','could','should','would','must','can'}\n    count = sum(1 for w in words if w in modals)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Enumerator density: counts occurrences of enumerator markers (firstly/first/1./1)) normalized by sentence count'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # count enumerator tokens\n    matches = re.findall(r'\\b(firstly|secondly|thirdly|first|second|third)\\b|\\b\\d{1,2}[.)]\\b', text, flags=re.IGNORECASE)\n    enum_count = len([m for m in matches if any(m)]) if matches else 0\n    # sentence count approximation\n    sentences = len(re.findall(r'[.!?]', text))\n    denom = float(sentences) if sentences > 0 else 1.0\n    return float(enum_count) / denom\n\n", "def feature(text: str) -> float:\n    'Fraction of hyphenated tokens (tokens containing a hyphen), indicating compound/technical terms'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyph = sum(1 for t in tokens if '-' in t and re.search(r'[A-Za-z0-9]-[A-Za-z0-9]', t))\n    return float(hyph) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average sentence length in tokens (words per sentence), robust to missing punctuation'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(words)) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Quoted material density: number of quote characters (single or double or typographic) per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    quotes = re.findall(r'[\"\\'\u201c\u201d\u2018\u2019]', text)\n    quote_count = len(quotes)\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(quote_count) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Internal capitalization ratio: fraction of tokens that start with a capital letter (proper nouns) excluding all-caps tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    cap_count = 0\n    for i, t in enumerate(tokens):\n        if len(t) >= 1 and t[0].isupper() and not t.isupper():\n            # try to exclude sentence-initial capitalization by checking context: if previous token ends with sentence punctuation, consider it likely start\n            if i == 0:\n                # first token: count it too (title or proper noun)\n                cap_count += 1\n            else:\n                prev = tokens[i-1]\n                # conservative: if previous token ends with sentence punctuation in original text, treat as sentence-start; otherwise count as internal capitalization\n                # fallback: check surrounding text snippet\n                snippet = text\n                if prev and not prev.endswith(('.', '!', '?')):\n                    cap_count += 1\n    return float(cap_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total word tokens (lexical diversity)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n", "def feature(text: str) -> float:\n    'Proportion of sentences that begin with a quotation mark (indicating dialogue starts)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    begins_with_quote = sum(1 for s in sents if s.lstrip().startswith(('\"', \"'\")))\n    return float(begins_with_quote) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count) \u2014 proxy for descriptive complexity'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sents = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    scount = max(1, len(sents))\n    return float(comma_count) / float(scount)\n\n", "def feature(text: str) -> float:\n    'Ratio of words ending in \"ly\" (adverb ratio) to total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?\", text)\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if w.lower().endswith('ly'))\n    return float(ly_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of internally capitalized words (capitalized words not at sentence-start) to total words'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    words_total = 0\n    internal_caps = 0\n    token_re = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n    for sent in sentences:\n        tokens = token_re.findall(sent)\n        if not tokens:\n            continue\n        # skip first token in sentence (likely capitalized by grammar)\n        for t in tokens[1:]:\n            words_total += 1\n            if len(t) > 1 and t[0].isupper() and t[1:].islower():\n                internal_caps += 1\n    if words_total == 0:\n        # fallback to overall words\n        words = token_re.findall(text)\n        if not words:\n            return 0.0\n        words_total = len(words)\n        internal_caps = sum(1 for t in words if len(t) > 1 and t[0].isupper() and t[1:].islower())\n    return float(internal_caps) / float(words_total)\n\n", "def feature(text: str) -> float:\n    'Ellipsis frequency: count of \"...\" occurrences normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    words = re.findall(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?\", text)\n    wcount = max(1, len(words))\n    return float(ellipses) / float(wcount)\n\n", "def feature(text: str) -> float:\n    'Function-word density: fraction of common function words present among all tokens (proxy for grammatical/stopword usage)'\n    import re\n    if not text:\n        return 0.0\n    stopset = {'the','and','of','to','a','in','that','it','is','was','for','as','with','on','at','by','an','be','this','which','or','from','but','not','are','his','her','their','they','he','she','you','i','we','my','your','our'}\n    words = re.findall(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?\", text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in stopset)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Normalized density of multi-word capitalized sequences (e.g., \"New York\", \"Lord Voldemort\") per sentence'\n    import re\n    if not text:\n        return 0.0\n    # find sequences of two or more consecutive Titlecase words\n    matches = re.findall(r'\\b(?:[A-Z][a-z]+(?:\\s+|$)){2,}', text)\n    sents = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    scount = max(1, len(sents))\n    return float(len(matches)) / float(scount)\n\n", "def feature(text: str) -> float:\n    'Proportion of commas that are immediately followed by a coordinating conjunction (\", and\", \", but\", etc.)'\n    import re\n    if not text:\n        return 0.0\n    comma_conj = re.findall(r',\\s+(?:and|but|or|so|yet|for|nor)\\b', text, flags=re.I)\n    comma_count = text.count(',')\n    if comma_count == 0:\n        return 0.0\n    return float(len(comma_conj)) / float(comma_count)\n", "def feature(text: str) -> float:\n    'Ellipsis frequency: occurrences of \"...\" or single ellipsis char per 100 words'\n    if not text:\n        return 0.0\n    import re\n    ellipses = len(re.findall(r'\\.{2,}|\\u2026', text))\n    words = max(1, len(text.split()))\n    return float(ellipses) * 100.0 / float(words)\n\n", "def feature(text: str) -> float:\n    'Irregular/auxiliary past-tense token ratio using a common verb list (captures \"was\",\"said\",\"went\", etc.)'\n    if not text:\n        return 0.0\n    import re\n    tokens = re.findall(r\"\\b[A-Za-z']+\\b\", text.lower())\n    past_verbs = {'was','were','had','said','saw','went','came','took','left','began','started','ran','stood','yelled','shouted','whispered','pushed','felt','heard','told','fell','gave','made','became','knew','thought','said'}\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in past_verbs)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation characters'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a personal pronoun (I,you,he,she,they,we,it)'\n    if not text:\n        return 0.0\n    import re\n    pronouns = {'i','you','he','she','they','we','it','who'}\n    # split into sentence-like chunks\n    raw_sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not raw_sentences:\n        return 0.0\n    starts = 0\n    for s in raw_sentences:\n        # get first word token\n        m = re.search(r\"[A-Za-z']+\", s)\n        if not m:\n            continue\n        first = m.group(0).lower()\n        if first in pronouns:\n            starts += 1\n    return float(starts) / float(len(raw_sentences))\n\n", "def feature(text: str) -> float:\n    'Filler-word density: fraction of tokens that are common discourse/filler words (really, just, actually, basically, truly, etc.)'\n    if not text:\n        return 0.0\n    import re\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    fillers = {'really','very','just','actually','basically','literally','truly','surely','perhaps','maybe','kindof','sortof','somewhat'}\n    count = sum(1 for t in tokens if t in fillers)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Dialogue-tag density: number of occurrences of common dialogue verbs (said, asked, replied, shouted, yelled, whispered) per sentence'\n    if not text:\n        return 0.0\n    import re\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    tag_verbs = {'said','asked','replied','shouted','yelled','whispered','muttered','cried','answered','added'}\n    tag_count = sum(1 for t in tokens if t in tag_verbs)\n    # approximate sentences\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(tag_count) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Sentence-end emphasis ratio: proportion of sentences ending in \"!\" or \"?\" (exclamation/question emphasis)'\n    if not text:\n        return 0.0\n    import re\n    # find sentence endings with their final punctuation\n    ends = re.findall(r'([^\\S\\r\\n]*)([^.!?]*)([.!?])', text)\n    if not ends:\n        return 0.0\n    total = len(ends)\n    emph = sum(1 for (_,_,p) in ends if p in ('!','?'))\n    return float(emph) / float(total)\n", "def feature(text: str) -> float:\n    'Normalized frequency of asterisk markup characters (detects bold/italic/markdown like **text** or *emphasis*)'\n    if not text:\n        return 0.0\n    asterisks = text.count('*')\n    return float(asterisks) / max(1.0, float(len(text)))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are long (length >= 8 characters) as a proxy for formal/complex vocabulary'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 8)\n    return float(long_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average comma usage measured as commas per lexical word (commas normalize for sentence complexity/clauses)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    word_count = len(re.findall(r'\\w+', text))\n    return float(comma_count) / max(1.0, float(word_count))\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that start with a capitalized name-like token followed immediately by a comma (e.g., \"Mark,\")'\n    import re\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    name_comma = 0\n    pattern = re.compile(r'^\\s*[A-Z][A-Za-z0-9_\\-]+\\s*,')\n    for ln in lines:\n        if pattern.search(ln):\n            name_comma += 1\n    return float(name_comma) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a personal pronoun (I, you, he, she, we, they, it, my, his, her, their)'\n    import re\n    if not text:\n        return 0.0\n    parts = re.split(r'[.!?]+', text)\n    pronouns = {'i','you','he','she','we','they','it','my','his','her','their','its','our','me','him','them'}\n    starts = 0\n    total = 0\n    for p in parts:\n        s = p.strip()\n        if not s:\n            continue\n        total += 1\n        first_word_match = re.match(r\"['\\\"\u201c\u201d\u2018\u2019]*([A-Za-z]+)\", s)\n        if first_word_match:\n            first = first_word_match.group(1).lower()\n            if first in pronouns:\n                starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts) / float(total)\n\n", "def feature(text: str) -> float:\n    'Variety of punctuation: number of distinct punctuation characters present normalized to [0,1] (more variety -> more stylistic punctuation)'\n    if not text:\n        return 0.0\n    puncts = set(c for c in text if not c.isalnum() and not c.isspace())\n    # normalize by an upper cap (10) to keep result in [0,1]\n    cap = 10.0\n    return float(min(len(puncts), cap)) / cap\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that begin with a quote and contain a short quoted fragment (<= 5 words) on that line'\n    import re\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    quoted_lines = 0\n    short_quoted = 0\n    for ln in lines:\n        stripped = ln.lstrip()\n        if not stripped:\n            continue\n        if stripped.startswith('\"') or stripped.startswith('\u201c') or stripped.startswith(\"''\") or stripped.startswith(\"'\"):\n            quoted_lines += 1\n            # find matching closing quote in the same line\n            # search for second quote character\n            quote_char = stripped[0]\n            # find the next same quote char after position 0\n            idx = stripped.find(quote_char, 1)\n            content = ''\n            if idx != -1:\n                content = stripped[1:idx]\n            else:\n                # if no closing, take rest of line after opening quote\n                content = stripped[1:]\n            words_inside = re.findall(r'\\w+', content)\n            if len(words_inside) <= 5:\n                short_quoted += 1\n    if quoted_lines == 0:\n        return 0.0\n    return float(short_quoted) / float(quoted_lines)\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines whose first non-space alphabetic character is lowercase (indicates continuation/fragments or unconventional capitalization)'\n    import re\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    lower_start = 0\n    total = 0\n    for ln in lines:\n        m = re.search(r'\\S', ln)\n        if not m:\n            continue\n        total += 1\n        first_nonspace = ln[m.start():].lstrip()\n        if first_nonspace:\n            ch = first_nonspace[0]\n            if ch.isalpha() and ch.islower():\n                lower_start += 1\n    if total == 0:\n        return 0.0\n    return float(lower_start) / float(total)\n", "def feature(text: str) -> float:\n    'Normalized profanity score: fraction of words that are common swear terms'\n    import re\n    swears = ['fuck', 'fucking', 'fucked', 'shit', 'damn', 'bitch', 'bastard', 'asshole', 'crap', 'motherfucker', 'prick']\n    if not text:\n        return 0.0\n    pattern = r'\\\\b(' + '|'.join(re.escape(w) for w in swears) + r')\\\\b'\n    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(len(matches)) / float(words)\n\n", "def feature(text: str) -> float:\n    'Binary-ish indicator of a title/byline signature like \"by ...\", especially referencing AI/Assistant'\n    import re\n    if not text:\n        return 0.0\n    # match explicit byline on its own line or mentions of \"by [name]\" and references to AI/assistant\n    if re.search(r'(?m)^\\s*by\\s+[^\\n]{1,50}\\s*$', text, flags=re.IGNORECASE):\n        return 1.0\n    if re.search(r'by\\s+(your\\s+)?ai|by\\s+assistant|your\\s+ai\\s+assistant', text, flags=re.IGNORECASE):\n        return 1.0\n    # also check second line pattern: Title line then \"by ...\".\n    lines = text.splitlines()\n    if len(lines) >= 2 and re.match(r'^\\s*by\\s+', lines[1], flags=re.IGNORECASE):\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Density of common lightweight markup tokens (markdown/metadata markers) per word'\n    import re\n    if not text:\n        return 0.0\n    tokens = ['######', '#####', '####', '###', '--', '---', '(#', '#dropcap', '```', '* * *', '---', '[]']\n    token_count = sum(text.count(t) for t in tokens)\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(token_count) / float(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that begin with a quotation mark (indicates line-by-line dialogue)'\n    import re\n    lines = text.splitlines()\n    non_empty = [l for l in lines if l.strip()]\n    if not non_empty:\n        return 0.0\n    starts = sum(1 for l in non_empty if re.match(r'^\\s*[\"\u201c\u201d\\']', l))\n    return float(starts) / float(len(non_empty))\n\n", "def feature(text: str) -> float:\n    'Ellipsis occurrence rate: number of \"...\" sequences per sentence (proxy for trailing/informal speech)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(ellipses) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for syntactic complexity)'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(commas) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Parenthetical density: number of parentheses characters per word (indicates asides, markup, meta-comments)'\n    import re\n    if not text:\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(paren_count) / float(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of quoted segments that are short (<=8 words): measures clipped dialogue/short turns'\n    import re\n    if not text:\n        return 0.0\n    # capture segments between straight and curly quotes\n    segments = re.findall(r'[\"\u201c\u201d]([^\"\u201c\u201d]+?)[\"\u201c\u201d]', text)\n    if not segments:\n        return 0.0\n    short = 0\n    for seg in segments:\n        wc = len(re.findall(r'\\w+', seg))\n        if wc <= 8:\n            short += 1\n    return float(short) / float(len(segments))\n", "def feature(text: str) -> float:\n    'Ratio of titlecase words that are not at obvious sentence starts (proxy for proper names inside sentences)'\n    import re\n    if not text:\n        return 0.0\n    total_words = len(re.findall(r'\\b\\w+\\b', text))\n    if total_words == 0:\n        return 0.0\n    # Match words starting with capital letter and lowercase letters, not immediately preceded by sentence end + space\n    matches = re.findall(r'(?<![.!?]\\s)\\b([A-Z][a-z]+)\\b', text)\n    return float(len(matches)) / total_words\n\n", "def feature(text: str) -> float:\n    'Normalized frequency of ellipses (\"...\" or longer) per sentence (ellipses_count / sentence_count)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    # count sentences by sentence-ending punctuation runs; treat absence as zero sentence -> return 0.0\n    sentence_marks = len(re.findall(r'[.!?]+', text))\n    if sentence_marks == 0:\n        return float(ellipses)  # if no terminal punctuation, return raw ellipses count (small text case)\n    return float(ellipses) / sentence_marks\n\n", "def feature(text: str) -> float:\n    'Asterisk emphasis density: number of \"*\" characters per word (captures markdown/inline emphasis)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = len(words)\n    if word_count == 0:\n        return 0.0\n    asterisks = text.count('*')\n    return float(asterisks) / word_count\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas indicate clause complexity and descriptive style)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # count sentences robustly; if none, treat whole text as one sentence\n    sentence_marks = len(re.findall(r'[.!?]+', text))\n    sentences = sentence_marks if sentence_marks > 0 else 1\n    return float(comma_count) / sentences\n\n", "def feature(text: str) -> float:\n    'Fraction of quoted spans that contain an explicit speaking verb (said, asked, replied, tapped, glared, etc.)'\n    import re\n    if not text:\n        return 0.0\n    # find double-quoted spans (common dialogue markers)\n    quoted_iter = list(re.finditer(r'\"([^\"]+)\"', text))\n    if not quoted_iter:\n        # also check for smart quotes\n        quoted_iter = list(re.finditer(r'\u201c([^\u201d]+)\u201d', text))\n    if not quoted_iter:\n        return 0.0\n    speaking_verbs = {'said','asked','replied','shouted','muttered','sighed','grinned','snapped','whispered',\n                      'tapped','glared','laughed','cried','yelled','barked','murmured','stammered','sobbed','demanded','replied'}\n    hit = 0\n    for m in quoted_iter:\n        # include a small window after the closing quote to catch tags like: \"...\" he said.\n        span = (m.group(1) or '') + text[m.end():m.end()+40]\n        span_lower = span.lower()\n        if any(re.search(r'\\b' + re.escape(v) + r'\\b', span_lower) for v in speaking_verbs):\n            hit += 1\n    return float(hit) / len(quoted_iter)\n\n", "def feature(text: str) -> float:\n    'Proportion of short sentences (<= 3 words) among all sentences - captures clipped, punchy style'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence-ending punctuation; keep non-empty fragments\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    short = 0\n    for p in parts:\n        words = re.findall(r'\\b\\w+\\b', p)\n        if len(words) <= 3 and len(words) > 0:\n            short += 1\n    return float(short) / len(parts)\n\n", "def feature(text: str) -> float:\n    'Modal verb ratio: frequency of modal verbs (will, would, shall, should, can, could, may, might, must) per token'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    modals = {'will','would','shall','should','can','could','may','might','must'}\n    count = sum(1 for w in words if w in modals)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Em-dash density: occurrences of em-dash (\u2014) or doubled-hyphen (--) per sentence (aside/parenthetical style indicator)'\n    import re\n    if not text:\n        return 0.0\n    emdash_count = text.count('\u2014') + text.count('--')\n    sentence_marks = len(re.findall(r'[.!?]+', text))\n    if sentence_marks == 0:\n        # if no sentence punctuation, normalize by word count to avoid division by zero\n        words = len(re.findall(r'\\b\\w+\\b', text))\n        return float(emdash_count) / words if words > 0 else 0.0\n    return float(emdash_count) / sentence_marks\n", "def feature(text: str) -> float:\n    'Fraction of first-person pronouns and possessives (I, we, my, our, me, us, mine, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    fp = {'i','we','me','us','my','our','mine','ours','myself','ourselves'}\n    count = sum(1 for t in tokens if t in fp)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Digit character density: fraction of characters that are digits (detects dates, years, numeric references)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    digits = sum(1 for c in text if c.isdigit())\n    return digits / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: fraction of tokens with auxiliary + past-participle pattern (was/was + xxed/xxen)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    # look for auxiliaries followed by a likely past participle (ending in ed/en/n)\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|has been|have been|had been)\\s+[a-zA-Z]+(?:ed|en|n)\\b', text.lower())\n    return len(matches) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Modal verb fraction: proportion of tokens that are modal verbs (may, might, could, would, should, can, will, must, shall)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'may','might','could','would','should','can','will','must','shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Formal connective density: fraction of tokens that are formal connectors (therefore, however, moreover, thus, consequently, furthermore, hence)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    connectors = {'therefore','however','moreover','thus','consequently','furthermore','hence','whereas','nevertheless','nonetheless'}\n    count = sum(1 for t in tokens if t in connectors)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Standard deviation of word lengths (measures variability in lexical token lengths)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if len(words) < 2:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / float(len(lengths))\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    return math.sqrt(var)\n\n", "def feature(text: str) -> float:\n    'Quote character density: fraction of characters that are typical quote marks (\", \\u201c, \\u201d, \\u2018, \\u2019)'\n    if not text:\n        return 0.0\n    quote_chars = {'\"', '\\u201c', '\\u201d', '\\u2018', '\\u2019', \"'\"}\n    total = len(text)\n    if total == 0:\n        return 0.0\n    count = sum(1 for c in text if c in quote_chars)\n    return count / float(total)\n\n", "def feature(text: str) -> float:\n    'Proper-noun proxy: fraction of whitespace-separated tokens (excluding the very first token) that start with an uppercase letter'\n    import string\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if len(tokens) < 2:\n        return 0.0\n    def strip_punct(tok):\n        return tok.strip(string.punctuation)\n    count = 0\n    total = 0\n    for i, tok in enumerate(tokens):\n        if i == 0:\n            continue\n        s = strip_punct(tok)\n        if not s:\n            continue\n        total += 1\n        if s[0].isupper() and not s.isupper():\n            count += 1\n    if total == 0:\n        return 0.0\n    return count / float(total)\n", "def feature(text: str) -> float:\n    'Estimated fraction of sentences using passive constructions (approximate)'\n    import re\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sents:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|being|had been|has been|have been|been)\\b\\s+\\w+ed\\b', re.IGNORECASE)\n    matches = sum(1 for s in sents if pattern.search(s))\n    return matches / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (simple stopword density)'\n    import re\n    stopwords = {'the','and','of','to','a','in','is','that','it','for','on','with','as','by','an','be','are','was','were','this','which','or','from','at','but','not','have','has','their','they','its','these'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    stop_count = sum(1 for t in tokens if t in stopwords)\n    return stop_count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are long (>25 words)'\n    import re\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sents:\n        return 0.0\n    long_count = 0\n    for s in sents:\n        words = re.findall(r'\\w+', s)\n        if len(words) > 25:\n            long_count += 1\n    return long_count / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are uppercase acronyms (2+ uppercase letters)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    acr = sum(1 for t in tokens if len(t) >= 2 and t.isupper() and any(ch.isalpha() for ch in t))\n    return acr / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Normalized lexical entropy over word types (0..1 approximate measure of unpredictability)'\n    import re, math\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    freq = {}\n    for t in tokens:\n        freq[t] = freq.get(t, 0) + 1\n    n = len(tokens)\n    entropy = 0.0\n    for c in freq.values():\n        p = c / n\n        entropy -= p * math.log(p) if p > 0 else 0.0\n    types = len(freq)\n    if types <= 1:\n        return 0.0\n    return entropy / math.log(types)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (stddev / mean) to capture lexical length variability'\n    import re, math\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = float(sum(lengths)) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence-like segments that end without terminal punctuation (incomplete sentence fraction)'\n    import re\n    segments = [s for s in re.findall(r'[^.!?]+[.!?]?', text)]\n    segments = [s.strip() for s in segments if s.strip()]\n    if not segments:\n        return 0.0\n    incomplete = sum(1 for s in segments if not s or s[-1] not in '.!?')\n    return incomplete / float(len(segments))\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the second-person pronoun \"you\" (robust to quotes/capitalization)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence-like chunks using punctuation or newlines\n    parts = [p.strip() for p in re.split(r'[.!?]+\\s+|\\n+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    starts_you = 0\n    for p in parts:\n        if re.match(r'^[\\'\"\\(\\[]*\\s*you\\b', p, re.I):\n            starts_you += 1\n    return float(starts_you) / len(parts)\n\n", "def feature(text: str) -> float:\n    'Density of line breaks: number of non-empty lines divided by total word count (higher for poems/lineated text)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    words_count = len(words)\n    if words_count == 0:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    lines_count = len(lines)\n    return float(lines_count) / words_count\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that are short (<=5 words) \u2014 captures poetic/fragmented formatting'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    short_lines = 0\n    for ln in lines:\n        words = re.findall(r\"[A-Za-z']+\", ln)\n        if len(words) <= 5:\n            short_lines += 1\n    return float(short_lines) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: count of sequences of two or more periods (e.g., \"...\") per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    words_count = len(words)\n    if words_count == 0:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{2,}', text))\n    return float(ellipses) / words_count\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (comma_count / sentence_count), 0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    # Split into sentences by punctuation or newlines\n    sentences = [s for s in re.split(r'[.!?]+\\s+|\\n+', text) if s.strip()]\n    sent_count = len(sentences)\n    if sent_count == 0:\n        return 0.0\n    return float(commas) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Lexical diversity: number of unique word forms (lowercased) divided by total word count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of words that start with an uppercase letter (captures title-casing or line-capitalization common in poetry)'\n    import re\n    if not text:\n        return 0.0\n    # Word tokens preserving original casing\n    tokens = re.findall(r\"\\b[A-Za-z'][A-Za-z']*\\b\", text)\n    if not tokens:\n        return 0.0\n    cap_count = sum(1 for t in tokens if t[0].isupper())\n    return float(cap_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<=6 words) \u2014 indicates terse, fragmentary, or poetic sentences'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s+|\\n+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short_sent = 0\n    for s in sentences:\n        words = re.findall(r\"[A-Za-z']+\", s)\n        if len(words) <= 6:\n            short_sent += 1\n    return float(short_sent) / len(sentences)\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count, robust to missing punctuation)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    return float(commas) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Lexical diversity: unique word count divided by total word count (based on simple alphanumeric tokens)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like a short title or heading (Title Case and short)'\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    title_like = 0\n    for ln in lines:\n        # consider short lines with titlecase style\n        words = ln.split()\n        if 0 < len(words) <= 8:\n            # istitle handles many Title Case forms like \"The End\"\n            if ln.istitle():\n                title_like += 1\n    return float(title_like) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Ratio of internal proper-name-like tokens (Titlecase) excluding obvious sentence starts'\n    import re\n    if not text:\n        return 0.0\n    words_all = re.findall(r'\\b\\w+\\b', text)\n    total_words = len(words_all)\n    if total_words == 0:\n        return 0.0\n    # find Titlecase words and exclude those that are at true sentence starts\n    count = 0\n    for m in re.finditer(r'\\b[A-Z][a-z]{1,}\\b', text):\n        start = m.start()\n        # look back to find previous non-space character\n        i = start - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        if i >= 0 and text[i] in '.!?':\n            # likely a sentence start -> skip\n            continue\n        if start == 0:\n            continue\n        count += 1\n    return float(count) / total_words\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per sentence (robust to zero sentences)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    return float(ellipses) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain quotation marks (dialogue density at sentence level)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences using punctuation followed by whitespace (keeps short texts robust)\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    with_quote = sum(1 for s in sentences if '\"' in s or '\u201c' in s or '\u201d' in s)\n    return float(with_quote) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Normalized standard deviation of paragraph lengths (words per paragraph): stddev / (mean+1) to avoid blowup'\n    import re\n    if not text:\n        return 0.0\n    # paragraphs separated by empty lines\n    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n    if not paragraphs:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', p)) for p in paragraphs]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = var ** 0.5\n    return float(std) / (mean + 1.0)\n\n", "def feature(text: str) -> float:\n    'Stopword density using a conservative small list of common English stopwords (fraction of tokens that are stopwords)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','of','to','a','in','is','it','that','he','she','they','was','were','for','on','with','as','by','at','from','this','an','be'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    sw_count = sum(1 for w in words if w in stopwords)\n    return float(sw_count) / len(words)\n", "def feature(text: str) -> float:\n    'Density of common markdown/formatting symbols (*, #, `_`, `) relative to text length'\n    if not text:\n        return 0.0\n    symbols = set('*#_`>')\n    count = sum(1 for c in text if c in symbols)\n    return count / max(1, len(text))\n\n", "def feature(text: str) -> float:\n    'Average number of ellipsis groups (\"...\" or longer) per sentence'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return ellipses / sentences\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that are short (<= 40 characters) \u2014 detects lists/poetic formatting'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip() != '']\n    if not lines:\n        return 0.0\n    short = sum(1 for ln in lines if len(ln.strip()) <= 40)\n    return short / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are capitalized mid-sentence (likely proper nouns or odd capitalization) relative to total tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    total = max(1, len(words))\n    mid_caps = 0\n    for m in re.finditer(r'\\b[A-Z][a-z]+\\b', text):\n        s = m.start()\n        # find previous non-space character\n        p = s - 1\n        while p >= 0 and text[p].isspace():\n            p -= 1\n        # if no previous char or previous char ends a sentence, treat as sentence-start\n        if p < 0 or text[p] in '.!?\"\\'':\n            continue\n        mid_caps += 1\n    return mid_caps / float(total)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas/sentence) \u2014 proxy for syntactic complexity'\n    import re\n    if not text:\n        return 0.0\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    commas = text.count(',')\n    return commas / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Density of spaced hyphen/dash occurrences (\" - \", em-dash, en-dash) per sentence'\n    import re\n    if not text:\n        return 0.0\n    occurrences = len(re.findall(r'(?:\\s-\\s|\\u2014|\\u2013)', text))\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return occurrences / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Skew proxy for sentence lengths: abs(mean - median) divided by std (0 if no variation) \u2014 captures asymmetry in sentence length distribution'\n    import re, math\n    if not text:\n        return 0.0\n    parts = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not parts:\n        return 0.0\n    lens = []\n    for s in parts:\n        words = re.findall(r'\\w+', s)\n        lens.append(len(words))\n    if not lens:\n        return 0.0\n    mean = sum(lens) / float(len(lens))\n    sorted_l = sorted(lens)\n    n = len(sorted_l)\n    if n % 2 == 1:\n        median = sorted_l[n//2]\n    else:\n        median = 0.5 * (sorted_l[n//2 - 1] + sorted_l[n//2])\n    # compute population std\n    var = sum((x - mean) ** 2 for x in lens) / float(len(lens))\n    std = math.sqrt(var)\n    if std == 0.0:\n        return 0.0\n    return abs(mean - median) / (std + 1e-12)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating conjunction or common sentence-level connector (and, but, or, so, however, also, then, because)'\n    import re\n    if not text:\n        return 0.0\n    connectors = {'and', 'but', 'or', 'so', 'however', 'also', 'then', 'because'}\n    # split on sentence enders and keep fragments\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    starts = 0\n    for p in parts:\n        first = re.findall(r'\\b\\w+\\b', p)\n        if first:\n            if first[0].lower() in connectors:\n                starts += 1\n    return starts / float(len(parts))\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total tokens (vocabulary richness)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+['\u2019]?\\w*|\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence terminators, avoids zero division)'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(commas) / sentences\n\n", "def feature(text: str) -> float:\n    'Density of sensory words (visual/auditory/tactile/olfactory/gustatory) as fraction of all tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\w+['\u2019]?\\w*|\\w+\", text.lower())\n    if not words:\n        return 0.0\n    sensory = {\n        'visual': {'see', 'saw', 'look', 'looked', 'eyes', 'vision', 'light', 'dark', 'darkness', 'glow', 'gaze'},\n        'auditory': {'hear', 'heard', 'sound', 'silent', 'silence', 'noise', 'whisper', 'shout', 'deafening'},\n        'tactile': {'touch', 'touched', 'cold', 'warm', 'hot', 'rough', 'smooth'},\n        'olfactory': {'smell', 'smelled', 'scent', 'odor', 'fragrance'},\n        'gustatory': {'taste', 'tasted', 'bitter', 'sweet', 'salty'}\n    }\n    sensory_set = set().union(*sensory.values())\n    count = sum(1 for w in words if w in sensory_set)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with -ly (heuristic adverb density)'\n    import re, string\n    if not text:\n        return 0.0\n    raw = re.findall(r\"\\S+\", text)\n    cleaned = []\n    for tok in raw:\n        tok = tok.strip(string.punctuation).lower()\n        if tok:\n            cleaned.append(tok)\n    if not cleaned:\n        return 0.0\n    ly_count = sum(1 for w in cleaned if w.endswith('ly'))\n    return float(ly_count) / len(cleaned)\n\n", "def feature(text: str) -> float:\n    'Average words per comma-separated clause (words / (commas + 1)), lower values indicate many short clauses'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\w+['\u2019]?\\w*|\\w+\", text)\n    words_count = len(words)\n    clauses = text.count(',') + 1\n    clauses = max(1, clauses)\n    return float(words_count) / clauses\n\n", "def feature(text: str) -> float:\n    'Lexical repetition rate: proportion of repeated tokens (1 - unique/total)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+['\u2019]?\\w*|\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    repetition = 1.0 - (unique / len(tokens))\n    return repetition\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of punctuation counts across sentences (captures sentence-level punctuation variability)'\n    import re, math\n    if not text:\n        return 0.0\n    # split roughly on sentence terminators to get sentence-like pieces\n    pieces = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not pieces:\n        return 0.0\n    punct_counts = []\n    for p in pieces:\n        punct_counts.append(sum(1 for c in p if not c.isalnum() and not c.isspace()))\n    mean = sum(punct_counts) / len(punct_counts)\n    if mean == 0:\n        return 0.0\n    var = sum((x - mean) ** 2 for x in punct_counts) / len(punct_counts)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Density of quotation marks (double quotes) per token - proxy for presence of direct dialogue'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text)\n    token_count = max(1, len(tokens))\n    quote_count = text.count('\"') + text.count('\u201c') + text.count('\u201d') + text.count(\"'\\\"\")  # include common quote chars\n    # also consider single quotes used as dialogue are ambiguous, but include them conservatively:\n    quote_count += text.count(\" '\") + text.count(\"' \")\n    return float(quote_count) / token_count\n\n", "def feature(text: str) -> float:\n    'Fraction of capitalized tokens that are likely proper nouns (capitalized not at sentence start)'\n    import re\n    if not text:\n        return 0.0\n    raw_tokens = re.findall(r\"\\b[^\\s]+\\b\", text)\n    if not raw_tokens:\n        return 0.0\n    proper_like = 0\n    capitalized_total = 0\n    sentence_boundaries = set()\n    for m in re.finditer(r'[.!?]+', text):\n        # mark index after punctuation as potential sentence start\n        sentence_boundaries.add(m.end())\n    for i, tok in enumerate(raw_tokens):\n        stripped = tok.strip('()[]{}\"\\'\u201c\u201d')  # remove surrounding punctuation\n        if not stripped:\n            continue\n        if stripped[0].isupper():\n            capitalized_total += 1\n            # compute approximate start index of token\n            try:\n                start_idx = text.index(tok)\n            except ValueError:\n                start_idx = None\n            # if not at very start and not immediately after sentence punctuation, count as proper-like\n            if start_idx is None:\n                proper_like += 1\n            else:\n                if start_idx != 0 and start_idx not in sentence_boundaries:\n                    proper_like += 1\n    if capitalized_total == 0:\n        return 0.0\n    return float(proper_like) / capitalized_total\n\n", "def feature(text: str) -> float:\n    'Smoothed ratio of common irregular past-tense verbs to total tokens (captures non-\"ed\" past usage)'\n    import re\n    if not text:\n        return 0.0\n    irregulars = {'was','were','had','saw','went','came','got','made','became','began','brought',\n                  'broke','chose','did','drank','drove','flew','found','gave','kept','knew','left',\n                  'lost','put','read','ran','said','sent','slept','spoke','took','told','thought',\n                  'won','felt','built','fell','held','heard','lost','paid','rode','rose','shot','shut'}\n    tokens = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in irregulars)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Occurrence density of ellipses (\"...\") per sentence - often used for dramatic or truncated text'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    return float(ellipses) / sent_count\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing an em-dash or double hyphen (\u2014 or --) to capture interrupted or dramatic phrasing'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b|[\u2014-]{2,}\", text)\n    if not tokens:\n        return 0.0\n    em_count = 0\n    for m in re.finditer(r'[\u2014]|--', text):\n        em_count += 1\n    return float(em_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of uncommon punctuation (colons, semicolons, parentheses, slashes) relative to all punctuation'\n    import re\n    if not text:\n        return 0.0\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    uncommon = sum(text.count(ch) for ch in ';:()[]{}\\\\/<>')\n    return float(uncommon) / float(total_punct)\n", "def feature(text: str) -> float:\n    'Density of ellipsis sequences (three or more periods) per 100 words to capture trailing/thoughtful pauses'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(ellipses) / float(words) * 100.0\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence-ending events that are ellipses (count of \"...\" divided by total sentence end punctuation) \u2014 captures trailing or dramatic endings'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    sentence_ends = text.count('.') + text.count('!') + text.count('?')\n    if sentence_ends == 0:\n        return 0.0\n    return float(ellipses) / float(sentence_ends)\n\n", "def feature(text: str) -> float:\n    'Ratio of asterisk characters to total tokens, indicating emphasis markers like *word* or surrounding asterisks'\n    import re\n    if not text:\n        return 0.0\n    asterisks = text.count('*')\n    tokens = max(1, len(re.findall(r'\\S+', text)))\n    return float(asterisks) / float(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are surrounded by or contain double-quote characters, approximating dialog density'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    dq = 0\n    for t in tokens:\n        if '\"' in t or t.startswith('\"') or t.endswith('\"'):\n            dq += 1\n    return float(dq) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of exclamation or question marks per sentence (punctuation intensity) to capture emotional or interrogative tone'\n    import re\n    if not text:\n        return 0.0\n    punctuation_count = text.count('!') + text.count('?')\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    if sentence_count == 0:\n        return float(punctuation_count)\n    return float(punctuation_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Ratio of capitalized words that appear mid-sentence (likely proper nouns or names) to total words, indicating named characters and narrative focus'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    caps_mid = 0\n    total = len(words)\n    # find capitalized word matches with letter sequence (start index used)\n    for m in re.finditer(r'\\b[A-Z][a-z]+\\b', text):\n        start = m.start()\n        # examine prefix trimmed of whitespace to decide if it's sentence-start\n        prefix = text[:start].rstrip()\n        if prefix == '':\n            continue\n        # if prefix ends with sentence-ending punctuation, treat as sentence start\n        if prefix.endswith('.') or prefix.endswith('!') or prefix.endswith('?'):\n            continue\n        # otherwise count as mid-sentence capitalized\n        caps_mid += 1\n    return float(caps_mid) / float(total)\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending in \"ing\" (gerund/progressive forms) among all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', text)\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.lower().endswith('ing'))\n    return float(ing_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending in \"ly\" (adverbial -ly usage) among all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', text)\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.lower().endswith('ly'))\n    return float(ly_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Distinct punctuation variety normalized (number of different non-alphanumeric punctuation characters divided by 10)'\n    if not text:\n        return 0.0\n    puncts = {c for c in text if not c.isalnum() and not c.isspace()}\n    # normalize to keep value in [0,1] roughly (max ~10 common punctuation marks)\n    return float(len(puncts)) / 10.0\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: count of \"...\" occurrences per 1000 characters'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    return float(ellipses) / max(1.0, len(text)) * 1000.0\n\n", "def feature(text: str) -> float:\n    'Ratio of sensory-descriptive words (see, hear, smell, taste, touch, vivid, bright, dark, moist, cold, warm, smell variants) to total tokens'\n    import re\n    sensory = {'see','saw','seen','hear','heard','listen','smell','scent','taste','touch','feel',\n               'vivid','vibrant','moist','dry','warm','cold','bright','dark','fragrant','silent','noisy'}\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a subordinating or clause-introducing conjunction (because, although, since, while, if, when, though, unless, after, before, as, once)'\n    import re\n    subs = {'because','although','since','while','if','when','though','unless','after','before','as','once'}\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [sent.strip() for sent in re.split(r'(?<=[.!?])\\s+', s) if sent.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for sent in sentences:\n        # remove leading non-word characters like quotes or dashes\n        sent_clean = re.sub(r'^[^\\w]+', '', sent).lower()\n        first_word_match = re.match(r'(\\w+)', sent_clean)\n        if first_word_match and first_word_match.group(1) in subs:\n            starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences ending with an exclamation or question mark (expressiveness/dialogue indicator)'\n    import re\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [sent for sent in re.split(r'(?<=[.!?])\\s+', s) if sent.strip()]\n    if not sentences:\n        return 0.0\n    special_end = sum(1 for sent in sentences if sent.rstrip().endswith('!') or sent.rstrip().endswith('?'))\n    return float(special_end) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Fraction of interior words (not the first word of a sentence) that start with an uppercase letter (proper nouns / title-casing irregularity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'([.!?]+\\s*)', text)\n    # reconstruct sentences to ensure splitting keeps content\n    chunks = []\n    temp = ''\n    for part in sentences:\n        temp += part\n        if re.search(r'[.!?]$', part.strip()):\n            chunks.append(temp.strip())\n            temp = ''\n    if temp:\n        chunks.append(temp.strip())\n    interior_capital = 0\n    interior_total = 0\n    for s in chunks:\n        words = re.findall(r'\\b\\w+\\b', s)\n        for w in words[1:]:\n            interior_total += 1\n            if w and w[0].isupper():\n                interior_capital += 1\n    if interior_total == 0:\n        return 0.0\n    return float(interior_capital) / float(interior_total)\n\n", "def feature(text: str) -> float:\n    'Density of exclamation marks per sentence (exclamation_count / sentence_count)'\n    import re\n    if not text:\n        return 0.0\n    exclamations = text.count('!')\n    # approximate sentence count by punctuation\n    sentence_count = len(re.findall(r'[.!?]+', text))\n    if sentence_count == 0:\n        # if there are exclamations but no sentence punctuation, treat as one sentence\n        return float(exclamations) if exclamations else 0.0\n    return float(exclamations) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens ending with \"ing\" (present participles / continuous aspect)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing') and len(t) > 3)\n    return float(ing_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that contain a hyphen or slash (hyphenated/compound tokens), indicating technical or creative compounds'\n    import re\n    if not text:\n        return 0.0\n    # split on whitespace to preserve tokens with punctuation like hyphens\n    raw_tokens = text.split()\n    if not raw_tokens:\n        return 0.0\n    count = sum(1 for t in raw_tokens if '-' in t or '/' in t)\n    return float(count) / float(len(raw_tokens))\n\n", "def feature(text: str) -> float:\n    'Normalized word-type entropy (Shannon entropy / log2(vocabulary_size)) to measure lexical variety'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\b\\w+\\b', text)]\n    if not tokens:\n        return 0.0\n    freq = {}\n    for t in tokens:\n        freq[t] = freq.get(t, 0) + 1\n    total = float(len(tokens))\n    vocab_size = len(freq)\n    if vocab_size <= 1:\n        return 0.0\n    entropy = 0.0\n    for c in freq.values():\n        p = c / total\n        entropy -= p * math.log2(p)\n    # normalize by max possible entropy log2(vocab_size)\n    return float(entropy) / float(math.log2(vocab_size))\n\n", "def feature(text: str) -> float:\n    'Fraction of non-space characters that are punctuation of unique types (distinct punctuation characters / total punctuation count)'\n    import re\n    if not text:\n        return 0.0\n    puncts = re.findall(r'[^\\w\\s]', text)\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    total = len(puncts)\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Ratio of lines that appear to be dialogue or line-based speech (lines starting with quotes, dashes, or em-dash)'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    count = 0\n    for l in lines:\n        s = l.lstrip()\n        if s.startswith(('\"','\u201c','\u201d',\"'\",'-','\u2014')):\n            count += 1\n    return float(count) / float(len(lines))\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence end punctuation, keep only non-empty segments\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # guard against division by zero if somehow no words in sentences\n    valid_counts = [c for c in word_counts if c > 0]\n    if not valid_counts:\n        return 0.0\n    return float(sum(valid_counts)) / float(len(valid_counts))\n\n", "def feature(text: str) -> float:\n    'Titlecase ratio excluding sentence-initial words (proper noun / mid-sentence capital use)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    total_checked = 0\n    titlecase_count = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if not words:\n            continue\n        # skip first word of sentence\n        for w in words[1:]:\n            total_checked += 1\n            if w[0].isupper():\n                titlecase_count += 1\n    if total_checked == 0:\n        return 0.0\n    return float(titlecase_count) / float(total_checked)\n\n", "def feature(text: str) -> float:\n    'Hyphenated token ratio: fraction of tokens containing hyphen-like characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_chars = ('-', '\u2013', '\u2014')\n    count = 0\n    for t in tokens:\n        if any(h in t for h in hyphen_chars):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Roman numeral token ratio (e.g., XVI, XVII, or ranges like XVI-XVII) as indicator of historical/academic style'\n    import re\n    if not text:\n        return 0.0\n    # Convert to uppercase to match roman numerals\n    text_up = text.upper()\n    tokens = re.findall(r'\\b[IVXLCDM]+(?:-[IVXLCDM]+)?\\b', text_up)\n    total_words = len(re.findall(r'\\w+', text_up))\n    if total_words == 0:\n        return 0.0\n    return float(len(tokens)) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Ellipsis frequency: average number of ellipses (\"...\" or \"\u2026\") per sentence'\n    import re\n    if not text:\n        return 0.0\n    ell_count = text.count('...') + text.count('\u2026')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    denom = max(1, len(sentences))\n    return float(ell_count) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation count'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    total = len(puncts)\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Repeated bigram ratio: fraction of bigrams that occur more than once (redundancy / formulaic phrasing)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [' '.join(pair) for pair in zip(tokens, tokens[1:])]\n    if not bigrams:\n        return 0.0\n    counts = Counter(bigrams)\n    repeated = sum(1 for b in bigrams if counts[b] > 1)\n    return float(repeated) / float(len(bigrams))\n\n", "def feature(text: str) -> float:\n    'Numeric token ratio: fraction of tokens containing digits (years, percentages, numerals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Normalized sentence-length variability: stddev(words per sentence) / (mean+1)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # sentences split by ., !, ?\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(s.split()) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = var ** 0.5\n    return float(std) / (mean + 1.0)\n\n", "def feature(text: str) -> float:\n    'Density of 4-digit year mentions (1900-2099) per word, common in academic citations'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = text.split()\n    if not words:\n        return 0.0\n    years = re.findall(r'\\b(?:19|20)\\d{2}\\b', text)\n    return float(len(years)) / len(words)\n\n", "def feature(text: str) -> float:\n    'Frequency of the pattern \"et al\" (indicative of scholarly citation) per word'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = text.split()\n    if not words:\n        return 0.0\n    etal = len(re.findall(r'\\bet\\.?\\s*al\\.?\\b', text, flags=re.IGNORECASE))\n    return float(etal) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that look like list items or headings (start with -, *, or a number + period)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    marker_pat = re.compile(r'^\\s*(?:[-*]|(?:\\d+\\.)|[A-Za-z]\\))\\s+')\n    counted = sum(1 for l in lines if marker_pat.match(l))\n    return float(counted) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Ratio of hyphenated tokens (contains -) to total tokens, capturing compound/technical terms'\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t and any(c.isalpha() for c in t))\n    return float(hyphen_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Stopword density: fraction of tokens that are common English stopwords (approximate)'\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {'the','and','is','in','it','of','to','a','that','this','for','on','with','as','by','an','be','are','was','were','from'}\n    tokens = [t.strip(\".,;:()[]\\\"'\").lower() for t in text.split() if t.strip(\".,;:()[]\\\"'\")]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Acronym density: fraction of tokens that are all-caps of length >= 2 (e.g., DNA, USA), common in formal/technical texts'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = [t.strip('.,;:()[]\\\"\\'') for t in text.split() if t.strip('.,;:()[]\\\"\\'')]\n    if not tokens:\n        return 0.0\n    def is_acronym(tok):\n        return len(tok) >= 2 and tok.isalpha() and tok.upper() == tok\n    count = sum(1 for t in tokens if is_acronym(t))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation that appears in long runs (>=3), e.g., ellipses or repeated !!!, indicating informal trailing or emphasis'\n    import re\n    if not text:\n        return 0.0\n    punct_runs = re.findall(r'([^\\w\\s])\\1{2,}', text)  # captures repeats of same punctuation >=3\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    return float(len(punct_runs)) / total_punct\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas indicate clause density)'\n    import re\n    comma_count = text.count(',')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    return float(comma_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain hyphens (compound/technical terms or ranges)'\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphens = sum(1 for t in tokens if '-' in t and any(ch.isalnum() for ch in t))\n    return float(hyphens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like numbered bracket citations (e.g., [1], [12])'\n    import re\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    bracket_cites = len(re.findall(r'\\[\\s*\\d+\\s*\\]', text))\n    return float(bracket_cites) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of repeated bigram occurrences relative to total bigrams (measures local repetition)'\n    import re\n    from collections import Counter\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [' '.join((tokens[i], tokens[i+1])) for i in range(len(tokens)-1)]\n    counts = Counter(bigrams)\n    # total extra occurrences beyond the first (i.e., repeats)\n    repeated_occurrences = sum(cnt - 1 for cnt in counts.values() if cnt > 1)\n    return float(repeated_occurrences) / float(len(bigrams))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain digits (numerical mentions, years, measurements)'\n    import re\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(digit_tokens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of colons and semicolons per character (formal/complex punctuation usage)'\n    count = text.count(':') + text.count(';')\n    denom = max(1, len(text))\n    return float(count) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: number of words that appear exactly once divided by total tokens'\n    import re\n    from collections import Counter\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    hapax = sum(1 for w, c in counts.items() if c == 1)\n    return float(hapax) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of first-person pronouns (I, me, my, we, us, our) among all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of academic/jargon words (small curated list) as fraction of tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    jargon = {'study', 'research', 'method', 'analysis', 'characterized', 'characterise',\n              'characteristic', 'evidence', 'participants', 'significant', 'aims', 'integrate',\n              'propose', 'theory', 'model', 'results', 'conclusion', 'objective', 'framework'}\n    count = sum(1 for t in tokens if t in jargon)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Nominalization rate: fraction of tokens that end in common noun-forming suffixes'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(tion|ment|ness|ity|ence|ization|isation)$')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Sentence-start diversity: distinct first words of sentences divided by sentence count'\n    import re\n    if not text:\n        return 0.0\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    first_words = []\n    for p in parts:\n        m = re.search(r'\\b\\w+\\b', p)\n        if m:\n            first_words.append(m.group(0).lower())\n    if not first_words:\n        return 0.0\n    return float(len(set(first_words))) / float(len(first_words))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentence terminators that are exclamation or question marks (vs periods)'\n    import re\n    if not text:\n        return 0.0\n    total_terms = len(re.findall(r'[.!?]', text))\n    if total_terms == 0:\n        return 0.0\n    exq = len(re.findall(r'[!?]', text))\n    return float(exq) / float(total_terms)\n\n", "def feature(text: str) -> float:\n    'Simple passive-voice indicator: occurrences of forms like \"was/ were/ is/ are\" followed by an -ed/-en token, normalized by sentences'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r'\\b(?:was|were|is|are|been|being|be)\\b\\s+\\w+(?:ed|en)\\b', text.lower())\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    denom = float(len(sentences)) if sentences else 1.0\n    return float(len(matches)) / denom\n\n", "def feature(text: str) -> float:\n    'Sensory-word ratio: fraction of tokens that are sensory or imagery-related (see/hear/taste/etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    sensory = {'see','saw','seen','look','hear','heard','listen','taste','smell','feel','touch',\n               'bright','dark','loud','quiet','sweet','bitter','faint','vivid','gaze','stare'}\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Numeric token ratio: fraction of whitespace-separated tokens that contain any digit'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(ch.isdigit() for ch in t):\n            count += 1\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (word counts); measures syntactic variability'\n    import re, math\n    raw_sents = re.split(r'[.!?]+', text)\n    sent_word_counts = [len(re.findall(r'\\w+', s)) for s in raw_sents if re.search(r'\\w', s)]\n    if not sent_word_counts:\n        return 0.0\n    mean = sum(sent_word_counts) / len(sent_word_counts)\n    var = sum((c - mean) ** 2 for c in sent_word_counts) / len(sent_word_counts)\n    return math.sqrt(var)\n\n\n", "def feature(text: str) -> float:\n    'Interior-capitalized token ratio: capitals not at sentence starts (proxy for named-entities density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # split by sentence boundaries preserving tokens\n    sents = re.split(r'(?<=[.!?])\\s+', text)\n    interior_caps = 0\n    interior_total = 0\n    for s in sents:\n        tokens = re.findall(r'\\b\\w+\\b', s)\n        if not tokens:\n            continue\n        # skip first token in sentence\n        for tok in tokens[1:]:\n            interior_total += 1\n            if tok[0].isupper():\n                interior_caps += 1\n    if interior_total == 0:\n        return 0.0\n    return float(interior_caps) / float(interior_total)\n\n\n", "def feature(text: str) -> float:\n    'Numeric token ratio: fraction of tokens that contain a digit (years, percentages, IDs)'\n    import re\n    tokens = re.findall(r'\\b[\\w/-]+\\b', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_tokens) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Hedging/softening density: fraction of tokens that are hedging words (may, might, suggests, likely, possibly, appears)'\n    import re\n    hedges = {'may', 'might', 'possibly', 'perhaps', 'appears', 'suggests', 'suggest', 'likely', 'approximately', 'seems', 'argues'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (comma density), with safe fallbacks'\n    import re\n    commas = text.count(',')\n    # count sentences by punctuation enders\n    sent_count = max(1, len([s for s in re.split(r'[.!?]+', text) if re.search(r'\\w', s)]))\n    return float(commas) / float(sent_count)\n\n\n", "def feature(text: str) -> float:\n    'Top-token dominance: frequency of the most common token divided by total tokens (higher means more repetition)'\n    import re\n    from collections import Counter\n    tokens = [t.lower() for t in re.findall(r'\\b\\w+\\b', text)]\n    if not tokens:\n        return 0.0\n    freqs = Counter(tokens)\n    top_freq = freqs.most_common(1)[0][1]\n    return float(top_freq) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Parenthetical citation density: normalized count of parentheses that contain a 4-digit year or an author-like comma (e.g., (Smith, 1996))'\n    import re\n    if not text:\n        return 0.0\n    parens = re.findall(r'\\([^)]{0,200}\\)', text)\n    if not parens:\n        return 0.0\n    matches = 0\n    for p in parens:\n        if re.search(r'\\b(1[0-9]{3}|20[0-9]{2})\\b', p) or re.search(r'[A-Z][a-z]+,\\s*\\d{4}', p):\n            matches += 1\n    return matches / float(len(parens))\n\n", "def feature(text: str) -> float:\n    'Nominalization ratio: fraction of tokens ending with common nominalizing suffixes (tion, sion, ment, ness, ity, ize/ise)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    nom = re.compile(r'(tion|sion|ment|ness|ity|ize|ise|ization|isation)$')\n    count = sum(1 for w in words if nom.search(w))\n    return count / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Semicolon and colon density per sentence: (count of ; and :) / max(1, sentence count)'\n    import re\n    if not text:\n        return 0.0\n    punct_count = text.count(';') + text.count(':')\n    # estimate sentences\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return punct_count / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Sentence length variability: standard deviation of sentence lengths measured in words'\n    import re, math\n    if not text:\n        return 0.0\n    # Split into sentences using .!? sequences\n    raw_sents = re.split(r'(?<=[.!?])\\s+', text.strip())\n    lengths = []\n    for s in raw_sents:\n        words = re.findall(r'\\w+', s)\n        if words:\n            lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    return math.sqrt(var)\n\n", "def feature(text: str) -> float:\n    'Section header fraction: fraction of paragraphs that begin with common headers like Introduction, Abstract, Conclusion'\n    import re\n    if not text:\n        return 0.0\n    paras = [p for p in re.split(r'\\n{2,}', text) if p.strip() != '']\n    if not paras:\n        return 0.0\n    header_re = re.compile(r'^\\s*(Introduction|Abstract|Conclusion|Background|Methods|Results|Discussion)\\b', re.IGNORECASE)\n    header_count = sum(1 for p in paras if header_re.search(p))\n    return header_count / float(len(paras))\n\n", "def feature(text: str) -> float:\n    'Hyphenated compound ratio: fraction of word tokens that contain a hyphen (e.g., long-term, state-of-the-art)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[^\\s]+\\b', text)\n    if not words:\n        return 0.0\n    hyph = sum(1 for w in words if '-' in w and re.search(r'\\w-\\w', w))\n    return hyph / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Repeated-punctuation score: proportion of punctuation occurrences that are repeated sequences (e.g., \"...\", \"!!\", \"--\", \"\u2014\")'\n    import re\n    if not text:\n        return 0.0\n    puncts = re.findall(r'[^\\w\\s]', text)\n    total_punct = len(puncts)\n    if total_punct == 0:\n        return 0.0\n    repeats = len(re.findall(r'(\\.\\.\\.|[!?.]{2,}|[-\u2013\u2014]{2,})', text))\n    return repeats / float(total_punct)\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\" or \"\u2026\") per word (ellipses occurrences normalized by word count)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = len(words)\n    ellipses = len(re.findall(r'\\.\\.\\.|\u2026', text))\n    return float(ellipses) / float(max(1, word_count))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, us, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    fp = {'i','me','my','mine','myself','we','us','our','ours','ourselves'}\n    count = sum(1 for t in tokens if t in fp)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of common sensory/scene words (see, hear, stare, misty, eerie, dim, etc.) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    sensory = {\n        'see','saw','seen','look','looked','stare','stared','gaze','gazed',\n        'hear','heard','listen','listened','sound','smell','smelled','smelling',\n        'taste','touch','felt','feel','piercing','shrill','eerie','misty','dark','dim',\n        'shadow','shadows','silence','jingle','scent','fog','foggy'\n    }\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that begin with a pronoun (I, you, he, she, they, we, etc.)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+|\\n+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    pronouns = {'i','you','he','she','they','we','it','who','someone','someone\\'s','he\\'s','she\\'s','they\\'re'}\n    starts = 0\n    for s in sentences:\n        m = re.search(r'\\b\\w+\\b', s)\n        if m and m.group(0).lower() in pronouns:\n            starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Average per-sentence lexical diversity: mean(unique words / words) across sentences'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+|\\n+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    ratios = []\n    for s in sentences:\n        tokens = [t.lower() for t in re.findall(r'\\b\\w+\\b', s)]\n        n = len(tokens)\n        if n == 0:\n            continue\n        ratios.append(len(set(tokens)) / n)\n    if not ratios:\n        return 0.0\n    return float(sum(ratios)) / float(len(ratios))\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation count (0 if none)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are long (>=7 characters) as a proxy for lexical complexity'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) >= 7)\n    return float(long_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Normalized density of common profanity tokens (simple lexicon-based)'\n    import re\n    if not text:\n        return 0.0\n    s = text.lower()\n    words = re.findall(r'\\w+', s)\n    if not words:\n        return 0.0\n    profane = {'fuck', 'fucking', 'shit', 'damn', 'bitch', 'crap', 'hell', 'asshole', 'fucker', 'bloody'}\n    count = sum(1 for w in words if w in profane)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ellipsis usage: count of \"...\" or Unicode ellipsis vs sentence count (expressive/truncated style)'\n    import re\n    if not text:\n        return 0.0\n    s = str(text)\n    ell_count = s.count('...') + s.count('\u2026')\n    # estimate sentence count\n    sent_count = max(1, s.count('.') + s.count('!') + s.count('?'))\n    return float(ell_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Ratio of standalone lowercase \"i\" tokens (indicative of informal/typo-prone lowercasing)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    i_count = sum(1 for t in tokens if t == 'i')\n    return float(i_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens containing 3+ repeated characters in a row (character elongation like \"soooo\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    rep_count = sum(1 for t in tokens if re.search(r'(.)\\1\\1', t))\n    return float(rep_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of simple formatting markers (*, _, `) per token (marks markdown/typed formatting presence)'\n    if not text:\n        return 0.0\n    s = str(text)\n    tokens = s.split()\n    token_count = max(1, len(tokens))\n    marker_count = s.count('*') + s.count('_') + s.count('`')\n    return float(marker_count) / float(token_count)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena rate: fraction of word types that occur exactly once (another lexical richness measure)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    counts = {}\n    for t in tokens:\n        counts[t] = counts.get(t, 0) + 1\n    hapax = sum(1 for v in counts.values() if v == 1)\n    # normalize by total token count to avoid inflation in very short texts\n    return float(hapax) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average length of repeated punctuation clusters (e.g., \"!!!\", \"???\", \"...\") \u2014 captures expressive punctuation'\n    import re\n    if not text:\n        return 0.0\n    s = str(text)\n    clusters = [len(m.group(0)) for m in re.finditer(r'([^\\w\\s])\\1+', s)]\n    if not clusters:\n        return 0.0\n    return float(sum(clusters)) / float(len(clusters))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are four-digit numbers (years/dates) \u2014 detects explicit date mentions like \"2046\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    year_count = sum(1 for t in tokens if re.fullmatch(r'\\d{4}', t))\n    return float(year_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Ratio of common English stopwords to total tokens (a proxy for function-word density and formal prose)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[^\\d\\W_][\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    stopwords = {'the','is','and','of','to','a','in','that','it','for','with','as','on','are','was','by','an','be','this','which','or','from','at','their','these','they','has','have','had'}\n    sw_count = sum(1 for t in tokens if t in stopwords)\n    return float(sw_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique lowercase word forms divided by total token count (lexical variety)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[^\\d\\W_][\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proper-noun density: fraction of tokens that are capitalized but not sentence-initial (proxy for names, places, titles)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    proper_count = 0\n    total = 0\n    for sent in sentences:\n        sent = sent.strip()\n        if not sent:\n            continue\n        words = re.findall(r\"\\b[^\\d\\W_][\\w'-]*\\b\", sent)\n        if not words:\n            continue\n        for i, w in enumerate(words):\n            total += 1\n            if i == 0:\n                continue\n            if w[0].isupper() and not w.isupper() and any(ch.isalpha() for ch in w):\n                proper_count += 1\n    if total == 0:\n        return 0.0\n    return float(proper_count) / total\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas indicate clause density and more complex, academic prose)'\n    import re\n    if not text:\n        return 0.0\n    # count sentences robustly; fall back to 1 if none to avoid division by zero\n    sentence_boundaries = re.findall(r'[.!?]+', text)\n    num_sentences = max(1, len(sentence_boundaries))\n    comma_count = text.count(',')\n    return float(comma_count) / num_sentences\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hedging/modal words (may, might, could, suggest, appears) indicating cautious/academic tone'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','seem','seems','appear','appears','suggest','suggests','likely','possibly','perhaps','tend','tends','arguably'}\n    tokens = re.findall(r\"\\b[^\\d\\W_][\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Citation-year density: fraction of sentences that contain a parenthesized year like (1999) or (2010), common in academic text'\n    import re\n    if not text:\n        return 0.0\n    year_matches = re.findall(r'[\\(\\[]\\s*(19|20)\\d{2}\\s*[\\)\\]]', text)\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(year_matches)) / sentence_count\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, we, us, our, mine, ours) to total tokens'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours', 'myself', 'ourselves', 'i\\'m', 'i\u2019d'}\n    count = sum(1 for w in words if w in first_person)\n    return count / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    total_sent = max(1, text.count('.') + text.count('!') + text.count('?'))\n    question_count = text.count('?')\n    return question_count / float(total_sent)\n\n", "def feature(text: str) -> float:\n    'Density of quotation-mark characters (\", \u201c, \u201d) per character, capturing dialogue or quoted text'\n    if not text:\n        return 0.0\n    quotes = sum(text.count(q) for q in ['\"', '\u201c', '\u201d', '\u201e', '\u00ab', '\u00bb'])\n    return quotes / float(len(text))\n\n", "def feature(text: str) -> float:\n    'Proportion of titlecase words (words starting with an uppercase followed by lowercase letters) among all tokens'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    titlecase = sum(1 for t in tokens if len(t) > 1 and t[0].isupper() and t[1:].islower())\n    return titlecase / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation count (0 if none)'\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return distinct / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain digits (useful to detect lists, dates, measurements, formal documents)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    num_with_digits = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return num_with_digits / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice marker ratio: count of \"be\" forms followed by an -ed word divided by token count'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    passive_matches = re.findall(r'\\b(?:is|are|was|were|be|been|being)\\b\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    return len(passive_matches) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a first-person marker or \"Personally\" (indicates personal stance)'\n    import re\n    # Split into sentences using punctuation followed by whitespace (keeps fragments too)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    first_person_starts = {'i', 'we', 'me', 'my', 'ours', 'our', 'mine', 'personally'}\n    count = 0\n    for s in sentences:\n        m = re.match(r'^\\W*([A-Za-z\\']+)', s)\n        if m:\n            first = m.group(1).lower()\n            if first in first_person_starts:\n                count += 1\n    return count / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Hyphenated token density: fraction of tokens containing a hyphen (compound/technical terms indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyph = sum(1 for t in tokens if '-' in t)\n    return float(hyph) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Numeric token ratio: fraction of tokens that contain at least one digit (counts numerals, measurements, lists)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(num) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Normalized occurrences of \"et al\" citations: count of \"et al\" (with or without period) per 100 words'\n    import re\n    if not text:\n        return 0.0\n    words = max(1, len(re.findall(r'\\w+', text)))\n    matches = len(re.findall(r'\\bet\\.?\\s+al\\b', text, flags=re.IGNORECASE))\n    # scale to per 100 words to keep magnitude readable\n    return (matches * 100.0) / words\n\n", "def feature(text: str) -> float:\n    'Passive-voice proxy: number of simple passive-like patterns (was/were/is/are + -ed) per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    matches = len(re.findall(r'\\b(?:was|were|is|are|has been|have been|had been|been|being)\\b\\s+\\w+ed\\b', text, flags=re.IGNORECASE))\n    return float(matches) / sentences\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences containing a colon or semicolon (complex clause/connective use)'\n    import re\n    if not text:\n        return 0.0\n    raw_sentences = [s for s in re.split(r'[.!?]', text) if s.strip()]\n    if not raw_sentences:\n        return 0.0\n    cnt = sum(1 for s in raw_sentences if ':' in s or ';' in s)\n    return float(cnt) / len(raw_sentences)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per sentence (trailing thoughts, truncation, summarization style)'\n    import re\n    if not text:\n        return 0.0\n    ell = text.count('...')\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(ell) / sentences\n\n", "def feature(text: str) -> float:\n    'Top-word concentration: frequency of the most common word divided by total words (lexical repetition/boilerplate)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    counts = Counter(words)\n    most_common_freq = counts.most_common(1)[0][1]\n    return float(most_common_freq) / len(words)\n", "def feature(text: str) -> float:\n    'Normalized count of URL/markdown-like tokens (http, www, [, ], *, parentheses) as a sign of links/metadata'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    denom = max(1, len(words))\n    marker_count = text.count('http') + text.count('www.') + text.count('[') + text.count(']') + text.count('*') + text.count('(') + text.count(')')\n    return float(marker_count) / float(denom)\n\n\n", "def feature(text: str) -> float:\n    'Average number of commas per (detected) sentence: comma_count / (sentence_count+epsilon) to estimate clause density'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(commas) / float(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Ratio of words that are archaic/formal tokens (whilst, hitherto, unto, hath, wherefore, hereby, thereof, henceforth, thou)'\n    import re\n    if not text:\n        return 0.0\n    archaic = {'whilst', 'hitherto', 'unto', 'hath', 'wherefore', 'hereby', 'thereof', 'henceforth', 'thou', 'ye', 'thine', 'ere'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in archaic)\n    return float(count) / float(len(words))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain an ellipsis (\"...\") as a marker of trailing/dramatic style'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(ellipses) / float(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Density of parenthetical/bracket characters ((), []) per word as a proxy for asides, citations or links'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    denom = max(1, len(words))\n    paren_count = text.count('(') + text.count(')') + text.count('[') + text.count(']')\n    return float(paren_count) / float(denom)\n\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total words (lower values indicate repetition)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n\n\n", "def feature(text: str) -> float:\n    'Normalized standard deviation of punctuation counts per sentence: stddev(punct_count_per_sentence) / (mean+1)'\n    import re\n    if not text:\n        return 0.0\n    # split sentences\n    sentences = [s for s in re.split(r'[.!?]+', text) if s is not None]\n    # filter out empties\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    punct_counts = []\n    for s in sentences:\n        punct = sum(1 for c in s if not c.isalnum() and not c.isspace())\n        punct_counts.append(punct)\n    n = len(punct_counts)\n    mean = sum(punct_counts) / float(n)\n    variance = sum((x - mean) ** 2 for x in punct_counts) / float(n)\n    stddev = variance ** 0.5\n    return float(stddev) / (mean + 1.0)\n\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (contain \"?\"), a proxy for rhetorical/questioning tone'\n    if not text:\n        return 0.0\n    question_marks = text.count('?')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(question_marks) / float(sentences)\n", "def feature(text: str) -> float:\n    'Fraction of short sentences (<= 3 words) among all sentence-like segments (split on .!? or newline)'\n    import re\n    if not text:\n        return 0.0\n    segments = [seg.strip() for seg in re.split(r'[.!?\\n]+', text) if seg.strip()]\n    if not segments:\n        return 0.0\n    short_count = 0\n    for seg in segments:\n        words = seg.split()\n        if len(words) <= 3:\n            short_count += 1\n    return float(short_count) / float(len(segments))\n\n", "def feature(text: str) -> float:\n    'Density of first-person pronouns among word tokens (I, me, my, mine, we, us, our)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    pronouns = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by number of sentences), 0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    # count sentence terminators as sentences; also consider newline as potential separator\n    sentences = [s for s in re.split(r'[.!?\\n]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of occurrences of \"...\" per sentence (normalized by sentence count)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = [s for s in re.split(r'[.!?\\n]+', text) if s.strip()]\n    denom = float(len(sentences)) if len(sentences) > 0 else 1.0\n    return float(ellipses) / denom\n\n", "def feature(text: str) -> float:\n    'Vowel-to-consonant ratio in alphabetic characters (a,e,i,o,u vs other letters)'\n    if not text:\n        return 0.0\n    vowels = set('aeiouAEIOU')\n    vcount = 0\n    ccount = 0\n    for ch in text:\n        if ch.isalpha():\n            if ch in vowels:\n                vcount += 1\n            else:\n                ccount += 1\n    denom = float(ccount) if ccount > 0 else 1.0\n    return float(vcount) / denom\n\n", "def feature(text: str) -> float:\n    'Mean length of repeated punctuation clusters (e.g., \"...\", \"!!!\"), averaged over all clusters; 0 if none'\n    import re\n    if not text:\n        return 0.0\n    clusters = re.findall(r'([^\\w\\s])\\1+', text)\n    # the above finds the characters but not full length; find full sequences\n    seqs = re.findall(r'([^\\w\\s])\\1+', text)\n    if not seqs:\n        return 0.0\n    # compute lengths by scanning all sequences of repeated punctuation\n    lengths = [len(m.group(0)) for m in re.finditer(r'([^\\w\\s])\\1+', text)]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for sentence complexity / subordinate clauses)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Average clause length in words where clauses are defined by commas (long clauses suggest more complex prose)'\n    import re\n    if not text:\n        return 0.0\n    clauses = [c.strip() for c in text.split(',') if c.strip()]\n    if not clauses:\n        words = text.split()\n        return float(sum(len(w) for w in words)) / float(max(1, len(words))) if words else 0.0\n    lengths = [len(re.findall(r'\\w+', c)) for c in clauses]\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Diversity of contractions: unique contraction forms divided by total contraction tokens (higher = varied contractions)'\n    import re\n    if not text:\n        return 0.0\n    # capture typical contractions like don't, I\\'m, he\\'ll, we\\'ve etc.\n    contractions = re.findall(r\"\\b[a-zA-Z]+\\'[a-zA-Z]{1,4}\\b\", text)\n    total = len(contractions)\n    if total == 0:\n        return 0.0\n    unique = len(set([c.lower() for c in contractions]))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Parenthesis usage density: number of parenthesis characters per sentence (signals asides or explanatory style)'\n    import re\n    if not text:\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(paren_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence-like segments that begin with a quotation mark (indicates direct dialogue turns)'\n    import re\n    if not text:\n        return 0.0\n    segments = [s for s in re.split(r'([.!?]+)', text) if s and not re.fullmatch(r'[.!?]+', s)]\n    if not segments:\n        return 0.0\n    starts_with_quote = 0\n    for s in segments:\n        s_strip = s.lstrip()\n        if s_strip.startswith('\"') or s_strip.startswith('\u201c') or s_strip.startswith(\"'\"):\n            starts_with_quote += 1\n    return float(starts_with_quote) / float(len(segments))\n\n", "def feature(text: str) -> float:\n    'Proportion of apostrophes that indicate possessive \\\"\\'s\\\" vs other apostrophe uses (possessive bias)'\n    import re\n    if not text:\n        return 0.0\n    total_apostrophe_uses = len(re.findall(r\"'\", text)) + len(re.findall(r\"\u2019\", text))\n    if total_apostrophe_uses == 0:\n        return 0.0\n    # Count occurrences of \"'s\" or \"\u2019s\" as possessive indicators (word-boundary aware)\n    possessive = len(re.findall(r\"\\b\\w+(?:'|\u2019)[sS]\\b\", text))\n    return float(possessive) / float(total_apostrophe_uses)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence-like segments that end with an ellipsis or that are not terminated by .!? (incomplete or trailing style)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like segments by sentence terminators while retaining trailing pieces\n    segments = [s for s in re.split(r'([.!?]+)', text) if s and not re.fullmatch(r'[.!?]+', s)]\n    if not segments:\n        return 0.0\n    count_incomplete = 0\n    for seg in segments:\n        seg_strip = seg.rstrip()\n        if seg_strip.endswith('...') or not re.search(r'[.!?]\\s*$', seg):\n            # treat interior segments that do not end with punctuation or that explicitly use ellipse as incomplete\n            count_incomplete += 1\n    return float(count_incomplete) / float(len(segments))\n", "def feature(text: str) -> float:\n    'Density of ellipses (three-dot sequences or single ellipsis char) per sentence'\n    import re\n    if not text:\n        return 0.0\n    ellipsis_count = text.count('...') + text.count('\u2026')\n    # approximate sentence count (safe lower bound)\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(ellipsis_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a digit (numeric-token ratio)'\n    import re\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(digit_tokens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of interior capitalized words (likely proper names) to total words \u2014 excludes first word of sentences'\n    import re\n    words_all = re.findall(r'\\w+', text)\n    total = len(words_all)\n    if total == 0:\n        return 0.0\n    # split into sentences, examine words that are not first in a sentence and start with uppercase\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    proper_interior = 0\n    for s in sentences:\n        tokens = re.findall(r\"\\b[\\w'-]+\\b\", s)\n        for i, tok in enumerate(tokens):\n            if i == 0:\n                continue  # skip sentence-initial capitalized tokens\n            if len(tok) > 1 and tok[0].isupper() and tok[1:].islower():\n                proper_interior += 1\n    return float(proper_interior) / float(total)\n\n", "def feature(text: str) -> float:\n    'Density of sensory/concrete words (sea, sand, metal, heart, eyes, gut, smell, taste, sight, sound, touch, cold, warm, blood) per word'\n    import re\n    if not text:\n        return 0.0\n    sens = {'sea', 'sand', 'metal', 'heart', 'eyes', 'eye', 'gut', 'smell', 'smelled', 'taste', 'sight', 'sound', 'touch', 'cold', 'warm', 'blood', 'ocean', 'salt', 'sand-encrusted'}\n    words = re.findall(r\"\\w+['\u2019]?\\w*|\\w+\", text.lower())\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if w in sens:\n            count += 1\n        # also check partial matches like 'sand-encrusted' split into parts\n        elif '-' in w:\n            parts = w.split('-')\n            if any(p in sens for p in parts):\n                count += 1\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of short sentences (fewer than 3 words) to total sentences (estimates fragments/brief lines)'\n    import re\n    if not text:\n        return 0.0\n    # split by sentence terminators, keep fragments\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip() != '']\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = [w for w in re.findall(r\"\\w+['\u2019]?\\w*|\\w+\", s)]\n        if len(words) < 3:\n            short += 1\n    return float(short) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Immediate repeated-word ratio: fraction of adjacent token pairs that are identical (captures stuttering/repetition)'\n    import re\n    tokens = re.findall(r\"\\w+['\u2019]?\\w*|\\w+\", text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(tokens)) if tokens[i] == tokens[i-1])\n    return float(repeats) / float(len(tokens) - 1)\n\n", "def feature(text: str) -> float:\n    'Log smoothed ratio of colons to semicolons (log((colons+0.5)/(semicolons+0.5))) to capture explanatory/claused style'\n    import math\n    if not text:\n        return 0.0\n    colons = text.count(':')\n    semis = text.count(';')\n    # smooth and take log to keep scale reasonable; returns 0 when equal counts\n    ratio = (colons + 0.5) / (semis + 0.5)\n    try:\n        return math.log(ratio)\n    except Exception:\n        return 0.0\n", "def feature(text: str) -> float:\n    'First-person pronoun fraction (I, me, my, mine, we, us, our, ours) as a marker of personal voice'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Proper-noun fraction (capitalized words not at sentence starts) \u2014 proxy for named entities and titles'\n    import re\n    sentences = re.split(r'[.!?]\\s*', text)\n    proper = 0\n    nonfirst = 0\n    for s in sentences:\n        if not s.strip():\n            continue\n        # keep original case\n        words = re.findall(r\"[A-Za-z][A-Za-z']*\", s)\n        if len(words) <= 1:\n            continue\n        for w in words[1:]:\n            nonfirst += 1\n            if len(w) > 1 and w[0].isupper() and not w.isupper():\n                proper += 1\n    if nonfirst == 0:\n        return 0.0\n    return float(proper) / nonfirst\n\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per sentence (0 if no sentences) \u2014 signals stylistic complexity'\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    semicolons = text.count(';')\n    return float(semicolons) / sentences\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain quotation marks (\", \u201c, \u201d) \u2014 dialogue or quoted material indicator'\n    import re\n    # split on sentence terminators; keep segments\n    parts = [p for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', '\u00ab', '\u00bb', \"'\")\n    quoted = sum(1 for p in parts if any(ch in p for ch in quote_chars))\n    return float(quoted) / len(parts)\n\n", "def feature(text: str) -> float:\n    'Average clause length (words per clause) splitting on , ; . ! ? \u2014 captures sub-sentence complexity'\n    import re\n    clauses = [c.strip() for c in re.split(r'[,\\;.!?]+', text) if c.strip()]\n    if not clauses:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', c)) for c in clauses]\n    counts = [c for c in counts if c > 0]\n    if not counts:\n        return 0.0\n    return float(sum(counts)) / len(counts)\n\n", "def feature(text: str) -> float:\n    'Hyphenated-word fraction (words containing -) \u2014 indicates compound terms or editorial style'\n    import re\n    words = re.findall(r'\\b\\S+\\b', text)\n    if not words:\n        return 0.0\n    hyph = sum(1 for w in words if '-' in w)\n    return float(hyph) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence starts that begin with common conjunctions (and, but, or, so, thus, however, then) \u2014 discourse starter pattern'\n    import re\n    starts = re.findall(r'(?:^|[.!?]\\s+)(\\w+)', text.lower())\n    if not starts:\n        return 0.0\n    conj = {'and','but','or','so','thus','however','then','yet','because'}\n    count = sum(1 for w in starts if w in conj)\n    return float(count) / len(starts)\n\n", "def feature(text: str) -> float:\n    'Normalized maximum consecutive repeated-word run (long duplicate runs normalized by total words) \u2014 catches editing artifacts or emphasis'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    max_run = 1\n    cur = 1\n    for a, b in zip(words, words[1:]):\n        if a == b:\n            cur += 1\n            if cur > max_run:\n                max_run = cur\n        else:\n            cur = 1\n    return float(max_run) / len(words)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a first-person \"I\" (captures \"I apologize\", \"I will not\", etc.)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    count = 0\n    for s in sents:\n        sstr = s.lstrip()\n        # get first token\n        m = re.match(r\"['\\\"\\(\\[]*\\s*([A-Za-z0-9_']+)\", sstr)\n        if m:\n            first = m.group(1).lower()\n            if first in ('i', \"i'm\", \"i\u2019ll\", \"i\u2019ll\", \"i\u2019ve\", \"i'd\", \"i\u2019ll\".lower()):\n                count += 1\n            elif first == \"i\":\n                count += 1\n    return float(count) / len(sents)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that explicitly refer to AI identity or system words (ai, assistant, anthropic, model, bot)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    ai_terms = {'ai', 'assistant', 'anthropic', 'model', 'bot', 'system'}\n    count = sum(1 for t in tokens if t in ai_terms)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that belong to a small safety/policy vocabulary (generate, story, prompt, harmful, unethical, safe, comfortable, response)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    vocab = {'generate', 'generating', 'story', 'prompt', 'response', 'harmful', 'unethical', 'safe', 'safety', 'policy', 'comfortable', 'comfort', 'provide'}\n    count = sum(1 for t in tokens if t in vocab)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio (first-person tokens +1) / (third-person tokens +1) to capture narrative perspective bias'\n    import re\n    if not text:\n        return 1.0\n    tokens = re.findall(r'\\w+', text.lower())\n    first = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    third = {'he', 'she', 'they', 'him', 'her', 'them', 'his', 'hers', 'their', 'theirs'}\n    fcount = sum(1 for t in tokens if t in first)\n    tcount = sum(1 for t in tokens if t in third)\n    return float(fcount + 1) / float(tcount + 1)\n\n", "def feature(text: str) -> float:\n    'Density of capitalized tokens that look like proper names (regex [A-Z][a-z]{2,}) normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    proper_matches = re.findall(r'\\b[A-Z][a-z]{2,}\\b', text)\n    return float(len(proper_matches)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of word-tokens that are present participles (ending with \"ing\" and length>3) \u2014 indicative of descriptive/narrative style'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if len(t) > 3 and t.lower().endswith('ing'))\n    return float(ing_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that mention prompt/story/generate together with a refusal-style negation (captures \"cannot generate this story\", \"will not generate the prompt\")'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip().lower() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    content_pattern = re.compile(r'\\b(prompt|story|generate|generation|provide)\\b')\n    refusal_pattern = re.compile(r\"\\b(not|no|never)\\b|n'?t\\b|\\bcan'?t\\b|\\bcannot\\b|\\bwon't\\b|\\bwill not\\b\")\n    count = 0\n    for s in sents:\n        if content_pattern.search(s) and refusal_pattern.search(s):\n            count += 1\n    return float(count) / len(sents)\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentence count) as a proxy for clause density and complexity'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences <= 0:\n        # fallback: normalize by word count if no sentence punctuation\n        words = text.split()\n        return float(commas) / max(1.0, float(len(words)))\n    return float(commas) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen (e.g., 25-year-old, long-term), indicates compound/adjective-heavy style'\n    if not text:\n        return 0.0\n    tokens = [t for t in text.split() if t]\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of colons and semicolons per character, capturing formal/academic punctuation usage'\n    if not text:\n        return 0.0\n    punct_count = text.count(':') + text.count(';')\n    return float(punct_count) / max(1.0, float(len(text)))\n\n", "def feature(text: str) -> float:\n    'Normalized frequency of modal-verb + adverb patterns (e.g., \"could potentially\", \"might also\") per word'\n    import re\n    if not text:\n        return 0.0\n    text_l = text.lower()\n    # look for modal + an -ly adverb immediately following (common hedging in formal/AI text)\n    matches = re.findall(r'\\b(?:could|might|may|would|should|can|must)\\s+\\w+ly\\b', text_l)\n    words = re.findall(r'\\w+', text_l)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Loose passive-voice indicator: count of auxiliary verbs followed by an -ed participle (is/was/were + Xed) normalized by words'\n    import re\n    if not text:\n        return 0.0\n    text_l = text.lower()\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|has|have|had)\\b\\s+\\w+ed\\b', text_l)\n    words = re.findall(r'\\w+', text_l)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of apostrophes that are typographic/unicode (\u2019 U+2019) vs total apostrophes, indicating edited/published text'\n    if not text:\n        return 0.0\n    uni = text.count('\u2019')\n    ascii = text.count(\"'\")\n    total = uni + ascii\n    if total == 0:\n        return 0.0\n    return float(uni) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like Roman numerals (II, III, IV, V, etc.), capturing historical references or formal numbering'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z]+\", text)\n    if not tokens:\n        return 0.0\n    roman_pattern = re.compile(r'^[IVXLCDM]+$', re.IGNORECASE)\n    roman_count = 0\n    for t in tokens:\n        # strip trailing/leading non-letters already handled; check uppercase form with valid roman letters\n        if roman_pattern.match(t.upper()):\n            # exclude single-letter tokens that are unlikely roman numerals except I and V etc.\n            roman_count += 1\n    return float(roman_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of titlecased tokens (Word with initial capital) to total tokens, approximating presence of proper nouns and named entities'\n    if not text:\n        return 0.0\n    tokens = [t.strip('.,:;!?()\"\\'') for t in text.split() if t.strip('.,:;!?()\"\\'')]\n    if not tokens:\n        return 0.0\n    title_count = sum(1 for t in tokens if t.istitle())\n    return float(title_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Ratio of tokens containing digits (dates, measurements, enumerations) to total tokens'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of titlecase tokens (Initial capital then lowercase), often proper nouns or section starts'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.istitle())\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), capturing formality and information density'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # Split into sentences by punctuation, keep non-empty fragments\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    words_per_sentence = [len(s.split()) for s in sentences]\n    return float(sum(words_per_sentence)) / len(words_per_sentence)\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: occurrences of \"be\" forms followed by -ed verbs per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    low = text.lower()\n    # crude pattern for passive: be-form + word ending in -ed\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|am)\\b\\s+\\w+ed\\b', low)\n    # count sentences for normalization\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    return float(len(matches)) / sent_count\n\n", "def feature(text: str) -> float:\n    'Fraction of hedging/modal/uncertainty words (may, might, could, possibly, likely, appear, seem, perhaps)'\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','would','should','can','possibly','perhaps','likely','appear','appears','seem','seems','tend','tends','suggests','suggest'}\n    tokens = [t.strip('.,;:()[]{}\"\\'').lower() for t in text.split()]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    unique = len(set(puncts))\n    total = len(puncts)\n    return float(unique) / total\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio (I, me, my, we, our) to total tokens, indicating personal vs impersonal tone'\n    if not text:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    tokens = [t.strip('.,;:()[]{}\"\\'').lower() for t in text.split()]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Estimated passive-voice incidence: count of \"be\" forms followed by a past-participial-looking token, normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    sents = max(1, len(re.findall(r'[.!?]+', text)))\n    pattern = re.compile(r'\\b(?:is|are|was|were|be|been|being|seem|seems|seemed)\\b\\s+\\w+(?:ed|en|n)\\b', re.IGNORECASE)\n    matches = len(pattern.findall(text))\n    return float(matches) / float(sents)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence end punctuation; keep segments that contain a word\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if re.search(r'\\w', p)]\n    if not parts:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', p)) for p in parts]\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Parenthetical density: number of parenthetical/grouped phrases or em-dash uses per sentence (indicates asides and qualifiers)'\n    import re\n    if not text:\n        return 0.0\n    sents = max(1, len(re.findall(r'[.!?]+', text)))\n    paren_count = text.count('(') + text.count(')') + text.count('\u2014') + text.count('--')\n    return float(paren_count) / float(sents)\n\n", "def feature(text: str) -> float:\n    'Citation-like token density: occurrences of bracket citations [1], parenthetical author-year citations (Smith, 2020), or \"et al.\" per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    bracket_cites = len(re.findall(r'\\[\\s*\\d+\\s*\\]', text))\n    author_year = len(re.findall(r'\\([A-Z][A-Za-z\\-\\']{1,20},\\s*\\d{4}\\)', text))\n    et_al = len(re.findall(r'\\bet\\s+al\\.?', text, re.IGNORECASE))\n    total = bracket_cites + author_year + et_al\n    return float(total) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Hedging/uncertainty word ratio: fraction of tokens that are hedging words (may, might, could, suggests, appears, likely, possibly)'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','possibly','suggests','suggest','appears','likely','tends','seem','seems','appeared','appearing','possible','probable','may be'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in hedges)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Uppercase acronym ratio: fraction of tokens that are all-uppercase acronyms (2+ letters), indicating technical terms or named entities'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    acr_count = sum(1 for w in words if re.match(r'^[A-Z]{2,}$', w))\n    return float(acr_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Function-word density: fraction of tokens that are common function words (the, of, and, to, in, a, is, that, it, for, on, with)'\n    import re\n    if not text:\n        return 0.0\n    stopset = {'the','of','and','to','in','a','is','that','it','for','on','with','as','by','an','be','are','was'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopset)\n    return float(count) / float(len(words))\n", "def feature(text: str) -> float:\n    'Normalized frequency of ellipses or long dot sequences (three or more dots) per word'\n    import re\n    words = re.findall(r'\\w+', text)\n    total = len(words)\n    if total == 0:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    return float(ellipses) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of bigrams that are repeated (repetition tendency in adjacent token patterns)'\n    import re\n    tokens = re.findall(r\"\\w+(?:'\\w+)?\", text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [tokens[i] + '\\u0000' + tokens[i+1] for i in range(len(tokens)-1)]\n    total = len(bigrams)\n    unique = len(set(bigrams))\n    repeats = total - unique\n    return float(repeats) / float(total)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are likely adverbs (end with ly) among all tokens'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    advs = sum(1 for t in tokens if len(t) > 2 and t.endswith('ly'))\n    return float(advs) / float(total)\n\n", "def feature(text: str) -> float:\n    'Normalized count of common modal verbs (would, could, should, might, must, will, shall)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    modals = {'would', 'could', 'should', 'might', 'must', 'will', 'shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(total)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (comma density as a proxy for sentence complexity)'\n    import re\n    commas = text.count(',')\n    # sentence count by punctuation; treat line breaks as potential sentence separators too\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(commas) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that match a trailing possessive-s pattern (possessive-s like words) among words'\n    import re\n    tokens = re.findall(r\"\\b\\w+'s\\b\", text.lower())\n    words = re.findall(r'\\w+', text)\n    total = len(words)\n    if total == 0:\n        return 0.0\n    return float(len(tokens)) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are long (more than 20 words) as a measure of long-sentence prevalence'\n    import re\n    sent_splits = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in sent_splits if s.strip()]\n    if not sentences:\n        return 0.0\n    long_count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) > 20:\n            long_count += 1\n    return float(long_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Variety of punctuation types used (unique punctuation characters normalized by a fixed max)'\n    import string\n    punct_set = set(c for c in text if c in string.punctuation)\n    # normalize by number of common punctuation characters to keep output in 0-1\n    max_types = float(len(string.punctuation)) if len(string.punctuation) > 0 else 1.0\n    return float(len(punct_set)) / max_types\n", "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that include an asterisk (*) (often used for emphasis or markup in informal/human text)'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '*' in t)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of repeated non-alphanumeric punctuation sequences (counts of sequences like \"!!\", \"?!\", \"...\" divided by word count)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = max(1, len(words))\n    seqs = re.findall(r'[^A-Za-z0-9\\s]{2,}', text)\n    return float(len(seqs)) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of detected sentences that begin with a quotation mark (\", \\', or typographic quotes), approximating dialog openings'\n    import re\n    if not text:\n        return 0.0\n    # split into rough sentences\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = ('\"', \"'\", '\u201c', '\u201d', '\u2018', '\u2019')\n    count = 0\n    for s in sentences:\n        s_stripped = s.lstrip()\n        if s_stripped and s_stripped[0] in quote_chars:\n            count += 1\n    return float(count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that appear inside parentheses (words enclosed in (...) ), indicating asides/parenthetical comments'\n    import re\n    if not text:\n        return 0.0\n    total_words = max(1, len(re.findall(r'\\w+', text)))\n    paren_groups = re.findall(r'\\((.*?)\\)', text, flags=re.S)\n    inside_words = 0\n    for g in paren_groups:\n        inside_words += len(re.findall(r'\\w+', g))\n    return float(inside_words) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain runs of 2 or more uppercase letters (captures ALL-CAPS or emphatic uppercase sequences)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'[A-Z]{2,}')\n    count = 0\n    for t in tokens:\n        if pattern.search(t):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of punctuation count per sentence (captures variability in expressive punctuation use)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    punct_counts = []\n    for s in sentences:\n        punct_counts.append(sum(1 for c in s if not c.isalnum() and not c.isspace()))\n    if not punct_counts:\n        return 0.0\n    mean = sum(punct_counts) / len(punct_counts)\n    if len(punct_counts) == 1:\n        return 0.0\n    var = sum((pc - mean) ** 2 for pc in punct_counts) / (len(punct_counts) - 1)\n    std = math.sqrt(var)\n    return std / (mean + 1e-9)\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that occur inside quoted spans (both double and single quotes combined), approximating dialogue density'\n    import re\n    if not text:\n        return 0.0\n    total_words = max(1, len(re.findall(r'\\w+', text)))\n    quoted_spans = []\n    quoted_spans += re.findall(r'\"(.*?)\"', text, flags=re.S)\n    # for single quotes, avoid simple contractions by requiring at least 2 words inside or a space\n    single_quotes = re.findall(r\"'(.*?)'\", text, flags=re.S)\n    for sq in single_quotes:\n        if re.search(r'\\s', sq) or len(re.findall(r'\\w+', sq)) > 1:\n            quoted_spans.append(sq)\n    words_in_quotes = 0\n    for span in quoted_spans:\n        words_in_quotes += len(re.findall(r'\\w+', span))\n    return float(words_in_quotes) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Smoothed ratio: (second-person pronoun count + 0.5) / (first-person pronoun count + 0.5)'\n    import re\n    if not text:\n        return 1.0\n    second = len(re.findall(r\"\\b(?:you|your|yours|you're|youve|you've|yall|y'all)\\b\", text, flags=re.I))\n    first = len(re.findall(r\"\\b(?:I|me|my|mine|we|us|our|ours|I'm|I'm|im)\\b\", text, flags=re.I))\n    return float(second + 0.5) / float(first + 0.5)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with a single short word (<=12 letters) followed immediately by a comma (e.g., \"Since, ...\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # split into sentences (simple)\n    sentences = re.split(r'(?<=[.!?])\\s+|\\n+', text.strip())\n    if not sentences:\n        return 0.0\n    count = 0\n    total = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        total += 1\n        if re.match(r\"^[A-Za-z]{1,12},\", s):\n            count += 1\n    return float(count) / float(total) if total else 0.0\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") measured as occurrences per 100 words'\n    import re\n    if not text:\n        return 0.0\n    words = max(1, len(re.findall(r'\\w+', text)))\n    ellipses = text.count('...')\n    return float(ellipses) / float(words) * 100.0\n\n", "def feature(text: str) -> float:\n    'Binary-ish feature: text ends with an ellipsis (\"...\") -> 1.0 else 0.0'\n    if not text:\n        return 0.0\n    return 1.0 if text.rstrip().endswith('...') else 0.0\n\n", "def feature(text: str) -> float:\n    'Proportion of very short sentences (<=5 words) to total sentences'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+|\\n+', text.strip())\n    total = 0\n    short = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        total += 1\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 5:\n            short += 1\n    return float(short) / float(total) if total else 0.0\n\n", "def feature(text: str) -> float:\n    'Paragraph break density: number of double newlines (\\\\n\\\\n) per 100 words'\n    import re\n    if not text:\n        return 0.0\n    words = max(1, len(re.findall(r'\\w+', text)))\n    pbreaks = text.count('\\n\\n')\n    return float(pbreaks) / float(words) * 100.0\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+|\\n+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    commas = text.count(',')\n    return float(commas) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Frequency of long punctuation runs (sequences of non-alnum chars length >=3) per word'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    cur = 0\n    for c in text:\n        if not c.isalnum() and not c.isspace():\n            cur += 1\n        else:\n            if cur >= 3:\n                count += 1\n            cur = 0\n    if cur >= 3:\n        count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain a double-quote character (indicative of dialogue usage)'\n    if not text:\n        return 0.0\n    import re\n    # split into sentences (approximate)\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    quoted = sum(1 for s in sents if '\"' in s)\n    return float(quoted) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a double-quote (common for spoken/dialogue lines)'\n    if not text:\n        return 0.0\n    import re\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    starts_with_quote = sum(1 for s in sents if s.lstrip().startswith('\"'))\n    return float(starts_with_quote) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that look like internal-abbreviations with embedded dots (e.g., \"S.S.\", \"U.S.\")'\n    if not text:\n        return 0.0\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if re.search(r'[A-Za-z]\\.[A-Za-z]', t):\n            # avoid counting pure ellipses or trailing punctuation-only tokens\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average sentence length measured in words (words per sentence, at least 1 sentence)'\n    if not text:\n        return 0.0\n    import re\n    words = text.split()\n    if not words:\n        return 0.0\n    # approximate sentence count by punctuation; ensure at least 1\n    sent_count = text.count('.') + text.count('!') + text.count('?')\n    if sent_count <= 0:\n        # fallback: split on newlines or treat whole text as one sentence\n        sent_count = max(1, len([s for s in re.split(r'\\n+', text) if s.strip()]))\n    sent_count = max(1, sent_count)\n    return float(len(words)) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing an elongated letter sequence (same letter repeated 3+ times, e.g., \"ummm\", \"soooo\")'\n    if not text:\n        return 0.0\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if re.search(r'(.)\\1{2,}', t.lower()):\n            count += 1\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique token count divided by total token count (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(n)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with \"I\" (case-insensitive) \u2014 proxy for first-person sentence openings'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s+|\\n+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts_with_I = 0\n    for s in sentences:\n        s_strip = s.lstrip()\n        if not s_strip:\n            continue\n        # check if first token is 'I' (allow \"I'\", \"I,\" etc.)\n        first_token = re.match(r\"^([A-Za-z]+)(?:\\b|['\\W])\", s_strip)\n        if first_token:\n            if first_token.group(1).lower() == 'i':\n                starts_with_I += 1\n    return float(starts_with_I) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (mean words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s+|\\n+', text) if s.strip()]\n    if not sentences:\n        # fallback to overall token average\n        tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text)\n        return float(len(tokens))\n    lengths = []\n    for s in sentences:\n        tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", s)\n        lengths.append(len(tokens))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Density of sensory verbs/lemmas (see/hear/smell/taste/feel/watch/look and common inflections) per token'\n    import re\n    if not text:\n        return 0.0\n    sensory = {'see','saw','seen','see','look','looked','seen','glimpse','glimpsed',\n               'hear','heard','listen','listened','sound','sounded',\n               'smell','smelled','scent','sniff','tasted','taste',\n               'feel','felt','touch','touched','watch','watched','notice','noticed','observe','observed'}\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per sentence (often used in dramatic/AI prose)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    # count sentences roughly\n    sentences = [s for s in re.split(r'[.!?]+\\s+|\\n+', text) if s.strip()]\n    denom = float(len(sentences)) if sentences else max(1.0, len(text.split()))\n    return float(ellipses) / denom\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (length >= 8), as a proxy for lexical complexity'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) >= 8)\n    return float(long_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Bigram repetition rate: fraction of bigrams that occur more than once (repetitive phrasing indicator)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = ['%s %s' % (tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    counts = Counter(bigrams)\n    repeated = sum(1 for b in counts.values() if b > 1)\n    total = len(bigrams)\n    return float(repeated) / float(total)\n", "def feature(text: str) -> float:\n    'Normalized density of ellipses (\"...\") per word (ellipses count / number of words)'\n    import re\n    words = re.findall(r\"\\w+\", text)\n    ellipses = text.count('...')\n    denom = max(1, len(words))\n    return float(ellipses) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like adverbs by ending with \"ly\" (simple adverb proxy)'\n    import re\n    words = re.findall(r\"\\w+\", text)\n    if not words:\n        return 0.0\n    cnt = sum(1 for w in words if len(w) > 2 and w.lower().endswith('ly'))\n    return float(cnt) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain a quotation mark (double-quote variants) to detect dialogue or quoted material'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences, keep pieces that have at least one word\n    parts = [p for p in re.split(r'[.!?]+', text) if re.search(r'\\w', p)]\n    if not parts:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d')\n    cnt = sum(1 for s in parts if any(q in s for q in quote_chars))\n    return float(cnt) / float(len(parts))\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation characters (0 if none)'\n    import string\n    punct_chars = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    if not punct_chars:\n        return 0.0\n    distinct = set(punct_chars)\n    return float(len(distinct)) / float(len(punct_chars))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common sensory verbs or forms (see, saw, feel, heard, smell, taste, touch, listen, feel variants)'\n    import re\n    sens = {'see','saw','seen','seeing','feel','felt','hear','heard','listen','listened','smell','smelled','taste','tasted','touch','touched','sight','felt','felt','perceive','perceived'}\n    words = re.findall(r\"\\w+\", text)\n    if not words:\n        return 0.0\n    cnt = sum(1 for w in words if w.lower() in sens)\n    return float(cnt) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (can, could, may, might, shall, should, will, would, must) as a proxy for modality'\n    import re\n    modals = {'can','could','may','might','shall','should','will','would','must'}\n    words = re.findall(r\"\\w+\", text)\n    if not words:\n        return 0.0\n    cnt = sum(1 for w in words if w.lower() in modals)\n    return float(cnt) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total number of word tokens (lexical diversity)'\n    import re\n    words = re.findall(r\"\\w+\", text.lower())\n    if not words:\n        return 0.0\n    unique = set(words)\n    return float(len(unique)) / float(len(words))\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain a double or curly quotation mark (dialog / quoted text)'\n    import re\n    # split into sentence-like segments\n    sents = re.split(r'[.!?]', text)\n    if not sents:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d')\n    qcount = 0\n    for s in sents:\n        if any(c in s for c in quote_chars):\n            qcount += 1\n    return float(qcount) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Lexical diversity: unique word tokens divided by total word tokens'\n    import re\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens matching a small sensory vocabulary (see, hear, feel, smell, taste, glow, etc.)'\n    import re\n    sensory = {'see','saw','seen','look','looked','look','hear','heard','listen','feel','felt','smell','smelled','taste',\n               'touch','glow','glowing','bright','dark','shimmer','shimmering','whisper','hiss','silent','silence',\n               'shine','shining','shadow','shadowed','glitter'}\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count, sentence count >=1)'\n    import re\n    commas = text.count(',')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    denom = max(1, sentences)\n    return float(commas) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation characters that are part of a repeated sequence (e.g., \"...\" or \"!!\")'\n    import re\n    puncts = re.findall(r'[^\\w\\s]', text)\n    total_punct = len(puncts)\n    if total_punct == 0:\n        return 0.0\n    repeated = 0\n    for m in re.finditer(r'([^\\w\\s])\\1+', text):\n        repeated += len(m.group(0))\n    return float(repeated) / float(total_punct)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like proper names (capitalized not at sentence starts or all-caps acronyms), excluding lone \"I\"'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip()) if s]\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    proper_count = 0\n    # build set of sentence-initial tokens to exclude\n    sentence_starts = set()\n    for s in sentences:\n        m = re.findall(r'\\b\\w+\\b', s)\n        if m:\n            sentence_starts.add(m[0])\n    for tok in tokens:\n        if tok == 'I':\n            continue\n        if tok in sentence_starts:\n            continue\n        if tok.isupper() and len(tok) > 1:\n            proper_count += 1\n        elif tok[0].isupper() and (len(tok) > 1 and tok[1:].islower()):\n            proper_count += 1\n    return float(proper_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of -ly adverbs (heuristic) per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    ly_count = len([w for w in words if w.endswith('ly') and len(w) > 2])\n    return float(ly_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of occurrences of progressive constructions (was/were/is/are/am + -ing) to words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    prog_matches = re.findall(r'\\b(?:was|were|is|are|am|been|being|be)\\s+\\w+ing\\b', text.lower())\n    return float(len(prog_matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are honorifics/titles (Dr., Mr., Mrs., Prof., Capt., Col., etc.)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+\\.?|\\S', text)\n    if not words:\n        return 0.0\n    honors = re.findall(r'\\b(?:Dr|Mr|Mrs|Ms|Prof|Captain|Capt|Col|Colonel|Sir|Madam)\\.?\\b', text, flags=re.IGNORECASE)\n    return float(len(honors)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ellipsis-style usage density: counts \"...\" or unicode ellipsis sequences per word'\n    import re\n    if not text:\n        return 0.0\n    # count three-dot variants including spaced dots and unicode ellipsis\n    ellipses = len(re.findall(r'\\.\\s*\\.\\s*\\.', text)) + text.count('\u2026')\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(ellipses) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Modal verb density (could/would/should/might/may/must/can/shall) per token'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    mods = re.findall(r'\\b(?:could|would|should|might|may|must|can|shall)\\b', text, flags=re.IGNORECASE)\n    return float(len(mods)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (length >= 8 characters) as a simple lexical sophistication measure'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 8)\n    return float(long_words) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average commas per sentence (commas divided by sentence-count), robust to missing punctuation'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # estimate sentences by punctuation; fallback to 1 to avoid division by zero\n    sentences = [s for s in re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip()) if s]\n    sentence_count = len(sentences)\n    if sentence_count == 0:\n        # if no sentence-ending punctuation, treat the whole text as one sentence\n        sentence_count = 1\n    return float(comma_count) / float(sentence_count)\n", "def feature(text: str) -> float:\n    'Ratio of tokens that are written in all-caps (length>=2 and at least two letters) \u2014 captures shouting/strong emphasis'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        if len(letters) < 2:\n            return False\n        return ''.join(letters).upper() == ''.join(letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of capitalized words that are NOT at sentence start (proxy for proper-noun density)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'[.!?]+\\s*', text)\n    total_words = 0\n    nonstart_caps = 0\n    for sent in sentences:\n        if not sent.strip():\n            continue\n        words = re.findall(r\"\\b[\\w'-]+\\b\", sent)\n        if not words:\n            continue\n        for idx, w in enumerate(words):\n            total_words += 1\n            if idx == 0:\n                continue\n            if w and w[0].isupper() and not w.isupper():\n                nonstart_caps += 1\n    if total_words == 0:\n        return 0.0\n    return float(nonstart_caps) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that end with \"ing\" (present-participial / gerundive usage)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        lw = w.lower()\n        if len(lw) > 3 and lw.endswith('ing') and any(c.isalpha() for c in lw[:-3]):\n            count += 1\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Maximum consecutive run of punctuation characters divided by word count (captures \"!!!\" or \"...\" emphasis)'\n    import re\n    if not text:\n        return 0.0\n    max_run = 0\n    cur = 0\n    for ch in text:\n        if not ch.isalnum() and not ch.isspace():\n            cur += 1\n            if cur > max_run:\n                max_run = cur\n        else:\n            cur = 0\n    word_count = len(re.findall(r'\\w+', text))\n    if word_count == 0:\n        return 0.0\n    return float(max_run) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentence-ending punctuation that is \"!\" or \"?\" (exclamatory/interrogative emphasis)'\n    if not text:\n        return 0.0\n    ex = text.count('!') + text.count('?')\n    total = text.count('.') + text.count('!') + text.count('?')\n    if total == 0:\n        return 0.0\n    return float(ex) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that contain any digit (numeric token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(ch.isdigit() for ch in t):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of common short stopwords (the,a,an,and,of,to,in,is,was,he,she,it,that,this) \u2014 captures function-word prominence'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','a','an','and','of','to','in','is','was','were','it','that','this','he','she','they','i','we','you','his','her','for','on','with','as','at','by','from'}\n    words = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stop)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that occur inside parentheses relative to total word count'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    inside_words = 0\n    # capture text inside parentheses (non-greedy)\n    for inner in re.findall(r'\\((.*?)\\)', text, flags=re.S):\n        inside_words += len(re.findall(r'\\w+', inner))\n    return float(inside_words) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of colons and semicolons per sentence (punctuation density indicating academic style)'\n    import re\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    colons_semis = text.count(':') + text.count(';')\n    return float(colons_semis) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Ratio of standalone four-digit numbers (years or citations) to total word tokens'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    years = len(re.findall(r'\\b\\d{4}\\b', text))\n    return float(years) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total tokens (lexical variety)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause density and syntactic complexity)'\n    import re\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    commas = text.count(',')\n    return float(commas) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Density of quotation marks (double and typographic quotes) per token, indicating quoted material or titles'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    quotes = len(re.findall(r'[\"\u201c\u201d]', text))\n    return float(quotes) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of first-person pronouns (I, me, my, we, our, etc.) relative to token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours',\"i'm\",\"i've\",\"i'd\",\"i'll\"}\n    count = 0\n    for w in tokens:\n        if w.lower() in first_person:\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of word tokens ending with -ed (approximate past-tense verb ratio)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    cnt = 0\n    for w in tokens:\n        wl = w.lower()\n        if len(wl) > 3 and wl.endswith('ed'):\n            cnt += 1\n    return float(cnt) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Binary indicator (1.0/0.0) for classic narrative openings such as \"once upon a time\", \"in the heart\", \"there lived\"'\n    if not text:\n        return 0.0\n    s = text.lower()\n    openings = ['once upon a time', 'in the heart', 'there lived', 'long ago', 'in a small town', 'in a small village', 'in the middle of']\n    for ph in openings:\n        if ph in s:\n            return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Density of common sensory/visual verbs (see, stare, look, hear, feel, watch, notice) relative to token count'\n    import re\n    if not text:\n        return 0.0\n    sensory = {'see','saw','seen','stare','stared','staring','stared','look','looked','looking','hear','heard','listen','listened','felt','feel','touch','touched','gaze','gazed','watch','watched','notice','noticed','watching','gazed','glanced','glance','peek','peered','observed'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    cnt = 0\n    for w in tokens:\n        if w.lower() in sensory:\n            cnt += 1\n    return float(cnt) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Normalized count of emotive/adjective words from a small curated lexicon (e.g., trembling, extraordinary, mundane)'\n    import re\n    if not text:\n        return 0.0\n    lex = {'trembling','extraordinary','mundane','significant','heartbroken','lonely','angry','terrified','hopeful','melancholy','struggling','brave','quiet','frayed','yellowed','tremulous','dreadful'}\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    cnt = 0\n    for w in tokens:\n        if w.lower().strip(\"'-\") in lex:\n            cnt += 1\n    return float(cnt) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of short sentence fragments (<=5 words) among detected sentences'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence terminators; include newlines as potential separators\n    parts = [s.strip() for s in re.split(r'[.!?]+\\s*|\\n+', text) if s.strip()]\n    if not parts:\n        return 0.0\n    short = 0\n    for s in parts:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if len(words) <= 5:\n            short += 1\n    return float(short) / len(parts)\n\n", "def feature(text: str) -> float:\n    'Pronoun variety: distinct pronouns used divided by total pronoun occurrences (higher = more varied pronoun use)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','you','your','yours','he','him','his','she','her','hers','they','them','their','theirs','it','its','who','whom','whose'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    found = []\n    for w in tokens:\n        lw = w.lower().strip(\"'-\")\n        if lw in pronouns:\n            found.append(lw)\n    if not found:\n        return 0.0\n    distinct = len(set(found))\n    return float(distinct) / len(found)\n\n", "def feature(text: str) -> float:\n    'Lexical repetition rate: 1 - (unique content words / total content words) for words >3 letters excluding common stopwords'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','a','an','and','or','but','in','on','at','to','of','for','with','as','is','was','were','be','been','by','that','this','it','its','he','she','they','i','you','we','his','her','their','my','our','your'}\n    tokens = re.findall(r\"\\b\\w+\\b\", text)\n    content = [w.lower() for w in tokens if len(w) > 3 and w.lower() not in stop]\n    if not content:\n        return 0.0\n    unique = len(set(content))\n    return 1.0 - (unique / len(content))\n", "def feature(text: str) -> float:\n    'Density of double/curly quotation mark characters normalized by text length (excludes straight apostrophes)'\n    if not text:\n        return 0.0\n    quote_chars = ['\"', '\\u201c', '\\u201d', '\\u00ab', '\\u00bb', '\\u201e', '\\u201f']\n    total_chars = max(1, len(text))\n    qcount = sum(text.count(c) for c in quote_chars)\n    return float(qcount) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Fraction of all words that appear inside paired double/unicode quotation marks (approximate dialog presence)'\n    import re\n    if not text:\n        return 0.0\n    all_words = re.findall(r'\\b\\w+\\b', text)\n    total_words = max(1, len(all_words))\n    # match content between common double-quote characters\n    matches = re.findall(r'[\"\\u201c\\u201d\\u00ab\\u00bb\\u201e\\u201f](.+?)[\"\\u201c\\u201d\\u00ab\\u00bb\\u201e\\u201f]', text, flags=re.S)\n    inside_words = sum(len(re.findall(r'\\b\\w+\\b', m)) for m in matches)\n    return float(inside_words) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Average number of exclamation marks per sentence (exclamation intensity) computed as total ! divided by sentence-end punctuation count'\n    import re\n    if not text:\n        return 0.0\n    exclamations = text.count('!')\n    sentence_end_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(exclamations) / float(sentence_end_count)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of ellipsis sequences (\"...\" or \"\u2026\") normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}|\u2026', text))\n    words = max(1, len(re.findall(r'\\b\\w+\\b', text)))\n    return float(ellipses) / float(words)\n\n", "def feature(text: str) -> float:\n    'Proper-noun density: fraction of tokens that are capitalized mid-sentence (heuristic for names and titles)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    total_tokens = max(1, len(tokens))\n    proper_count = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if not words:\n            continue\n        # exclude first word of sentence\n        for w in words[1:]:\n            if len(w) > 0 and w[0].isupper() and w.lower() != 'i' and any(ch.islower() for ch in w[1:]):\n                proper_count += 1\n    return float(proper_count) / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (under 6 words) \u2014 captures terse/dialogue style versus flowing narration'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if len(words) < 6:\n            short += 1\n    return float(short) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ratio of modal verbs (can/could/will/would/shall/should/may/might/must) to all tokens \u2014 signals speculative or advisory tone'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice indicator per sentence: counts occurrences of auxiliaries + -ed past participle (approx) normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    # look for patterns like \"was Xed\", \"were Xed\", \"is Xed\", \"has been Xed\" (simple heuristic)\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|be|being|has been|have been|had been|was been)\\b\\s+\\w+ed\\b', flags=re.I)\n    matches = len(pattern.findall(text))\n    sentence_end = max(1, len(re.findall(r'[.!?]', text)))\n    return float(matches) / float(sentence_end)\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like dialogue lines (start with quote, em-dash or hyphen)'\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    dialogue_start_chars = ('\"', \"'\", '\u201c', '\u2018', '-', '\u2014')\n    dialogue_lines = 0\n    for l in lines:\n        ls = l.lstrip()\n        if ls.startswith(dialogue_start_chars):\n            dialogue_lines += 1\n    return float(dialogue_lines) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Average number of written ellipses \"...\" per sentence (ellipses divided by sentence count)'\n    import re\n    ellipses = text.count('...')\n    # approximate sentence count\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences <= 0:\n        return float(ellipses)\n    return float(ellipses) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Indicator (1.0/0.0) whether the text contains a header-like \"by <Name>\" line (common in bylines)'\n    import re\n    if re.search(r'(?m)^\\s*by\\s+\\S', text, re.I):\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Ratio: frequency of the most common capitalized token divided by total capitalized tokens (repeated proper names)'\n    import re\n    from collections import Counter\n    caps = re.findall(r'\\b[A-Z][a-z]+\\b', text)\n    if not caps:\n        return 0.0\n    counts = Counter(caps)\n    most = counts.most_common(1)[0][1]\n    return float(most) / float(len(caps))\n\n", "def feature(text: str) -> float:\n    'Proportion of bigram tokens that are repeated occurrences beyond their first appearance (bigram repetition density)'\n    import re\n    from collections import Counter\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    bigrams = list(zip(words, words[1:]))\n    counts = Counter(bigrams)\n    repeated_tokens = sum((c - 1) for c in counts.values() if c > 1)\n    return float(repeated_tokens) / float(len(bigrams))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (comma density)'\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    commas = text.count(',')\n    if sentences <= 0:\n        # if no clear sentence punctuation, normalize by token count to avoid division by zero\n        import re\n        tokens = max(1, len(re.findall(r'\\w+', text)))\n        return float(commas) / float(tokens)\n    return float(commas) / float(max(1, sentences))\n\n", "def feature(text: str) -> float:\n    'Number of parenthetical/bracket characters per token (indicator of asides or meta text)'\n    import re\n    paren_chars = sum(text.count(c) for c in '()[]{}')\n    tokens = max(1, len(re.findall(r'\\w+', text)))\n    return float(paren_chars) / float(tokens)\n\n", "def feature(text: str) -> float:\n    'Average length in characters of the first clause of each sentence (substring before first comma) \u2014 measures clause brevity'\n    import re\n    pieces = re.split(r'[.!?]+', text)\n    lengths = []\n    for p in pieces:\n        s = p.strip()\n        if not s:\n            continue\n        clause = s.split(',', 1)[0].strip()\n        lengths.append(len(clause))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain quotation marks (dialogue density)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    if not sentences:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', \"\u00ab\", \"\u00bb\")\n    quoted = sum(1 for s in sentences if any(q in s for q in quote_chars))\n    return float(quoted) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Density of newline characters (newlines per character) \u2014 indicates lists/structured fragments'\n    if not text:\n        return 0.0\n    return float(text.count('\\n')) / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like list or metadata lines (start with -, number., or \\'Word:\\')'\n    import re\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    non_empty = [ln for ln in lines if ln.strip()]\n    if not non_empty:\n        return 0.0\n    def is_list_like(ln: str) -> bool:\n        s = ln.lstrip()\n        if s.startswith(('-', '*', '\u2022')):\n            return True\n        if re.match(r'^\\s*\\d+[\\.\\)]\\s+', ln):\n            return True\n        if re.match(r'^\\s*[\\w\\'-]+:', ln):\n            return True\n        return False\n    count = sum(1 for ln in non_empty if is_list_like(ln))\n    return float(count) / float(len(non_empty))\n\n", "def feature(text: str) -> float:\n    'Rate of ellipses (\\'...\\', unicode ellipsis) per token \u2014 indicates trailing thoughts/truncation or dramatic pauses'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    ellipsis_count = text.count('...') + text.count('\u2026')\n    return float(ellipsis_count) / float(max(1, len(tokens)))\n\n", "def feature(text: str) -> float:\n    'Fraction of apostrophe-bearing tokens that are negative contractions containing \\\"n\\'t\\\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    apostrophe_tokens = [t for t in tokens if \"'\" in t]\n    if not apostrophe_tokens:\n        return 0.0\n    count_nt = sum(1 for t in apostrophe_tokens if \"n't\" in t.lower())\n    return float(count_nt) / float(len(apostrophe_tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<=5 words) \u2014 short sentences often indicate dialogue or informal style'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = sum(1 for s in sentences if len(s.split()) <= 5)\n    return float(short) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Normalized count of distinct punctuation characters used (divided by 10, capped at 1.0) \u2014 punctuation variety'\n    if not text:\n        return 0.0\n    puncts = set(c for c in text if not c.isalnum() and not c.isspace())\n    score = float(len(puncts)) / 10.0\n    return float(score if score <= 1.0 else 1.0)\n", "def feature(text: str) -> float:\n    'Ratio of tokens that are common sensory words (see/hear/smell/taste/touch and related adjectives/nouns)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    sensory = {'see','saw','seen','seeings','look','looked','looked','look','hear','heard','listen','smell','scent','scented','taste','tasted','touch','felt','feel','sweaty','bright','vibrant','hue','hues','color','colors','sight','sightless','sound','shout','scream','screamed','shouted','crowd','stampede','grasp','door','desk','book','birthday','pink','slip','geyser'}\n    cnt = sum(1 for t in tokens if t in sensory)\n    return float(cnt) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain direct-quote characters (indicative of dialogue presence)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences by punctuation followed by whitespace or end\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', \"'\")\n    cnt = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            cnt += 1\n    return float(cnt) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause density and syntactic complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of word tokens ending in \"ly\" (heuristic measure of adverb usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly'))\n    return float(ly_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of honorifics/titles (Mr, Mrs, Dr, Captain, Detective, etc.) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    titles = re.findall(r'\\b(?:mr|mrs|ms|dr|sir|miss|captain|detective|prof|mrs)\\.?\\b', text, flags=re.IGNORECASE)\n    return float(len(titles)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain parentheses or dashes (extraorial asides or parenthetical structure)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    cnt = 0\n    for s in sentences:\n        if '(' in s or ')' in s or '\u2014' in s or ' - ' in s:\n            cnt += 1\n    return float(cnt) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation count (higher = more varied punctuation)'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(len(puncts))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<=5 words), indicating fragmentation or clipped dialogue'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s.strip() for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if len(words) <= 5:\n            short += 1\n    return float(short) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Proportion of non-empty lines that begin with one or more asterisks (markdown/manifest markers like \"*\", \"**\")'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip() != '']\n    if not lines:\n        return 0.0\n    starts_with_asterisk = sum(1 for ln in lines if ln.lstrip().startswith('*'))\n    return float(starts_with_asterisk) / float(len(lines))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens (length >=2) that are ALL CAPS (acronym/initialism density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    caps = sum(1 for t in tokens if len(t) >= 2 and t.isupper())\n    return float(caps) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Density of ellipsis occurrences (\"...\") normalized by sentence-like punctuation count (ellipses per sentence)'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentence_marks = text.count('.') + text.count('!') + text.count('?')\n    denom = max(1, sentence_marks)\n    return float(ellipses) / float(denom)\n\n\n", "def feature(text: str) -> float:\n    'Frequency of long repeated punctuation runs (e.g., \"!!!\", \"...\", \"???\" or \"***\") per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    runs = re.findall(r'([!?._\\-\\*])\\1{2,}', text)\n    denom = max(1, len(tokens))\n    return float(len(runs)) / float(denom)\n\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of unique word forms that occur exactly once (vocabulary novelty)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    counts = {}\n    for w in words:\n        counts[w] = counts.get(w, 0) + 1\n    unique_words = len(counts)\n    if unique_words == 0:\n        return 0.0\n    hapax = sum(1 for v in counts.values() if v == 1)\n    return float(hapax) / float(unique_words)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen \"-\" (hyphenated-token ratio)'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphenated = sum(1 for t in tokens if '-' in t and any(ch.isalpha() for ch in t.replace('-', '')))\n    return float(hyphenated) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are \"meta\" words often indicating prompts or system/user/AI discussion (prompt, assistant, ai, gpt, generated, model, user)'\n    import re\n    if not text:\n        return 0.0\n    meta_set = {'prompt', 'assistant', 'user', 'ai', 'gpt', 'generated', 'model', 'system', 'response'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    meta_count = sum(1 for t in tokens if t in meta_set)\n    return float(meta_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Ratio of tokens ending with \"ing\" (approximate progressive/gerund usage) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text)\n    if not tokens:\n        return 0.0\n    cnt = sum(1 for t in tokens if len(t) >= 4 and t.lower().endswith('ing'))\n    return float(cnt) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending in \"ly\" (approximate adverb/adjective -ly usage) among all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text)\n    if not tokens:\n        return 0.0\n    cnt = sum(1 for t in tokens if len(t) >= 3 and t.lower().endswith('ly'))\n    return float(cnt) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of ellipses: number of \"...\" occurrences per sentence (sentences split on terminal punctuation)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    # split into sentences with punctuation followed by whitespace (keeps edge cases reasonable)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    s_count = max(1, len(sentences))\n    return float(ellipses) / float(s_count)\n\n", "def feature(text: str) -> float:\n    'Density of common dialogue/speech tags (said, asked, replied, whispered, shouted, murmured) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    tags = {'said','asked','replied','whispered','shouted','murmured','cried','answered'}\n    cnt = sum(1 for t in tokens if t in tags)\n    return float(cnt) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of capitalized words that are likely named entities (capitalized not immediately after sentence end) per token'\n    import re\n    if not text:\n        return 0.0\n    token_count = max(1, len(re.findall(r'\\w+', text)))\n    ents = 0\n    for m in re.finditer(r'\\b([A-Z][a-z]+)\\b', text):\n        start = m.start()\n        # look left for previous non-space character (if none, treat as sentence start)\n        i = start - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        prev_char = text[i] if i >= 0 else ''\n        # if previous non-space char is a sentence terminator, treat as sentence-start and skip\n        if prev_char in '.!?':\n            continue\n        ents += 1\n    return float(ents) / float(token_count)\n\n", "def feature(text: str) -> float:\n    'Approximate punctuation variety: unique punctuation characters divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if (not c.isalnum() and not c.isspace())]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    unique = len(set(puncts))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain a quotation mark (indicative of dialogue presence), using double or single quotes'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    cnt = sum(1 for s in sentences if '\"' in s or \"'\" in s)\n    return float(cnt) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that are \"complex\" by containing two or more commas (approximate syntactic complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    complex_sentences = sum(1 for s in sentences if s.count(',') >= 2)\n    return float(complex_sentences) / float(len(sentences))\n", "def feature(text: str) -> float:\n    \"Frequency of ellipses ('...') per sentence, capturing trailing/fragmentary style\"\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    # Count sentences robustly\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    # If no conventional sentence-ending punctuation, fall back to newline count\n    if sentences == 0:\n        sentences = max(1, text.count('\\n') + 1)\n    return ellipses / float(max(1.0, sentences))\n\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (distinct lowercased word forms / total words) as a measure of lexical diversity'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text, flags=re.UNICODE)\n    if not words:\n        return 0.0\n    lowered = [w.lower() for w in words]\n    distinct = len(set(lowered))\n    return distinct / float(len(lowered))\n\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence, approximating clause/coordination complexity'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences == 0:\n        # treat whole text as one sentence if no punctuation\n        sentences = 1\n    return commas / float(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Normalized standard deviation of word lengths (std / mean) to capture lexical length variability'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text, flags=re.UNICODE)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if len(lengths) < 2 or mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / (len(lengths) - 1)\n    std = math.sqrt(var)\n    return std / mean\n\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that contain digits (years, dates, enumerations), often present in historical/encyclopedic text'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[\\w-]+\\b', text, flags=re.UNICODE)\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return digit_tokens / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Estimated character-level Shannon entropy (bits per character), higher values indicate more varied character usage'\n    import math\n    if not text:\n        return 0.0\n    counts = {}\n    total = 0\n    for ch in text:\n        counts[ch] = counts.get(ch, 0) + 1\n        total += 1\n    if total == 0:\n        return 0.0\n    entropy = 0.0\n    for v in counts.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    return entropy\n", "def feature(text: str) -> float:\n    'Titlecase token density outside sentence-starts: fraction of capitalized words that are not simply sentence-initial (proxy for named entities)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    count_title_nonstart = 0\n    total_tokens = 0\n    for m in re.finditer(r'\\b[A-Z][a-zA-Z]+\\b', text):\n        total_tokens += 1\n        # find previous non-space char\n        i = m.start() - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        prev_char = text[i] if i >= 0 else ''\n        # if previous non-space char is a sentence end, treat as sentence-start capitalization\n        if prev_char in '.!?':\n            continue\n        count_title_nonstart += 1\n    denom = float(len(re.findall(r\"[A-Za-z']+\", text))) or 1.0\n    return float(count_title_nonstart) / denom\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences normalized by sentence count'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(ellipses) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Pronoun density: fraction of tokens that are pronouns/possessives (I, you, he, she, they, my, our, their, etc.)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','you','he','him','she','her','we','us','they','them','my','your','his','her','its','our','their','mine','yours','hers','ours','theirs'}\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in pronouns)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    \"Adverb '-ly' ratio: fraction of tokens that look like typical English adverbs ending in 'ly'\"\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if len(w) > 2 and w.lower().endswith('ly'))\n    return float(ly_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Colon/semicolon density: frequency of \":\" and \";\" per character (formal/academic punctuation tendency)'\n    if not text:\n        return 0.0\n    denom = max(1, len(text))\n    count = text.count(':') + text.count(';')\n    return float(count) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Passive-voice heuristic: count of \"be\" forms followed by -ed words (e.g., \"was done\") normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    pattern = r'\\b(?:had been|has been|have been|was|were|is|are|be|been|being)\\s+[A-Za-z]+ed\\b'\n    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n    words = re.findall(r\"[A-Za-z']+\", text)\n    denom = float(len(words)) if words else 1.0\n    return float(len(matches)) / denom\n\n", "def feature(text: str) -> float:\n    'Average alphabetic token length (ignores punctuation and numerals) \u2014 longer averages may indicate formal/complex wording'\n    import re\n    if not text:\n        return 0.0\n    alpha_tokens = re.findall(r'[A-Za-z]+', text)\n    if not alpha_tokens:\n        return 0.0\n    return float(sum(len(t) for t in alpha_tokens)) / float(len(alpha_tokens))\n\n", "def feature(text: str) -> float:\n    'Common stopword density: fraction of tokens that are common function words (the, of, and, in, to, for, on, with, as, by, etc.)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','of','and','in','to','for','on','with','as','by','is','are','was','were','that','this','these','those','an','a','be'}\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in stopwords)\n    return float(count) / float(len(words))\n", "def feature(text: str) -> float:\n    'Density of parenthetical author-year citation patterns like \"(Smith, 2006)\" per 1000 words'\n    import re\n    if not text:\n        return 0.0\n    words = max(1, len(re.findall(r'\\w+', text)))\n    pattern_count = len(re.findall(r'\\([A-Z][A-Za-z\\-\\.\\s]{1,40},\\s*\\d{4}\\)', text))\n    return (pattern_count / words) * 1000.0\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: count of \"...\" sequences per 1000 characters (captures intentional trailing/thought ellipses)'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    return float(ellipses) / (len(text) + 1e-9) * 1000.0\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns to total word count (I, we, me, my, our, us, mine, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    first_set = {'i','we','me','my','our','us','mine','ours','myself','ourselves'}\n    first_count = sum(1 for t in tokens if t in first_set)\n    return first_count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation of sentence lengths (stddev / mean) using sentences split on .!? to capture uniformity'\n    import re, math\n    if not text:\n        return 0.0\n    # Split into sentences conservatively\n    sent_boundaries = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in sent_boundaries if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return std / (mean + 1e-9)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word types that occur exactly once (singletons / total tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    counts = Counter(tokens)\n    hapax = sum(1 for v in counts.values() if v == 1)\n    return hapax / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of opening parentheses that contain a digit (heuristic for numeric citations/years) among all parentheses occurrences'\n    import re\n    if not text:\n        return 0.0\n    opens = text.count('(')\n    if opens == 0:\n        return 0.0\n    # find contents inside parentheses\n    contents = re.findall(r'\\(([^)]{0,100})\\)', text)\n    if not contents:\n        return 0.0\n    numeric_inside = sum(1 for c in contents if re.search(r'\\d', c))\n    return numeric_inside / float(opens)\n\n", "def feature(text: str) -> float:\n    'Passive-voice marker density: occurrences of auxiliaries followed by -ed (e.g., \"was produced\") per sentence'\n    import re\n    if not text:\n        return 0.0\n    # count potential passive constructs\n    markers = re.findall(r'\\b(?:was|were|is|are|been|being|be)\\b\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    # sentence count\n    sents = max(1, len(re.findall(r'[.!?]', text)))\n    return len(markers) / float(sents)\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation characters that are special Unicode punctuation (em-dash, en-dash, ellipsis char, curly quotes)'\n    if not text:\n        return 0.0\n    special = set(['\\u2014','\\u2013','\\u2026','\\u201c','\\u201d','\\u2018','\\u2019'])\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    special_count = sum(1 for c in text if c in special)\n    return special_count / float(total_punct)\n", "def feature(text: str) -> float:\n    'Normalized count of common salutation/closing tokens (Dear, Sincerely, Regards, Atn, Attn) indicative of letters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    keywords = {'dear', 'sincerely', 'regards', 'attn', 'atn', 'yours'}\n    count = sum(1 for t in tokens if t in keywords)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Count of short bracketed metadata tags (e.g. [translated], (note)) normalized by number of sentences'\n    import re\n    if not text:\n        return 0.0\n    tags = re.findall(r'\\[[^\\]]{1,60}\\]|\\([^\\)]{1,60}\\)|\\<[^>]{1,60}\\>', text)\n    sentences = max(1.0, float(len(re.findall(r'[.!?]', text)) or 1))\n    return len(tags) / sentences\n\n", "def feature(text: str) -> float:\n    'Ratio of multi-word capitalized sequences (titlecase sequences length>=2) to token count; captures named entities like \"Heinrich B\u00f6ll\"'\n    import re\n    if not text:\n        return 0.0\n    sequences = re.findall(r'\\b[A-Z][A-Za-z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff]+\\b(?:\\s+\\b[A-Z][A-Za-z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff]+\\b)+', text)\n    tokens = max(1.0, float(len(re.findall(r'\\b\\w+\\b', text)) or 1))\n    return len(sequences) / tokens\n\n", "def feature(text: str) -> float:\n    'Density of numeric tokens (tokens containing digits) to capture dates, addresses, years'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_tokens) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Binary-like indicator (0.0 or 1.0) for address-like structure: lines that look like \"Number Street, CITY, ST ZIP\" or contain \"St\" and digits'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    for ln in lines:\n        if re.search(r'\\b\\d{1,5}\\b.*\\b(St|Street|Ave|Avenue|Blvd|Rd|Road|Ln|Lane)\\b', ln, re.IGNORECASE):\n            return 1.0\n        if re.search(r'\\b[A-Z]{2,}\\b,\\s*[A-Z]{2,}\\s*\\d{3,5}', ln):  # pattern like \"NY, NY 10012\"\n            return 1.0\n        if re.search(r'\\b(Atn|Attn|Attention)\\b', ln, re.IGNORECASE):\n            return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Modal verb density (can, could, may, might, must, should, would, shall) normalized by tokens to capture hedging or tentative style'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'must', 'should', 'would', 'shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average clause length in words: split on commas/semicolons/colons to approximate clause lengths (words per clause)'\n    import re\n    if not text:\n        return 0.0\n    clauses = [c.strip() for c in re.split(r'[,:;]', text) if c.strip()]\n    if not clauses:\n        return 0.0\n    word_counts = [len(re.findall(r'\\b\\w+\\b', c)) for c in clauses]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts)) / max(1.0, len(word_counts))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that are all-caps or contain multiple uppercase letters (acronyms, headings) to token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    def many_upper(tok):\n        ups = sum(1 for c in tok if c.isupper())\n        return ups >= 2 or tok.isupper()\n    count = sum(1 for t in tokens if many_upper(t))\n    return float(count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Adjacent-line rhyme similarity: fraction of adjacent non-empty lines whose last words share the last 3 letters'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if len(lines) < 2:\n        return 0.0\n    def last_suffix(line):\n        words = re.findall(r'\\w+', line.lower())\n        if not words:\n            return ''\n        w = words[-1]\n        return w[-3:] if len(w) >= 3 else w\n    suffixes = [last_suffix(ln) for ln in lines]\n    matches = sum(1 for i in range(len(suffixes)-1) if suffixes[i] and suffixes[i+1] and suffixes[i] == suffixes[i+1])\n    return float(matches) / max(1.0, float(len(suffixes)-1))\n\n", "def feature(text: str) -> float:\n    'Density of quotation characters per word (counts standard and smart quotes) \u2014 higher for dialog-heavy texts'\n    import re\n    if not text:\n        return 0.0\n    quote_chars = re.findall(r'[\"\\u201C\\u201D\\u2018\\u2019]', text)\n    words = re.findall(r'\\w+', text)\n    return float(len(quote_chars)) / max(1.0, float(len(words)))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that end with an ellipsis (\"...\") \u2014 indicates trailing/thoughtful style'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.\\.\\.', text))\n    # count sentence endings as groups of sentence-ending punctuation\n    sentence_ends = len(re.findall(r'[.!?]+', text))\n    return float(ellipses) / max(1.0, float(sentence_ends))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count \u2014 lexical variety indicator'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    total = len(words)\n    if total == 0:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Proportion of non-empty lines that are short (<= 6 words) \u2014 captures stanza/poem-like line brevity'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    short = 0\n    for ln in lines:\n        wc = len(re.findall(r'\\w+', ln))\n        if wc <= 6:\n            short += 1\n    return float(short) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Punctuation density excluding quotes: count of punctuation characters (not alnum/space/quotes) divided by text length'\n    import re\n    if not text:\n        return 0.0\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace() and c not in '\"\\u201C\\u201D\\u2018\\u2019\\'')\n    return float(punct_count) / max(1.0, float(len(text)))\n\n", "def feature(text: str) -> float:\n    'Parenthetical token density: number of parentheses characters \"(\" or \")\" per 100 words (scaled) \u2014 unusual structural marker'\n    import re\n    if not text:\n        return 0.0\n    parens = text.count('(') + text.count(')')\n    words = len(re.findall(r'\\w+', text))\n    return (float(parens) / max(1.0, float(words))) * 100.0\n\n", "def feature(text: str) -> float:\n    'Adjacent duplicate-word bigram ratio: proportion of consecutive identical tokens (e.g., \"the the\") among all bigrams'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    dup = sum(1 for i in range(len(tokens)-1) if tokens[i] == tokens[i+1])\n    total_bigrams = len(tokens)-1\n    return float(dup) / float(total_bigrams)\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are first-person pronouns (I, me, my, we, us, our, mine)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', \"i'm\", \"i've\", \"i'd\", 'we', 'us', 'our', 'ours', \"we're\"}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of common profane words (fraction of tokens matching a small profanity list)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    profanities = r'\\b(?:fuck|shit|damn|hell|bitch|crap|asshole|bastard)\\b'\n    matches = re.findall(profanities, text, flags=re.IGNORECASE)\n    return float(len(matches)) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain an ellipsis (\"...\" or unicode ellipsis) \u2014 indicates trailing thought/colloquial pauses'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[^.!?]+[.!?]?', text)\n    if not sentences:\n        return 0.0\n    ellipsis_count = text.count('...') + text.count('\u2026')\n    # Cap ellipsis_count at number of sentences to avoid >1 per sentence skew\n    ellipsis_count = min(ellipsis_count, len(sentences))\n    return float(ellipsis_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that start with a quotation mark (dialogue line starts)'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    quote_starts = ('\"', '\u201c', '\u201d', \"'\")\n    count = 0\n    for l in lines:\n        s = l.lstrip()\n        if any(s.startswith(q) for q in quote_starts):\n            count += 1\n    return float(count) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (lexical diversity): unique word forms divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that end with an exclamation or question mark (exclamatory/interrogative sentence rate)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[^.!?]+[.!?]?', text)\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        s_stripped = s.strip()\n        if s_stripped.endswith('!') or s_stripped.endswith('?'):\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Density of parenthetical or dash-style asides (counts of \"(\" or \")\" or long dash patterns per token)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    aside_marks = text.count('(') + text.count(')') + text.count('\u2014') + text.count('--') + text.count(' - ')\n    return float(aside_marks) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like capitalized names (Start with uppercase followed by at least two lowercase letters)'\n    import re\n    if not text:\n        return 0.0\n    name_matches = re.findall(r'\\b[A-Z][a-z]{2,}\\b', text)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    return float(len(name_matches)) / len(tokens)\n", "def feature(text: str) -> float:\n    'Normalized count of ellipses (\"...\" or longer) per sentence (trailing-off or dramatic pause)'\n    import re\n    if not text:\n        return 0.0\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    return float(ellipses) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that are filler/interjection tokens like \"umm\", \"uh\", \"hmm\", \"erm\" (case-insensitive)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    filler_pattern = re.compile(r'\\b(?:um+|uh+|hmm+|erm+|uhm+|ehm+|mmmh+|ahh+)\\b', re.IGNORECASE)\n    fillers = sum(1 for w in re.findall(r'\\b\\w+\\b', text) if filler_pattern.fullmatch(w))\n    return float(fillers) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that contain a character repeated three or more times in a row (e.g., \"Ummm\", \"soooo\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    repeated = 0\n    pattern = re.compile(r'(.)\\1{2,}')\n    for w in words:\n        if pattern.search(w):\n            repeated += 1\n    return float(repeated) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of title-like tokens (Senator, President, Brigadier, Dr, Mr, Mrs, Captain, Colonel, etc.) per token'\n    import re\n    if not text:\n        return 0.0\n    titles = {'senator','president','brigadier','dr','mr','mrs','ms','captain','colonel','sir','lady','lord','governor','minister'}\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    title_count = 0\n    for w in words:\n        if w.lower().rstrip('.').strip() in titles:\n            title_count += 1\n    return float(title_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a quote mark or a dash (heuristic for direct dialogue lines)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences more robustly using punctuation and newlines\n    parts = re.split(r'(?<=[.!?])\\s+|\\n+', text)\n    sentences = [p for p in parts if p.strip()]\n    if not sentences:\n        return 0.0\n    starts_dialog = 0\n    for s in sentences:\n        lead = s.lstrip()[:1]\n        if lead in {'\"', '\u201c', '\u201d', \"'\", '\u2014', '-'}:\n            starts_dialog += 1\n    return float(starts_dialog) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences containing parentheses \"(\" which often signal asides or meta-comments'\n    import re\n    if not text:\n        return 0.0\n    parts = re.split(r'(?<=[.!?])\\s+|\\n+', text)\n    sentences = [p for p in parts if p.strip()]\n    if not sentences:\n        return 0.0\n    paren_count = sum(1 for s in sentences if '(' in s or ')' in s)\n    return float(paren_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that end with an ellipsis or trailing dash (incomplete or trailing-off sentences)'\n    import re\n    if not text:\n        return 0.0\n    parts = re.split(r'(?<=[.!?])\\s+|\\n+', text)\n    sentences = [p for p in parts if p.strip()]\n    if not sentences:\n        return 0.0\n    trailing = 0\n    for s in sentences:\n        s_strip = s.rstrip()\n        if s_strip.endswith('...') or s_strip.endswith('\u2014') or s_strip.endswith('-'):\n            trailing += 1\n    return float(trailing) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Density of explicit dialogue attribution verbs after quotes (e.g., \\\"...\\\", said/asked/replied) per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    pattern = re.compile(r'\"\\s*(?:said|asked|replied|shouted|whispered|muttered|exclaimed|cried|answered)\\b', re.IGNORECASE)\n    matches = len(pattern.findall(text))\n    return float(matches) / float(sentences)\n", "def feature(text: str) -> float:\n    'Ratio of common English stopwords to total tokens (proxy for function-word usage)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he',\n        'as','you','do','at','this','but','his','by','from','they','we','say','her','she','or','an',\n        'will','my','one','all','would','there','their','what','so','up','out','if','about','who',\n        'get','which','go','me'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    stop_count = sum(1 for t in tokens if t in stopwords)\n    return float(stop_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of titlecase tokens (Word.istitle()) as a rough named-entity / proper-noun signal'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z]+\", text)\n    if not tokens:\n        return 0.0\n    title_count = sum(1 for t in tokens if t.istitle())\n    return float(title_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Square-bracket citation density: number of [ or ] characters normalized by text length (proxy for references)'\n    if not text:\n        return 0.0\n    bracket_count = text.count('[') + text.count(']')\n    # Normalize by character length to avoid large values on short texts\n    return float(bracket_count) / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation count'\n    import string\n    if not text:\n        return 0.0\n    punct_chars = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    total_punct = len(punct_chars)\n    if total_punct == 0:\n        return 0.0\n    distinct = len(set(punct_chars))\n    return float(distinct) / float(total_punct)\n\n", "def feature(text: str) -> float:\n    'Proportion of question marks relative to sentence count (questions per sentence), robust to short text'\n    if not text:\n        return 0.0\n    q_marks = text.count('?')\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    if sentence_count == 0:\n        # treat as either 0 or single sentence; use 0 to indicate no questions\n        return 0.0\n    return float(q_marks) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen (compound words like \"self-esteem\"), normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    # split on whitespace to preserve tokens like \"state-of-the-art\"\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Clause density: fraction of sentences that contain at least one comma, semicolon, or colon (proxy for syntactic complexity)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    complex_sentences = 0\n    for s in sentences:\n        if (',' in s) or (';' in s) or (':' in s):\n            complex_sentences += 1\n    return float(complex_sentences) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Density of ellipses occurrences (\\\"...\\\" or Unicode ellipsis) per 100 characters'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.\\.\\.|\u2026', text))\n    denom = max(1.0, len(text))\n    return float(ellipses) / denom * 100.0\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with \\\"ly\\\" (heuristic adverb/adjectival modifiers measure)'\n    import re\n    tokens = re.findall(r\"[A-Za-z'-]+\", text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        tl = t.lower()\n        if len(tl) >= 3 and tl.endswith('ly'):\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation characters that are directly followed by a letter/digit with no intervening space (punctuation-spacing errors)'\n    import re\n    if not text:\n        return 0.0\n    # count punctuation occurrences\n    punct_chars = re.findall(r'[^\\w\\s]', text)\n    total_punct = len(punct_chars)\n    if total_punct == 0:\n        return 0.0\n    # find punctuation followed by letter/digit with no space\n    bad = len(re.findall(r'[^\\w\\s](?=[A-Za-z0-9])', text))\n    return float(bad) / total_punct\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique (lowercased) word tokens divided by total tokens'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    lower = [t.lower() for t in tokens]\n    unique = len(set(lower))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of adjacent duplicate tokens (case-insensitive) to capture stuttering/repetition'\n    import re\n    tokens_raw = re.findall(r'\\w+', text)\n    tokens = [t.lower() for t in tokens_raw]\n    if len(tokens) < 2:\n        return 0.0\n    dup = 0\n    for a, b in zip(tokens, tokens[1:]):\n        if a == b:\n            dup += 1\n    return float(dup) / (len(tokens) - 1)\n\n", "def feature(text: str) -> float:\n    'Proportion of punctuation characters that belong to repeated punctuation runs (e.g., \\\"!!!\\\",\\\"??\\\",\\\"...\\\")'\n    import re\n    if not text:\n        return 0.0\n    punct = re.findall(r'[^\\w\\s]', text)\n    total = len(punct)\n    if total == 0:\n        return 0.0\n    repeats = re.findall(r'([^\\w\\s])\\1+', text)\n    # repeats counts distinct runs; compute run lengths sum\n    run_lengths = 0\n    for m in re.finditer(r'([^\\w\\s])\\1+', text):\n        run_lengths += len(m.group(0))\n    return float(run_lengths) / total\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (mean words per sentence), skipping empty sentences'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence end punctuation, allow multiple punctuation marks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if words:\n            lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Second-person pronoun fraction (you/your/youre/you\\'re etc.) as a sign of direct address'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\w+'?\\w*|\\w+\", text.lower())\n    if not words:\n        return 0.0\n    second_person = {'you', 'your', 'yours', \"you're\", 'youre', \"y'all\", 'yall'}\n    count = sum(1 for w in words if w in second_person)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing digits (years, counts, identifiers) indicating citations/numeric content'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    num_with_digits = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_with_digits) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per sentence (common in narrative or trailing-off style)'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    # estimate sentence count, ensure at least 1 to avoid division by zero\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(ellipses) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated (contain a hyphen and at least one letter) indicating compound tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphenated = 0\n    for t in tokens:\n        if '-' in t and re.search(r'[A-Za-z]', t):\n            # require letters on at least one side of hyphen\n            parts = [p for p in t.split('-') if p]\n            if any(re.search(r'[A-Za-z]', p) for p in parts):\n                hyphenated += 1\n    return float(hyphenated) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Modal verb fraction (can/could/may/might/must/shall/should/will/would) as a marker of modality or speculation'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for w in words if w in modals)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Comma-to-other-punctuation ratio: commas / (commas + . ! ? ; :) to capture listiness vs sentence-ending style'\n    if not text:\n        return 0.0\n    commas = float(text.count(','))\n    other = float(sum(1 for c in text if c in '.!?;:'))\n    total = commas + other\n    if total <= 0.0:\n        return 0.0\n    return commas / total\n\n", "def feature(text: str) -> float:\n    'Indicator (0.0/1.0) whether the text contains common academic section headers like \"Introduction\" or \"Abstract\" at line starts'\n    import re\n    if not text:\n        return 0.0\n    if re.search(r'(?im)^[ \\t]*(Introduction|Abstract|Background|Methods|Conclusion|Results)\\b', text):\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, we, me, us, my, our, mine, ours) to total word tokens'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','we','me','us','my','mine','our','ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice score: number of \"be\" + past-participles patterns per sentence'\n    import re\n    if not text:\n        return 0.0\n    # Look for patterns like \"was created\", \"has been transformed\", \"is discussed\"\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|being|has been|have been|had been|will be|would be|should be)\\b\\s+\\w+(?:ed|en)\\b', re.I)\n    matches = pattern.findall(text)\n    sentences = max(1.0, float(text.count('.') + text.count('!') + text.count('?')))\n    return float(len(matches)) / sentences\n\n", "def feature(text: str) -> float:\n    'Comma density: number of commas divided by number of word tokens (captures clause usage / complexity)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    tokens = len(re.findall(r'\\w+', text))\n    return float(commas) / max(1.0, tokens)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total tokens (lexical diversity)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Normalized count of parenthesized four-digit years like \"(1998)\" as an indicator of citations or academic references'\n    import re\n    if not text:\n        return 0.0\n    years = len(re.findall(r'\\(\\s*\\d{4}\\s*\\)', text))\n    tokens = max(1, len(re.findall(r'\\w+', text)))\n    return float(years) / float(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of hyphenated words (words containing one or more hyphens) among all tokens'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    hyphens = len(re.findall(r'\\b\\w+(?:-\\w+)+\\b', text))\n    return float(hyphens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average clause-like separators (commas, semicolons, colons) per sentence as a measure of sentence complexity'\n    if not text:\n        return 0.0\n    separators = text.count(',') + text.count(';') + text.count(':')\n    sentences = max(1.0, float(text.count('.') + text.count('!') + text.count('?')))\n    return float(separators) / sentences\n", "def feature(text: str) -> float:\n    'Proxy for passive voice: counts occurrences of \"was/were/is/are/been\" followed by -ed verbs per word'\n    import re\n    if not text:\n        return 0.0\n    words = len(re.findall(r'\\w+', text))\n    # simple heuristic: auxiliary + past-participle (ending in -ed)\n    passive_hits = re.findall(r'\\b(?:was|were|is|are|has been|have been|had been|been|be)\\b\\s+\\w+ed\\b', text.lower())\n    return 0.0 if words == 0 else float(len(passive_hits)) / float(words)\n\n", "def feature(text: str) -> float:\n    'Average sentence length measured in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence end punctuation, keep non-empty segments\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # guard against empty sentence tokens\n    word_counts = [c for c in word_counts if c > 0]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts)) / float(len(word_counts))\n\n", "def feature(text: str) -> float:\n    'Ratio of hyphenated tokens (e.g., \"long-term\", \"rights-based\") to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t and not t.strip('-').isdigit())\n    return float(hyphen_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of parenthetical groups that contain numeric content (years or numeric refs)'\n    import re\n    if not text:\n        return 0.0\n    parens = re.findall(r'\\(([^)]*)\\)', text)\n    if not parens:\n        return 0.0\n    numeric_parens = 0\n    for p in parens:\n        # consider it numeric if it has a year-like 4-digit or any digit (ref numbers)\n        if re.search(r'\\d{4}', p) or re.search(r'\\d+', p):\n            numeric_parens += 1\n    return float(numeric_parens) / float(len(parens))\n\n", "def feature(text: str) -> float:\n    'Proportion of adverbial -ly words (words ending with \"ly\") among tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n    return float(ly_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas complexity proxy)'\n    import re\n    if not text:\n        return 0.0\n    # count commas and sentences\n    comma_count = text.count(',')\n    # sentence count using .,!,? as heuristics\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = len(sentences) if len(sentences) > 0 else 1\n    return float(comma_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Lexical density: fraction of tokens that are likely content words (not in a small stopword list)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','of','in','to','a','an','for','is','are','was','were','that','this','it','as','on','by','with','be','has','have','had','from','or','which','such','these','those','their','its','at','into','about'}\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    content = sum(1 for t in tokens if t not in stopwords)\n    return float(content) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Passive voice pattern density: count of simple passive constructions per sentence'\n    import re\n    if not text:\n        return 0.0\n    # Simple heuristic: auxiliaries + optional \"been\" + past-participle ending in -ed\n    passive_matches = re.findall(r'\\b(?:was|were|is|are|has|have|had|will|would|should|could|might|must)\\s+(?:been\\s+)?\\w+ed\\b', text, flags=re.IGNORECASE)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    denom = max(1, len(sentences))\n    return float(len(passive_matches)) / denom\n\n", "def feature(text: str) -> float:\n    'Acronym density: fraction of tokens that are all-caps acronyms of length>=2 (e.g., DVT, USA)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    acronyms = [t for t in tokens if re.fullmatch(r'[A-Z]{2,}', t)]\n    return float(len(acronyms)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    '4-digit year density: fraction of tokens that are 4-digit numbers (useful for historical/academic texts)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    years = re.findall(r'\\b\\d{4}\\b', text)\n    return float(len(years)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Parenthetical density: number of parenthesis characters (() ) normalized by word count'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    parens = text.count('(') + text.count(')')\n    return float(parens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Hyphenated token density: fraction of tokens containing a hyphen (captures ranges and compound terms)'\n    import re\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyph = sum(1 for t in tokens if '-' in t)\n    return float(hyph) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Modal verb density: fraction of tokens that are modal verbs (can, could, may, might, must, shall, should, will, would)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can','could','may','might','must','shall','should','will','would'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Hedging/qualifier density: fraction of tokens that are hedging or cautious lexical items (suggest, likely, possibly, appear, indicate, tend, may, might)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    hedges = {'suggest','suggests','suggested','appear','appears','appeared','likely','possibly','tend','tends','tended','indicate','indicates','indicated','may','might','could'}\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Stopword density: fraction of tokens that are common stopwords (short function using a compact stopword set)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    stopwords = {'the','and','of','to','a','in','is','that','it','for','on','as','with','by','an','be','are','was','were'}\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Indicator (1.0/0.0) whether the text begins with the heading \"Introduction\" (case-insensitive, after leading whitespace)'\n    if not text:\n        return 0.0\n    s = text.lstrip()\n    return 1.0 if s[:12].lower().startswith('introduction') else 0.0\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words; 0 for <2 sentences or empty text'\n    import re, math\n    if not text:\n        return 0.0\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    lengths = [len(re.findall(r'\\w+', p)) for p in parts]\n    lengths = [l for l in lengths if l > 0]\n    n = len(lengths)\n    if n < 2:\n        return 0.0\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    std = math.sqrt(var)\n    if mean == 0:\n        return 0.0\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (three or more consecutive dots) per word'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(ellipses) / float(words)\n\n", "def feature(text: str) -> float:\n    'Proportion of repeated adjacent word bigrams (1 - unique_bigrams/total_bigrams), 0 if no bigrams'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    total = max(0, len(words) - 1)\n    if total == 0:\n        return 0.0\n    bigrams = [' '.join((words[i], words[i+1])) for i in range(len(words)-1)]\n    unique = len(set(bigrams))\n    return 1.0 - (unique / float(total))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are titlecase words (Word.istitle()), as a proxy for presence of titles/proper nouns'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    titlecase = sum(1 for w in words if w.istitle())\n    return float(titlecase) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average length (characters) of parenthetical spans normalized by number of words; 0 if no parenthetical spans'\n    import re\n    if not text:\n        return 0.0\n    spans = re.findall(r'\\(([^)]*)\\)', text)\n    if not spans:\n        return 0.0\n    avg_span_len = sum(len(s) for s in spans) / float(len(spans))\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(avg_span_len) / float(words)\n\n", "def feature(text: str) -> float:\n    'Ratio of lowercase letters to uppercase letters ( (lower+1)/(upper+1) ) to avoid division by zero'\n    if not text:\n        return 0.0\n    lower = sum(1 for c in text if c.islower())\n    upper = sum(1 for c in text if c.isupper())\n    return float(lower + 1) / float(upper + 1)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are percentage indicators (\"%\") or the word \"percent\"'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+%?|\\S', text)\n    if not words:\n        return 0.0\n    pct_words = len(re.findall(r'%', text)) + len(re.findall(r'\\bpercent\\b', text.lower()))\n    return float(pct_words) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Parenthetical density: count of parentheses characters per token (captures citations or aside notes)'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    paren_count = text.count('(') + text.count(')')\n    denom = max(1, len(words))\n    return float(paren_count) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Density of academic/research trigger words (study, research, published, journal, survey, findings, book, author)'\n    if not text:\n        return 0.0\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    triggers = re.findall(r'\\b(study|research|published|journal|analysis|author|authors|dataset|survey|findings|book|researcher)\\b', text.lower())\n    return float(len(triggers)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (clause complexity indicator)'\n    if not text:\n        return 0.0\n    import re\n    comma_count = text.count(',')\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Average length of contents inside parentheses (captures length of citations or parenthetical notes)'\n    if not text:\n        return 0.0\n    import re\n    contents = re.findall(r'\\((.*?)\\)', text, flags=re.DOTALL)\n    if not contents:\n        return 0.0\n    lengths = [len(c.strip()) for c in contents]\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Lexical repetition rate: 1 - (unique_word_count / total_word_count), higher means more repetition'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    total = len(words)\n    return 1.0 - (float(unique) / float(total))\n\n", "def feature(text: str) -> float:\n    'Density of punctuation clusters (sequences of 2+ punctuation chars like \"...\" or \"--\") per token'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    clusters = re.findall(r'[^\\w\\s]{2,}', text)\n    denom = max(1, len(words))\n    return float(len(clusters)) / float(denom)\n", "def feature(text: str) -> float:\n    'Quotation density: count of quotation mark characters (\", \u201c, \u201d) normalized by word count'\n    import re\n    words = max(1, len(re.findall(r'\\w+', text)))\n    quotes = len(re.findall(r'[\"\u201c\u201d]', text))\n    return float(quotes) / float(words)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique normalized word forms divided by total words (lexical diversity)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    return float(len(set(tokens))) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Passive-voice proxy: count of \"be\" forms followed by \"-ed\" words per sentence (approximate passive indicator)'\n    import re\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    pattern = r'\\b(?:is|are|was|were|be|been|being|am|\\'m)\\s+\\w+ed\\b'\n    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Comma density per sentence: average number of commas per sentence (comma_count / sentence_count)'\n    import re\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    commas = text.count(',')\n    return float(commas) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Proper-noun proxy: fraction of words that are capitalized but not sentence-initial (approximate proper noun ratio)'\n    import re\n    text_stripped = text.strip()\n    if not text_stripped:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text_stripped)\n    total_words = 0\n    capital_non_initial = 0\n    for s in sentences:\n        tokens = re.findall(r'\\b\\w+\\b', s)\n        if not tokens:\n            continue\n        total_words += len(tokens)\n        for tok in tokens[1:]:\n            if tok and tok[0].isupper():\n                capital_non_initial += 1\n    total_words = max(1, total_words)\n    return float(capital_non_initial) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Function-word density: fraction of common high-frequency function words (the, and, to, of, in, etc.) per word'\n    import re\n    stopwords = {'the','a','an','and','to','of','in','that','it','is','for','on','with','as','by','at','from','this','these','those','be','was','were','are','or','but'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if t in stopwords:\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Character entropy (normalized): Shannon entropy of character distribution normalized by log2(unique_chars)'\n    import re, math\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    freq = {}\n    for c in chars:\n        freq[c] = freq.get(c, 0) + 1\n    n = len(chars)\n    entropy = 0.0\n    for v in freq.values():\n        p = v / n\n        entropy -= p * math.log2(p)\n    unique = len(freq)\n    if unique <= 1:\n        return 0.0\n    return float(entropy) / float(math.log2(unique))\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, mine, we, us, our, ours) to total words'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    matches = re.findall(r'\\b(?:i|me|my|mine|we|us|our|ours)\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Frequency of auxiliary past markers \"was\"/\"were\" per word (simple proxy for past narration/passive contexts)'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    was_matches = re.findall(r'\\b(?:was|were)\\b', text, flags=re.IGNORECASE)\n    return float(len(was_matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of common speech-reporting verbs (said, asked, whispered, intoned, etc.) per word'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    pattern = r'\\b(?:said|asked|replied|whispered|shouted|intoned|muttered|cried|yelled|exclaimed|sobbed|remarked|added)\\b'\n    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ellipsis frequency: count of sequences of three or more dots per word (indicates trailing/thoughtful style)'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    ellipses = re.findall(r'\\.{3,}', text)\n    return float(len(ellipses)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are capitalized but not the first word of their sentence (proxy for named-entity or mid-sentence capitalization)'\n    if not text:\n        return 0.0\n    import re\n    # Split into sentences heuristically\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    total_words = 0\n    mid_capitalized = 0\n    for sent in sentences:\n        sent_words = re.findall(r\"\\b[\\w'-]+\\b\", sent)\n        if not sent_words:\n            continue\n        total_words += len(sent_words)\n        # skip first word of sentence\n        for w in sent_words[1:]:\n            if w[0].isupper() and not w.isupper():  # ignore full acronyms\n                mid_capitalized += 1\n    if total_words == 0:\n        return 0.0\n    return float(mid_capitalized) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Proportion of occurrences where a speech-reporting verb immediately precedes an opening quote (e.g., said, asked, whispered + \",) per word'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    pattern = r'\\b(?:said|asked|replied|whispered|intoned|muttered|cried|yelled|exclaimed|remarked|added)\\b\\s*,?\\s*\"' \n    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Indicator of archaic or common misspelling tokens (whilst, thou, ben, alot, recieve, etc.) per word'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    tokens = ['whilst', 'thou', 'thee', 'hath', 'doth', 'art', 'ere', 'ben', 'alot', 'recieve', 'seperate', 'definately', 'teh', 'adn']\n    pattern = r'\\b(?:' + '|'.join(re.escape(t) for t in tokens) + r')\\b'\n    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of sensory/mental verbs (saw, heard, felt, noticed, dream(ed), woke, sensed) to total words (proxy for introspective/dream narration)'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    verbs = ['saw', 'heard', 'felt', 'felt', 'touched', 'smelled', 'tasted', 'noticed', 'sensed', 'dreamed', 'dreamt', 'woke', 'awoke', 'realized', 'remembered']\n    pattern = r'\\b(?:' + '|'.join(re.escape(v) for v in verbs) + r')\\b'\n    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(len(words))\n", "def feature(text: str) -> float:\n    'Fraction of capitalized words that appear mid-sentence (not immediately following sentence-ending punctuation)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    total_words = len(words)\n    if total_words == 0:\n        return 0.0\n    mid_cap = 0\n    # find all capitalized words (simple proper-noun pattern)\n    for m in re.finditer(r'\\b[A-Z][a-z]+\\b', text):\n        start = m.start()\n        # find preceding non-space character (if any)\n        i = start - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        if i < 0:\n            # start of text -> treat as sentence-start\n            continue\n        if text[i] in '.!?':\n            # after sentence end -> treat as sentence-start\n            continue\n        # else consider it mid-sentence\n        mid_cap += 1\n    return float(mid_cap) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = set(words)\n    return float(len(unique)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Variance of sentence lengths (in words) using .!? as sentence delimiters (0 if fewer than 2 sentences)'\n    import re\n    if not text:\n        return 0.0\n    # split on runs of sentence-ending punctuation\n    raw_sentences = re.split(r'[.!?]+', text)\n    lengths = []\n    for s in raw_sentences:\n        s = s.strip()\n        if s:\n            lengths.append(len(re.findall(r'\\w+', s)))\n    if len(lengths) < 2:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    variance = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(variance)\n\n", "def feature(text: str) -> float:\n    'Ratio of modal verbs (can/could/may/might/must/should/would/will/shall) to total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total = len(words)\n    if total == 0:\n        return 0.0\n    modals = re.findall(r'\\b(?:can|could|may|might|must|should|would|will|shall)\\b', text, flags=re.I)\n    return float(len(modals)) / float(total)\n\n", "def feature(text: str) -> float:\n    'Frequency of ellipsis or multi-dot sequences (3+ dots) per word as a measure of trailing/thought-like punctuation'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total = len(words) if words else 1\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    return float(ellipses) / float(total)\n\n", "def feature(text: str) -> float:\n    'Diversity of apostrophe-containing tokens: unique contractions/possessive tokens divided by total apostrophe tokens (0 if none)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+'\\w+\\b\", text)\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    unique = set(t.lower() for t in tokens)\n    return float(len(unique)) / float(total)\n\n", "def feature(text: str) -> float:\n    'Ratio of past-continuous patterns (was/were + -ing verb) to total words as an indicator of narrative tense style'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total = len(words)\n    if total == 0:\n        return 0.0\n    matches = re.findall(r'\\b(?:was|were)\\s+[A-Za-z]+ing\\b', text, flags=re.I)\n    return float(len(matches)) / float(total)\n", "def feature(text: str) -> float:\n    'Average number of words inside each quoted segment (\"\"), 0 if no quoted segments'\n    import re\n    if not text:\n        return 0.0\n    segments = re.findall(r'\"(.*?)\"', text, re.S)\n    if not segments:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', seg)) for seg in segments]\n    # avoid zero division; return average\n    return float(sum(word_counts)) / float(len(word_counts)) if word_counts else 0.0\n\n", "def feature(text: str) -> float:\n    'Proportion of words that are capitalized but not at sentence-start (internal proper-noun-like capitalization density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    cap_iter = list(re.finditer(r'\\b[A-Z][a-zA-Z]+\\b', text))\n    internal_caps = 0\n    for m in cap_iter:\n        idx = m.start()\n        # look back for sentence boundary (.,!,? plus whitespace) or start of text\n        start_ok = False\n        if idx == 0:\n            start_ok = True\n        else:\n            prev = text[max(0, idx-3):idx]\n            # consider it sentence-start if previous ends with . ! ? followed by whitespace, or beginning\n            if re.search(r'(^|[\\.!\\?]\\s)$', prev):\n                start_ok = True\n        if not start_ok:\n            internal_caps += 1\n    return float(internal_caps) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, we, us, our) to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text, re.I)\n    if not words:\n        return 0.0\n    matches = re.findall(r'\\b(i|me|my|mine|we|us|our|ours)\\b', text, re.I)\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proportion of word tokens that end with \"ing\" (present participle / gerund density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text, re.I)\n    if not words:\n        return 0.0\n    ing = [w for w in words if w.lower().endswith('ing')]\n    return float(len(ing)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences normalized by sentence count (0 if no sentences)'\n    import re\n    if not text:\n        return 0.0\n    ell = text.count('...')\n    sents = [s for s in re.findall(r'[^\\.!?]+[\\.!?]?', text, re.S) if s.strip()]\n    denom = float(len(sents)) if sents else 1.0\n    return float(ell) / denom\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<6 words) \u2014 higher values indicate choppy or dialog-heavy style'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^\\.!?]+[\\.!?]?', text, re.S) if s.strip()]\n    if not sents:\n        return 0.0\n    short = 0\n    for s in sents:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if len(words) < 6:\n            short += 1\n    return float(short) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Median-to-mean sentence length ratio (words): values <1 indicate right-skewed long sentences, >1 left-skewed'\n    import re, statistics\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^\\.!?]+[\\.!?]?', text, re.S) if s.strip()]\n    if not sents:\n        return 0.0\n    lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in sents]\n    lengths = [l for l in lengths if l > 0]\n    if not lengths:\n        return 0.0\n    mean = statistics.mean(lengths)\n    median = statistics.median(lengths)\n    return float(median) / float(mean) if mean != 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Quoted-turns per sentence: number of quoted segments (pairs of \") divided by sentence count (approx dialogue turns density)'\n    import re\n    if not text:\n        return 0.0\n    segments = re.findall(r'\"', text)\n    quoted_segments = len(segments) // 2\n    sents = [s for s in re.findall(r'[^\\.!?]+[\\.!?]?', text, re.S) if s.strip()]\n    denom = float(len(sents)) if sents else 1.0\n    return float(quoted_segments) / denom\n", "def feature(text: str) -> float:\n    'Average clause length in words when splitting on commas (avg words per comma-separated clause)'\n    import re\n    if not text:\n        return 0.0\n    clauses = [c.strip() for c in re.split(r',', text) if c.strip()]\n    if not clauses:\n        return 0.0\n    lengths = []\n    for c in clauses:\n        words = re.findall(r\"\\b[\\w']+\\b\", c)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n", "def feature(text: str) -> float:\n    'Normalized ellipsis frequency: count of \"...\" per word'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    words = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not words:\n        return 0.0\n    return float(ellipses) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of repeated trigrams (3-word sequences) that occur more than once'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if len(tokens) < 3:\n        return 0.0\n    trigrams = [' '.join(tokens[i:i+3]) for i in range(len(tokens)-2)]\n    from collections import Counter\n    counts = Counter(trigrams)\n    repeated = sum(1 for c in counts.values() if c > 1)\n    total = len(trigrams)\n    return float(repeated) / total\n\n", "def feature(text: str) -> float:\n    'Proportion of common function words (articles, prepositions, auxiliaries, conjunctions) to all tokens'\n    import re\n    if not text:\n        return 0.0\n    func = {'the','a','an','and','or','but','if','while','of','to','in','for','on','with','as','by','at','from','that','this','it','is','are','was','were','be','have','has','had','do','does','did','not','so'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in func)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of non-period punctuation characters (punctuation excluding \".\" ) normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not words:\n        return 0.0\n    nonperiod_punct = sum(1 for c in text if (not c.isalnum()) and (not c.isspace()) and c != '.')\n    return float(nonperiod_punct) / len(words)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that end with an exclamation or question mark (exclamation/question density)'\n    import re\n    if not text:\n        return 0.0\n    # conservative sentence split, keep trailing punctuation if present\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        if s[-1] in '?!':\n            count += 1\n    return float(count) / len(sentences)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that appear to start with a quotation mark or dash (dialogue sentence fraction)'\n    import re\n    if not text:\n        return 0.0\n    # Split into candidate sentences by punctuation boundaries\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    dlg = 0\n    for s in sentences:\n        s_stripped = s.lstrip()\n        if not s_stripped:\n            continue\n        if s_stripped[0] in ('\"', \"'\", '\u201c', '\u201d', '\u2014', '-', '\u2013'):\n            dlg += 1\n    return float(dlg) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Normalized balance of present vs past auxiliary verbs: (present-past)/(present+past) in [-1,1]'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    present = 0\n    past = 0\n    present_set = {'is', 'are', 'am', 'has', 'have', 'does', 'do'}\n    past_set = {'was', 'were', 'had', 'did'}\n    for t in tokens:\n        if t in present_set:\n            present += 1\n        elif t in past_set:\n            past += 1\n    denom = present + past\n    if denom == 0:\n        return 0.0\n    return float(present - past) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Density of modal verbs (can/could/may/might/must/shall/should/will/would) per word'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of capitalized tokens that occur NOT at sentence-start (interior capitalized token ratio)'\n    import re\n    if not text:\n        return 0.0\n    matches = list(re.finditer(r'\\b\\w+\\b', text))\n    if not matches:\n        return 0.0\n    interior_caps = 0\n    total_tokens = 0\n    for m in matches:\n        token = m.group(0)\n        total_tokens += 1\n        # Determine if token is at sentence start by looking for previous non-space char\n        idx = m.start()\n        i = idx - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        # If previous non-space char is a sentence terminator or none, treat as sentence-initial\n        sentence_initial = (i < 0) or (text[i] in '.!?')\n        if token and token[0].isupper() and not sentence_initial:\n            interior_caps += 1\n    return float(interior_caps) / float(total_tokens) if total_tokens > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentence_count)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # Count sentences by terminal punctuation, fallback to 1 if none to avoid division by zero\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sentence_count = len(sentences) if len(sentences) > 0 else 1\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Density of exclamation marks per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    excl = text.count('!')\n    return float(excl) / float(wc)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a first-person subject (start with \"I\" or \"i\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]*', text) if s.strip()]\n    if not sents:\n        return 0.0\n    starts = 0\n    for s in sents:\n        # get first token-like piece\n        m = re.match(r\"['\\\"]*([A-Za-z]+)\", s)\n        if m and m.group(1).lower() == 'i':\n            starts += 1\n    return float(starts) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Modal verb density: proportion of tokens that are modal verbs (can/could/will/would/may/might/should/must)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text)\n    if not tokens:\n        return 0.0\n    modals = {'can','could','will','would','may','might','should','must','shall'}\n    low = [t.lower() for t in tokens]\n    count = sum(1 for t in low if t in modals)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: average number of \"...\" occurrences per sentence (captures trailing suspense/omission)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    ellipsis_count = text.count('...')\n    sents = [s for s in re.findall(r'[^.!?]+[.!?]*', text) if s.strip()]\n    num_sents = len(sents) if sents else 1\n    return float(ellipsis_count) / float(num_sents)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences containing progressive verb forms (was/am/is/are/been + \\\\w+ing)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]*', text) if s.strip()]\n    if not sents:\n        return 0.0\n    pattern = re.compile(r\"\\b(?:am|is|are|was|were|be|been|being)\\s+[A-Za-z]+ing\\b\", re.I)\n    matched = sum(1 for s in sents if pattern.search(s))\n    return float(matched) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Passive-voice proxy: fraction of sentences containing simple passive pattern (was/were/is/are + \\\\w+ed)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]*', text) if s.strip()]\n    if not sents:\n        return 0.0\n    pattern = re.compile(r\"\\b(?:was|were|is|are|been|be|being)\\s+[A-Za-z]+ed\\b\", re.I)\n    matched = sum(1 for s in sents if pattern.search(s))\n    return float(matched) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<= 6 words), indicating sentence fragments or terse narrative beats'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]*', text) if s.strip()]\n    if not sents:\n        return 0.0\n    short = 0\n    for s in sents:\n        words = re.findall(r\"[A-Za-z']+\", s)\n        if len(words) <= 6:\n            short += 1\n    return float(short) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Density of sensory/perceptual verbs (see/hear/feel/notice/smell/taste/look/watch/listen) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text)\n    if not tokens:\n        return 0.0\n    sensory = {'see','saw','seen','look','looked','hear','heard','listen','listened','feel','felt','notice','noticed','smell','smelled','taste','tasted','watch','watched'}\n    low = [t.lower() for t in tokens]\n    count = sum(1 for t in low if t in sensory)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating conjunction (and, but, so, yet, or) signaling informal/narrative continuations'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]*', text) if s.strip()]\n    if not sents:\n        return 0.0\n    conj = {'and','but','so','yet','or'}\n    starts = 0\n    for s in sents:\n        m = re.match(r\"['\\\"]*([A-Za-z]+)\", s)\n        if m and m.group(1).lower() in conj:\n            starts += 1\n    return float(starts) / float(len(sents))\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\" or longer) per token (indicates trailing/reflective style)'\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    tokens = re.findall(r'\\w+|\\S', text)\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    return float(ellipses) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are pure numeric tokens (integers) \u2014 presence of specific numeric details'\n    if not text:\n        return 0.0\n    numeric_tokens = len(re.findall(r'\\b\\d+\\b', text))\n    tokens = re.findall(r'\\w+|\\S', text)\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    return float(numeric_tokens) / float(total)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain double-quote characters (dialogue or quoted text density)'\n    if not text:\n        return 0.0\n    # split into sentences roughly\n    sentences = re.findall(r'[^.!?]+[.!?]?', text, flags=re.S)\n    if not sentences:\n        return 0.0\n    quote_hits = sum(1 for s in sentences if re.search(r'[\"\\u201c\\u201d]', s))\n    return float(quote_hits) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Average sentence length measured in words (words per sentence)'\n    if not text:\n        return 0.0\n    # split into sentences (naive)\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Lexical diversity: unique word token ratio (unique_words / total_words)'\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\w+', text)]\n    total = len(words)\n    if total == 0:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation occurrences'\n    if not text:\n        return 0.0\n    punctuation_chars = [c for c in text if not c.isalnum() and not c.isspace()]\n    total_punc = len(punctuation_chars)\n    if total_punc == 0:\n        return 0.0\n    unique_punc = len(set(punctuation_chars))\n    return float(unique_punc) / float(total_punc)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a first-person pronoun (I, we, we\\'re, I\\'m) \u2014 narrative sentence openings'\n    if not text:\n        return 0.0\n    # Split into sentences\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        # look at beginning token\n        m = re.match(r'^\\W*([A-Za-z\\']+)', s)\n        if not m:\n            continue\n        first = m.group(1).lower()\n        if first in {'i', \"i'm\", \"we\", \"we're\", \"we've\", \"we'd\", \"i've\", \"i'd\"}:\n            starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Adverb density: fraction of word tokens ending with -ly (common in descriptive prose)'\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[A-Za-z]+\\b', text)\n    total = len(words)\n    if total == 0:\n        return 0.0\n    ly_count = sum(1 for w in words if len(w) >= 3 and w.lower().endswith('ly'))\n    return float(ly_count) / float(total)\n", "def feature(text: str) -> float:\n    'Ratio of capitalized-title-like words in the first non-empty line (captures presence of a short Title line)'\n    import re\n    if not text:\n        return 0.0\n    for line in text.splitlines():\n        line = line.strip()\n        if line:\n            words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", line)\n            if not words or len(words) > 8:\n                return 0.0\n            cap = sum(1 for w in words if w[0].isupper())\n            return cap / len(words)\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of paragraph word counts (paragraphs split on blank lines)'\n    import re, math\n    if not text:\n        return 0.0\n    paras = [p for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n    counts = [len(re.findall(r'\\w+', p)) for p in paras if re.findall(r'\\w+', p)]\n    if len(counts) < 2:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Fraction of pronouns that are first-person (I, me, my, we, us, our) versus total first+third person pronouns'\n    import re\n    if not text:\n        return 0.0\n    t = text.lower()\n    first = len(re.findall(r'\\b(i|me|my|mine|we|us|our|ours)\\b', t))\n    third = len(re.findall(r'\\b(he|she|they|them|their|theirs|his|her|its)\\b', t))\n    denom = first + third\n    if denom == 0:\n        return 0.0\n    return first / denom\n\n", "def feature(text: str) -> float:\n    'Ratio of word tokens that are simple regular past-tense forms ending with \"ed\" (heuristic for past-tense density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    eds = len(re.findall(r'\\b\\w+ed\\b', text.lower()))\n    return eds / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that begin like dialog (start with quotation marks, em-dash, or hyphen)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    dialog_starts = 0\n    for ln in lines:\n        s = ln.lstrip()\n        if re.match(r'^[\u201c\"\\'\\-\\\u2014]', s):\n            dialog_starts += 1\n    return dialog_starts / len(lines)\n\n", "def feature(text: str) -> float:\n    'Fraction of repeated trigrams (overlapping 3-word sequences) indicating verbatim repetition patterns'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 6:\n        return 0.0\n    trigrams = [' '.join(words[i:i+3]) for i in range(len(words)-2)]\n    counts = Counter(trigrams)\n    repeated = sum(v - 1 for v in counts.values() if v > 1)\n    total = len(trigrams)\n    if total == 0:\n        return 0.0\n    return repeated / total\n\n", "def feature(text: str) -> float:\n    'Density of semicolons, colons, and parentheses per word (punctuation style indicator)'\n    import re\n    if not text:\n        return 0.0\n    punc_count = text.count(';') + text.count(':') + text.count('(') + text.count(')')\n    words = len(re.findall(r'\\w+', text))\n    if words == 0:\n        return 0.0\n    return punc_count / words\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like chapter/section markers (start with \"Chapter\" or Roman numerals like \"I.\")'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    pattern = re.compile(r'^\\s*(chapter\\b|chapter\\s+\\w+|\\b[IVXLCDM]+\\.|\\bchapter\\s*\\d+)', re.I)\n    matches = sum(1 for ln in lines if pattern.search(ln))\n    return matches / len(lines)\n", "def feature(text: str) -> float:\n    'Average number of ellipsis occurrences (\"...\" or longer) per sentence'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return ellipses / float(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain at least one quotation mark (indicative of dialogue presence)'\n    import re\n    if not text:\n        return 0.0\n    # split roughly into sentences\n    sentences = [s for s in re.split(r'[.!?]\\s*', text) if s.strip() != '']\n    if not sentences:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', '\\'')  # include single quote to detect dialogue with single quotes too\n    count = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            count += 1\n    return count / float(len(sentences))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that end with a truncation marker (trailing \"...\" or multiple dots or a trailing dash/em-dash)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    trunc_count = 0\n    for t in tokens:\n        if re.search(r'(\\.{2,}|\u2026|\u2014|\u2013|-)$', t):\n            trunc_count += 1\n    return trunc_count / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Density of dialogue-tag verbs (said, asked, replied, whispered, shouted, exclaimed, etc.) per sentence'\n    import re\n    if not text:\n        return 0.0\n    tags = re.findall(r'\\b(said|asked|replied|whispered|shouted|exclaimed|muttered|added|cried|yelled|remarked|answered)\\b', text, flags=re.I)\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return len(tags) / float(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Diversity of contraction forms: unique contraction tokens divided by total contraction-like tokens (higher = more varied contraction usage)'\n    import re\n    if not text:\n        return 0.0\n    contractions = re.findall(r\"\\b\\w+['\u2019]\\w+\\b\", text)\n    total = len(contractions)\n    if total == 0:\n        return 0.0\n    unique = len(set(c.lower() for c in contractions))\n    return unique / float(total)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation occurrences that are part of multi-character punctuation runs (e.g., \"!!\", \"?!\", \"...\")'\n    import re\n    if not text:\n        return 0.0\n    multi_runs = re.findall(r'[^\\w\\s]{2,}', text)\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    # count each multi-run as its length (to reflect weight)\n    multi_len = sum(len(m) for m in multi_runs)\n    return multi_len / float(total_punct)\n\n\n", "def feature(text: str) -> float:\n    'Average number of words in lines that contain quotes (useful to capture average spoken line length in dialogue)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    quote_lines = [l for l in lines if '\"' in l or '\u201c' in l or '\u201d' in l or l.strip().startswith(('-', '\u2014'))]\n    if not quote_lines:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', l)) for l in quote_lines]\n    avg = sum(lengths) / float(len(lengths))\n    return avg\n\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are \"fancy\" punctuation or typographic characters (curly quotes, em-dash, ellipsis char), indicating human-style typographic variation'\n    if not text:\n        return 0.0\n    fancy_chars = set('\u201c\u201d\u2018\u2019\u2014\u2013\u2026')  # include common typographic punctuation\n    total = len(text)\n    if total == 0:\n        return 0.0\n    count = sum(1 for c in text if c in fancy_chars)\n    return count / float(total)\n", "def feature(text: str) -> float:\n    'Proportion of quoted segments that contain any fully capitalized word (shouting, emphasis)'\n    import re\n    if not text:\n        return 0.0\n    quotes = re.findall(r'\"([^\"]+)\"', text)\n    if not quotes:\n        return 0.0\n    cap_count = 0\n    for q in quotes:\n        for token in re.findall(r'\\b[A-Z]{2,}\\b', q):\n            if token:\n                cap_count += 1\n                break\n    return float(cap_count) / len(quotes)\n\n", "def feature(text: str) -> float:\n    'Proportion of short sentences (<4 words) among all sentences'\n    import re\n    if not text:\n        return 0.0\n    # crude sentence split retaining fragments\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) < 4:\n            short += 1\n    return float(short) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of quoted lines that are accompanied by a nearby speaker-reporting verb (said, asked, replied, shouted, whispered, etc.)'\n    import re\n    if not text:\n        return 0.0\n    quotes_iter = list(re.finditer(r'\"([^\"]+)\"', text))\n    if not quotes_iter:\n        return 0.0\n    reporting = {'said','asked','replied','shouted','whispered','muttered','cried','exclaimed','answered','sighed','added','rejoined','remarked'}\n    pattern = re.compile(r'\\b(' + r'|'.join(re.escape(v) for v in reporting) + r')\\b', re.IGNORECASE)\n    matched = 0\n    for m in quotes_iter:\n        start, end = m.start(), m.end()\n        window_start = max(0, start - 60)\n        window_end = min(len(text), end + 60)\n        window = text[window_start:window_end]\n        if pattern.search(window):\n            matched += 1\n    return float(matched) / len(quotes_iter)\n\n", "def feature(text: str) -> float:\n    'Density of non-verbal tokens (parentheses/brackets notes, bracketed or hyphen-surrounded tokens) per 100 words'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\([^)]+\\)|\\[[^\\]]+\\]|-\\w+-|\\*\\w+\\*', text)\n    words = len(re.findall(r'\\w+', text))\n    return float(len(tokens) * 100) / (words + 1)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (clause density indicator)'\n    import re\n    if not text:\n        return 0.0\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    commas = text.count(',')\n    return float(commas) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Ratio of hyphenated or em-dash-containing tokens to total words (stilistic fragments like \"Scooby-Doo\", \"well-known\", \"\u2014\" usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    hyphen_tokens = 0\n    for t in tokens:\n        if '-' in t or '\u2014' in t or '\u2013' in t:\n            # avoid counting lone dashes used as separators too aggressively\n            if re.search(r'[A-Za-z0-9].*[-\u2014\u2013].*[A-Za-z0-9]', t):\n                hyphen_tokens += 1\n    words = len(re.findall(r'\\w+', text))\n    return float(hyphen_tokens) / (words + 1)\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (length > 10) among all word tokens \u2014 measures verbosity or formal diction'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 10)\n    return float(long_count) / len(words)\n", "def feature(text: str) -> float:\n    'Density of definitional/encyclopedic phrases (e.g., \"refers to\", \"is a\", \"is defined as\") per sentence'\n    if not text:\n        return 0.0\n    import re\n    patterns = [r'\\brefers to\\b', r'\\bis (?:a|an|the)\\b', r'\\bis defined as\\b', r'\\bcan be\\b', r'\\bconsists of\\b', r'\\bis called\\b']\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    match_count = 0\n    joined = ' '.join(sentences)  # fallback safe use\n    for s in sentences:\n        s_low = s.lower()\n        if any(re.search(p, s_low) for p in patterns):\n            match_count += 1\n    return float(match_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hedge/qualifying words (often, may, might, could, appears, seems, many, various)'\n    if not text:\n        return 0.0\n    import re\n    HEDGES = {'often', 'many', 'various', 'sometimes', 'may', 'might', 'could', 'seems', 'appears', 'generally', 'commonly', 'typically', 'usually', 'suggests', 'suggest', 'tend'}\n    tokens = re.findall(r\"\\w+(?:'\\w+)?\", text.lower())\n    if not tokens:\n        return 0.0\n    hedge_count = sum(1 for t in tokens if t in HEDGES)\n    return float(hedge_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average length of consecutive runs of stopwords (gives sense of function-word heavy sequences)'\n    if not text:\n        return 0.0\n    import re\n    STOP = {'the','a','an','and','or','but','if','when','while','of','in','on','for','to','with','by','as','is','are','was','were','be','been','being','that','which','this','these','those','it','its'}\n    tokens = re.findall(r\"\\w+(?:'\\w+)?\", text.lower())\n    if not tokens:\n        return 0.0\n    runs = []\n    current = 0\n    for t in tokens:\n        if t in STOP:\n            current += 1\n        else:\n            if current > 0:\n                runs.append(current)\n                current = 0\n    if current > 0:\n        runs.append(current)\n    if not runs:\n        return 0.0\n    return float(sum(runs)) / len(runs)\n\n", "def feature(text: str) -> float:\n    'Density of quotes and bracket characters (single/double quotes, curly/square/angle brackets) per character'\n    if not text:\n        return 0.0\n    chars = len(text)\n    if chars == 0:\n        return 0.0\n    QUOTES = set(['\"', \"'\", '\u201c', '\u201d', '\u2018', '\u2019', '\u00ab', '\u00bb', '<', '>', '(', ')', '[', ']', '{', '}', '\u2014', '\u2013'])\n    count = sum(1 for c in text if c in QUOTES)\n    return float(count) / chars\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing hyphens or slashes (hyphenated or slash-joined terms proportion)'\n    if not text:\n        return 0.0\n    import re\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    special = sum(1 for t in tokens if '-' in t or '/' in t)\n    return float(special) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Estimated passive-voice indicator: fraction of sentences containing a \"be\" auxiliary followed by an -ed participle'\n    if not text:\n        return 0.0\n    import re\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|am)\\b\\s+\\w+ed\\b', re.I)\n    passive_count = 0\n    for s in sentences:\n        if pattern.search(s):\n            passive_count += 1\n    return float(passive_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the most common sentence-initial word (captures formulaic/templated openings)'\n    if not text:\n        return 0.0\n    import re, collections\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = []\n    for s in sentences:\n        m = re.match(r'\\s*[\"\\(\\[]*\\s*([A-Za-z0-9\\']+)', s)\n        if m:\n            starts.append(m.group(1).lower())\n    if not starts:\n        return 0.0\n    counter = collections.Counter(starts)\n    most_common_count = counter.most_common(1)[0][1]\n    return float(most_common_count) / len(starts)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a first-person pronoun (I / We), highlighting sentence-level narrative stance'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    if not sentences:\n        return 0.0\n    fp_starts = 0\n    for s in sentences:\n        m = re.search(r\"^\\s*([\\w']+)\", s)\n        if m:\n            if m.group(1).lower() in {'i', \"i'm\", 'we', \"we're\"}:\n                fp_starts += 1\n    return float(fp_starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are titlecase-like (start with uppercase and have lowercase following), proxy for proper names and factual writing'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    def is_titlecase(tok):\n        return len(tok) > 1 and tok[0].isupper() and tok[1:].islower()\n    count = sum(1 for t in tokens if is_titlecase(t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit, indicating presence of factual/numeric content'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of ellipses (\"...\") per sentence, capturing trailing/thoughtful stylistic pauses common in fiction'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = re.split(r'[.!?]+', text)\n    # count non-empty sentence-like fragments\n    sentence_count = sum(1 for s in sentences if s.strip())\n    if sentence_count == 0:\n        return float(ellipses)\n    return float(ellipses) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in \"ly\" (case-insensitive), a simple proxy for adverb usage and descriptive prose'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.lower().endswith('ly') and len(t) > 2)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique token count divided by total tokens, a measure of lexical diversity'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"\\b[\\w'-]+\\b\", text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen, indicating compound or technical terms'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (small curated list), approximating functional vs content-word focus'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','of','to','a','in','that','is','it','for','on','was','with','as','by','an','be','are','this','which','from','at','or','but','were','he','she','they'}\n    tokens = [t.lower() for t in re.findall(r\"\\b[\\w']+\\b\", text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Density of digit characters (digits / total characters), useful for tables and numbered verses'\n    if not text:\n        return 0.0\n    digits = sum(1 for c in text if c.isdigit())\n    total = max(1, len(text))\n    return digits / float(total)\n\n", "def feature(text: str) -> float:\n    'Proportion of characters that are part of multi-space runs (two or more spaces), indicating tables/columns'\n    import re\n    if not text:\n        return 0.0\n    runs = re.findall(r' {2,}', text)\n    if not runs:\n        return 0.0\n    run_chars = sum(len(r) for r in runs)\n    total = max(1, len(text))\n    return run_chars / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are 4-digit years (1000-2099), common in academic citations and dated material'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(1[0-9]{3}|20[0-9]{2})\\b', text)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return len(years) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of short abbreviations ending with a period (e.g., \"vol.\", \"pp.\", \"Dr.\") to all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\S+\\b', text)\n    if not tokens:\n        return 0.0\n    abbrev_count = 0\n    for t in tokens:\n        # strip trailing punctuation except the dot we care about\n        if re.match(r'^[A-Za-z]{1,4}\\.$', t):\n            abbrev_count += 1\n    return abbrev_count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are all-uppercase (>=2 letters), highlighting headings or table headers'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[A-Za-z]{2,}\\b', text)\n    if not words:\n        return 0.0\n    up = sum(1 for w in words if w.isupper())\n    return up / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word tokens / total word tokens) as a simple lexical richness measure'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return unique / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Normalized count of ellipses (\"...\") occurrences per character, capturing trailing thoughts or redactions'\n    if not text:\n        return 0.0\n    count = text.count('...')\n    return count / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Average fraction of unique characters per word (unique_chars/word_length), a lightweight measure of word-internal variety'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    ratios = []\n    for w in words:\n        L = len(w)\n        if L == 0:\n            continue\n        uniq = len(set(w))\n        ratios.append(uniq / float(L))\n    if not ratios:\n        return 0.0\n    return sum(ratios) / float(len(ratios))\n", "def feature(text: str) -> float:\n    'Normalized frequency of \"et al.\" mentions (common in academic texts)'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    matches = re.findall(r'\\bet\\s+al\\.?', text, flags=re.I)\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of strong intra-sentence punctuation (colons and semicolons) per sentence'\n    import re\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    punct_count = text.count(':') + text.count(';')\n    return float(punct_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of common English function words (small stopword set) as a proxy for syntactic density'\n    import re\n    stopwords = {'the','and','of','to','a','in','is','for','on','by','with','that','as','are','be','this','it','an','from','or','which'}\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    cnt = sum(1 for w in words if w.lower() in stopwords)\n    return float(cnt) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of first-person pronouns (I, we, our, my, me, us) as a sign of personal vs. academic voice'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    first_person = {'i','we','our','us','my','me','mine','myself','ourselves'}\n    cnt = sum(1 for w in words if w.lower() in first_person)\n    return float(cnt) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of common inline abbreviations (e.g., e.g., i.e., etc., cf., vs.) normalized by word count'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    matches = re.findall(r'\\b(e\\.g\\.|i\\.e\\.|etc\\.|cf\\.|vs\\.)', text, flags=re.I)\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas indicate clause complexity)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = max(1, len(sentences))\n    return float(comma_count) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for s in sentences:\n        total_words += len(s.split())\n    return float(total_words) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that are common transition/adversative adverbs (e.g., however, therefore, moreover)'\n    import re\n    if not text:\n        return 0.0\n    transitions = {'however','therefore','moreover','furthermore','consequently','nevertheless','nonetheless','thus','additionally','meanwhile','subsequently','alternatively','hence'}\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in transitions)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of mid-sentence capitalized tokens (proxy for named-entity / proper-noun density)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    tokens_total = 0\n    mid_caps = 0\n    for sent in sentences:\n        words = re.findall(r\"\\b[\\w'-]+\\b\", sent)\n        for i, w in enumerate(words):\n            if not w:\n                continue\n            tokens_total += 1\n            # consider as mid-sentence capitalized if capitalized and not the first token in the sentence\n            if i != 0 and w[0].isupper():\n                mid_caps += 1\n    if tokens_total == 0:\n        return 0.0\n    return float(mid_caps) / float(tokens_total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen or dash (hyphenated compounds and multi-word modifiers)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t or '\u2013' in t or '\u2014' in t)\n    return float(hyphen_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of explicit quotation marks (double quotes and typographic quotes) per token'\n    import re\n    if not text:\n        return 0.0\n    quote_chars = ['\"', '\u201c', '\u201d', '\u00ab', '\u00bb', '\u201e', '\u201f']\n    quote_count = sum(text.count(q) for q in quote_chars)\n    tokens = re.findall(r\"\\b\\S+\\b\", text)\n    if not tokens:\n        return 0.0\n    return float(quote_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in -ing (gerunds/participles), which can indicate nominal/progressive constructions'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[A-Za-z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing') and len(t) > 3)\n    return float(ing_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are four-digit years in the 1900s or 2000s (e.g., 1945, 2001)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\d{4}\\b\", text)\n    if not tokens:\n        return 0.0\n    # Count only those between 1900 and 2099 as likely year-like tokens\n    year_count = sum(1 for t in tokens if 1900 <= int(t) <= 2099)\n    total_tokens = max(1, len(re.findall(r\"\\b\\S+\\b\", text)))\n    return float(year_count) / float(total_tokens)\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w']+\\b\", text)\n    nwords = len(words)\n    # Count sentences by terminal punctuation groups; treat absence as one sentence if words exist\n    sentences = re.findall(r'[.!?]+', text)\n    nsent = len(sentences) if sentences else (1 if nwords > 0 else 0)\n    if nsent == 0:\n        return 0.0\n    return nwords / nsent\n\n", "def feature(text: str) -> float:\n    'Comma density per sentence: number of commas divided by sentence count'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = re.findall(r'[.!?]+', text)\n    nsent = len(sentences) if sentences else 1\n    return commas / nsent\n\n", "def feature(text: str) -> float:\n    'Possessive apostrophe ratio: fraction of tokens that are possessives (e.g., company\u2019s, boss\\'s)'\n    import re\n    if not text:\n        return 0.0\n    # tokens including apostrophes and unicode right single quote\n    tokens = re.findall(r\"\\b[\\w\u2019']+\\b\", text)\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    possessive_pattern = re.compile(r\"(?i).+([\u2019']s)$\")\n    possessives = sum(1 for t in tokens if possessive_pattern.search(t))\n    return possessives / n\n\n", "def feature(text: str) -> float:\n    'Heading-like line ratio: fraction of non-empty lines that look like short title/headings'\n    import re\n    if not text:\n        return 0.0\n    lines = [line.strip() for line in text.splitlines() if line.strip()]\n    if not lines:\n        return 0.0\n    def is_heading(line):\n        words = re.findall(r\"\\b[\\w'\u2019]+\\b\", line)\n        if not words or len(words) > 7:\n            return False\n        # require most words to start with capital letter (Title-case / Proper heading)\n        caps = sum(1 for w in words if w[0].isupper())\n        return caps >= max(1, len(words) // 2)\n    count = sum(1 for l in lines if is_heading(l))\n    return count / len(lines)\n\n", "def feature(text: str) -> float:\n    'Hyphenated token ratio: fraction of tokens containing ASCII or Unicode dashes'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'\u2019\\-\u2013\u2014]+\\b\", text)\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t or '\u2013' in t or '\u2014' in t)\n    return hyphen_count / n\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word types that occur exactly once'\n    import re\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r\"\\b[\\w']+\\b\", text)]\n    n = len(words)\n    if n == 0:\n        return 0.0\n    freq = {}\n    for w in words:\n        freq[w] = freq.get(w, 0) + 1\n    hapaxes = sum(1 for v in freq.values() if v == 1)\n    # Return hapaxes normalized by token count (not types) to reflect rarity per sample\n    return hapaxes / n\n\n", "def feature(text: str) -> float:\n    'Long content-word ratio: fraction of tokens that are long (>6 chars) and not a common stopword'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','and','a','an','in','on','at','by','for','with','about','against','between','into',\n        'through','during','before','after','above','below','to','from','up','down','of','is','are',\n        'was','were','be','been','being','that','this','it','as','or','if','but','not','they','their',\n        'his','her','its','we','you','I','he','she'\n    }\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    content_long = sum(1 for t in tokens if len(t) > 6 and t not in stopwords)\n    return content_long / n\n\n", "def feature(text: str) -> float:\n    'Mid-sentence titlecase ratio: fraction of tokens that are Titlecase but not the first token of a sentence (proper noun signal)'\n    import re\n    if not text:\n        return 0.0\n    # split sentences on terminal punctuation followed by whitespace (keeps abbreviations imperfectly)\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    total_tokens = 0\n    mid_titlecase = 0\n    token_pattern = re.compile(r\"\\b[\\w'\u2019]+\\b\")\n    for sent in sentences:\n        if not sent:\n            continue\n        tokens = token_pattern.findall(sent)\n        if not tokens:\n            continue\n        for i, tok in enumerate(tokens):\n            total_tokens += 1\n            if i == 0:\n                continue\n            # consider Titlecase where first char is uppercase and rest are lowercase (common for proper nouns)\n            if len(tok) > 0 and tok[0].isupper() and tok[1:].islower():\n                mid_titlecase += 1\n    if total_tokens == 0:\n        return 0.0\n    return mid_titlecase / total_tokens\n", "def feature(text: str) -> float:\n    'Passive-voice heuristic: number of \"be\" forms followed by -ed participles per sentence (approximate)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|am)\\s+[A-Za-z-]+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    return float(len(matches)) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Comma density per sentence: average number of commas in each sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback to punctuation density if no clear sentences\n        return float(text.count(',')) if text else 0.0\n    return float(text.count(',')) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word types that occur exactly once in the text'\n    import re\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not words:\n        return 0.0\n    freq = {}\n    for w in words:\n        freq[w] = freq.get(w, 0) + 1\n    hapaxes = sum(1 for v in freq.values() if v == 1)\n    return float(hapaxes) / float(len(freq))\n\n", "def feature(text: str) -> float:\n    'First-person plural density: fraction of tokens that are \"we\" or \"We\" (indicator of authorial framing)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\w+\", text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() == 'we')\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Parenthetical density: fraction of words that occur inside parentheses (captures asides, acronyms, references)'\n    import re\n    if not text:\n        return 0.0\n    # find all parenthetical contents\n    parens = re.findall(r'\\(([^)]*)\\)', text)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    paren_words = 0\n    for p in parens:\n        paren_words += len(re.findall(r'\\w+', p))\n    return float(paren_words) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Colon and semicolon usage per sentence: (count of \":\" and \";\") divided by number of sentences'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    delim_count = text.count(':') + text.count(';')\n    if not sentences:\n        return float(delim_count)\n    return float(delim_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Hyphenated compound density: fraction of word tokens that are hyphenated compounds (e.g., \"multi-disciplinary\")'\n    import re\n    if not text:\n        return 0.0\n    # count words and hyphenated tokens\n    all_tokens = re.findall(r'\\b[\\w-]+\\b', text)\n    if not all_tokens:\n        return 0.0\n    hyphenated = sum(1 for t in all_tokens if '-' in t and re.search(r'[A-Za-z0-9]-[A-Za-z0-9]', t))\n    return float(hyphenated) / float(len(all_tokens))\n", "def feature(text: str) -> float:\n    'Passive-voice proxy: count of \"be\"-forms followed shortly by an -ed token per sentence (approximate passive usage)'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    # approximate pattern: be-form within up to 2 intervening tokens before an -ed verb\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|am|has been|have been|had been|has|have|had)\\b(?:\\s+\\w+){0,2}\\s+\\w+ed\\b', lowered)\n    sentence_count = max(1.0, float(lowered.count('.') + lowered.count('!') + lowered.count('?')))\n    return float(len(matches)) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Average sentence length (words per sentence) using simple punctuation split'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    sentence_count = max(1.0, float(text.count('.') + text.count('!') + text.count('?')))\n    return float(len(words)) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Variance of sentence lengths (words), indicating uniformity or variability of sentence construction'\n    import re, math\n    if not text:\n        return 0.0\n    raw_sentences = re.split(r'[.!?]+', text)\n    sent_lengths = []\n    for s in raw_sentences:\n        s = s.strip()\n        if not s:\n            continue\n        sent_lengths.append(len(re.findall(r'\\w+', s)))\n    if len(sent_lengths) <= 1:\n        return 0.0\n    mean = sum(sent_lengths) / len(sent_lengths)\n    var = sum((l - mean) ** 2 for l in sent_lengths) / len(sent_lengths)\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Parenthetical abbreviation density: fraction of parenthetical groups that look like abbreviations/acronyms (e.g., (DVT), (COVID-19)) per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    parens = re.findall(r'\\(([^)]{1,60})\\)', text)\n    if not parens:\n        return 0.0\n    abbrev_count = 0\n    for p in parens:\n        p_stripped = p.strip()\n        # consider acronyms/abbreviations: mostly uppercase letters and digits, optional hyphens, short length\n        if re.fullmatch(r'[A-Z0-9\\-]{1,20}', p_stripped):\n            # ensure contains at least one letter OR digit and more than one char (to avoid single letters)\n            if len(p_stripped) > 1:\n                abbrev_count += 1\n        # also treat short uppercase phrases like \"e.g.\" as abbreviation-like\n        elif len(p_stripped) <= 6 and any(c.isupper() for c in p_stripped):\n            abbrev_count += 1\n    return float(abbrev_count) / float(max(1, len(words)))\n\n", "def feature(text: str) -> float:\n    'Uppercase-acronym density: fraction of tokens that are all-caps acronyms of length >=2 (e.g., DVT, NASA) per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    acronyms = re.findall(r'\\b[A-Z]{2,}\\b', text)\n    # filter out purely numeric tokens captured (none here because pattern requires letters)\n    count = 0\n    for a in acronyms:\n        if any(ch.isalpha() for ch in a):\n            count += 1\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Colon and semicolon density per sentence: average number of \":\" and \";\" characters per sentence (style marker for lists/clauses)'\n    if not text:\n        return 0.0\n    punctuation_count = float(text.count(':') + text.count(';'))\n    sentence_count = max(1.0, float(text.count('.') + text.count('!') + text.count('?')))\n    return punctuation_count / sentence_count\n\n", "def feature(text: str) -> float:\n    'First-person pronoun density: fraction of tokens that are first-person pronouns (I, we, our, my, me, us) per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    matches = re.findall(r'\\b(?:i|me|my|mine|we|our|ours|us)\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice indicator: number of passive constructions per sentence (approx.)'\n    import re\n    if not text:\n        return 0.0\n    text_l = text.lower()\n    # approximate passive: auxiliary (was/were/is/are/has been/have been/had been/been/being) + past participle ending in ed/en\n    matches = re.findall(r'\\b(?:was|were|is|are|has been|have been|had been|been|being|be)\\b\\s+\\w+(?:ed\\b|en\\b)', text_l)\n    sentence_count = len(re.findall(r'[.!?]+', text_l))\n    if sentence_count == 0:\n        sentence_count = 1\n    return float(len(matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas as proxy for clause density/complexity)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentence_count = len(re.findall(r'[.!?]+', text))\n    if sentence_count == 0:\n        sentence_count = 1\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Lexical density measured as fraction of content words (non-function words) among tokens'\n    import re\n    if not text:\n        return 0.0\n    function_words = {'the','a','an','of','and','to','in','for','on','with','as','by','is','are','was','were','be','that','which','who','whom','this','these','those','it','its','their','they','them','at','from','or','but','if','than','so','then','such'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    function_count = sum(1 for t in tokens if t in function_words)\n    content_count = len(tokens) - function_count\n    return float(content_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio: fraction of tokens that are first-person pronouns (I, we, me, my, our, us)'\n    import re\n    if not text:\n        return 0.0\n    first_person = {'i','we','me','my','our','us','mine','myself','ourselves','ours'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Estimated passive-voice pattern density: count of \"is/are/was/etc. <word>ed\" per sentence'\n    import re\n    if not text:\n        return 0.0\n    sent_count = max(1, len(re.findall(r'[.!?]', text)))\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being)\\s+\\w+ed\\b', text, flags=re.I)\n    return float(len(matches)) / sent_count\n\n", "def feature(text: str) -> float:\n    'Proportion of titlecase words appearing mid-sentence (possible proper nouns or institutional names)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    mid_title_count = 0\n    total_words = 0\n    for s in sentences:\n        words = re.findall(r\"\\b\\w+\\b\", s)\n        if not words:\n            continue\n        total_words += len(words)\n        # skip first word of each sentence when counting titlecase mid-sentence\n        for w in words[1:]:\n            if len(w) > 1 and w[0].isupper() and (w[1:].islower() or w[1:].isalpha()):\n                mid_title_count += 1\n    if total_words == 0:\n        return 0.0\n    return float(mid_title_count) / total_words\n\n", "def feature(text: str) -> float:\n    'Average number of parenthetical groups per sentence (presence of parenthetical citations or asides)'\n    import re\n    if not text:\n        return 0.0\n    paren_groups = len(re.findall(r'\\([^)]{1,500}\\)', text))\n    sent_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(paren_groups) / sent_count\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause density / sentence complexity)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sent_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(comma_count) / sent_count\n\n", "def feature(text: str) -> float:\n    'Density of capitalized words that are not at sentence starts (proxy for organization names, proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    mid_caps = 0\n    total_tokens = 0\n    for s in sentences:\n        tokens = re.findall(r\"\\b\\w+\\b\", s)\n        if not tokens:\n            continue\n        total_tokens += len(tokens)\n        for tok in tokens[1:]:\n            if tok[0].isupper() and (len(tok) == 1 or tok[1:].islower()):\n                mid_caps += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(mid_caps) / total_tokens\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain digits (numbers, citations, model identifiers)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are nominalizations (words ending in -tion/-sion/-ment/-ness/-ity/-ence/-ance)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('tion','sion','ment','ness','ity','ence','ance')\n    count = 0\n    for t in tokens:\n        for s in suffixes:\n            if t.endswith(s) and len(t) > len(s) + 1:\n                count += 1\n                break\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with a first-person pronoun (I, we, my, our, me, mine, us, ours)'\n    import re\n    if not text:\n        return 0.0\n    parts = re.split(r'[.!?]+', text)\n    starts = 0\n    valid = 0\n    first_person = {'i','we','my','our','me','mine','us','ours'}\n    for part in parts:\n        s = part.strip()\n        if not s:\n            continue\n        m = re.search(r'\\b\\w+\\b', s)\n        if not m:\n            continue\n        valid += 1\n        if m.group(0).lower() in first_person:\n            starts += 1\n    if valid == 0:\n        return 0.0\n    return float(starts) / valid\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are internally capitalized (capitalized not because they begin a sentence) - proxy for named-entity density'\n    import re\n    if not text:\n        return 0.0\n    capitalized_iter = re.finditer(r'\\b[A-Z][a-z]+\\b', text)\n    internal_caps = 0\n    for m in capitalized_iter:\n        start = m.start()\n        # find last non-space character before this token\n        i = start - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        if i < 0:\n            # beginning of text -> sentence start\n            continue\n        if text[i] in '.!?':\n            # likely sentence-initial; skip\n            continue\n        internal_caps += 1\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    return float(internal_caps) / total\n\n", "def feature(text: str) -> float:\n    'Proportion of punctuation characters that belong to repeated punctuation sequences like \"!!\", \"??\", \"...\"'\n    import re\n    if not text:\n        return 0.0\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    repeat_groups = list(re.finditer(r'([!?\\.])\\1{1,}', text))\n    repeat_chars = sum(len(m.group(0)) for m in repeat_groups)\n    return float(repeat_chars) / total_punct\n\n", "def feature(text: str) -> float:\n    'Ratio of short words (<=3 letters) to long words (>7 letters) approximated as short_count / (1 + long_count)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    short_count = sum(1 for t in tokens if len(t) <= 3)\n    long_count = sum(1 for t in tokens if len(t) > 7)\n    return float(short_count) / (1.0 + float(long_count))\n\n", "def feature(text: str) -> float:\n    'Fraction of the document\\'s words that fall inside parentheses or square brackets'\n    import re\n    if not text:\n        return 0.0\n    total_words = len(re.findall(r'\\b\\w+\\b', text))\n    if total_words == 0:\n        return 0.0\n    inside_matches = re.findall(r'[\\(\\[]([^)\\]]+)[\\)\\]]', text)\n    inside_words = 0\n    for s in inside_matches:\n        inside_words += len(re.findall(r'\\b\\w+\\b', s))\n    return float(inside_words) / total_words\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are modal verbs (can/could/may/might/must/will/would/shall/should)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can','could','may','might','must','will','would','shall','should'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count) as a crude measure of sentence complexity'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences == 0:\n        # if no sentence punctuation, treat the whole text as one sentence\n        sentences = 1\n    return float(commas) / float(sentences)\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), sentences split on .!?'\n    import re\n    if not text:\n        return 0.0\n    # count sentences by punctuation and also try to split to avoid empty segments\n    sentences = re.split(r'[.!?]+', text)\n    sentences = [s for s in sentences if s.strip()]\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not sentences:\n        return float(len(words))\n    return float(len(words)) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (measures syntactic complexity/clauses)'\n    import re\n    if not text:\n        return 0.0\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    comma_count = text.count(',')\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Unmatched parenthesis/bracket imbalance normalized by text length (higher values indicate truncation or citation artifacts)'\n    if not text:\n        return 0.0\n    paren_diff = abs(text.count('(') - text.count(')'))\n    bracket_diff = abs(text.count('[') - text.count(']'))\n    total_diff = paren_diff + bracket_diff\n    return float(total_diff) / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Average occurrences of trailing ellipses per sentence (counts \"...\" presence) to capture truncation/ellipsis usage'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sentence_count = max(1, len(sentences))\n    return float(ellipses) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Proportion of lines that look like headings (short line starting with capitalized word)'\n    import re\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    heading_pattern = re.compile(r'^[A-Z][A-Za-z0-9 \\-]{0,60}\\s*$')\n    heading_count = sum(1 for l in lines if heading_pattern.match(l.strip()))\n    return float(heading_count) / max(1.0, len(lines))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    return float(len(set(tokens))) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: number of \"be + past-participle\" patterns per sentence'\n    import re\n    if not text:\n        return 0.0\n    # Sentence count fallback\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    # approximate passive patterns\n    patterns = re.compile(r'\\b(?:was|were|is|are|been|has been|have been|had been|was being|were being|is being|are being)\\b\\s+\\w+(?:ed|en|n)\\b', re.IGNORECASE)\n    matches = patterns.findall(text)\n    return float(len(matches)) / float(max(1, sentence_count))\n\n", "def feature(text: str) -> float:\n    'Density of structural/technical punctuation characters: ()[]{}:;/ per character'\n    if not text:\n        return 0.0\n    chars = len(text)\n    if chars == 0:\n        return 0.0\n    special = set('()[]{}:;/')\n    count = sum(1 for c in text if c in special)\n    return float(count) / float(chars)\n\n", "def feature(text: str) -> float:\n    'Frequency of citation-like patterns (numeric bracket citations or \"et al.\" or parenthetical 4-digit years) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    c1 = len(re.findall(r'\\[\\s*\\d+\\s*\\]', text))\n    c2 = len(re.findall(r'\\bet\\s+al\\b', text, flags=re.IGNORECASE))\n    c3 = len(re.findall(r'\\(\\s*(?:19|20)\\d{2}\\s*(?:[,;:)].*)?\\)', text))\n    total = c1 + c2 + c3\n    return float(total) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Rate of repeated adjacent word bigrams (fraction of bigrams that appear more than once)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [' '.join((tokens[i], tokens[i+1])) for i in range(len(tokens)-1)]\n    counts = Counter(bigrams)\n    repeated = sum(1 for b in counts if counts[b] > 1)\n    return float(repeated) / float(len(bigrams))\n\n", "def feature(text: str) -> float:\n    'Hapax legomena fraction: proportion of word tokens that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w[\\w-]*\\b', text.lower())\n    if not words:\n        return 0.0\n    counts = Counter(words)\n    hapaxes = sum(1 for w, c in counts.items() if c == 1)\n    return float(hapaxes) / len(words)\n\n", "def feature(text: str) -> float:\n    'Sentence-start formal fraction: fraction of sentences that begin with academic/function words like \"This\",\"These\",\"The\",\"In\"'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    formal_starts = {'this', 'these', 'the', 'in', 'our', 'we', 'however', 'moreover', 'furthermore'}\n    count = 0\n    for s in sentences:\n        m = re.match(r'\\s*([A-Za-z]+)', s)\n        if m and m.group(1).lower() in formal_starts:\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Citation pattern density: normalized count of common citation patterns like [1], (Smith et al., 2020), (Smith, 2020)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    denom = max(1, len(words))\n    numeric_cites = re.findall(r'\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]', text)\n    etal_cites = re.findall(r'\\([A-Za-z][A-Za-z\\'\\s\\.]{0,60}?et al\\.,\\s*\\d{4}\\)', text)\n    parenthetical_year = re.findall(r'\\([A-Z][a-zA-Z\\-]+,\\s*\\d{4}\\)', text)\n    total = len(numeric_cites) + len(etal_cites) + len(parenthetical_year)\n    return float(total) / denom\n\n", "def feature(text: str) -> float:\n    'Colon and semicolon density per sentence: (\":\" + \";\") count normalized by number of sentences'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = max(1, len(sentences))\n    punc_count = text.count(':') + text.count(';')\n    return float(punc_count) / num_sent\n\n", "def feature(text: str) -> float:\n    'Hyphenated word fraction: fraction of tokens containing a hyphen (e.g., \"long-term\", \"pre-existing\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w[\\w-]*\\b', text)\n    if not words:\n        return 0.0\n    hyph = sum(1 for w in words if '-' in w)\n    return float(hyph) / len(words)\n\n", "def feature(text: str) -> float:\n    'Sentence length variance: sample variance of words-per-sentence (0.0 if fewer than 2 sentences)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences]\n    if len(lengths) < 2:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / (len(lengths) - 1)\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Stopword ratio: fraction of tokens that are common English function words (simple stoplist)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','of','in','to','a','is','this','that','for','with','by','on','as','are','was','we','our','be','it'}\n    words = re.findall(r'\\b\\w[\\w-]*\\b', text.lower())\n    if not words:\n        return 0.0\n    sw_count = sum(1 for w in words if w in stopwords)\n    return float(sw_count) / len(words)\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Average number of words per comma-separated segment (words / (commas+1)) to capture clause density'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    words = len(tokens)\n    segments = text.count(',') + 1\n    if words == 0:\n        return 0.0\n    return float(words) / float(segments)\n\n", "def feature(text: str) -> float:\n    'Proportion of words that end with \"ly\" (adverb-heavy prose indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly'))\n    return float(ly_count) / float(total)\n\n", "def feature(text: str) -> float:\n    'Dialogue density: number of double-quote characters (dialogue markers) per token'\n    if not text:\n        return 0.0\n    words = max(1, len(text.split()))\n    quote_marks = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    return float(quote_marks) / float(words)\n\n", "def feature(text: str) -> float:\n    'Mid-sentence titlecase ratio: fraction of non-initial words in sentences that start with uppercase (proper nouns / odd caps)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    total_noninitial = 0\n    cap_noninitial = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if len(words) <= 1:\n            continue\n        for w in words[1:]:\n            total_noninitial += 1\n            if w and w[0].isupper():\n                cap_noninitial += 1\n    if total_noninitial == 0:\n        return 0.0\n    return float(cap_noninitial) / float(total_noninitial)\n\n", "def feature(text: str) -> float:\n    'Distinct pronoun usage divided by total tokens (measures pronoun diversity vs verbosity)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','you','your','yours','he','him','his','she','her','hers','they','them','their','theirs','it','its'}\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    distinct_prons = len(set(t for t in tokens if t in pronouns))\n    return float(distinct_prons) / float(total)\n\n", "def feature(text: str) -> float:\n    'Parenthetical/dash density: occurrences of parentheses or em-dashes per token (stylistic interruptions)'\n    if not text:\n        return 0.0\n    words = max(1, len(text.split()))\n    punc_count = text.count('(') + text.count(')') + text.count('\u2014') + text.count('--') + text.count(' - ')\n    return float(punc_count) / float(words)\n", "def feature(text: str) -> float:\n    'Non-initial Titlecase ratio: fraction of titlecase words that are not the first word of a sentence'\n    import re\n    if not text:\n        return 0.0\n    # all titlecase-looking tokens (e.g., Evangeline, John)\n    title_tokens = re.findall(r'\\b[A-Z][a-z]{2,}\\b', text)\n    # find sentence starts and their first words\n    sentences = [s.strip() for s in re.split(r'[.!?]\\s+|\\n+' , text) if s.strip()]\n    first_words = []\n    for s in sentences:\n        m = re.match(r'\\b([A-Z][a-z]{2,})\\b', s)\n        if m:\n            first_words.append(m.group(1))\n    non_initial = 0\n    for tok in title_tokens:\n        if tok not in first_words:\n            non_initial += 1\n    words = len(re.findall(r'\\b\\w+\\b', text))\n    return float(non_initial) / max(words, 1)\n\n", "def feature(text: str) -> float:\n    'Long-word ratio: fraction of words with length >= 8 characters'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 8)\n    return float(long_count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Commas per sentence: average number of commas divided by sentence count (robust to zero-punctuation)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # split into sentences by punctuation or newlines\n    sentences = [s for s in re.split(r'[.!?]+\\s+|\\n+', text) if s.strip()]\n    sent_count = max(len(sentences), 1)\n    return float(comma_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena rate: fraction of word types that occur exactly once (lowercased)'\n    import re\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\b\\w+\\b', text)]\n    if not words:\n        return 0.0\n    from collections import Counter\n    freqs = Counter(words)\n    hapaxes = sum(1 for v in freqs.values() if v == 1)\n    types = len(freqs)\n    return float(hapaxes) / max(types, 1)\n\n", "def feature(text: str) -> float:\n    'Sentence-start common-word bias: fraction of sentences that begin with common opener words (the, in, as, while, after, when, on)'\n    import re\n    if not text:\n        return 0.0\n    openers = {'the', 'in', 'as', 'while', 'after', 'when', 'on'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s+|\\n+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.match(r'\\b([A-Za-z]+)\\b', s)\n        if m and m.group(1).lower() in openers:\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Short-sentence fraction: fraction of sentences with fewer than 4 words (captures choppy/sentence-fragment style)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s+|\\n+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if len(words) < 4:\n            short += 1\n    return float(short) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Acronym/all-caps token ratio: fraction of tokens that are 2+ uppercase letters (e.g., \"SS\", \"NASA\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    acronyms = sum(1 for t in tokens if re.fullmatch(r'[A-Z]{2,}', t))\n    return float(acronyms) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Punctuation-cluster ratio: occurrences of consecutive punctuation characters (like \"?!\", \"--\", \"...\") divided by word count'\n    import re\n    if not text:\n        return 0.0\n    clusters = re.findall(r'[^\\w\\s]{2,}', text)\n    words = len(re.findall(r'\\b\\w+\\b', text))\n    return float(len(clusters)) / max(words, 1)\n", "def feature(text: str) -> float:\n    'Density of multi-word Title Case sequences (e.g., \"The Words\", proper-name or decorative titles per token)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = max(1, len(words))\n    # Match two or more consecutive Title-Case words (Capitalized then lowercase)\n    matches = re.findall(r'\\b(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\\b', text)\n    return float(len(matches)) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing digits (numeric tokens like ages, years, counts)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(digit_tokens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of color and sensory-descriptive words (small lexicon of colors and sensory adjectives per token)'\n    import re\n    if not text:\n        return 0.0\n    lex = {'red','crimson','black','white','brown','gold','silver','blue','green','scarlet','pale','dark','bright',\n           'thick','thin','cold','warm','hot','soft','hard','rough','smooth','scent','scented','smell','smelled',\n           'hear','heard','sound','glisten','glistening','plume','drip','dripped','bleed','blood','bloody','smear'}\n    tokens = [t.lower() for t in re.findall(r'\\b\\w+\\b', text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in lex)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of capitalized tokens that occur mid-sentence (heuristic for proper nouns / named entities)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences, then inspect tokens excluding the first token of each sentence\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    mid_caps = 0\n    mid_tokens = 0\n    for s in sentences:\n        tokens = re.findall(r'\\b\\w+\\b', s)\n        if len(tokens) <= 1:\n            continue\n        for tok in tokens[1:]:\n            mid_tokens += 1\n            if tok[0].isupper() and tok[1:].islower():\n                mid_caps += 1\n    if mid_tokens == 0:\n        return 0.0\n    return float(mid_caps) / float(mid_tokens)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: count of \"...\" or single unicode ellipsis per token (indicative of trailing/thoughtful style)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = max(1, len(words))\n    ellipses = text.count('...') + text.count('\u2026')\n    return float(ellipses) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of words longer than 6 characters (long-word ratio; proxy for a more ornate vocabulary)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) > 6)\n    return float(long_words) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with common adjective suffixes (ous,ful,ive,less,able,al,ic,ary) indicating descriptive register'\n    import re\n    if not text:\n        return 0.0\n    suffixes = ('ous','ful','ive','less','able','al','ic','ary')\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        for s in suffixes:\n            if t.endswith(s) and len(t) > len(s) + 1:\n                count += 1\n                break\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that start with a quotation mark (dialogue lines density using line breaks)'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    quote_starts = 0\n    for l in lines:\n        s = l.lstrip()\n        if s.startswith('\"') or s.startswith('\u201c') or s.startswith(\"'\"):\n            quote_starts += 1\n    return float(quote_starts) / float(len(lines))\n", "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (detects diacritics/typographic quotes)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return non_ascii / total\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens containing a hyphen (compound or stylistic hyphenation density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff0-9'\u2019\\-]+\", text)\n    if not tokens:\n        return 0.0\n    hyphen_tokens = sum(1 for t in tokens if '-' in t and len(t.strip('-')) > 0)\n    return hyphen_tokens / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of titlecased tokens that are not the first token of their sentence (approx named-entity density)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_non_initial_tokens = 0\n    non_initial_titlecase = 0\n    for s in sentences:\n        words = re.findall(r\"[A-Za-z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff0-9'\u2019\\-]+\", s)\n        for i, w in enumerate(words):\n            if i == 0:\n                continue\n            total_non_initial_tokens += 1\n            # istitle() checks that first char uppercase and others lowercase; also check capitalized-like\n            if w.istitle():\n                non_initial_titlecase += 1\n    if total_non_initial_tokens == 0:\n        return 0.0\n    return non_initial_titlecase / total_non_initial_tokens\n\n", "def feature(text: str) -> float:\n    'Diversity of punctuation: distinct punctuation characters divided by total punctuation occurrences'\n    import re\n    if not text:\n        return 0.0\n    punctuation_chars = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(punctuation_chars)\n    if total == 0:\n        return 0.0\n    distinct = len(set(punctuation_chars))\n    return distinct / total\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are inside parentheses or brackets (parenthetical verbosity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff0-9'\u2019\\-(){}\\[\\]]+\", text)\n    if not tokens:\n        return 0.0\n    parenthetical_tokens = 0\n    for t in tokens:\n        if '(' in t or ')' in t or '[' in t or ']' in t or '{' in t or '}' in t:\n            parenthetical_tokens += 1\n    return parenthetical_tokens / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of all apostrophe characters that are Unicode curly right single quotation marks (\u2019), indicating typographic styling'\n    if not text:\n        return 0.0\n    total_apos = text.count(\"'\") + text.count(\"\u2019\")\n    if total_apos == 0:\n        return 0.0\n    unicode_apos = text.count(\"\u2019\")\n    return unicode_apos / total_apos\n\n", "def feature(text: str) -> float:\n    'Density of common short function words (approximate stopword density using a compact list)'\n    import re\n    if not text:\n        return 0.0\n    common = {'the','and','of','to','a','in','that','it','is','was','for','with','as','on','by','an','be','this','are','were','or','which'}\n    tokens = re.findall(r\"[A-Za-z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff0-9'\u2019\\-]+\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in common)\n    return count / len(tokens)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z'-]+\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours', \"i'm\", \"i'd\", \"i've\", \"i'll\"}\n    cnt = sum(1 for t in tokens if t in first_person)\n    return float(cnt) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence \u2014 indicates clause/comma complexity'\n    import re\n    if not text:\n        return 0.0\n    # split sentences by end punctuation groups; if none, treat whole text as one sentence\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        sentences = [text]\n    commas = text.count(',')\n    return float(commas) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that contain a hyphen (e.g., \"man-thing\") indicating creative compounds or editorial style'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z'-]+\", text)\n    if not tokens:\n        return 0.0\n    hyphen_tokens = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_tokens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Normalized count of ellipses (three or more dots) per sentence \u2014 often used in informal/dramatic human writing'\n    import re\n    if not text:\n        return 0.0\n    ellipsis_matches = re.findall(r'\\.{3,}', text)\n    ellipsis_count = len(ellipsis_matches)\n    # sentence count fallback\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(ellipsis_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<=4 words) \u2014 indicates fragments, snappy dialogue, or informal style'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences; if no punctuation, treat whole text as one sentence\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    short_count = 0\n    for s in sentences:\n        words = re.findall(r\"[A-Za-z'-]+\", s)\n        if len(words) <= 4 and len(words) > 0:\n            short_count += 1\n    return float(short_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (a small, fixed list) \u2014 measures filler/functional-word usage'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','a','an','and','or','but','if','then','so','because','as','at','by','for','from','in','into','on','of','to','with','that','this','these','those','is','are','was','were','be','have','has','had','do','does','did','it','he','she','they','you','we','i','me','my','your'}\n    tokens = re.findall(r\"[A-Za-z'-]+\", text.lower())\n    if not tokens:\n        return 0.0\n    cnt = sum(1 for t in tokens if t in stopwords)\n    return float(cnt) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of word lengths across the whole text \u2014 indicates lexical variability'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z'-]+\", text)\n    lengths = [len(w) for w in words if w]\n    if not lengths:\n        return 0.0\n    mean = float(sum(lengths)) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are titlecase (First letter uppercase, rest lowercase) \u2014 rough proxy for proper nouns and named entities'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z'-]+\", text)\n    if not tokens:\n        return 0.0\n    titlecase = 0\n    for t in tokens:\n        if len(t) > 1 and t[0].isupper() and t[1:].islower():\n            titlecase += 1\n        elif len(t) == 1 and t.isupper():\n            # single-letter capitals like 'I' are counted separately (but include them here)\n            titlecase += 1\n    return float(titlecase) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Density of double-quote characters (standard and curly) per token, as a proxy for dialogue presence'\n    if not text:\n        return 0.0\n    toks = text.split()\n    n = len(toks) if toks else 1\n    quotes = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    return float(quotes) / float(n)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain any digit (numbers, ordinals like 11th, or enumerations)'\n    import re\n    if not text:\n        return 0.0\n    toks = text.split()\n    n = len(toks)\n    if n == 0:\n        return 0.0\n    count = 0\n    for t in toks:\n        if re.search(r'\\d', t):\n            count += 1\n    return float(count) / float(n)\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that do not end with terminal punctuation (.,!,?, closing quotes or ellipsis) \u2014 indicates fragments/poetic lines'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    total = len(lines)\n    if total == 0:\n        return 0.0\n    nonterminal = 0\n    for l in lines:\n        s = l.rstrip()\n        if not s:\n            nonterminal += 1\n            continue\n        # treat ellipsis and closing quotes as terminal\n        if s.endswith('...') or s.endswith('\u2026') or s[-1] in '.!?' or s[-1] in ['\"', '\u201d', \"'\", '\u2019', '\u201d']:\n            continue\n        nonterminal += 1\n    return float(nonterminal) / float(total)\n\n", "def feature(text: str) -> float:\n    'Ratio of distinct punctuation symbols used to total punctuation characters (higher = more variety relative to amount)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    unique = len(set(puncts))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like list items (start with a quote, dash, bullet, or a leading numeral+dot) \u2014 detects lists/search results/posts formatting'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    total = len(lines)\n    if total == 0:\n        return 0.0\n    pattern = re.compile(r'^\\s*([\"\\-\\*\\u2022]|\\d+\\.)')\n    count = 0\n    for l in lines:\n        if pattern.match(l):\n            count += 1\n    return float(count) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence-like segments whose first alphabetical character is lowercase (captures poetic/lowercase-sentence starts or informal line breaks)'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence enders or newlines\n    segments = re.split(r'[.!?]+\\s+|\\n+', text)\n    segs = [s for s in segments if s.strip()]\n    total = len(segs)\n    if total == 0:\n        return 0.0\n    lower_start = 0\n    for s in segs:\n        m = re.search(r'[A-Za-z]', s)\n        if m:\n            ch = m.group(0)\n            if ch.islower():\n                lower_start += 1\n    return float(lower_start) / float(total)\n", "def feature(text: str) -> float:\n    'Normalized frequency of filler/interjection tokens like \"um\", \"uh\", \"hmm\", \"erm\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'\\b(?:um+|uh+|erm|hmm+|ugh+|uhm|umm+|eh|ah)\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    return float(len(matches)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Count of ellipses or contiguous-dot sequences per word (e.g., \"...\" or \"..\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    seqs = re.findall(r'\\.{2,}', text)\n    if not tokens:\n        return 0.0\n    return float(len(seqs)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens containing elongated character runs (3+ same character), e.g., \"soooo\", \"Ummm\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    elongated = 0\n    pattern = re.compile(r'([A-Za-z0-9])\\1\\1')\n    for t in tokens:\n        if pattern.search(t):\n            elongated += 1\n    return float(elongated) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain at least one double-quote character (dialogue sentence ratio)'\n    import re\n    if not text:\n        return 0.0\n    # Count sentence-like splits\n    sentence_separators = re.findall(r'[.!?]+', text)\n    # Fallback: if no sentence separators, treat whole text as one sentence\n    sentence_count = max(1, len(sentence_separators))\n    quote_sentences = 0\n    # Split on sentence end markers to inspect fragments\n    fragments = re.split(r'(?<=[.!?])\\s+', text)\n    for frag in fragments:\n        if '\"' in frag:\n            quote_sentences += 1\n    return float(min(quote_sentences, sentence_count)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of first-person pronoun tokens (I, me, my, mine, we, us, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'im', \"i'm\", 'ive', \"i've\", 'id', \"i'd\", 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of multi-character punctuation sequences (e.g., \"!!!\", \"?!\", \"--\", \"..\") per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    punct_seqs = re.findall(r'(?:[!?.]{2,}|[-]{2,}|[,;:]{2,}|\\?!|!\\?)', text)\n    return float(len(punct_seqs)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter (after stripping leading quotes/spaces) \u2014 indicates informal or unedited style'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence fragments\n    fragments = re.split(r'[.!?]+', text)\n    starts = 0\n    total = 0\n    for frag in fragments:\n        s = frag.lstrip(' \\t\\n\"\\'' + '([{')  # strip common leading chars\n        if not s:\n            continue\n        total += 1\n        first_char = s[0]\n        if first_char.isalpha() and first_char.islower():\n            starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts) / float(total)\n\n", "def feature(text: str) -> float:\n    'Normalized count of common contractions written without an apostrophe (e.g., \"Im\", \"dont\", \"thats\") indicating casual/mistyped tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    forms = {'dont','cant','wont','shouldnt','couldnt','wouldnt','im','ive','id','isnt','theres','thats','whos','whats','lets','aint','hes','shes','hes','hes','ive','ill','ill'}\n    count = sum(1 for t in tokens if t in forms)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Frequency of ellipses per sentence (counts \"...\" and single-character ellipsis \"\u2026\")'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...') + text.count('\u2026')\n    # approximate sentence count by splitting on sentence-ending punctuation; ensure at least 1\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    return float(ellipses) / max(1, len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain double-quote characters (dialog indicator)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence-like segments (rough)\n    segments = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    total = max(1, len(segments))\n    dq_count = sum(1 for s in segments if '\"' in s or '\u201c' in s or '\u201d' in s)\n    return float(dq_count) / total\n\n", "def feature(text: str) -> float:\n    'Heuristic past-tense verb density: fraction of tokens ending with \"ed\" (len>3) among all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    ed_tokens = [t for t in tokens if len(t) > 3 and t.endswith('ed')]\n    return float(len(ed_tokens)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio: fraction of tokens that are first-person pronouns'\n    import re\n    if not text:\n        return 0.0\n    fp = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in fp)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of repeated proper names: occurrences of capitalized words that appear more than once divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    total = max(1, len(tokens))\n    caps = re.findall(r'\\b[A-Z][a-z]{2,}\\b', text)\n    if not caps:\n        return 0.0\n    counts = {}\n    for c in caps:\n        counts[c] = counts.get(c, 0) + 1\n    repeated_occurrences = sum(v for v in counts.values() if v > 1)\n    return float(repeated_occurrences) / total\n\n", "def feature(text: str) -> float:\n    'Density of vivid/sensory adjectives from a small handcrafted lexicon (cold, rusty, blistered, acrid, jagged, etc.)'\n    import re\n    if not text:\n        return 0.0\n    lex = {'cold', 'chill', 'rusty', 'blistered', 'sulfurous', 'acrid', 'broken', 'jagged',\n           'overgrown', 'abandoned', 'empty', 'anxious', 'trembling', 'bloodshot', 'rusty', 'ragged', 'blistered'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in lex)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average within-sentence type-token ratio (unique words / words) averaged across sentences'\n    import re\n    if not text:\n        return 0.0\n    segments = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not segments:\n        return 0.0\n    ratios = []\n    for s in segments:\n        tokens = re.findall(r'\\w+', s.lower())\n        if not tokens:\n            continue\n        uniques = len(set(tokens))\n        ratios.append(uniques / len(tokens))\n    if not ratios:\n        return 0.0\n    return float(sum(ratios)) / len(ratios)\n\n", "def feature(text: str) -> float:\n    'Proportion of long words (length > 7 characters) indicating denser vocabulary'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) > 7)\n    return float(long_count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Average number of words per quoted segment (mean quoted-turn length)'\n    import re\n    if not text:\n        return 0.0\n    quotes = re.findall(r'\"([^\"]+)\"', text)\n    if not quotes:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', q)) for q in quotes]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Density of common dialogue speaker verbs (said/asked/replied/etc.) per word'\n    import re\n    if not text:\n        return 0.0\n    pattern = r'\\b(?:said|asked|replied|whispered|muttered|cried|shouted|answered|added|remarked|sighed|exclaimed)\\b'\n    matches = len(re.findall(pattern, text, flags=re.I))\n    total_words = max(1, len(re.findall(r'\\w+', text)))\n    return float(matches) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Normalized count of ellipses or multi-dot hesitation sequences per word'\n    import re\n    if not text:\n        return 0.0\n    sequences = re.findall(r'\\.{3,}', text)\n    count = len(sequences)\n    total_words = max(1, len(re.findall(r'\\w+', text)))\n    return float(count) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Ratio of occurrences of a space directly before common punctuation (e.g., \" ,\" or \" .\") to all punctuation \u2014 formatting oddities'\n    import re\n    if not text:\n        return 0.0\n    # punctuation considered\n    puncts = set('.,;:!?()[]\"\\'' )\n    space_before = len(re.findall(r' [\\.\\,\\;\\:\\!\\?\\)\\]\\\\\"\\'\"]', text))\n    total_punct = sum(1 for c in text if c in puncts)\n    if total_punct == 0:\n        return 0.0\n    return float(space_before) / float(total_punct)\n\n", "def feature(text: str) -> float:\n    'Fraction of quoted segments that include or are adjacent to a speaker tag (evidence of dialogue attribution)'\n    import re\n    if not text:\n        return 0.0\n    quote_iter = list(re.finditer(r'\"([^\"]+)\"', text))\n    n = len(quote_iter)\n    if n == 0:\n        return 0.0\n    speaker_pattern = re.compile(r'\\b(?:said|asked|replied|whispered|muttered|cried|shouted|answered|added|remarked|sighed|exclaimed)\\b', flags=re.I)\n    count_with_speaker = 0\n    for m in quote_iter:\n        start, end = m.start(), m.end()\n        # look after and before the quote for a speaker tag within ~30 chars\n        after = text[end:end+40]\n        before = text[max(0, start-40):start]\n        if speaker_pattern.search(after) or speaker_pattern.search(before):\n            count_with_speaker += 1\n    return float(count_with_speaker) / float(n)\n\n", "def feature(text: str) -> float:\n    'Variety of punctuation characters used: distinct punctuation types divided by total punctuation (higher = diverse punctuation use)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    unique = len(set(puncts))\n    total = len(puncts)\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Diversity of contraction forms: number of distinct contraction token types (with apostrophes) normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    # catch words with internal apostrophes (straight or curly)\n    matches = re.findall(r\"\\b[\\w]+['\u2019][\\w]+\\b\", text)\n    total_words = max(1, len(re.findall(r'\\w+', text)))\n    unique = len(set(m.lower() for m in matches))\n    return float(unique) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that begin with a quotation mark and then a lowercase letter (unusual capitalization pattern in dialogue)'\n    import re\n    if not text:\n        return 0.0\n    # find sentence starts optionally after .!? and whitespace\n    matches = re.findall(r'(?:^|[\\.!\\?]\\s+)([\"\\'])(\\w)', text)\n    # keep only those that actually start with a quote\n    quote_starts = [(q, ch) for (q, ch) in matches if q]\n    if not quote_starts:\n        return 0.0\n    lower_count = sum(1 for q, ch in quote_starts if ch.islower())\n    return float(lower_count) / float(len(quote_starts))\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that start with a quote, dash, or em-dash (dialogue turn indicator)'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines()]\n    non_empty = [ln for ln in lines if ln.strip()]\n    if not non_empty:\n        return 0.0\n    starts = 0\n    for ln in non_empty:\n        s = ln.lstrip()\n        if not s:\n            continue\n        if s[0] in ['\"', \"'\", '\u201c', '\u2014', '-']:\n            starts += 1\n    return float(starts) / float(len(non_empty))\n\n", "def feature(text: str) -> float:\n    'Normalized count of ellipsis sequences (\"...\" or longer) per word (ellipsis-heavy style)'\n    import re\n    if not text:\n        return 0.0\n    words = text.split()\n    if not words:\n        return 0.0\n    ellipses = len(re.findall(r'\\.\\.\\.+', text))\n    return float(ellipses) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<=3 words) and end with an exclamation (one-word exclamations like \"Holy!\" )'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence-like chunks (keep trailing punctuation if present)\n    parts = re.findall(r'[^.!?]+[.!?]?', text, flags=re.DOTALL)\n    sentences = [p.strip() for p in parts if p.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        words = s.split()\n        if words and s.rstrip().endswith('!') and len(words) <= 3:\n            count += 1\n    return float(count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ratio of mid-sentence capitalized words (capitalization not following sentence end) to all words'\n    import re\n    if not text:\n        return 0.0\n    matches = list(re.finditer(r'\\b\\w+\\b', text))\n    if not matches:\n        return 0.0\n    mid_caps = 0\n    total = 0\n    for m in matches:\n        w = m.group(0)\n        if not w:\n            continue\n        total += 1\n        # find previous non-space character\n        i = m.start() - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        prev = text[i] if i >= 0 else ''\n        # treat as mid-sentence if previous non-space char is not a typical sentence terminator or newline/quote\n        if w[0].isupper() and prev not in ('.', '!', '?', '\\n', '\"', \"'\", '\u201c', '\u201d', ''):\n            mid_caps += 1\n    if total == 0:\n        return 0.0\n    return float(mid_caps) / float(total)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that contain a repeated character run of length >=3 (e.g., \"!!!\", \"loooool\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    rep = 0\n    for t in tokens:\n        if re.search(r'(.)\\1{2,}', t):\n            rep += 1\n    return float(rep) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are colloquial/informal tokens (gonna, wanna, gotta, ain\\'t, dunno, lemme)'\n    import re\n    if not text:\n        return 0.0\n    colloquial = {'gonna', 'wanna', 'gotta', \"ain't\", 'aint', 'dunno', 'lemme', 'ya', 'yall', \"y'all\"}\n    words = [w.lower().strip('.,!?:;\"()[]') for w in re.findall(r'\\b[\\w\\'\u2019]+\\b', text)]\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in colloquial)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines whose last non-space character is not a terminal sentence punctuation (.!?), indicating line-fragment/poetic layout'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    frag = 0\n    for ln in lines:\n        s = ln.rstrip()\n        # find last non-space char\n        i = len(s) - 1\n        while i >= 0 and s[i].isspace():\n            i -= 1\n        last = s[i] if i >= 0 else ''\n        if last not in ('.', '!', '?', '\u2026'):\n            frag += 1\n    return float(frag) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Distinct punctuation variety normalized by total punctuation count (higher = more varied punctuation characters)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    if not puncts:\n        return 0.0\n    distinct = set(puncts)\n    return float(len(distinct)) / float(len(puncts))\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count)'\n    import re\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    comma_count = text.count(',')\n    return float(comma_count) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<= 3 words)'\n    import re\n    # split roughly on sentence enders, keep fragments\n    raw_sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in raw_sentences if s.strip() != '']\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 3:\n            short += 1\n    return float(short) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of word tokens that are long (length >= 7), proxy for lexical complexity'\n    import re\n    words = re.findall(r\"\\w+\", text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 7)\n    return float(long_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per sentence (ellipses divided by sentence count)'\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    ellipses = text.count('...')\n    return float(ellipses) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Normalized punctuation variety: number of distinct punctuation characters divided by total punctuation occurrences'\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    total = len(puncts)\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Ratio of exclamation marks to sentence count (exclamation intensity)'\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    excl = text.count('!')\n    return float(excl) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Quote density: number of double-quote characters (\") normalized by word tokens'\n    import re\n    words = re.findall(r\"\\w+\", text)\n    if not words:\n        return 0.0\n    quote_chars = text.count('\"')\n    return float(quote_chars) / float(len(words))\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause complexity and subordination)'\n    import re\n    if not text:\n        return 0.0\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    commas = text.count(',')\n    return float(commas) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of modal verbs (could, would, should, might, may, must, can, shall) as indicator of argumentative/academic tone'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'could', 'would', 'should', 'might', 'may', 'must', 'can', 'shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens with internal capitalization (proper nouns, named entities) excluding first token and all-uppercase acronyms'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for i, tok in enumerate(tokens):\n        if i == 0:\n            continue\n        if tok and tok[0].isupper() and not tok.isupper():\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like abbreviations or contain internal periods (e.g., U.S., Fig., e.g.)'\n    import re\n    if not text:\n        return 0.0\n    abbr_matches = re.findall(r'\\b\\w+(?:\\.\\w+)+\\b', text)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    return float(len(abbr_matches)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per sentence (indicator of complex sentence embedding)'\n    import re\n    if not text:\n        return 0.0\n    semis = text.count(';')\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(semis) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Coverage of the top-5 most frequent tokens: sum(counts of top 5 tokens) / total tokens (measures repetition concentration)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    freq = {}\n    for t in tokens:\n        freq[t] = freq.get(t, 0) + 1\n    top_counts = sorted(freq.values(), reverse=True)[:5]\n    return float(sum(top_counts)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Standard deviation of words per sentence (lower for mechanically uniform prose)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if len(counts) == 0:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    return float(math.sqrt(var))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total tokens)'\n    import re\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common stopwords (the, and, of, to, a, in, is, that)'\n    import re\n    stopwords = {'the','and','of','to','a','in','is','that','it','for','on','with','as','are','was','be','by','an','this'}\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain a colon or semicolon (formal/explanatory structuring)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    sentinel_count = 0\n    for s in sentences:\n        if ':' in s or ';' in s:\n            sentinel_count += 1\n    return float(sentinel_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of bigrams that occur more than once (reused phrase fragments)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = ['%s %s' % (tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    from collections import Counter\n    counts = Counter(bigrams)\n    repeated_total = sum(cnt for bg, cnt in counts.items() if cnt > 1)\n    # normalized by total bigrams\n    return float(repeated_total) / float(len(bigrams))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal/hedging words (may, might, could, should, perhaps, possibly, likely)'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','would','should','can','perhaps','possibly','likely','unlikely','seem','seems','appears','appear','tend','tends'}\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Concentration of punctuation: fraction of all punctuation characters made up by the most frequent punctuation (monotony)'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    from collections import Counter\n    counts = Counter(puncts)\n    most = counts.most_common(1)[0][1]\n    return float(most) / float(len(puncts))\n", "def feature(text: str) -> float:\n    'Ellipsis frequency per word (counts occurrences of \"...\" normalized by token count)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    words_count = max(1, len(words))\n    ellipses = text.count('...')\n    return float(ellipses) / float(words_count)\n\n", "def feature(text: str) -> float:\n    'Normalized variance of sentence lengths (variance of words per sentence divided by (mean+1))'\n    import re, math\n    if not text:\n        return 0.0\n    # Split into sentences by terminal punctuation\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    if not sentences or len(sentences) == 1:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(var) / (mean + 1.0)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (list/enumeration density)'\n    import re\n    if not text:\n        return 0.0\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+\\s*', text) if s.strip()]))\n    comma_count = text.count(',')\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Passive-voice heuristic: occurrences of forms like \"is/was/were/be/been/being\" + past-participle (-ed) per sentence'\n    import re\n    if not text:\n        return 0.0\n    # Look for auxiliaries followed by a word ending in \"ed\" as a rough passive indicator\n    matches = re.findall(r'\\b(?:is|am|are|was|were|be|being|been|has been|have been|had been)\\b\\s+\\b\\w+ed\\b', text, flags=re.I)\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+\\s*', text) if s.strip()]))\n    return float(len(matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio (I, me, my, we, us, our) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation symbols divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    unique = len(set(puncts))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a conjunction/transition (and, but, so, however, then, also, moreover, thus, therefore)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    conj = {'and', 'but', 'so', 'however', 'then', 'also', 'moreover', 'thus', 'therefore', 'then'}\n    for s in sentences:\n        first = re.findall(r'\\b\\w+\\b', s)\n        if first:\n            if first[0].lower() in conj:\n                starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Citation-year ratio: fraction of tokens that look like 4-digit years (1000-2099), common in scholarly text'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z0-9'-]+\", text)\n    if not words:\n        return 0.0\n    years = re.findall(r'\\b(?:1[0-9]{3}|20[0-9]{2})\\b', text)\n    return float(len(years)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Multi-word capitalized sequence density: fraction of words that belong to sequences of 2+ Titlecase tokens (author names, proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff'-]+\", text)\n    if not tokens:\n        return 0.0\n    is_title = [1 if re.match(r'^[A-Z][a-z\\-\\'\u2019]+$', t) else 0 for t in tokens]\n    # count tokens that are part of runs of length >= 2\n    in_run_count = 0\n    run_len = 0\n    for flag in is_title + [0]:\n        if flag:\n            run_len += 1\n        else:\n            if run_len >= 2:\n                in_run_count += run_len\n            run_len = 0\n    return float(in_run_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Hyphenated token ratio: fraction of tokens that contain a hyphen (common in technical compound terms like \"proximal-distal\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9'-]+(?:-[A-Za-z0-9'-]+)*\", text)\n    if not tokens:\n        return 0.0\n    hyph_count = sum(1 for t in tokens if '-' in t)\n    return float(hyph_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Numeric token ratio: fraction of tokens containing a digit (equations, years, measurements)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9\\.\\-/%]+\", text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average parenthetical content length (characters): average length of text inside parentheses, 0 if none'\n    import re\n    if not text:\n        return 0.0\n    groups = re.findall(r'\\((.*?)\\)', text, flags=re.S)\n    if not groups:\n        return 0.0\n    lengths = [len(g.strip()) for g in groups if g is not None]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words: mean number of words per sentence (heuristic splitting on .!?), returns 0 on empty'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = []\n    for s in sentences:\n        tokens = re.findall(r\"[A-Za-z0-9'-]+\", s)\n        if tokens:\n            word_counts.append(len(tokens))\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts)) / float(len(word_counts))\n\n", "def feature(text: str) -> float:\n    'Commas per sentence: number of commas divided by sentence count (captures clause density and complex prose)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sentence_count = max(1, len(sentences))\n    return float(comma_count) / float(sentence_count)\n", "def feature(text: str) -> float:\n    'Ratio of tokens that contain digits (years, dosages, percentages, measurements) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num_tokens) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are acronyms or all-caps sequences (>=2 consecutive uppercase letters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    acronyms = sum(1 for w in words if re.fullmatch(r'[A-Z]{2,}', w))\n    return float(acronyms) / len(words)\n\n", "def feature(text: str) -> float:\n    'Proportion of short header-like lines (short standalone titles like \"Introduction\") among all lines'\n    import re\n    if not text:\n        return 0.0\n    lines = [l.strip() for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    header_like = 0\n    for l in lines:\n        # header-like: short (<40 chars), few punctuation, often title-cased or single word\n        if len(l) <= 40 and len(re.findall(r'[.!?;:]', l)) == 0:\n            words = l.split()\n            if 1 <= len(words) <= 6 and sum(1 for w in words if w[0].isupper()) >= 1:\n                header_like += 1\n    return float(header_like) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice density: count of \"be\" forms followed by past-participle-like tokens normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    # common be-forms possibly indicating passive constructions\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|has been|have been|had been)\\b\\s+\\w+(?:ed|en)\\b', text, flags=re.I)\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(matches)) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Density of modal/hedging verbs (may, might, could, should, would, can, must) per token'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    modals = set(['may','might','could','should','would','can','must','shall'])\n    count = sum(1 for w in words if w in modals)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of capitalized words that appear not at sentence starts (approximate named-entity density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[A-Za-z][A-Za-z\\'-]*\\b', text)\n    if not words:\n        return 0.0\n    # find capitalized tokens and check if they are at sentence start\n    count_mid = 0\n    total = 0\n    for m in re.finditer(r'\\b[A-Za-z][A-Za-z\\'-]*\\b', text):\n        token = m.group(0)\n        total += 1\n        if token[0].isupper():\n            # look for previous non-space character\n            i = m.start()\n            j = i - 1\n            while j >= 0 and text[j].isspace():\n                j -= 1\n            if j >= 0 and text[j] not in '.!?':\n                count_mid += 1\n            elif j < 0:\n                # start of text, treat as sentence start (do not count)\n                pass\n            else:\n                # previous char is sentence end -> treat as sentence start (do not count)\n                pass\n    return float(count_mid) / total if total else 0.0\n\n", "def feature(text: str) -> float:\n    'Density of double-quote usage (\", \u201c, \u201d) per token as a proxy for quoting/citation behavior'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\S+', text)\n    if not words:\n        return 0.0\n    quote_count = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    return float(quote_count) / len(words)\n", "def feature(text: str) -> float:\n    'Non-ASCII punctuation density: fraction of characters that are punctuation and non-ASCII (captures smart quotes, em-dashes, ellipses)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    count = 0\n    for c in text:\n        if not c.isalnum() and not c.isspace() and ord(c) > 127:\n            count += 1\n    return float(count) / total_chars\n\n", "def feature(text: str) -> float:\n    'Titlecase word ratio: fraction of tokens that are titlecased (e.g., \"Intel\", \"Introduction\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.istitle())\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'List/bullet line density: fraction of non-empty lines that look like list items (starting with -, *, or numbered lists)'\n    import re\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    nonempty = [ln for ln in lines if ln.strip() != '']\n    if not nonempty:\n        return 0.0\n    bullets = 0\n    for ln in nonempty:\n        s = ln.lstrip()\n        if s.startswith(('-', '*')) or re.match(r'^\\d+[\\.\\)]\\s+', s):\n            bullets += 1\n    return float(bullets) / len(nonempty)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), using .!? as sentence delimiters; 0 for empty text'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = len(words)\n    # count sentences by punctuation but also split to get more robust sentence count\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip() != '']\n    sent_count = len(sentences)\n    if sent_count == 0:\n        return float(word_count)\n    return float(word_count) / sent_count\n\n", "def feature(text: str) -> float:\n    'Quotation sentence density: fraction of sentences that contain quotation marks (straight or curly)'\n    import re\n    if not text:\n        return 0.0\n    quote_chars = '\"\\'\\u201c\\u201d\\u2018\\u2019'\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip() != '']\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Punctuation skew: ratio of the most frequent punctuation character count to total punctuation count (1.0 if only one kind used)'\n    if not text:\n        return 0.0\n    from collections import Counter\n    puncts = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    if not puncts:\n        return 0.0\n    ctr = Counter(puncts)\n    most = ctr.most_common(1)[0][1]\n    total = len(puncts)\n    return float(most) / total\n\n", "def feature(text: str) -> float:\n    'Digit-token ratio: fraction of tokens that contain at least one digit (captures dates, years, enumerations)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Fraction of alphabetic words that start with an uppercase letter but are not at an obvious sentence start (internal-capitalization density)'\n    import re\n    if not text:\n        return 0.0\n    words_iter = list(re.finditer(r'\\b[A-Za-z][A-Za-z0-9_]*\\b', text))\n    total = float(len(words_iter))\n    if total == 0.0:\n        return 0.0\n    internal_caps = 0\n    for m in words_iter:\n        start = m.start()\n        # look backwards for the previous non-space char (if any)\n        i = start - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        prev_char = text[i] if i >= 0 else ''\n        # if previous non-space char is a sentence terminator or quote or start of text, consider sentence-start\n        if prev_char in ('.', '!', '?', '\\n', '\"', \"'\"):\n            continue\n        # otherwise, if the matched word starts with uppercase, count it\n        if m.group(0)[0].isupper():\n            internal_caps += 1\n    return internal_caps / total\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (...) occurrences per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_words = float(len(words))\n    if total_words == 0.0:\n        return 0.0\n    ellipses = len(re.findall(r'\\.\\.\\.+', text))\n    return ellipses / total_words\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas used as a proxy for clause complexity)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # approximate sentences by ., !, ?\n    sentence_count = float(max(1, len(re.findall(r'[.!?]', text))))\n    return comma_count / sentence_count\n\n", "def feature(text: str) -> float:\n    'Fraction of words that appear inside parentheses or square brackets (parenthetical density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_words = float(len(words))\n    if total_words == 0.0:\n        return 0.0\n    inside = 0\n    for span in re.findall(r'\\([^)]*\\)', text):\n        inside += len(re.findall(r'\\w+', span))\n    for span in re.findall(r'\\[[^\\]]*\\]', text):\n        inside += len(re.findall(r'\\w+', span))\n    return inside / total_words\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = float(len(tokens))\n    if total == 0.0:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return count / total\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<= 3 words), indicating choppiness or telegraphic style'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences by punctuation sequences; keep any non-empty trimmed segments\n    raw_sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip() != '']\n    if not raw_sentences:\n        return 0.0\n    short_count = 0\n    for s in raw_sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 3:\n            short_count += 1\n    return short_count / float(len(raw_sentences))\n\n", "def feature(text: str) -> float:\n    'Normalized imbalance between counts of exclamation and question sentence endings (0 = balanced, 1 = extreme imbalance)'\n    import re\n    if not text:\n        return 0.0\n    ex = len(re.findall(r'!', text))\n    q = len(re.findall(r'\\?', text))\n    total_end = ex + q\n    if total_end == 0:\n        return 0.0\n    return abs(ex - q) / float(total_end)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word types that occur exactly once (indicator of variety vs repetition)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    total = float(len(tokens))\n    if total == 0.0:\n        return 0.0\n    from collections import Counter\n    counts = Counter(tokens)\n    hapaxes = sum(1 for c in counts.values() if c == 1)\n    # return hapaxes normalized by total distinct types to reflect uniqueness among types\n    distinct = float(len(counts))\n    if distinct == 0.0:\n        return 0.0\n    return hapaxes / distinct\n", "def feature(text: str) -> float:\n    'Lexical diversity: number of unique word tokens divided by total word tokens'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return unique / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<= 6 words) \u2014 measures sentence-length variability'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short_count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 6 and len(words) > 0:\n            short_count += 1\n    return short_count / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Comma density: number of commas divided by total words (captures descriptive, clause-heavy style)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = max(1, len(words))\n    comma_count = text.count(',')\n    return comma_count / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a first-person pronoun (I, me, my, we, our, us)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    pronoun_pattern = re.compile(r'^\\s*(?:\"|\\\u201c|\\\u201d)?\\s*(i|me|my|mine|we|us|our|ours)\\b', re.I)\n    starts = sum(1 for s in sentences if pronoun_pattern.search(s))\n    return starts / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Present-progressive proxy: fraction of word-tokens ending with \"ing\" (approx continuous action usage)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return ing_count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis frequency: occurrences of \"...\" or \"\u2026\" per sentence (indicates trailing/fragmented style)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    sentence_count = max(1, len(sentences))\n    ellipses = len(re.findall(r'\\.\\.\\.|\u2026', text))\n    return ellipses / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Mid-sentence capitalized-word ratio: fraction of non-initial words in sentences that start with uppercase (proper nouns or odd caps)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    total_mid = 0\n    cap_mid = 0\n    for s in sentences:\n        words = re.findall(r\"[A-Za-z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff']+\", s)\n        if len(words) <= 1:\n            continue\n        mid_words = words[1:]\n        total_mid += len(mid_words)\n        cap_mid += sum(1 for w in mid_words if w[0].isupper())\n    if total_mid == 0:\n        return 0.0\n    return cap_mid / float(total_mid)\n\n", "def feature(text: str) -> float:\n    'Punctuation repetition score: number of repeated-punctuation runs (e.g., \"!!\", \"??\", \"--\") per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    sentence_count = max(1, len(sentences))\n    repeats = len(re.findall(r'([!?.,;:\\-])\\1{1,}', text))\n    return repeats / float(sentence_count)\n", "def feature(text: str) -> float:\n    'Proportion of words that appear inside double quotes (dialogue density using double quotes)'\n    import re\n    if not text:\n        return 0.0\n    total_words = len(re.findall(r'\\w+', text))\n    if total_words == 0:\n        return 0.0\n    # sum words in every odd segment when splitting on double quotes\n    parts = text.split('\"')\n    quoted_words = 0\n    for i in range(1, len(parts), 2):\n        quoted_words += len(re.findall(r'\\w+', parts[i]))\n    return quoted_words / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, we, our, etc.) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','myself','we','us','our','ours','ourselves'}\n    count = sum(1 for t in tokens if t in first_person)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Passive-voice proxy: occurrences of be-forms followed by -ed token per sentence (heuristic)'\n    import re\n    if not text:\n        return 0.0\n    # count candidate passive patterns\n    matches = re.findall(r'\\b(?:was|were|is|are|be|been|being)\\s+[A-Za-z]+ed\\b', text, flags=re.IGNORECASE)\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return len(matches) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total words) as a measure of lexical diversity'\n    import re\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text)]\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return unique / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Variety of punctuation: number of distinct punctuation characters divided by total punctuation count'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total_p = len(puncts)\n    if total_p == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return distinct / float(total_p)\n\n", "def feature(text: str) -> float:\n    'Fraction of words that occur inside parentheses (parenthetical content density)'\n    import re\n    if not text:\n        return 0.0\n    paren_matches = re.findall(r'\\([^)]*\\)', text)\n    total_words = max(1, len(re.findall(r'\\w+', text)))\n    words_in_paren = sum(len(re.findall(r'\\w+', m)) for m in paren_matches)\n    return words_in_paren / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Indicator of an author byline (e.g., lines starting with \"by <Name>\" or \"by Your AI Assistant\")'\n    import re\n    if not text:\n        return 0.0\n    # look for \"by <word...>\" at start of text or start of a line, case-insensitive\n    if re.search(r'(?im)^(?:\\s*by\\s+[^\\n]{1,60})', text):\n        return 1.0\n    # also check for explicit \"Your AI Assistant\" anywhere\n    if re.search(r'Your\\s+AI\\s+Assistant', text, flags=re.IGNORECASE):\n        return 1.0\n    return 0.0\n", "def feature(text: str) -> float:\n    'Ratio of titlecase words that are not the first word in the text (proxy for proper names and titles)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    count = 0\n    for i, w in enumerate(words):\n        # skip the very first word to avoid sentence-initial capitalization bias\n        if i == 0:\n            continue\n        if w[0].isupper() and not w.isupper():\n            count += 1\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of mid-sentence punctuation (colons, semicolons, and dashes) per sentence'\n    import re\n    if not text:\n        return 0.0\n    mid_punct = text.count(':') + text.count(';') + text.count('\u2014') + text.count('\u2013') + text.count(' - ')\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(mid_punct) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Proportion of words that are inside or adjacent to parentheses (counts \"(\" or \")\" toward token proportion)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    paren_marks = text.count('(') + text.count(')')\n    # normalize by word count to get a per-token signal\n    return float(paren_marks) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (a proxy for clause density and syntactic complexity)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(commas) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique lowercased word forms divided by total words) as a measure of lexical diversity'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating or discourse conjunction (and, but, so, however, then, yet, also, moreover)'\n    import re\n    if not text:\n        return 0.0\n    conj = {'and','but','so','then','however','yet','also','moreover','thus','meanwhile'}\n    # split into sentence-like chunks\n    raw_sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s.strip() for s in raw_sentences if s and re.search(r'\\w', s)]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        # remove leading non-letters and quotes\n        s_stripped = re.sub(r'^[^A-Za-z0-9]+', '', s)\n        first_word_match = re.match(r'([A-Za-z]+)', s_stripped)\n        if first_word_match:\n            if first_word_match.group(1).lower() in conj:\n                count += 1\n    return float(count) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Density of double/curly double quotes per token (signals quoting of titles, citations)'\n    if not text:\n        return 0.0\n    words = len(re.findall(r'\\w+', text))\n    quote_chars = ['\"', '\u201c', '\u201d']\n    qcount = sum(text.count(ch) for ch in quote_chars)\n    return qcount / float(max(1, words))\n\n", "def feature(text: str) -> float:\n    \"Indicator for explicit translation tags like '[translated]' (1.0 if present, else 0.0)\"\n    if not text:\n        return 0.0\n    return 1.0 if '[translated]' in text.lower() else 0.0\n\n", "def feature(text: str) -> float:\n    'Proper noun density excluding sentence-initial words: capitalized tokens not at sentence start / total tokens'\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    total_tokens = 0\n    proper_count = 0\n    for s in sentences:\n        tokens = re.findall(r\"\\b[\\w'\u2019-]+\\b\", s)\n        if not tokens:\n            continue\n        for i, t in enumerate(tokens):\n            total_tokens += 1\n            if i == 0:\n                continue\n            # capitalized with at least one lowercase letter after first char (avoid ACRONYMS)\n            if t[0].isupper() and any(c.islower() for c in t[1:]):\n                proper_count += 1\n    if total_tokens == 0:\n        return 0.0\n    return proper_count / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: count of \"...\" or unicode ellipsis characters per token'\n    if not text:\n        return 0.0\n    words = len(re.findall(r'\\w+', text))\n    ell = text.count('...') + text.count('\u2026')\n    return ell / float(max(1, words))\n\n", "def feature(text: str) -> float:\n    'Academic lexicon density: proportion of tokens that match a small curated set of scholarly/critical words'\n    if not text:\n        return 0.0\n    lexicon = {'novella','novel','poem','poetry','critique','criticism','analysis','analyze','framework','photograph','essay','theme','motif','narrative','character','motivation','interpretation','reading','author','text','discuss','discussing','explores','serves','provides','insight'}\n    tokens = re.findall(r\"\\b[\\w'\u2019-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.strip(\"'\u2019-\") in lexicon)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Colon/semicolon usage normalized by sentence count (signals headings, clauses, academic style)'\n    if not text:\n        return 0.0\n    sent_count = max(1, len(re.split(r'(?<=[.!?])\\s+', text.strip())))\n    punct_count = text.count(':') + text.count(';')\n    return punct_count / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Hyphenated-token ratio: proportion of tokens containing ASCII or unicode dashes (compound words)'\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'\u2019-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    dash_chars = ('-', '\u2013', '\u2014')\n    count = sum(1 for t in tokens if any(ch in t for ch in dash_chars))\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Short-sentence fraction: proportion of sentences with 5 or fewer words (narrative fragments, headings, teaser style)'\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if 0 < len(words) <= 5:\n            short += 1\n    return short / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (indicates clause complexity and narrative style)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (lexical diversity): unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence-like segments that start with a lowercase letter (informality/poetic style)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks on ., !, ?, or newlines\n    segments = [s.strip() for s in re.split(r'[.!?]\\s+|\\n+', text) if s.strip()]\n    if not segments:\n        return 0.0\n    lower_start = 0\n    for seg in segments:\n        # find first alphabetic character\n        m = re.search(r'[A-Za-z]', seg)\n        if m and seg[m.start()].islower():\n            lower_start += 1\n    return float(lower_start) / float(len(segments))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens containing digits (captures numeric/text-speak usage like \"2\" or \"every1\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    has_digit = 0\n    for t in tokens:\n        if any(ch.isdigit() for ch in t):\n            has_digit += 1\n    return float(has_digit) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Stopword density: proportion of common function words among all word tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','a','an','in','on','to','of','is','it','that','he','she','they','we','i','you','was','were','as','for','with','at','by','from','this','be','are'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    sw_count = sum(1 for t in tokens if t in stopwords)\n    return float(sw_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: count of \"...\" sequences per sentence (indicates trailing thoughts/colloquial pacing)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(ellipses) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain double-quote characters (indicator of direct speech/dialogue)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences roughly\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d')\n    count = sum(1 for s in sentences if any(q in s for q in quote_chars))\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Normalized variety of punctuation characters used (distinct punctuation types / total common punctuation)'\n    if not text:\n        return 0.0\n    punctuation_chars = r'''!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~'''\n    used = set(c for c in text if (not c.isalnum()) and (not c.isspace()))\n    if not punctuation_chars:\n        return 0.0\n    # normalize to 0..1 by dividing by length of typical punctuation set\n    return float(len(used)) / max(1.0, len(punctuation_chars))\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of the first-word lengths across sentences (variability in sentence openings)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    first_lengths = []\n    for s in sentences:\n        words = [w for w in re.findall(r\"\\b[\\w'-]+\\b\", s)]\n        if words:\n            first_lengths.append(len(re.sub(r'[^A-Za-z]', '', words[0])))\n    if not first_lengths:\n        return 0.0\n    mean = sum(first_lengths) / len(first_lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((x - mean) ** 2 for x in first_lengths) / len(first_lengths)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that contain parentheses or dashes (indicators of asides/em-dashes/parenthetical style)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    markers = ('(', ')', '\u2014', '--', '\u2013')\n    count = sum(1 for s in sentences if any(m in s for m in markers))\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Density of ellipses or multi-dot sequences (count of \"...\" or \"\u2026\") normalized by characters'\n    import re\n    if not text:\n        return 0.0\n    ellipses = re.findall(r'\\.{2,}|\u2026', text)\n    return float(len(ellipses)) / max(1, len(text))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens containing a character repeated three or more times consecutively (e.g., \"sooo\", \"noooo\")'\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    repeats = 0\n    for t in tokens:\n        if re.search(r'(.)\\1\\1', t):\n            repeats += 1\n    return float(repeats) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that look like contractions with missing apostrophes (dont, cant, im, youve, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    patterns = {'dont','cant','wont','shouldnt','couldnt','wouldnt','im','ive','ill','youre','youve','youd','hes','shes','theres','thats','lets','isnt'}\n    count = sum(1 for t in tokens if t in patterns)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of short sentences (fewer than 4 word tokens) which can indicate informal fragments or dialog'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[^.!?]+(?:[.!?]|$)', text, flags=re.M)\n    if not sentences:\n        return 0.0\n    short = 0\n    total = 0\n    for s in sentences:\n        words = s.split()\n        if not words:\n            continue\n        total += 1\n        if len(words) < 4:\n            short += 1\n    return float(short) / total if total else 0.0\n\n", "def feature(text: str) -> float:\n    'Density of double-quote characters (\") per token \u2014 indicates quoting of phrases or reported speech'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    return float(text.count('\"')) / max(1, len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of first-person pronoun tokens (I, me, my, we, us, our) indicating personal/subjective narrative'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average length of punctuation runs immediately following words (e.g., \"word...\" gives 3) \u2014 higher values show emphatic/elliptical style'\n    import re\n    if not text:\n        return 0.0\n    runs = re.findall(r'\\w+([^\\w\\s]+)', text)\n    if not runs:\n        return 0.0\n    total_len = sum(len(r) for r in runs)\n    return float(total_len) / len(runs)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a first-person pronoun (I, we, me, my, ours) \u2014 signal of personal narration'\n    import re\n    if not text:\n        return 0.0\n    # split into candidate sentences\n    sentences = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    first_person = 0\n    for s in sentences:\n        m = re.match(r\"\\s*([A-Za-z']+)\", s)\n        if m:\n            w = m.group(1).lower()\n            if w in {'i', 'we', 'me', 'my', 'mine', 'our', 'ours'}:\n                first_person += 1\n    return float(first_person) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Density of sensory words (sight/hearing/smell/touch/taste verbs and adjectives) per token'\n    import re\n    if not text:\n        return 0.0\n    sensory = {'see','saw','seen','look','looked','gazed','stare','stared','glance','glanced',\n               'hear','heard','listen','listened','sound','whisper','whispered','shout','shouted',\n               'smell','smelled','scent','taste','tasted','feel','felt','touch','touched','dimly','dark','bright','rusty','cold','warm','silent','silence'}\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Maximum normalized token repetition: frequency of the most common token divided by total tokens'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    most_common = counts.most_common(1)[0][1]\n    return float(most_common) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of word tokens ending with -ing (progressive/gerund density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    ing = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return float(ing) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average punctuation marks per sentence (punctuation density normalized by sentence count)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if not sentences:\n        return float(punct_count)\n    return float(punct_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Dash/em-dash usage per sentence: counts em-dash or double-hyphen or spaced hyphen as interruption/dialog markers'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    dash_matches = re.findall(r'\u2014|--| - ', text)\n    if not sentences:\n        return float(len(dash_matches))\n    return float(len(dash_matches)) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the definite article \"the\" (case-insensitive) \u2014 common in descriptive openings'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    the_count = 0\n    for s in sentences:\n        m = re.match(r\"\\s*([A-Za-z']+)\", s)\n        if m and m.group(1).lower() == 'the':\n            the_count += 1\n    return float(the_count) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain double-quote characters (indicator of dialog)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences using punctuation followed by whitespace (robust fallback)\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        # treat entire text as one sentence\n        sentences = [text]\n    quote_count = sum(1 for s in sentences if '\"' in s or '\u201c' in s or '\u201d' in s)\n    return float(quote_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas density reflecting clause complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_total = text.count(',')\n    return float(comma_total) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens containing digits (ages, years, numeric references)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of common speech verbs (said, told, asked, replied, whispered, etc.) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    speech_verbs = {'said', 'told', 'asked', 'replied', 'whispered', 'murmured', 'shouted', 'cried', 'sighed', 'yelled', 'answered'}\n    count = sum(1 for t in tokens if t in speech_verbs)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of simple past auxiliaries (was, were, had, did) to tokens (captures past narrative scaffolding)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    past_aux = {'was', 'were', 'had', 'did', 'went', 'came'}\n    count = sum(1 for t in tokens if t in past_aux)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of hyphenated tokens (tokens containing \"-\"), useful for compound adjectives or editorial style'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of passive-like patterns: (was|were|is|are|had been|has been) followed by -ed word per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|had been|has been|have been|had|has|have)\\b\\s+\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    return float(len(matches)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of explicit dialogue tags like \"<pronoun> said/told/asked\" per token (he said, she told, they asked)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'\\b(?:he|she|they|i|we|you|him|her|them)\\s+(?:said|told|asked|replied|whispered|murmured|shouted|sighed)\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    return float(len(matches)) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\w+['\u2019]?\\w*|\\w+\", text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i', \"i'm\", 'im', \"i've\", \"i'd\", 'me', 'my', 'mine', 'we', \"we're\", 'us', 'our', 'ours'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total word tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / len(words)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentence_count), sentences split on .!?'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    return float(commas) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: distinct punctuation characters divided by total punctuation characters (0 if none)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    unique = len(set(puncts))\n    return float(unique) / total\n\n", "def feature(text: str) -> float:\n    'Average length (in characters) of the first word of each sentence (sentences split on .!?); 0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    pieces = re.split(r'[.!?]+\\s*', text)\n    lengths = []\n    for p in pieces:\n        p = p.strip()\n        if not p:\n            continue\n        m = re.search(r'\\w+', p)\n        if m:\n            lengths.append(len(m.group(0)))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain explicit quotation marks (double quotes or typographic quotes)'\n    import re\n    if not text:\n        return 0.0\n    # crude sentence split keeping empty drops\n    sentences = [s for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = set(['\"', '\u201c', '\u201d', '\u00bb', '\u00ab'])\n    count = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of capitalized word tokens that occur mid-sentence (not immediately after a sentence boundary)'\n    import re\n    if not text:\n        return 0.0\n    words_iter = list(re.finditer(r'\\b\\w+\\b', text))\n    if not words_iter:\n        return 0.0\n    total = 0\n    mid_caps = 0\n    for m in words_iter:\n        w = m.group(0)\n        total += 1\n        if not w:\n            continue\n        if not w[0].isupper():\n            continue\n        # find previous non-space character before this match\n        idx = m.start()\n        # consider it's sentence-start if beginning of text or previous non-space char is . ! ? or newline\n        prev_idx = idx - 1\n        while prev_idx >= 0 and text[prev_idx].isspace():\n            prev_idx -= 1\n        if prev_idx < 0:\n            # sentence start\n            continue\n        if text[prev_idx] in '.!?\\\"\u201c\u201d\\'':\n            # treat as sentence boundary\n            continue\n        # otherwise count as mid-sentence capitalized word\n        mid_caps += 1\n    if total == 0:\n        return 0.0\n    return float(mid_caps) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that contain at least one numeric digit'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    num_count = sum(1 for w in words if any(ch.isdigit() for ch in w))\n    return float(num_count) / len(words)\n", "def feature(text: str) -> float:\n    'Count of ellipses (three or more consecutive dots) normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    total_words = len(re.findall(r'\\w+', text))\n    if total_words == 0:\n        return float(ellipses)\n    return float(ellipses) / total_words\n\n", "def feature(text: str) -> float:\n    'Proportion of sentence-terminating punctuation that are question marks (indicative of interrogative/dialog style)'\n    import re\n    if not text:\n        return 0.0\n    total_term = len(re.findall(r'[.!?]', text))\n    if total_term == 0:\n        return 0.0\n    question_marks = len(re.findall(r'\\?', text))\n    return float(question_marks) / float(total_term)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence starts that begin with a quotation mark (shows dialog-driven sentences)'\n    import re\n    if not text:\n        return 0.0\n    # Count approximate sentence starts (start of text or after .!? plus whitespace)\n    sentence_starts = re.findall(r'(?:^|[.!?]\\s+)(?=[\"\\'])', text, re.S)\n    total_starts = len(re.findall(r'(?:^|[.!?]\\s+)', text, re.S))\n    # Avoid zero denominator: treat no sentences as 0\n    if total_starts == 0:\n        return 0.0\n    return float(len(sentence_starts)) / float(total_starts)\n\n", "def feature(text: str) -> float:\n    'Normalized frequency of repeated punctuation sequences (e.g., \"..\", \"...\", \"!!\", \"??\") per word'\n    import re\n    if not text:\n        return 0.0\n    repeats = re.findall(r'([!?\\.]{2,}|[-]{2,}|[,]{2,})', text)\n    total_words = len(re.findall(r'\\w+', text))\n    if total_words == 0:\n        return float(len(repeats))\n    return float(len(repeats)) / total_words\n\n", "def feature(text: str) -> float:\n    'Estimated proportion of lines that look like dialogue turns (lines starting with quotes/dashes or short quoted lines)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    dialogue_like = 0\n    for l in lines:\n        s = l.lstrip()\n        if s.startswith(('\"', \"'\", '-')):\n            dialogue_like += 1\n            continue\n        # short lines containing quotes are likely parts of dialog\n        if '\"' in l or \"'\" in l:\n            if len(l.strip()) < 80:\n                dialogue_like += 1\n                continue\n        # lines that are short and end with a quote or punctuation typical of dialog\n        if len(l.strip()) <= 40 and (l.strip().endswith('\"') or l.strip().endswith(\"'\")):\n            dialogue_like += 1\n    return float(dialogue_like) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (captures descriptive vs terse styles)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    # approximate sentence count by sentence terminators\n    sentence_count = len(re.findall(r'[.!?]', text))\n    if sentence_count == 0:\n        # if no sentence terminators, treat whole text as one sentence\n        sentence_count = 1\n    return float(commas) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Proportion of capitalized words that are not the first word of a sentence (mid-sentence capitalization ratio)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[A-Za-z][a-z]+\\b', text)\n    total_words = len(re.findall(r'\\w+', text))\n    if total_words == 0:\n        return 0.0\n    # all capitalized words\n    cap_words = re.findall(r'\\b[A-Z][a-z]+\\b', text)\n    cap_total = len(cap_words)\n    # capitalized words that appear at sentence starts\n    sentence_start_caps = re.findall(r'(?:^|[.!?]\\s+)([A-Z][a-z]+)', text)\n    sentence_start_caps_count = len(sentence_start_caps)\n    mid_sentence_caps = max(0, cap_total - sentence_start_caps_count)\n    return float(mid_sentence_caps) / float(total_words)\n", "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that contain asterisks (markdown emphasis/formatting usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    star_tokens = sum(1 for t in tokens if '*' in t)\n    return float(star_tokens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of common filler/interjection tokens (um/uh/hmm/yeah/etc.) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+'\\w+|\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    filler_re = re.compile(r'\\b(?:um+|uh+|hmm+|mmm+|uhm+|yeah|yep|nah|huh)\\b', re.I)\n    filler_count = sum(1 for t in tokens if filler_re.search(t))\n    return float(filler_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that are very short (<= 3 words) - conversational/snappy style'\n    import re\n    if not text:\n        return 0.0\n    # split sentences by punctuation markers; keep non-empty\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 3:\n            short += 1\n    return float(short) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation sequences that are repeated (e.g., \"!!\", \"...\", \"??\") indicating expressive punctuation'\n    import re\n    if not text:\n        return 0.0\n    # total punctuation characters\n    punct_count = sum(1 for c in text if (not c.isalnum() and not c.isspace()))\n    if punct_count == 0:\n        return 0.0\n    # repeated same punctuation char sequences (length>=2)\n    repeated = re.findall(r'([^\\w\\s])\\1+', text)\n    return float(len(repeated)) / float(punct_count)\n\n", "def feature(text: str) -> float:\n    'Density of em-dash / en-dash / double-hyphen occurrences per sentence (stylized punctuation usage)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sentence_count = max(1, len(sentences))\n    dash_matches = re.findall(r'(--|\u2014|\u2013)', text)\n    return float(len(dash_matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Diversity of apostrophe-containing tokens: unique contraction-like forms divided by total such tokens'\n    import re\n    if not text:\n        return 0.0\n    # tokens containing an internal apostrophe (e.g., don't, I'm, it\\'s)\n    contraction_tokens = re.findall(r\"\\b\\w+'\\w+\\b\", text)\n    if not contraction_tokens:\n        return 0.0\n    unique = len(set(t.lower() for t in contraction_tokens))\n    return float(unique) / float(len(contraction_tokens))\n\n", "def feature(text: str) -> float:\n    'Estimated dialogue turns per sentence: paired double-quote occurrences divided by sentence count'\n    import re\n    if not text:\n        return 0.0\n    quote_chars = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    # each pair of quotes roughly marks a turn; use integer pairs\n    turns = quote_chars // 2\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sentence_count = max(1, len(sentences))\n    return float(turns) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause density / syntactic complexity)'\n    import re\n    if not text:\n        return 0.0\n    # approximate sentence boundaries\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sentences = max(1, len(sentences))\n    commas = text.count(',')\n    return float(commas) / float(num_sentences)\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per sentence to capture trailing/interruptive style or narrative cliffing'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    denom = max(1, len(sentences))\n    return float(ellipses) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours','myself','ourselves'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with a lowercase letter (after trimming leading quotes/whitespace) indicating informal/dialogue formatting'\n    import re\n    if not text:\n        return 0.0\n    parts = [p for p in re.split(r'(?<=[.!?])\\s+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    lower_start = 0\n    total = 0\n    for p in parts:\n        s = p.lstrip(' \\t\\n\\r\"\\'')\n        if not s:\n            continue\n        total += 1\n        first = s[0]\n        if first.isalpha() and first.islower():\n            lower_start += 1\n    if total == 0:\n        return 0.0\n    return float(lower_start) / float(total)\n\n", "def feature(text: str) -> float:\n    'Ratio of immediate repeated word bigrams (e.g., \"the the\") to total word count (captures simple editing errors or spoken transcriptions)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    repeats = len(re.findall(r'\\b(\\w+)\\s+\\1\\b', text.lower()))\n    return float(repeats) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average count of parenthetical markers (parentheses and dashes) per sentence as a proxy for asides and interrupted syntax'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = max(1, len(sentences))\n    count = text.count('(') + text.count(')') + text.count('\u2014') + text.count('--')\n    return float(count) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of vocabulary types that occur only once (an indicator of lexical richness vs repetition)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    c = Counter(tokens)\n    vocab = len(c)\n    if vocab == 0:\n        return 0.0\n    hapax = sum(1 for k,v in c.items() if v == 1)\n    return float(hapax) / float(vocab)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a run of the same character of length 3 or more (elongated words like \"noooo\" or \"Darrrrrrling\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    runs = 0\n    for w in tokens:\n        if re.search(r'(.)\\1{2,}', w):\n            runs += 1\n    return float(runs) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Number of distinct punctuation characters used in the text (diverse punctuation usage indicator)'\n    if not text:\n        return 0.0\n    puncts = {c for c in text if not c.isalnum() and not c.isspace()}\n    return float(len(puncts))\n\n", "def feature(text: str) -> float:\n    'Density of the word \"like\" (as a crude proxy for simile/simile-like phrasing) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    likes = len(re.findall(r'\\blike\\b', text.lower()))\n    return float(likes) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of hapax legomena (tokens that occur exactly once) \u2014 a fine-grained lexical novelty measure'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    freqs = Counter(tokens)\n    hapaxes = sum(1 for t, c in freqs.items() if c == 1)\n    return float(hapaxes) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain a dash-style interruption marker (\u2014, --, or spaced hyphen), indicating dialog or interruption'\n    import re\n    if not text:\n        return 0.0\n    # crude sentence split\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    dash_count = 0\n    for s in sentences:\n        if '\u2014' in s or '--' in s or re.search(r'\\s-\\s', s):\n            dash_count += 1\n    return float(dash_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (three dots \"...\" or unicode ellipsis \"\u2026\") per token as an indicator of trailing or dramatic style'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    ell = text.count('...') + text.count('\u2026')\n    return float(ell) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of first-person pronouns (I/me/my/we/us/our) among tokens \u2014 stylistic viewpoint indicator'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    first = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t in first)\n    return float(count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are the second-person pronoun \"you\" (case-insensitive)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    you_count = sum(1 for t in tokens if t == 'you')\n    return you_count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common modal verbs (can/could/may/might/must/shall/should/will/would)'\n    import re\n    if not text:\n        return 0.0\n    modals = {'can','could','may','might','must','shall','should','will','would'}\n    tokens = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in modals)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen (hyphenated words), normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w'-]*\\b\", text)\n    if not tokens:\n        return 0.0\n    hyphenated = sum(1 for t in tokens if '-' in t and any(c.isalpha() for c in t.replace('-', '')))\n    return hyphenated / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of ellipses (\"...\") per sentence (ellipses count divided by sentence count)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    # sentence count: count ., !, ? as rough sentence separators\n    sent_count = text.count('.') + text.count('!') + text.count('?')\n    sent_count = max(1, sent_count)\n    return ellipses / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a run of three or more consonants (indicative of invented names or clusters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'[bcdfghjklmnpqrstvwxyz]{3,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average vowel proportion across words: total vowels divided by total alphabetic characters (0..1)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    vowels = set('aeiouAEIOU')\n    total_v = 0\n    total_chars = 0\n    for w in words:\n        for ch in w:\n            if ch.isalpha():\n                total_chars += 1\n                if ch in vowels:\n                    total_v += 1\n    if total_chars == 0:\n        return 0.0\n    return total_v / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that end with an exclamation or question mark (exclam/question emphasis ratio)'\n    if not text:\n        return 0.0\n    exq = text.count('!') + text.count('?')\n    sent_count = text.count('.') + text.count('!') + text.count('?')\n    if sent_count == 0:\n        return 0.0\n    return exq / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like acronyms/initialisms (all-uppercase tokens of length>=2), e.g., \"UK\", \"UN\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[A-Za-z][A-Za-z0-9'-]*\\b\", text)\n    if not tokens:\n        return 0.0\n    acr = sum(1 for t in tokens if len(t) >= 2 and t.isupper() and any(c.isalpha() for c in t))\n    return acr / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (may, might, could, would, should, must, can)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'may', 'might', 'could', 'would', 'should', 'must', 'can', 'shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are adverbs (heuristic: tokens ending with -ly)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of capitalized tokens that appear inside sentences (not as the first word) \u2014 proxy for named entities or title-case headings'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    total_tokens = 0\n    internal_caps = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if not words:\n            continue\n        for i, w in enumerate(words):\n            total_tokens += 1\n            if i > 0 and w[0].isupper() and w[1:].islower():\n                internal_caps += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(internal_caps) / total_tokens\n\n", "def feature(text: str) -> float:\n    'Hapax legomena fraction: proportion of unique word types that occur exactly once'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    ctr = Counter(tokens)\n    unique = len(ctr)\n    if unique == 0:\n        return 0.0\n    hapax = sum(1 for v in ctr.values() if v == 1)\n    return float(hapax) / unique\n\n", "def feature(text: str) -> float:\n    'Fraction of short lines (<=12 words) that end with a question mark or colon \u2014 catches titles, headings or question intros'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip() != '']\n    if not lines:\n        # fallback to first sentence\n        lines = re.split(r'(?<=[.!?])\\s+', text.strip())\n    if not lines:\n        return 0.0\n    short_marked = 0\n    for ln in lines:\n        words = re.findall(r'\\w+', ln)\n        if len(words) <= 12 and ln.rstrip().endswith((':', '?')):\n            short_marked += 1\n    return float(short_marked) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Enumerator pattern density: frequency of list/item markers like \"1.\", \"2)\", or lines starting with \"-\" normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    token_count = max(1, len(tokens))\n    enumerators = re.findall(r'(?m)^\\s*[-\\u2022]|\\b\\d+\\s*[\\.\\)]', text)\n    return float(len(enumerators)) / token_count\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a discourse/transition word (however, moreover, therefore, furthermore, additionally, consequently)'\n    import re\n    if not text:\n        return 0.0\n    transitions = ('however', 'moreover', 'therefore', 'furthermore', 'additionally', 'consequently', 'nonetheless', 'nevertheless', 'similarly', 'alternatively', 'conversely', 'indeed')\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        first_word_match = re.match(r'^\\W*([A-Za-z]+)', s)\n        if first_word_match:\n            w = first_word_match.group(1).lower()\n            if w in transitions:\n                starts += 1\n    return float(starts) / len(sentences)\n", "def feature(text: str) -> float:\n    'Proxy for passive voice: fraction of \"be\" auxiliaries followed by likely past participles (be + .*ed or *en)'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    # Match common 'be' forms followed by a token that ends with ed or en (approximate past participle)\n    matches = re.findall(r'\\b(?:is|are|was|were|am|be|been|being)\\s+\\w+(?:ed|en)\\b', lowered)\n    # Normalise by token count to avoid bias with very long/short texts\n    tokens = re.findall(r'\\b\\w+\\b', lowered)\n    denom = max(1, len(tokens))\n    return float(len(matches)) / denom\n\n", "def feature(text: str) -> float:\n    'Density of citation-like patterns: year-in-parentheses (e.g., (2019)) or \"et al.\" occurrences per token'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    year_cits = re.findall(r'\\(\\s*(1[89]\\d{2}|20\\d{2}|21\\d{2})\\s*\\)', lowered)  # conservative year range\n    etal = re.findall(r'\\bet al\\.\\b', lowered)\n    total_matches = len(year_cits) + len(etal)\n    tokens = re.findall(r'\\b\\w+\\b', lowered)\n    return float(total_matches) / max(1, len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hedge/qualifying words (e.g., suggests, appears, likely, possibly, indicate)'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'suggest', 'suggests', 'suggests', 'suggested', 'appear', 'appears', 'appeared', 'likely', 'probable', 'probably', 'may', 'might', 'could', 'possibly', 'indicate', 'indicates', 'indicated', 'seem', 'seems', 'seemed', 'tend', 'tends', 'tended', 'potential', 'potentially'}\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique token count divided by total token count (lexical variety)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with first-person indicators (I, we, my, our, me, us) \u2014 signals personal / reflective style'\n    import re\n    if not text:\n        return 0.0\n    parts = [s.strip() for s in re.split(r'[.!?]+[\\s\\n]*', text.strip()) if s.strip()]\n    if not parts:\n        return 0.0\n    first_person = {'i', 'we', 'my', 'our', 'me', 'us'}\n    count = 0\n    for s in parts:\n        m = re.match(r'^\\W*([A-Za-z]+)', s)\n        if m and m.group(1).lower() in first_person:\n            count += 1\n    return float(count) / len(parts)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing numeric characters (digits) \u2014 indicates data/experimental style or list-heavy content'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w\\-/.]+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of colons and semicolons per sentence (proxy for clause-heavy/formal punctuated sentences)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+[\\s\\n]*', text.strip()) if s.strip()]\n    sep_count = text.count(':') + text.count(';')\n    denom = max(1, len(sentences))\n    return float(sep_count) / denom\n", "def feature(text: str) -> float:\n    'Density of discourse markers (however, therefore, moreover, thus, consequently, furthermore) per sentence'\n    import re\n    if not text:\n        return 0.0\n    markers = ['however', 'therefore', 'moreover', 'furthermore', 'thus', 'consequently', 'in addition', 'on the other hand']\n    lower = text.lower()\n    matches = 0\n    for m in markers:\n        # count whole-word occurrences; for multiword markers allow a simple substring search anchored by word boundaries\n        matches += len(re.findall(r'\\b' + re.escape(m) + r'\\b', lower))\n    # sentence count fallback\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(matches) / float(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of non-initial tokens in sentences that are capitalized (proxy for proper nouns and named entities)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    total_tokens = 0\n    cap_tokens = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if not words:\n            continue\n        # ignore first token of sentence\n        for w in words[1:]:\n            total_tokens += 1\n            if w and w[0].isupper():\n                cap_tokens += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(cap_tokens) / float(total_tokens)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with an instructional/imperative verb (e.g., \"Discuss\", \"Describe\", \"Explain\")'\n    import re\n    if not text:\n        return 0.0\n    cues = {'discuss','describe','compare','explain','analyze','analyse','evaluate','outline','summarize','summarise','consider'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    hits = 0\n    for s in sentences:\n        m = re.match(r\"['\\\"\\(\\[]*\\s*([A-Za-z]+)\\b\", s)\n        if m:\n            first = m.group(1).lower()\n            if first in cues:\n                hits += 1\n    return float(hits) / float(len(sentences))\n\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total tokens (lexical variety)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (semicolon usage indicates clause complexity)'\n    import re\n    if not text:\n        return 0.0\n    semis = text.count(';')\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(semis) / float(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain digits (numbers, dates, percentages) indicating quantitative or citation-rich writing'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = 0\n    for t in tokens:\n        if re.search(r'\\d', t):\n            num += 1\n    return float(num) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Average number of words before the first comma in each sentence (measures length of introductory/leading phrases)'\n    import re\n    if not text:\n        return 0.0\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    counts = []\n    for p in parts:\n        idx = p.find(',')\n        if idx >= 0:\n            head = p[:idx]\n        else:\n            head = p\n        words = re.findall(r'\\b\\w+\\b', head)\n        counts.append(len(words))\n    if not counts:\n        return 0.0\n    return float(sum(counts)) / float(len(counts))\n\n\n", "def feature(text: str) -> float:\n    'Diversity of punctuation types used (number of distinct non-alphanumeric punctuation characters per sentence)'\n    import re\n    if not text:\n        return 0.0\n    puncts = set(c for c in text if not c.isalnum() and not c.isspace())\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(len(puncts)) / float(sentence_count)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain quotation marks (dialogue indicator)'\n    try:\n        import re\n        # split into sentence-like segments\n        sentences = re.split(r'(?<=[.!?])\\s+', text)\n        if not sentences or (len(sentences) == 1 and sentences[0].strip() == ''):\n            return 0.0\n        quote_chars = re.compile(r'[\"\\u201c\\u201d]')\n        qcount = sum(1 for s in sentences if quote_chars.search(s))\n        return float(qcount) / float(len(sentences))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that end with \"ly\" (adverb-heavy descriptive style)'\n    try:\n        import re\n        tokens = re.findall(r'\\w+', text)\n        if not tokens:\n            return 0.0\n        count = sum(1 for t in tokens if t.lower().endswith('ly'))\n        return float(count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a pronoun (I/you/he/she/they/we/it), helps identify personal/narrative voice'\n    try:\n        import re\n        pronouns = {'i', 'you', 'he', 'she', 'they', 'we', 'it', 'me', 'him', 'her', 'them', 'us'}\n        sentences = re.split(r'(?<=[.!?])\\s+', text)\n        sentences = [s for s in sentences if s.strip() != '']\n        if not sentences:\n            return 0.0\n        count = 0\n        for s in sentences:\n            m = re.search(r'\\b\\w+\\b', s)\n            if m and m.group(0).lower() in pronouns:\n                count += 1\n        return float(count) / float(len(sentences))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence endings that are ? or ! (expressive sentence endings vs neutral periods)'\n    try:\n        s_count = text.count('.') + text.count('!') + text.count('?')\n        if s_count <= 0:\n            return 0.0\n        excited = text.count('!') + text.count('?')\n        return float(excited) / float(s_count)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain uncommon letters like x or z (useful to spot invented names like \"Zxalthian\")'\n    try:\n        import re\n        tokens = re.findall(r'\\w+', text)\n        if not tokens:\n            return 0.0\n        count = sum(1 for t in tokens if ('x' in t.lower() or 'z' in t.lower()))\n        return float(count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per sentence (creative pauses/omissions indicator)'\n    try:\n        import re\n        ellipses = text.count('...')\n        s_count = text.count('.') + text.count('!') + text.count('?')\n        # consider at least one sentence for normalization\n        denom = float(s_count) if s_count > 0 else 1.0\n        return float(ellipses) / denom\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Average number of words per comma-separated clause (avg clause length between commas)'\n    try:\n        import re\n        clauses = text.split(',')\n        if not clauses:\n            return 0.0\n        clause_word_counts = [len(re.findall(r'\\w+', cl)) for cl in clauses]\n        # avoid division by zero if all clauses empty\n        total_clauses = len(clause_word_counts)\n        if total_clauses == 0:\n            return 0.0\n        return float(sum(clause_word_counts)) / float(total_clauses)\n    except Exception:\n        return 0.0\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ly\" (common adverb suffix) \u2014 may capture stylistic adverb use'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ly'))\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\b\\w+\\b', text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Ratio of capitalized words that are not sentence-initial (approximate proper-noun density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    total_words = len(words)\n    if total_words == 0:\n        return 0.0\n    # find capitalized words with their positions to test preceding punctuation\n    count = 0\n    for m in re.finditer(r'\\b[A-Z][a-zA-Z]+\\b', text):\n        start = m.start()\n        # skip if at very start\n        if start == 0:\n            continue\n        # look back for last non-space character before this word\n        prefix = text[:start].rstrip()\n        if not prefix:\n            continue\n        if prefix[-1] in '.!?':\n            # likely sentence-initial, skip\n            continue\n        count += 1\n    # normalize by total words to get comparable scale\n    return float(count) / total_words\n\n\n", "def feature(text: str) -> float:\n    'Passive-voice proxy: fraction of sentences containing a common auxiliary + past participle (was/were/is + .*ed|.*en)'\n    import re\n    if not text:\n        return 0.0\n    # count matches of auxiliary + verb ending in ed/en\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|had been|has been|have been)\\s+\\w+(?:ed|en)\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(matches)) / sentence_count\n\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation symbols used divided by total punctuation characters'\n    import string\n    if not text:\n        return 0.0\n    punct_chars = [c for c in text if c in string.punctuation]\n    total = len(punct_chars)\n    if total == 0:\n        return 0.0\n    distinct = len(set(punct_chars))\n    return float(distinct) / total\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are ellipses \"...\" (captures trailing/thoughtful pauses or excerpt style)'\n    import re\n    if not text:\n        return 0.0\n    ellipsis_count = text.count('...')\n    word_count = max(1, len(re.findall(r'\\w+', text)))\n    return float(ellipsis_count) / word_count\n\n\n", "def feature(text: str) -> float:\n    'Normalized skew-like measure of word length distribution: (mean - median) / std of word lengths (0 if not defined)'\n    import re, math\n    if not text:\n        return 0.0\n    lengths = [len(w) for w in re.findall(r'\\b\\w+\\b', text)]\n    n = len(lengths)\n    if n < 2:\n        return 0.0\n    mean = sum(lengths) / n\n    sorted_l = sorted(lengths)\n    mid = n // 2\n    if n % 2 == 1:\n        median = float(sorted_l[mid])\n    else:\n        median = (sorted_l[mid - 1] + sorted_l[mid]) / 2.0\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    std = math.sqrt(var)\n    if std == 0:\n        return 0.0\n    return float((mean - median) / std)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a gerund/participle word (first token ends with \"ing\")'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    count = 0\n    for s in sents:\n        parts = s.split()\n        if parts:\n            first = re.sub(r'^[^\\w]+|[^\\w]+$', '', parts[0])  # strip non-word chars around\n            if first.lower().endswith('ing') and len(first) > 3:\n                count += 1\n    return float(count) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain an ellipsis (\"...\") anywhere'\n    import re\n    if not text:\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sents:\n        # still check whole text for ellipsis\n        return 1.0 if '...' in text else 0.0\n    count = sum(1 for s in sents if '...' in s)\n    return float(count) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a digit (helps detect dates, codes like COVID-19 or numeric-heavy text)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+[-\\w]*', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total token count (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\b\\w+\\b', text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proxy for passive voice: fraction of sentences containing an auxiliary + past-participle pattern like \"was Xed\" or \"were Xed\"'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|has been|have been|had been|been|be)\\b\\s+\\w+ed\\b', re.IGNORECASE)\n    count = sum(1 for s in sents if pattern.search(s))\n    return float(count) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Density of colons and semicolons: number of \":\" or \";\" characters divided by total characters (0 if empty)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    count = text.count(':') + text.count(';')\n    return float(count) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Number of distinct punctuation characters present in the text (counts different punctuation marks, returned as float)'\n    import string, re\n    if not text:\n        return 0.0\n    punct_chars = set(ch for ch in text if ch in string.punctuation)\n    return float(len(punct_chars))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are very long words (length >= 10), indicating higher lexical sophistication'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) >= 10)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Density of mid-sentence capitalized words (heuristic proper-name density) = caps not immediately after .!?'\n    import re\n    if not text:\n        return 0.0\n    # find capitalized words like \"John\" or \"Mars\"\n    caps = list(re.finditer(r'\\b[A-Z][a-z]+\\b', text))\n    if not caps:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_tokens = len(words) if words else 1\n    count = 0\n    for m in caps:\n        start = m.start()\n        # look back to find last non-space character before this word\n        i = start - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        if i < 0:\n            # likely start of text -> treat as sentence start -> skip\n            continue\n        if text[i] in '.!?':\n            # likely sentence start -> skip\n            continue\n        count += 1\n    return float(count) / total_tokens\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain double-quote characters (proxy for dialogue presence)'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence-ending punctuation\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip() != '']\n    if not sentences:\n        return 0.0\n    dialogue_sentences = sum(1 for s in sentences if '\"' in s or '\u201c' in s or '\u201d' in s)\n    return float(dialogue_sentences) / len(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with -ly (adverbial density), returns 0 if no tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.endswith('ly'))\n    return float(count) / len(words)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with -ing (present-participle/gerund density), simple proxy for progressive or descriptive style'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.endswith('ing'))\n    return float(count) / len(words)\n\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique token count divided by total token count (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Density of sensory-verb and sensory-root words among tokens (see, hear, smell, taste, feel, listen, look, gaze, glance, etc.)'\n    import re\n    if not text:\n        return 0.0\n    sensory = {'see', 'saw', 'seen', 'look', 'looked', 'looked', 'gaze', 'gazed', 'glance', 'glanced',\n               'hear', 'heard', 'listen', 'listened', 'smell', 'smelled', 'taste', 'tasted', 'feel', 'felt',\n               'touch', 'touched', 'sound', 'sounded', 'view', 'viewed', 'watch', 'watched'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<= 8 words), giving a measure of choppiness/brief-sentence usage'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip() != '']\n    if not sentences:\n        return 0.0\n    short_count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 8:\n            short_count += 1\n    return float(short_count) / len(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Punctuation variability: number of distinct punctuation characters divided by total punctuation characters (0 if none)'\n    import re\n    if not text:\n        return 0.0\n    puncs = re.findall(r'[^\\w\\s]', text)\n    if not puncs:\n        return 0.0\n    distinct = len(set(puncs))\n    total = len(puncs)\n    return float(distinct) / total\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per sentence (sentences counted by .!?), 0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences == 0:\n        return 0.0\n    return ellipses / sentences\n\n", "def feature(text: str) -> float:\n    'Progressive -ing token fraction: fraction of tokens ending with \"ing\" (approx. continuous actions or gerunds)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing'))\n    return ing_count / n\n\n", "def feature(text: str) -> float:\n    'Simile marker fraction: normalized count of \"like\" tokens plus occurrences of \"as ... as\" per token count'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    tokens = re.findall(r'\\b\\w+\\b', lower)\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    like_count = len(re.findall(r'\\blike\\b', lower))\n    # crude count of \"as ... as\" patterns allowing up to 4 words between\n    asas_count = len(re.findall(r'\\bas\\b(?:\\s+\\w+){0,4}\\s+\\bas\\b', lower))\n    return (like_count + asas_count) / n\n\n", "def feature(text: str) -> float:\n    'Sentence-start first-person ratio: fraction of sentences that begin with \"I\" (or contractions like \"I\\'m\")'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence terminators (keep simple)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    total = len(sentences)\n    if total == 0:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        # strip leading quotes/parentheses/whitespace\n        m = re.match(r'^\\s*[\"\\'(\\[\\{]*\\s*(\\S+)', s)\n        if not m:\n            continue\n        first = m.group(1)\n        if re.match(r'^(I|I\\'m|I\u2019m|I\\')\\b', first, re.I):\n            starts += 1\n    return starts / total\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word types that occur exactly once (unique singletons / total tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    freq = {}\n    for w in tokens:\n        freq[w] = freq.get(w, 0) + 1\n    hapax = sum(1 for c in freq.values() if c == 1)\n    return hapax / n\n\n", "def feature(text: str) -> float:\n    'Conjunction-start ratio: fraction of sentences that begin with coordinating conjunctions (and, but, or, so, yet, for, nor)'\n    import re\n    if not text:\n        return 0.0\n    conj = {'and','but','or','so','yet','for','nor'}\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    total = len(sentences)\n    if total == 0:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        m = re.match(r'^\\s*[\"\\'(\\[\\{]*\\s*([A-Za-z]+)', s)\n        if not m:\n            continue\n        first = m.group(1).lower()\n        if first in conj:\n            starts += 1\n    return starts / total\n\n", "def feature(text: str) -> float:\n    'Average clause length by commas: average number of words in comma-separated segments (0 if no words)'\n    import re\n    if not text:\n        return 0.0\n    segments = [seg.strip() for seg in text.split(',')]\n    # filter out empty segments\n    segments = [s for s in segments if s]\n    if not segments:\n        return 0.0\n    lens = []\n    for s in segments:\n        words = re.findall(r'\\w+', s)\n        lens.append(len(words))\n    if not lens:\n        return 0.0\n    return sum(lens) / len(lens)\n", "def feature(text: str) -> float:\n    'Adverb density: fraction of tokens that end with \"ly\" (heuristic for adverb use)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of ellipses (\"...\" or unicode \u2026) per sentence (0 if no sentences)'\n    import re\n    ell_count = text.count('...') + text.count('\u2026')\n    # sentence count using punctuation .!? sequences\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    return float(ell_count) / sent_count\n\n", "def feature(text: str) -> float:\n    'Gerund/participle density: fraction of word tokens ending with \"ing\" (len>4 to reduce short-word noise)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.endswith('ing') and len(t) > 4)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Numeric token fraction: fraction of tokens that contain at least one digit'\n    import re\n    tokens = re.findall(r'\\b[\\w,.-]+\\b', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Dialogue quote fraction: fraction of sentences that contain double-quote characters (\", \u201c, \u201d)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d')\n    count = sum(1 for s in sentences if any(q in s for q in quote_chars))\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Sensory-verb density: fraction of tokens that are common sensory or emotive verbs (see/hear/smell/feel/etc.)'\n    import re\n    sensory = {'see','saw','seen','look','looked','look','hear','heard','listen','listened','smell','smelled',\n               'taste','tasted','feel','felt','gaze','gazed','stare','stared','glance','glanced','touch','touched',\n               'sigh','sighed','gasp','gasped','whisper','whispered','murmur','murmured','shiver','shivered','shudder','shuddered'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Short-sentence fraction: fraction of sentences that are very short (<= 3 words)'\n    import re\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    short = 0\n    for p in parts:\n        words = [w for w in re.findall(r'\\b\\w+\\b', p)]\n        if len(words) <= 3:\n            short += 1\n    return float(short) / len(parts)\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation characters (0 if none)'\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    unique = len(set(puncts))\n    total = len(puncts)\n    return float(unique) / total\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain any numeric digit (numbers, years, measurements)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters used divided by total punctuation occurrences (0 if none)'\n    import string\n    if not text:\n        return 0.0\n    puncts = [c for c in text if c in string.punctuation]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that contain a hyphen (heuristic for compound/adjective use)'\n    import re\n    if not text:\n        return 0.0\n    raw_tokens = text.split()\n    if not raw_tokens:\n        return 0.0\n    count = 0\n    for t in raw_tokens:\n        tn = re.sub(r\"^[^\\w-]+|[^\\w-]+$\", \"\", t)\n        # count tokens with internal hyphen(s)\n        if '-' in tn and any(ch.isalpha() for ch in tn.replace('-', '')):\n            count += 1\n    return float(count) / float(len(raw_tokens))\n", "def feature(text: str) -> float:\n    'Em-dash or double-dash frequency normalized by word count (heuristic for stylistic punctuation like \"\u2014\" or \"--\")'\n    import re\n    if not text:\n        return 0.0\n    dash_count = text.count('\u2014') + text.count('--')\n    words = re.findall(r'\\w+', text)\n    return float(dash_count) / max(1.0, len(words))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word types / total tokens), lowercased, as a measure of lexical variety'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word types that occur exactly once (unique singletons / total tokens)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    cnt = Counter(tokens)\n    hapax = sum(1 for w,c in cnt.items() if c == 1)\n    return float(hapax) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of short sentences (<=5 words) \u2014 captures fragmentary, staccato narrative style'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like spans\n    sentences = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text, flags=re.MULTILINE)]\n    sentences = [s for s in sentences if s]\n    if not sentences:\n        return 0.0\n    short_count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 5:\n            short_count += 1\n    return float(short_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Ellipsis frequency (instances of \"...\" or \"\u2026\") normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    ell = text.count('...') + text.count('\u2026')\n    sentence_markers = max(1.0, len(re.findall(r'[.!?]', text)))\n    return float(ell) / sentence_markers\n\n", "def feature(text: str) -> float:\n    'First-person pronoun fraction (i, me, my, we, us, our variants) as a proxy for narrative perspective'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    prs = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t in prs)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in \"ing\" (lowercased) \u2014 proxy for progressive/generative verbal style'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing'))\n    return float(ing_count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences normalized by token count'\n    import re\n    ellipses = text.count('...')\n    tokens = re.findall(r'\\w+', text)\n    return ellipses / max(1, len(tokens))\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio (I, me, my, mine, we, us, our) as fraction of tokens'\n    import re\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    fp = sum(1 for t in tokens if t in {'i','me','my','mine','we','us','our','ours'})\n    return fp / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Gerund/continuous form ratio: fraction of tokens ending with \"ing\" (heuristic for progressive/continuous phrasing)'\n    import re\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    ing = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return ing / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Definite-article frequency: fraction of tokens equal to \"the\"'\n    import re\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    the_count = sum(1 for t in tokens if t == 'the')\n    return the_count / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Unique-token ratio: number of distinct word tokens divided by total tokens (lexical diversity)'\n    import re\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    return len(set(tokens)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Semicolon density: count of \";\" divided by number of sentences (sentences counted by .!?), 0 if none'\n    import re\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    semis = text.count(';')\n    return semis / sentence_count\n\n", "def feature(text: str) -> float:\n    'Sensory-word density: fraction of tokens that are sensory verbs/nouns/adjectives (hear, see, smell, taste, touch, sound, sight, feel, listen, visual, auditory, olfactory, tactile)'\n    import re\n    sensory = {'hear','heard','hearing','listen','listened','listening','sound','sounds','auditory',\n               'see','saw','seen','sight','visual','watch','watched','smell','smelled','olfactory',\n               'taste','tasted','tasting','touch','touched','touching','feel','felt','feeling','tactile'}\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    sens = sum(1 for t in tokens if t in sensory)\n    return sens / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Sentence-start diversity: number of distinct sentence-initial words divided by sentence count (1.0 = every sentence starts differently)'\n    import re\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = []\n    for s in sentences:\n        m = re.findall(r'\\w+', s)\n        if m:\n            starts.append(m[0].lower())\n    if not starts:\n        return 0.0\n    return len(set(starts)) / len(starts)\n", "def feature(text: str) -> float:\n    'Citation year density: fraction of tokens that are 4-digit years in the 1900-2099 range'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(?:19|20)\\d{2}\\b', text)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(len(years)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of ellipsis-like sequences (three or more dots or single ellipsis char) per character'\n    import re\n    if not text:\n        return 0.0\n    ellipses = re.findall(r'\\.{3,}|\u2026', text)\n    if not text:\n        return 0.0\n    return float(len(ellipses)) / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Dialogue/quoted-sentence ratio: fraction of sentence-like spans that contain quotation marks (double quotes or curly quotes)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks (keeps trailing punctuation optional)\n    sentences = re.findall(r'[^.!?]+[.!?]?', text)\n    if not sentences:\n        return 0.0\n    quote_chars = set(['\"', '\u201c', '\u201d'])\n    quoted = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            quoted += 1\n    return float(quoted) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Hyphenated word ratio: fraction of word tokens that contain internal hyphens (e.g., man-woman, long-term)'\n    import re\n    if not text:\n        return 0.0\n    hyphenated = re.findall(r'\\b\\w+(?:-\\w+)+\\b', text)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(len(hyphenated)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Semicolon+colon density among punctuation: fraction of punctuation characters that are ; or : (academic/structured prose indicator)'\n    import re\n    if not text:\n        return 0.0\n    sc_count = text.count(';') + text.count(':')\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if punct_count == 0:\n        return 0.0\n    return float(sc_count) / float(punct_count)\n\n", "def feature(text: str) -> float:\n    'Titlecase word ratio: fraction of multi-character words that start with uppercase followed by all-lowercase (proper nouns, headings)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if len(w) > 1 and w[0].isupper() and w[1:].islower():\n            count += 1\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Passive-voice proxy: fraction of token positions where a form of \"be\" is immediately followed by a word ending in \"ed\"'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    be_forms = {'is', 'are', 'was', 'were', 'be', 'been', 'being', 'am'}\n    count = 0\n    for i in range(len(words) - 1):\n        if words[i] in be_forms and words[i + 1].endswith('ed'):\n            count += 1\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Numeric token ratio: fraction of tokens that contain at least one digit (tables, citations, measurements)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated and contain at least one capitalized component (e.g., Shapiro-Stiglitz)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[\\w-]+\\b', text)\n    if not tokens:\n        return 0.0\n    hyphen_caps = 0\n    for t in tokens:\n        if '-' in t:\n            parts = [p for p in t.split('-') if p]\n            if any(p[0].isupper() for p in parts if p):\n                hyphen_caps += 1\n    return float(hyphen_caps) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Normalized difference between semicolon and colon counts per sentence (semicolon minus colon) / sentences'\n    import re\n    if not text:\n        return 0.0\n    semicolons = text.count(';')\n    colons = text.count(':')\n    sentences = max(1.0, float(len(re.findall(r'[.!?]', text))))\n    return float(semicolons - colons) / sentences\n\n\n", "def feature(text: str) -> float:\n    'Density of all-cap acronyms (tokens of 2+ uppercase letters) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    acronyms = re.findall(r'\\b[A-Z]{2,}\\b', text)\n    return float(len(acronyms)) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = max(1.0, float(len(re.findall(r'[.!?]', text))))\n    return float(commas) / sentences\n\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that start with an enumerated marker like \"1.\" or \"2)\" (numbered lists)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    markers = re.findall(r'(?m)^\\s*\\d+[\\.\\)]', text)\n    return float(len(markers)) / float(len(lines))\n\n\n", "def feature(text: str) -> float:\n    'Occurrences of definitional patterns per sentence (phrases like \"is a\", \"is an\", \"is defined as\", \"refers to\")'\n    import re\n    if not text:\n        return 0.0\n    patterns = re.findall(r'\\b(is a|is an|is defined as|refers to|can be described as|is referred to as)\\b', text.lower())\n    sentences = max(1.0, float(len(re.findall(r'[.!?]', text))))\n    return float(len(patterns)) / sentences\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one hyphen (detects age ranges, compound adjectives, e.g., \"73-year-old\")'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Binary-ish score (0.0 or 1.0) indicating presence of short heading-like lines (e.g., \"Introduction\")'\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    for ln in lines:\n        # heading heuristic: short line, few words, no terminal punctuation, first word capitalized\n        words = ln.split()\n        if 1 <= len(words) <= 6 and len(ln) <= 60 and ln[-1] not in '.!?;:' and words[0][0].isupper():\n            # avoid counting normal sentence fragments (require no lowercase-only at start)\n            return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Density of numeric tokens: fraction of tokens that contain at least one digit'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Normalized frequency of em-dash or dash runs and ellipses (occurrences per character), capturing informal truncation or emphatic punctuation'\n    if not text:\n        return 0.0\n    count = 0\n    # count common dash/ellipsis markers\n    count += text.count('...')      # ellipses\n    count += text.count('\u2014')        # em dash (unicode)\n    count += text.count('\u2013')        # en dash\n    count += text.count('--')       # ASCII dash runs\n    # normalize by number of characters to keep scale small and robust\n    denom = max(1, len(text))\n    return float(count) / denom\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (can, could, may, might, must, shall, should, will, would) indicating speculative/permission language'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    modals = {'can','could','may','might','must','shall','should','will','would'}\n    count = 0\n    for t in tokens:\n        w = t.strip('.,:;!?()[]\"\\'' ).lower()\n        if w in modals:\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Punctuation variability: number of distinct punctuation characters used divided by total punctuation count (0 if no punctuation)'\n    if not text:\n        return 0.0\n    punct_chars = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(punct_chars)\n    if total == 0:\n        return 0.0\n    distinct = len(set(punct_chars))\n    return float(distinct) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in \"ing\" (present participles/gerunds), a marker of descriptive/continuous aspect usage'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    ing_count = 0\n    for t in tokens:\n        w = t.strip('.,:;!?()[]\"\\'' ).lower()\n        if len(w) > 3 and w.endswith('ing'):\n            ing_count += 1\n    return float(ing_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Normalized Shannon entropy of common function words distribution (higher = more varied use of stopwords), normalized to [0,1]'\n    if not text:\n        return 0.0\n    import math, re\n    # small stopword set for robustness\n    stopwords = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from','they','we','my','our','is','are'}\n    tokens = [t.strip('.,:;!?()[]\"\\'' ).lower() for t in text.split() if t.strip('.,:;!?()[]\"\\'' )]\n    if not tokens:\n        return 0.0\n    freqs = {}\n    total_sw = 0\n    for w in tokens:\n        if w in stopwords:\n            freqs[w] = freqs.get(w, 0) + 1\n            total_sw += 1\n    if total_sw == 0:\n        return 0.0\n    # compute entropy\n    ent = 0.0\n    for v in freqs.values():\n        p = v / total_sw\n        ent -= p * math.log2(p) if p > 0 else 0.0\n    # normalize by maximum possible entropy given the full stopword set\n    max_ent = math.log2(len(stopwords)) if len(stopwords) > 1 else 1.0\n    return float(ent) / max_ent if max_ent > 0 else 0.0\n", "def feature(text: str) -> float:\n    'Ratio of tokens containing digits (numeric token ratio), catches years and enumerations'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    numeric = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(numeric) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of interior capitalized words (proper-noun-like tokens not at sentence starts)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    tokens_all = re.findall(r'\\b\\w+\\b', text)\n    if not tokens_all:\n        return 0.0\n    count = 0\n    total = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        for i, w in enumerate(words):\n            total += 1\n            # consider interior capitalized words: start with uppercase and followed by lowercase letters\n            if i > 0 and len(w) > 1 and w[0].isupper() and w[1:].islower():\n                count += 1\n    return float(count) / float(total) if total > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (comma density), proxy for clause complexity'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    sent_count = max(1, len(sentences))\n    commas = text.count(',')\n    return float(commas) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Function-word pronoun density: fraction of tokens that are common pronouns (I, you, he, she, we, they, etc.)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','you','your','yours',\n                'he','him','his','she','her','hers','they','them','their','theirs','it','its'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of adjacent capitalized word pairs (e.g., \"Girl, Interrupted\" pattern) per sentence'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    sent_count = max(1, len(sentences))\n    bigram_count = 0\n    for s in sentences:\n        words = re.findall(r'\\b[A-Za-z][a-z]+\\b', s)\n        for i in range(len(words) - 1):\n            if words[i][0].isupper() and words[i+1][0].isupper():\n                bigram_count += 1\n    return float(bigram_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with common discourse-starter words (however, moreover, therefore, also, but, etc.)'\n    import re\n    if not text:\n        return 0.0\n    starters = {'however','moreover','therefore','furthermore','additionally','consequently',\n                'thus','meanwhile','also','but','and','finally','first','second','in addition'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        s0 = s.strip().lower()\n        # take first token (word) of the sentence\n        first = re.findall(r'^\\W*([a-z]+)', s0)\n        if first and first[0] in starters:\n            count += 1\n    return float(count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Normalized sentence-length variability: coefficient of variation (std/mean) of sentence word counts'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences if s.strip()]\n    if not lengths or len(lengths) == 1:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Fraction of short tokens (<= 3 characters) as a proxy for simple/common-word usage'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    short = sum(1 for t in tokens if len(t) <= 3)\n    return float(short) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text)\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t.lower() in first_person)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of em-dash characters (\u2014) in the text (count per character)'\n    if not text:\n        return 0.0\n    count = text.count('\u2014')\n    return count / float(max(1, len(text)))\n\n", "def feature(text: str) -> float:\n    'Average number of ellipses (\"...\") per sentence (ellipses indicate informal or rhetorical pauses)'\n    import re\n    if not text:\n        return 0.0\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    ellipses = text.count('...')\n    return ellipses / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is not uppercase (inconsistency in sentence-initial capitalization)'\n    import re\n    if not text:\n        return 0.0\n    parts = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not parts:\n        return 0.0\n    bad = 0\n    alpha_sentences = 0\n    for s in parts:\n        s_strip = s.lstrip()\n        # find first alphabetic char\n        m = re.search(r'[A-Za-z]', s_strip)\n        if not m:\n            continue\n        alpha_sentences += 1\n        first_char = s_strip[m.start()]\n        if not first_char.isupper():\n            bad += 1\n    return bad / float(max(1, alpha_sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens matching common citation patterns (e.g., [1], (Smith, 2020)) indicating academic sourcing'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    # patterns: numeric bracket citations and author-year parenthetical citations\n    bracket_cites = re.findall(r'\\[\\s*\\d+\\s*\\]', text)\n    author_year = re.findall(r'\\([A-Z][a-zA-Z\\-]+,\\s*\\d{4}\\)', text)\n    matches = len(bracket_cites) + len(author_year)\n    return matches / float(max(1, len(tokens)))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas per sentence), approximating clause density and syntactic complexity'\n    import re\n    if not text:\n        return 0.0\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    comma_count = text.count(',')\n    return comma_count / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are nominalizations (common noun-forming suffixes like -ion, -ment, -ness, -ity, -ization)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    pattern = re.compile(r'\\b\\w+(ion|ment|ness|ity|ization|isation)\\b')\n    count = sum(1 for w in words if pattern.match(w))\n    return count / float(len(words))\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause density and complex sentence structure)'\n    import re\n    if not text:\n        return 0.0\n    # Rough sentence split on .!? and newlines\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback: treat whole text as one sentence\n        sentences = [text.strip()]\n    comma_count = text.count(',')\n    return float(comma_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Nominalization ratio: fraction of tokens ending with common nominalizing suffixes (tion, sion, ment, ness, ity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'\u2019-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('tion', 'sion', 'ment', 'ness', 'ity', 'ence', 'ance')\n    count = 0\n    for t in tokens:\n        for s in suffixes:\n            if t.endswith(s):\n                count += 1\n                break\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Passive-voice indicator: fraction of sentences matching simple passive patterns (was/were/is/are + past participle)'\n    import re\n    if not text:\n        return 0.0\n    # coarse sentence split\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|has been|have been|had been|been|being)\\b(?:\\s+\\w+){0,2}\\s+\\w+(?:ed|en)\\b', re.I)\n    matches = 0\n    for s in sentences:\n        if pattern.search(s):\n            matches += 1\n    return float(matches) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Hedging/modality ratio: fraction of tokens that are hedging or modal words (may, might, could, seems, suggests, possibly, etc.)'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','possibly','perhaps','seem','seems','appear','appears','suggest','suggests','indicate','indicates','likely','probable','arguably','tend','tends'}\n    tokens = re.findall(r\"\\b[\\w'\u2019-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Heading/section line fraction: fraction of lines that look like section headings (short, title-cased, no terminal punctuation)'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    heading_like = 0\n    for ln in lines:\n        s = ln.strip()\n        # short line, no terminal punctuation, and starts with uppercase letter\n        words = s.split()\n        if 1 <= len(words) <= 12 and s[0].isupper() and not s.endswith(('.', '!', '?', ':', ';')) and any(ch.isalpha() for ch in s):\n            # penalize lines that are long or end with a colon (often lists) by not counting as headings\n            heading_like += 1\n    return float(heading_like) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Unicode apostrophe usage: fraction of apostrophes that are typographic (\u2019) vs total apostrophes (\u2019 + \\')'\n    if not text:\n        return 0.0\n    uni = text.count('\u2019')\n    ascii_ap = text.count(\"'\")\n    total = uni + ascii_ap\n    if total == 0:\n        return 0.0\n    return float(uni) / float(total)\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct non-alphanumeric punctuation characters divided by total punctuation count (captures typographic richness)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(len(puncts))\n\n", "def feature(text: str) -> float:\n    'Proper-noun-like ratio: fraction of tokens that are capitalized inside sentences (not sentence-initial), a proxy for named entities and academic references'\n    import re\n    if not text:\n        return 0.0\n    # coarse sentence split preserving words\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    cap_inside = 0\n    total_inside = 0\n    for s in sentences:\n        words = re.findall(r\"\\b[^\\W\\d_][\\w'-]*\\b\", s)  # words starting with a letter\n        if not words:\n            continue\n        for i, w in enumerate(words):\n            if i == 0:\n                continue  # skip sentence-initial capitalization\n            total_inside += 1\n            # count tokens that start with uppercase and are not all uppercase (to exclude acronyms)\n            if w[0].isupper() and not w.isupper():\n                cap_inside += 1\n    if total_inside == 0:\n        return 0.0\n    return float(cap_inside) / float(total_inside)\n", "def feature(text: str) -> float:\n    'Comma density per sentence: average number of commas per sentence (proxy for clause-complexity)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # count non-empty sentence fragments\n    sents = [s for s in re.split(r'[.!?]', text) if s.strip()]\n    num_sents = len(sents) if len(sents) > 0 else 1\n    return float(comma_count) / float(num_sents)\n\n", "def feature(text: str) -> float:\n    'Semicolon density per sentence: semicolons often indicate formal/complex sentence structure'\n    import re\n    if not text:\n        return 0.0\n    sc_count = text.count(';')\n    sents = [s for s in re.split(r'[.!?]', text) if s.strip()]\n    num_sents = len(sents) if len(sents) > 0 else 1\n    return float(sc_count) / float(num_sents)\n\n", "def feature(text: str) -> float:\n    'Ellipsis frequency: count of \"...\" sequences per sentence (indicative of truncation or informal trailing thoughts)'\n    import re\n    if not text:\n        return 0.0\n    ell = text.count('...')\n    sents = [s for s in re.split(r'[.!?]', text) if s.strip()]\n    num_sents = max(1, len(sents))\n    return float(ell) / float(num_sents)\n\n", "def feature(text: str) -> float:\n    'Gerund/continuous verb usage: fraction of tokens ending with \"ing\" (proxy for progressive aspect or nominalized verbs)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Mid-sentence Titlecase ratio: fraction of words that start with uppercase but are not the first word of a sentence (indicates proper nouns, section names)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    mid_titlecase = 0\n    total_tokens = 0\n    for sent in sentences:\n        words = re.findall(r'\\b\\w+\\b', sent)\n        if not words:\n            continue\n        for w in words[1:]:\n            total_tokens += 1\n            # exclude all-caps acronyms\n            if w[0].isupper() and not w.isupper():\n                mid_titlecase += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(mid_titlecase) / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Average vowel fraction per word: average proportion of vowels (aeiou) in each token (proxy for Latinate/technical vocabulary)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    vowels = set('aeiou')\n    fractions = []\n    for t in tokens:\n        vcount = sum(1 for ch in t if ch in vowels)\n        if len(t) == 0:\n            continue\n        fractions.append(float(vcount) / float(len(t)))\n    if not fractions:\n        return 0.0\n    return sum(fractions) / float(len(fractions))\n\n", "def feature(text: str) -> float:\n    'Presence of structural/academic markers: returns 1.0 if words like \"chapter\", \"section\", \"figure\", \"table\", \"appendix\", \"et al.\" are present, else 0.0'\n    if not text:\n        return 0.0\n    lower = text.lower()\n    keywords = ('chapter', 'section', 'figure', 'fig.', 'table', 'appendix', 'et al', 'et al.', 'references')\n    for k in keywords:\n        if k in lower:\n            return 1.0\n    return 0.0\n", "def feature(text: str) -> float:\n    'Heading density: fraction of lines that look like short title/headings (one-4 words, title-case)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    heading_count = 0\n    for ln in lines:\n        words = ln.split()\n        if 1 <= len(words) <= 4:\n            # consider title-case or a single word starting with uppercase\n            if all(w[0].isupper() for w in words if w):\n                heading_count += 1\n    return heading_count / len(lines)\n\n", "def feature(text: str) -> float:\n    'Semicolon density per sentence: average number of semicolons per detected sentence'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences by punctuation .!? (keep robustness)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # if no sentences found, use whole text as single sentence\n        sentences = [text]\n    semicolons = text.count(';')\n    return semicolons / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Average words per clause: split on commas, semicolons and em/en-dashes to estimate clause length'\n    import re\n    if not text:\n        return 0.0\n    # Normalize common dash characters\n    normalized = text.replace('\u2014', '-').replace('\u2013', '-')\n    clauses = [c.strip() for c in re.split(r'[,-;:()]+', normalized) if c.strip()]\n    if not clauses:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', c)) for c in clauses]\n    if not word_counts:\n        return 0.0\n    return sum(word_counts) / len(word_counts)\n\n", "def feature(text: str) -> float:\n    'Passive-voice heuristic: fraction of sentences that contain a simple passive pattern (was/were/is/are + past participle)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    passive_pattern = re.compile(r'\\b(?:was|were|is|are|been|being|be|had been|has been|have been)\\b\\s+\\w+(?:ed|en|n)\\b', re.I)\n    passive_count = 0\n    for s in sentences:\n        if passive_pattern.search(s):\n            passive_count += 1\n    return passive_count / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Named-entity proxy: fraction of tokens that are capitalized (Titlecase) but not at sentence starts'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences, then examine tokens not at start of sentence\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    total_tokens = 0\n    cap_tokens = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        for i, w in enumerate(words):\n            total_tokens += 1\n            if i != 0 and w[0].isupper() and w[1:].islower():\n                cap_tokens += 1\n    if total_tokens == 0:\n        return 0.0\n    return cap_tokens / total_tokens\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters used divided by total punctuation count'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if (not c.isalnum() and not c.isspace())]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return distinct / len(puncts)\n\n", "def feature(text: str) -> float:\n    'Enumeration density: fraction of lines that start with numbered or lettered list markers (e.g., \"1.\", \"a)\", \"-\")'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        # also check inline enumerations like \"1. \" anywhere\n        matches = re.findall(r'(?:^|\\s)(?:\\d+\\.|\\d+\\)|[a-zA-Z]\\))\\s+', text)\n        return len(matches) / max(1.0, len(text.splitlines()))\n    marker_count = 0\n    for ln in lines:\n        if re.match(r'^\\s*(?:\\d+\\.|\\d+\\)|[a-zA-Z]\\)|[-*\u2022])\\s+', ln):\n            marker_count += 1\n    return marker_count / len(lines)\n\n", "def feature(text: str) -> float:\n    'Acronym ratio: fraction of tokens that are all-caps acronyms (length >=2), e.g., \"NASA\", \"AI\", \"USA\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[A-Z]{2,}\\b', text)\n    all_tokens = re.findall(r'\\b\\w+\\b', text)\n    if not all_tokens:\n        return 0.0\n    return len(tokens) / len(all_tokens)\n", "def feature(text: str) -> float:\n    'Density of common academic/Latin abbreviations (e.g., \"e.g.\", \"i.e.\", \"et al.\") per token'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    patterns = ['e.g.', 'eg,', 'i.e.', 'ie,', 'et al', 'cf.', 'viz.', 'ibid', 'op. cit.']\n    count = 0\n    for p in patterns:\n        count += lower.count(p)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proxy for passive constructions: auxiliary (is/was/has ... been) + past-participle (-ed) occurrences per sentence'\n    import re\n    if not text:\n        return 0.0\n    # match auxiliaries optionally + \"been\" then a word ending with ed\n    matches = re.findall(r'\\b(?:is|are|was|were|am|be|been|being|has|have|had)(?:\\s+been)?\\s+\\w+ed\\b', text, flags=re.I)\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(matches)) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (clause-complexity indicator)'\n    import re\n    if not text:\n        return 0.0\n    semis = text.count(';')\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(semis) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of parenthetical groups that look like citations (contain years, \"et al\", \"pp.\", \"vol.\", \"doi\")'\n    import re\n    if not text:\n        return 0.0\n    groups = re.findall(r'\\(([^)]{0,300})\\)', text)\n    if not groups:\n        return 0.0\n    hits = 0\n    for g in groups:\n        if re.search(r'\\b\\d{3,4}\\b|\\bet al\\b|\\bpp\\.|\\bvol\\.|\\bdoi\\b', g, flags=re.I):\n            hits += 1\n    return float(hits) / float(len(groups))\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, we, me, us, my, our, mine, ours) to total tokens (narrativity / personal stance)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i','we','me','us','my','our','mine','ours','myself','ourselves'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Type-Token Ratio: number of distinct lowercased word types divided by total token count (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    types = len(set(words))\n    return float(types) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Colon density per sentence (use of definitions, lists, or formal clauses) measured as \":\" occurrences per sentence'\n    import re\n    if not text:\n        return 0.0\n    colons = text.count(':')\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(colons) / float(sentences)\n", "def feature(text: str) -> float:\n    'Median sentence length in words (robust to outliers)'\n    import re, math\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+|\\n+', s)\n    lens = []\n    for sent in sentences:\n        sent = sent.strip()\n        if not sent:\n            continue\n        words = re.findall(r'\\w+', sent)\n        lens.append(len(words))\n    if not lens:\n        return 0.0\n    lens.sort()\n    n = len(lens)\n    if n % 2 == 1:\n        return float(lens[n//2])\n    else:\n        return (lens[n//2 - 1] + lens[n//2]) / 2.0\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas indicate clause complexity)'\n    import re\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+|\\n+', s)\n    sentences = [sent for sent in sentences if sent.strip()]\n    if not sentences:\n        # fallback: treat whole text as one sentence\n        sentences = [s]\n    comma_count = text.count(',')\n    return comma_count / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique lowercased word types divided by total word tokens'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    types = set(tokens)\n    return len(types) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Semicolon frequency per sentence (proxy for complex syntactic embedding)'\n    import re\n    s = text.strip()\n    if not s:\n        return 0.0\n    # count sentences roughly\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+|\\n+', s)\n    sentences = [sent for sent in sentences if sent.strip()]\n    if not sentences:\n        sentences = [s]\n    semi_count = text.count(';')\n    return semi_count / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Maximum bigram repetition ratio: frequency of the most common consecutive word pair divided by total bigrams (captures phrase reuse)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = []\n    for i in range(len(tokens)-1):\n        bigrams.append(tokens[i] + ' ' + tokens[i+1])\n    from collections import Counter\n    c = Counter(bigrams)\n    most = c.most_common(1)[0][1]\n    total = len(bigrams)\n    return most / float(total)\n\n", "def feature(text: str) -> float:\n    'Personal pronoun density: fraction of tokens that are personal pronouns (I, we, you, he, she, they) \u2014 low in impersonal/academic tone'\n    import re\n    pronouns = {'i','we','you','he','she','they','me','us','him','her','them','my','our','your','their','ours','yours','theirs'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in pronouns)\n    return count / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentence_count), sentences min 1'\n    if not text:\n        return 0.0\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    comma_count = text.count(',')\n    return float(comma_count) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of hedging/abstract phrase occurrences normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    phrases = ['significant', 'cannot be overstated', 'seeks to', 'aims to', 'is rooted in', 'is characterized by', 'has significant', 'cannot be overstated']\n    tokens = re.findall(r'\\b\\w+\\b', lowered)\n    if not tokens:\n        return 0.0\n    count = 0\n    for p in phrases:\n        # count overlapping occurrences for multiword and single-word phrases\n        count += len(re.findall(re.escape(p), lowered))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of colons and semicolons per sentence ((: + ;) / sentence_count)'\n    if not text:\n        return 0.0\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    punct_count = text.count(':') + text.count(';')\n    return float(punct_count) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are Titlecase (initial capital, typical of proper nouns/headlines)'\n    import re\n    if not text:\n        return 0.0\n    raw_tokens = re.findall(r\"[A-Za-z][A-Za-z']*\", text)\n    if not raw_tokens:\n        return 0.0\n    title_count = sum(1 for t in raw_tokens if t.istitle())\n    return float(title_count) / len(raw_tokens)\n\n", "def feature(text: str) -> float:\n    'Density of explicit citation-like tokens (4-digit years 1900-2099 or \"et al\") per word'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    year_matches = re.findall(r'\\b(?:19|20)\\d{2}\\b', text)\n    etal_matches = re.findall(r'\\bet al\\b', text.lower())\n    count = len(year_matches) + len(etal_matches)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of common formal transition words (however, therefore, moreover, furthermore, nevertheless, consequently, in addition)'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    tokens = re.findall(r'\\b\\w+\\b', lowered)\n    if not tokens:\n        return 0.0\n    transitions = ['however', 'therefore', 'moreover', 'furthermore', 'nevertheless', 'consequently', 'in addition', 'nonetheless', 'thus']\n    count = 0\n    for t in transitions:\n        count += len(re.findall(re.escape(t), lowered))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing hyphens or em-dashes (indicates brand names, compounds, editorial style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t or '\u2014' in t or '\u2013' in t)\n    return float(hyphen_count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common prepositions (of, in, on, with, by, for, to, from, about, as, at)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    preps = {'of', 'in', 'on', 'with', 'by', 'for', 'to', 'from', 'about', 'as', 'at', 'into', 'through', 'between', 'among', 'over', 'against', 'during', 'without', 'within', 'toward', 'towards'}\n    count = sum(1 for t in tokens if t in preps)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of modal verbs (may, might, must, should, could, would, shall, can) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'may', 'might', 'must', 'should', 'could', 'would', 'shall', 'can', 'cannot'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Frequency of hedging or evaluative boosters (words like often, typically, crucial, significant) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    keywords = {'often', 'frequently', 'typically', 'commonly', 'rarely', 'crucial', 'critical', 'important', 'significant', 'notable', 'notably', 'especially', 'particularly', 'vital', 'key', 'impact', 'role', 'influence'}\n    count = sum(1 for t in tokens if t in keywords)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio (i, me, my, we, our, us) as a proxy for personal vs. impersonal voice'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    firstp = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for t in tokens if t in firstp)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / n\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation character types used divided by total punctuation count (0 if none)'\n    import re\n    if not text:\n        return 0.0\n    # consider common punctuation characters\n    puncts = re.findall(r\"[^\\w\\s]\", text)\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a discourse connective (however, moreover, furthermore, in addition, additionally, also, consequently, therefore)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences by punctuation\n    raw_sents = re.split(r'[.!?]+', text)\n    sents = [s.strip() for s in raw_sents if s.strip()]\n    if not sents:\n        return 0.0\n    connectives = ('however', 'moreover', 'furthermore', 'in addition', 'additionally', 'also', 'consequently', 'therefore', 'nonetheless', 'nevertheless', 'similarly', 'alternatively', 'instead')\n    count = 0\n    for s in sents:\n        low = s.lower()\n        # check multi-word connectors first\n        matched = False\n        for c in connectives:\n            if low.startswith(c):\n                count += 1\n                matched = True\n                break\n        if matched:\n            continue\n    return float(count) / len(sents)\n\n", "def feature(text: str) -> float:\n    'Clause density: average number of clause-punctuating separators (commas, semicolons, colons) per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    separators = text.count(',') + text.count(';') + text.count(':')\n    return float(separators) / sentence_count\n", "def feature(text: str) -> float:\n    'Fraction of mid-sentence capitalized tokens (likely proper nouns) excluding sentence-initial capitals'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    total = 0\n    for m in re.finditer(r'\\b[A-Z][a-z]+\\b', text):\n        start = m.start()\n        # look back one char to see if this is sentence start\n        if start == 0:\n            continue\n        prev_char = text[start-1]\n        # treat as mid-sentence if previous char is not sentence terminator or newline\n        if prev_char not in '.!? \\n\\r\\t':\n            count += 1\n        total += 1\n    # normalize by token count to be comparable across lengths\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Count of 4-digit year-like tokens (e.g., 1999, 2001) normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    years = re.findall(r'\\b(1[5-9]\\d{2}|20\\d{2}|21\\d{2})\\b', text)\n    return float(len(years)) / len(words)\n\n", "def feature(text: str) -> float:\n    'Presence density of explicit scholarly citation markers like \"et al.\" normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+|\\S', text)\n    if not tokens:\n        return 0.0\n    matches = re.findall(r'\\bet al\\.?\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per sentence (sentence count minimum 1)'\n    import re\n    if not text:\n        return 0.0\n    semis = text.count(';')\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(semis) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Density of formal discourse markers (however, therefore, moreover, consequently, furthermore, nevertheless, nonetheless, additionally, similarly, conversely)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    markers = {'however','therefore','moreover','consequently','furthermore','nevertheless','nonetheless','additionally','similarly','conversely','hence','thus'}\n    count = sum(1 for t in tokens if t in markers)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Content-word ratio: fraction of tokens that are not common stopwords (approximate lexical density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    stopwords = {'the','and','of','to','a','in','is','that','it','for','on','with','as','by','an','be','are','this','which','or','from','at','was','were','has','had'}\n    content = sum(1 for t in tokens if t not in stopwords)\n    return float(content) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total tokens (lexical diversity)'\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / n\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are discourse/subordinate connectives (however, moreover, therefore, although, because, since, whereas, thus, etc.)'\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    connectives = {'however','moreover','furthermore','therefore','thus','hence','although','though','whereas','because','since','nevertheless','nonetheless','conversely','instead','additionally'}\n    cnt = sum(1 for t in tokens if t in connectives)\n    return float(cnt) / n\n\n\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per sentence (sentences counted by .!?), sentences min 1'\n    if not text:\n        return 0.0\n    semis = text.count(';')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(semis) / sentences\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, we, me, us, my, our, mine, ours)'\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+'?\\w*\", text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    firstprs = {'i','we','me','us','my','our','mine','ours','myself','ourselves'}\n    cnt = sum(1 for t in tokens if t in firstprs)\n    return float(cnt) / n\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain digits (years, percentages, numbered items)'\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+|\\S', text)\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    cnt = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(cnt) / n\n\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation occurrences (smaller=repetitive, larger=varied)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / total\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, us, our) as a proxy for personal vs. impersonal tone'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / len(words)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal or hedging words (may, might, could, should, would, perhaps, possibly, likely, seem)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    hedges = {'may', 'might', 'could', 'should', 'would', 'perhaps', 'possibly', 'likely', 'seem', 'seems', 'appear', 'appears', 'suggests'}\n    count = sum(1 for w in words if w in hedges)\n    return float(count) / len(words)\n\n\n", "def feature(text: str) -> float:\n    'Average count of colons or semicolons per sentence (formal punctuation usage)'\n    import re\n    if not text:\n        return 0.0\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    puncts = text.count(':') + text.count(';')\n    return float(puncts) / sentence_count\n\n\n", "def feature(text: str) -> float:\n    'Normalized count of numeric ranges or year ranges (e.g., \"1500 to 1750\", \"1990-2000\") per sentence'\n    import re\n    if not text:\n        return 0.0\n    # match patterns like 1500-1750, 1500 to 1750, 2019\u20132020\n    ranges = re.findall(r'\\b\\d{3,4}\\s*(?:-|\u2013|\u2014|to)\\s*\\d{2,4}\\b', text)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(ranges)) / sentence_count\n\n\n", "def feature(text: str) -> float:\n    'Indicator (0.0/1.0) of a title-like first line followed by a blank line (common in essays or article starts)'\n    import re\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    # find first non-empty line index\n    idx = 0\n    while idx < len(lines) and not lines[idx].strip():\n        idx += 1\n    if idx >= len(lines):\n        return 0.0\n    first = lines[idx].strip()\n    # require a following blank line (or very short second line) to suggest title paragraph break\n    next_blank = (idx + 1 < len(lines) and not lines[idx + 1].strip())\n    # evaluate title-like capitalization: majority of words start with uppercase letter (allow small words)\n    words = re.findall(r\"[A-Za-z']+\", first)\n    if not words:\n        return 0.0\n    cap_count = sum(1 for w in words if w[0].isupper())\n    title_like = (5 <= len(first) <= 120) and (cap_count >= max(1, len(words) // 2)) and next_blank\n    return 1.0 if title_like else 0.0\n\n\n", "def feature(text: str) -> float:\n    'Average number of subordinating conjunctions per sentence (although, because, while, since, unless, despite, whereas, though) as a proxy for clause complexity'\n    import re\n    if not text:\n        return 0.0\n    subs = {'although', 'because', 'while', 'since', 'unless', 'despite', 'whereas', 'though', 'after', 'before', 'if', 'when', 'whenever', 'where', 'provided'}\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    total_subs = sum(1 for w in words if w in subs)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(total_subs) / sentence_count\n\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word tokens that occur exactly once (vocabulary uniqueness indicator)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    ctr = Counter(words)\n    hapax = sum(1 for w, c in ctr.items() if c == 1)\n    return float(hapax) / len(words)\n\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (std dev / mean) as a measure of variability in lexical complexity'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+\", text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    # population std deviation\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std) / mean\n", "def feature(text: str) -> float:\n    'Frequency of scholarly citation patterns per sentence: counts parenthetical citations like (Smith, 2010) or numeric [1] normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    # count common citation patterns: (Author, 2001), (Smith et al., 2020), [1], [12]\n    paren_cites = re.findall(r'\\([A-Za-z][^\\)]*\\d{4}[^\\)]*\\)', text)\n    et_al_cites = re.findall(r'\\([A-Za-z][^\\)]*et al\\.[^\\)]*\\)', text, flags=re.IGNORECASE)\n    bracket_cites = re.findall(r'\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]', text)\n    matches = len(paren_cites) + len(et_al_cites) + len(bracket_cites)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(matches) / float(sentence_count)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hedging/modal/qualifying words common in academic writing (may, might, could, suggest, indicate, likely, possibly, appears, tends)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    hedges = {'may','might','could','can','suggest','suggests','suggested','indicate','indicates','indicated','likely','possibly','potentially','appear','appears','appeared','tend','tends','tended','suggesting','indicating'}\n    count = sum(1 for w in words if w in hedges)\n    return float(count) / float(len(words))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated compounds (contain an internal hyphen between word characters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+|\\w+-\\w+|\\S', text)\n    hyphens = re.findall(r'\\w+-\\w+', text)\n    token_count = max(1, len(re.findall(r'\\w+', text)))\n    return float(len(hyphens)) / float(token_count)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of occurrences of Latin abbreviations (e.g., i.e., e.g., et al., cf.) normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    lc = text.lower()\n    patterns = ['e.g.', 'eg,', 'i.e.', 'ie,', 'et al.', 'cf.', 'viz.']\n    count = sum(lc.count(p) for p in patterns)\n    tokens = max(1, len(re.findall(r'\\w+', text)))\n    return float(count) / float(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: fraction of distinct punctuation characters present out of total punctuation characters (measure of varied punctuation use)'\n    import re\n    if not text:\n        return 0.0\n    puncts = re.findall(r'[^\\w\\s]', text)\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    total = len(puncts)\n    return float(distinct) / float(total)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with -ing (progressive/gerund forms) as a proxy for continuous/action-oriented phrasing'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.endswith('ing'))\n    return float(count) / float(len(words))\n\n\n", "def feature(text: str) -> float:\n    'Average count of colons and semicolons per sentence (proxy for clause-complex punctuation use)'\n    import re\n    if not text:\n        return 0.0\n    colons_semis = text.count(':') + text.count(';')\n    # sentence count fallback to at least 1\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(colons_semis) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Normalized count of multiword Titlecase sequences (>=2 consecutive Titlecase tokens not at sentence start) indicating named entities/technical terms'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences to identify sentence starts\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    seq_count = 0\n    total_tokens = 0\n    for sent in sentences:\n        if not sent:\n            continue\n        words = re.findall(r\"\\b\\w+\\b\", sent)\n        if not words:\n            continue\n        total_tokens += len(words)\n        # ignore the first token in the sentence when checking for Titlecase runs\n        run = 0\n        for w in words[1:]:\n            if w[0].isupper() and w[1:].islower():\n                run += 1\n            else:\n                if run >= 2:\n                    seq_count += 1\n                run = 0\n        if run >= 2:\n            seq_count += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(seq_count) / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common discourse markers (however, therefore, moreover, furthermore, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    markers = {'however', 'therefore', 'moreover', 'furthermore', 'additionally', 'consequently', 'nevertheless', 'nonetheless', 'similarly', 'notwithstanding'}\n    count = sum(1 for t in tokens if t in markers)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are content words by a lightweight stopword filter (proxy for lexical density)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the', 'and', 'of', 'to', 'a', 'in', 'is', 'that', 'for', 'on', 'with', 'as', 'by', 'an', 'be', 'are', 'at', 'from', 'this', 'which', 'or', 'it'}\n    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    content = sum(1 for t in tokens if t not in stopwords)\n    return float(content) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of definitional/nominalizing patterns like \"is a\", \"is an\", \"is the\", \"refers to\", \"is defined as\" per sentence (indicator of explanatory/academic phrasing)'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    patterns = [' is a ', ' is an ', ' is the ', ' refers to ', ' is defined as ', ' consists of ', ' are known as ', ' can be ', ' is known as ']\n    count = sum(lower.count(p) for p in patterns)\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are digits (proxy for presence of years, numeric data, or enumerations)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    digits = sum(1 for c in text if c.isdigit())\n    return float(digits) / float(total_chars)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are extremely long (length > 12 characters) \u2014 catches technical names and compound terms'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) > 12)\n    return float(long_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Unigram token entropy (Shannon entropy in bits) computed from word tokens; higher when vocabulary use is more evenly distributed'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not words:\n        return 0.0\n    freq = {}\n    for w in words:\n        freq[w] = freq.get(w, 0) + 1\n    n = len(words)\n    entropy = 0.0\n    for c in freq.values():\n        p = c / n\n        entropy -= p * math.log(p, 2)\n    return float(entropy)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with common academic/discourse openers (The, This, These, That, There, In) \u2014 proxies for formal exposition'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences by punctuation marks\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    starters = ('the','this','these','that','there','in')\n    count = 0\n    for s in sents:\n        first_word_match = re.match(r\"\\s*([A-Za-z']+)\", s)\n        if first_word_match:\n            if first_word_match.group(1).lower() in starters:\n                count += 1\n    return float(count) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a digit (numeric tokens) \u2014 academic/technical passages often include numbers, years, statistics'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(num_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens ending with -ly (adverb usage rate)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower().endswith('ly'))\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters used divided by total punctuation count (0 if none)'\n    import re\n    if not text:\n        return 0.0\n    puncts = re.findall(r'[^\\w\\s]', text)\n    if not puncts:\n        return 0.0\n    unique = len(set(puncts))\n    total = len(puncts)\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain an internal capital letter (capital letter not at first character) \u2014 catches scientific names, acronyms embedded in words'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # ignore single-character tokens\n        if len(t) > 1 and any(ch.isupper() for ch in t[1:]):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Clause separator density: average number of clause-separating punctuation marks (commas, semicolons, colons) per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sentence_count = max(1, len(sentences))\n    separators = text.count(',') + text.count(';') + text.count(':')\n    return float(separators) / float(sentence_count)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a digit or look like numeric measurements (percent, mg, ml, cm) \u2014 numeric density'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    num_patterns = re.compile(r'\\d|%|mg\\b|ml\\b|cm\\b|km\\b|mm\\b|kg\\b|g\\b|\u00b5g\\b|ml\\)|\\b\\(\\d')\n    count = 0\n    for t in tokens:\n        if num_patterns.search(t.lower()):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average count of subordinating conjunctions (because, although, since, while, whereas, though, unless, until, as) per sentence (robust to zero sentences)'\n    import re\n    if not text:\n        return 0.0\n    # approximate sentence count\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)) )\n    conj_pattern = re.compile(r'\\b(because|although|since|while|whereas|though|unless|until|as\\s+that|as\\s+if)\\b', re.IGNORECASE)\n    conj_count = len(conj_pattern.findall(text))\n    return float(conj_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing hyphens or slashes (compound terms, ranges, ratios) as a proxy for technical compounds'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t or '/' in t)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are discourse-transition/academic connectors (however, moreover, therefore, furthermore, consequently, nevertheless, nonetheless, additionally)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    transitions = set(['however','moreover','therefore','furthermore','consequently','nevertheless','nonetheless','additionally','subsequently','alternatively','conversely','hence'])\n    count = sum(1 for t in tokens if t in transitions)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of words in comma-separated segments (average clause/chunk length created by commas); if no commas, returns average sentence length'\n    import re\n    if not text:\n        return 0.0\n    # segments split by commas\n    comma_segments = [seg.strip() for seg in text.split(',')]\n    # compute words per segment\n    seg_word_counts = [len(re.findall(r'\\w+', seg)) for seg in comma_segments if seg.strip() != '']\n    if seg_word_counts:\n        return float(sum(seg_word_counts)) / float(len(seg_word_counts))\n    # fallback: average sentence length\n    sentences = re.split(r'[.!?]+', text)\n    sent_word_counts = [len(re.findall(r'\\w+', s)) for s in sentences if s.strip() != '']\n    if not sent_word_counts:\n        return 0.0\n    return float(sum(sent_word_counts)) / float(len(sent_word_counts))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are demonstratives (this, that, these, those) \u2014 signals explicit referential phrasing'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    demos = set(['this','that','these','those'])\n    count = sum(1 for t in tokens if t in demos)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average vowel density per alphabetic character (vowels / letters) across tokens \u2014 proxy for polysyllabic, Latinate vocabulary'\n    import re\n    if not text:\n        return 0.0\n    letter_chars = re.findall(r'[A-Za-z]', text)\n    if not letter_chars:\n        return 0.0\n    vowels = sum(1 for c in letter_chars if c.lower() in 'aeiou')\n    return float(vowels) / float(len(letter_chars))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like academic abbreviations/citation markers (e.g., e.g., i.e., et al., Fig., Table, doi, http) per token'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    tokens = re.findall(r'\\w+|\\S', text)\n    if not tokens:\n        return 0.0\n    patterns = [r'\\be\\.g\\.\\b', r'\\bi\\.e\\.\\b', r'\\bet al\\b', r'\\bfig\\.\\b', r'\\btable\\b', r'\\bdoi\\b', r'http[s]?://', r'\\bref(s)?\\b']\n    count = 0\n    for pat in patterns:\n        found = re.findall(pat, lower)\n        # some patterns return tuples; count total matches\n        count += len(found)\n    # normalize by token count but cap at 1.0 possible if many matches\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Citation-like density: counts bracketed numeric refs, parenthetical author-year citations and \"et al.\" per sentence'\n    import re\n    if not text:\n        return 0.0\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    bracket_refs = len(re.findall(r'\\[\\s*\\d{1,4}\\s*\\]', text))\n    paren_author_year = len(re.findall(r'\\([A-Za-z][^)]{0,60},\\s*\\d{4}\\)', text))\n    et_al = len(re.findall(r'\\bet al\\.', text.lower()))\n    total = bracket_refs + paren_author_year + et_al\n    return float(total) / sent_count\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated (contain a hyphen), normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    word_tokens = re.findall(r'\\w+', text)\n    denom = max(1, len(word_tokens))\n    hyphen_count = sum(1 for t in tokens if '-' in t and any(ch.isalpha() for ch in t))\n    return float(hyphen_count) / denom\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens with medium length (4-7 characters) \u2014 often common in formal prose (not short function words nor very long technical terms)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if 4 <= len(t) <= 7)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of explicit exemplification markers per sentence (e.g., \"e.g.\", \"i.e.\", \"for example\")'\n    import re\n    if not text:\n        return 0.0\n    text_l = text.lower()\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    patterns = re.findall(r'\\be\\.g\\.\\b', text_l) + re.findall(r'\\bi\\.e\\.\\b', text_l) + re.findall(r'\\bfor example\\b', text_l)\n    return float(len(patterns)) / sent_count\n\n", "def feature(text: str) -> float:\n    'Relative clause density: occurrences of relative pronouns (which, that, who, whom, whose, where, when) per sentence'\n    import re\n    if not text:\n        return 0.0\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    count = len(re.findall(r'\\b(which|that|who|whom|whose|where|when)\\b', text.lower()))\n    return float(count) / sent_count\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are short all-caps acronyms (2-5 uppercase letters), common in technical/academic texts'\n    import re\n    if not text:\n        return 0.0\n    all_word_tokens = re.findall(r'\\w+', text)\n    if not all_word_tokens:\n        return 0.0\n    acronyms = re.findall(r'\\b[A-Z]{2,5}\\b', text)\n    return float(len(acronyms)) / len(all_word_tokens)\n", "def feature(text: str) -> float:\n    'Average number of transitional/adversative adverbs (however, moreover, therefore, etc.) per sentence'\n    import re\n    if not text:\n        return 0.0\n    trans = {'however','moreover','furthermore','therefore','consequently','nevertheless','additionally','subsequently','meanwhile','thus','hence'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    trans_count = sum(1 for t in tokens if t in trans)\n    return float(trans_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ly\" (proxy for adverb/adverbial density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ly'))\n    return float(ly_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of bigrams that are repeated (number of distinct bigrams with freq>1 divided by total bigrams)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    from collections import Counter\n    bigrams = [' '.join((tokens[i], tokens[i+1])) for i in range(len(tokens)-1)]\n    counts = Counter(bigrams)\n    repeated = sum(1 for k,v in counts.items() if v > 1)\n    total = len(bigrams)\n    return float(repeated) / float(total) if total > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with the word \"in\" (case-insensitive), capturing \"In ...\" sentence openings'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts_in = 0\n    for s in sentences:\n        s_low = s.lstrip(' \"\\'').lower()\n        if s_low.startswith('in '):\n            starts_in += 1\n    return float(starts_in) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Frequency of em-dash or double-hyphen occurrences per sentence (\u2014 or --), capturing formal punctuation use'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    em_count = text.count('\u2014') + text.count('--')\n    return float(em_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters present divided by total punctuation count (0 if no punctuation)'\n    import string, re\n    if not text:\n        return 0.0\n    punct_set = set(string.punctuation) | {'\u2014'}\n    puncts = [c for c in text if c in punct_set]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Average clause length in words: total words divided by number of clauses (clauses split on , ; : \u2014)'\n    import re\n    if not text:\n        return 0.0\n    clauses = [c.strip() for c in re.split(r'[,:;\u2014]+', text) if c.strip()]\n    words = re.findall(r'\\b\\w+\\b', text)\n    total_words = len(words)\n    clause_count = max(1, len(clauses))\n    return float(total_words) / float(clause_count)\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like headings (short lines with title-cased words or that end with a question/exclamation)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    heading_count = 0\n    for ln in lines:\n        words = ln.split()\n        if not words:\n            continue\n        # candidate heading: short and many title-cased initials, or ends with ?/!\n        if len(words) <= 8:\n            title_initials = sum(1 for w in words if w[0].isupper())\n            if title_initials >= max(1, len(words) // 2):\n                heading_count += 1\n                continue\n        if ln.endswith('?') or ln.endswith('!'):\n            heading_count += 1\n    return float(heading_count) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with common expository function words (the, this, there, in, on, to, as, by, for, when)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences by .,!,?\n    raw_sents = re.split(r'[.!?]+', text)\n    sents = [s.strip() for s in raw_sents if s.strip()]\n    if not sents:\n        return 0.0\n    starters = set(['the', 'this', 'there', 'in', 'on', 'to', 'as', 'by', 'for', 'when', 'with', 'at', 'from'])\n    match = 0\n    for s in sents:\n        words = re.findall(r\"\\w+\", s.lower())\n        if not words:\n            continue\n        if words[0] in starters:\n            match += 1\n    return float(match) / len(sents)\n\n", "def feature(text: str) -> float:\n    'Sentence length dispersion: coefficient of variation (std / mean) of sentence lengths in words (0 if no sentences)'\n    import re, math\n    if not text:\n        return 0.0\n    raw_sents = re.split(r'[.!?]+', text)\n    sents = [s.strip() for s in raw_sents if s.strip()]\n    lengths = []\n    for s in sents:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n", "def feature(text: str) -> float:\n    'Normalized frequency of the existential phrases \"there is\" or \"there are\" (common in explanatory writing)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    # count overlapping occurrences of \"there is\" or \"there are\"\n    pair_count = 0\n    for i in range(len(words) - 1):\n        if words[i] == 'there' and words[i+1] in ('is', 'are'):\n            pair_count += 1\n    return float(pair_count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Proportion of repeated sentence-start trigrams (first up to 3 words) among all sentences (template repetition proxy)'\n    import re\n    if not text:\n        return 0.0\n    raw_sents = re.split(r'[.!?]+', text)\n    sents = [s.strip() for s in raw_sents if s.strip()]\n    if not sents:\n        return 0.0\n    starts = []\n    for s in sents:\n        words = re.findall(r'\\w+', s.lower())\n        if not words:\n            starts.append('')\n        else:\n            starts.append(' '.join(words[:3]))\n    total = len(starts)\n    unique = len(set(starts))\n    repeats = total - unique\n    return float(repeats) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are formal academic connectives (however, moreover, therefore, thus, furthermore, consequently, nevertheless)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    connectives = set(['however', 'moreover', 'therefore', 'thus', 'furthermore', 'consequently', 'nevertheless', 'additionally', 'similarly', 'notwithstanding'])\n    count = sum(1 for w in words if w in connectives)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Acronym density: fraction of tokens that are all-caps acronyms of length >=2 (e.g., \"USA\", \"API\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    acr_count = 0\n    for t in tokens:\n        if len(t) >= 2 and t.isupper() and re.search(r'[A-Z]', t):\n            # avoid counting numbers-only tokens\n            if not t.isdigit():\n                acr_count += 1\n    return float(acr_count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a discourse marker or transitional phrase (however, moreover, in addition, etc.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    markers = ('however', 'moreover', 'furthermore', 'additionally', 'nevertheless', 'nonetheless',\n               'in addition', 'in contrast', 'on the other hand', 'overall', 'importantly', 'notably',\n               'conversely', 'first', 'second', 'third', 'finally')\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    hit = 0\n    for s in sents:\n        sl = s.lower().lstrip('\"\\'' ).strip()\n        for m in markers:\n            if sl.startswith(m + ' ') or sl == m:\n                hit += 1\n                break\n    return float(hit) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Proportion of bigram tokens that are repeated beyond their first occurrence (repeat-density of bigrams)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[a-zA-Z0-9]+\\b', text.lower())\n    if len(words) < 2:\n        return 0.0\n    bigrams = ['%s %s' % (words[i], words[i+1]) for i in range(len(words)-1)]\n    freq = Counter(bigrams)\n    total = len(bigrams)\n    repeated_tokens = sum((f - 1) for f in freq.values() if f > 1)\n    return float(repeated_tokens) / float(total) if total > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Variance of sentence lengths in words (population variance), 0 if fewer than 2 sentences'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    lengths = []\n    for s in sents:\n        words = re.findall(r'\\b\\w+\\b', s)\n        lengths.append(len(words))\n    if len(lengths) < 2:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((L - mean) ** 2 for L in lengths) / len(lengths)\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with weak-existential or dummy-subject openings (there, it, this)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    starts = 0\n    for s in sents:\n        m = re.match(r'\\s*[\"\\']?\\s*([A-Za-z]+)', s)\n        if m:\n            first = m.group(1).lower()\n            if first in ('there', 'it', 'this', 'these', 'those'):\n                starts += 1\n    return float(starts) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that mention decades or years in decade form (e.g., 1970s, 1990s, 70s)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[\\w%]+\\b', text)\n    if not tokens:\n        return 0.0\n    patterns = re.findall(r'\\b\\d{2,4}s\\b', text.lower())\n    # also accept patterns like \"1970s\" or \"70s\"\n    count = len(patterns)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that use listing or exemplification phrases (include, such as, including, consist of, comprise)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    patterns = (' include ', ' including ', ' such as ', ' consist of ', ' comprise ', ' composed of ', ' for example', ' for instance', 'e.g.')\n    sents = [s.strip() for s in re.split(r'[.!?]+', text.lower()) if s.strip()]\n    if not sents:\n        return 0.0\n    hits = 0\n    for s in sents:\n        for p in patterns:\n            if p in s:\n                hits += 1\n                break\n    return float(hits) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Normalized unigram entropy (Shannon entropy / log(V)) of token distribution, 0 when too few types'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[a-zA-Z0-9]+\\b', text.lower())\n    N = len(words)\n    if N == 0:\n        return 0.0\n    freq = {}\n    for w in words:\n        freq[w] = freq.get(w, 0) + 1\n    V = len(freq)\n    if V < 2:\n        return 0.0\n    entropy = -sum((count / N) * math.log(count / N) for count in freq.values())\n    norm = math.log(V) if V > 1 else 1.0\n    return float(entropy) / float(norm)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain scholarly/citation cues like \"et al\", \"e.g.\", \"i.e.\", \"cf.\" (approximate)'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    cues = 0\n    cues += len(re.findall(r'\\bet al\\b', lower))\n    cues += len(re.findall(r'\\be\\.g\\.\\b', lower))\n    cues += len(re.findall(r'\\bi\\.e\\.\\b', lower))\n    cues += len(re.findall(r'\\bcf\\.\\b', lower))\n    cues += len(re.findall(r'\\bsee\\b', lower))\n    tokens = re.findall(r'\\b[\\w%]+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(cues) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of sentences that look \"definition-like\" (linking verb such as \"is/are/can be/are used to/is defined as\" followed by \"that\" or a defining construction)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    pattern = re.compile(r'\\b(?:is|are|can be|are used to|serves to|serve to|refers to|is defined as|are defined as|acts as|can serve as)\\b.*\\bthat\\b', flags=re.I)\n    count = 0\n    for s in sentences:\n        if pattern.search(s):\n            count += 1\n    return float(count) / float(len(sentences)) if sentences else 0.0\n\n", "def feature(text: str) -> float:\n    'Nominalization density: fraction of word tokens that end with common nominalization suffixes (tion, sion, ment, ness, ence, ance, ism, ity)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('tion', 'sion', 'ment', 'ness', 'ence', 'ance', 'ism', 'ity')\n    count = 0\n    for t in tokens:\n        for sfx in suffixes:\n            if t.endswith(sfx) and len(t) > len(sfx) + 1:\n                count += 1\n                break\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Gerund/present-participial density: fraction of tokens ending in \"ing\" (proxy for continuous/procedural phrasing)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 4 and t.endswith('ing'))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are coordinating conjunctions (and, or, but, nor, yet, so, for) capturing enumerative/listing style'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    conjs = {'and', 'or', 'but', 'nor', 'yet', 'so', 'for'}\n    count = sum(1 for t in tokens if t in conjs)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of explicit example markers per sentence (phrases like \"such as\", \"for example\", \"for instance\", \"e.g.\", \"i.e.\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    sentence_count = max(1, len(sentences))\n    matches = re.findall(r'(?:\\bfor example\\b|\\bsuch as\\b|\\bfor instance\\b|e\\.g\\.|i\\.e\\.)', text, flags=re.I)\n    return float(len(matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Average length of consecutive runs of common stopwords (the, and, of, to, a, in, is, that, for, with, on, as, by) \u2014 higher values indicate denser function-word runs'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stop = {'the','and','of','to','a','in','is','that','for','with','on','as','by'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    runs = []\n    cur = 0\n    for t in tokens:\n        if t in stop:\n            cur += 1\n        else:\n            if cur > 0:\n                runs.append(cur)\n                cur = 0\n    if cur > 0:\n        runs.append(cur)\n    if not runs:\n        return 0.0\n    return float(sum(runs)) / float(len(runs))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that show simple syntactic parallelism: at least two commas and a coordinating conjunction (commas + \"and\"/\"or\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    conj_pattern = re.compile(r'\\b(?:and|or)\\b', flags=re.I)\n    for s in sentences:\n        if s.count(',') >= 2 and conj_pattern.search(s):\n            count += 1\n    return float(count) / float(len(sentences)) if sentences else 0.0\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words \u2014 measures uniformity/repetitiveness of sentence length'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    raw_sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    sentences = raw_sentences if raw_sentences else [text.strip()]\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    if mean <= 0.0:\n        return 0.0\n    var = sum((L - mean) ** 2 for L in lengths) / float(len(lengths))\n    sd = math.sqrt(var)\n    return sd / mean\n", "def feature(text: str) -> float:\n    'Density of transition/adverbial connectors (however, moreover, therefore, furthermore, additionally, in addition) per token'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    transitions = {'however','moreover','therefore','furthermore','additionally','consequently','nevertheless','nonetheless','in addition','thus','hence'}\n    count = 0\n    for t in tokens:\n        if t in transitions:\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are coordinating/subordinating conjunctions (and, or, but, because, although, which, that) as proxy for clause linking density'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    conjunctions = {'and','or','but','because','since','although','though','while','whereas','which','that','if','when','whilst'}\n    count = sum(1 for t in tokens if t in conjunctions)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average token-overlap ratio between adjacent sentences (how often content words are repeated across neighboring sentences)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if len(sentences) < 2:\n        return 0.0\n    stopwords = {'the','and','of','to','a','in','is','that','it','for','on','with','as','are','was','were','be','by','an','this','these','those','which','or','but','we','they','their','its','has','have','had'}\n    ratios = []\n    for i in range(1, len(sentences)):\n        s1 = re.findall(r\"\\b[\\w']+\\b\", sentences[i-1].lower())\n        s2 = re.findall(r\"\\b[\\w']+\\b\", sentences[i].lower())\n        set1 = set(w for w in s1 if w not in stopwords and len(w) > 2)\n        set2 = set(w for w in s2 if w not in stopwords and len(w) > 2)\n        if not set1:\n            ratios.append(0.0)\n        else:\n            ratios.append(len(set1 & set2) / float(len(set1)))\n    return float(sum(ratios) / len(ratios))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences containing a passive-with-agent pattern (\"was ... by\", \"were ... by\", \"is ... by\") as a focused passive-agent proxy'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    pat = re.compile(r'\\b(?:was|were|is|are|has been|had been|been|being)\\b[^.?!]{0,120}\\bby\\b', re.I)\n    count = 0\n    for s in sentences:\n        if pat.search(s):\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences where a colon introduces a list/lead-in (colon followed by capital or digit), marking enumerative explanatory style'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        if re.search(r':\\s*[A-Z0-9]', s):\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Variance of word lengths (normalized by mean+1) \u2014 captures uniformity vs. lexical variety in token lengths'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    # normalize to avoid large raw values\n    return float(var / (mean + 1.0))\n", "def feature(text: str) -> float:\n    'Density of Latinate/technical suffixes (words ending with -ize/-ise/-ic/-al/-ary/-ous/-ive) as a proxy for formal/academic tone'\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w[\\w'-]*\", text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ize','ise','ic','al','ary','ous','ive','ent','ant')\n    count = 0\n    for t in tokens:\n        for suf in suffixes:\n            if len(t) > len(suf) + 1 and t.endswith(suf):\n                count += 1\n                break\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of academic citation-like patterns per sentence (e.g., (Smith, 2002), (Jones et al., 1999), or [1])'\n    if not text or not text.strip():\n        return 0.0\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)) )\n    # patterns: (Author, 2000), (Author et al., 2000), [1], [12]\n    pattern1 = re.findall(r'\\([A-Z][A-Za-z-]+(?:\\s+et\\s+al\\.)?,\\s*\\d{4}\\)', text)\n    pattern2 = re.findall(r'\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]', text)\n    total = len(pattern1) + len(pattern2)\n    return float(total) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with an existential/\u201cthere\u201d construction (there is/are/has/have etc.), a common academic framing style'\n    if not text or not text.strip():\n        return 0.0\n    raw_sentences = re.split(r'(?<=[.!?])\\s+', text)\n    sentences = [s.strip() for s in raw_sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        words = re.findall(r\"\\w[\\w'-]*\", s.lower())\n        if not words:\n            continue\n        if words[0] == 'there' and len(words) > 1 and words[1] in ('is','are','was','were','has','have','had','seems','appears','remains'):\n            starts += 1\n        if words[0] in (\"there's\",\"thereve\",\"theres\") and len(words) > 0:\n            starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Distinct punctuation character diversity normalized by total punctuation count (how varied punctuation usage is)'\n    if not text:\n        return 0.0\n    punct_chars = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not punct_chars:\n        return 0.0\n    distinct = len(set(punct_chars))\n    total = len(punct_chars)\n    # normalized to (0,1], higher means more varied punctuation relative to quantity\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like acronyms or initials (ALLCAPS tokens length>=2 or sequences like U.S.A., e.g., M.L.A.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"\\b\\S+\\b\", text)\n    if not tokens:\n        return 0.0\n    pattern1 = re.compile(r'^[A-Z]{2,}$')        # ALL CAPS\n    pattern2 = re.compile(r'^(?:[A-Z]\\.){2,}$')  # A.B.C.\n    count = 0\n    for t in tokens:\n        # Strip surrounding punctuation for check\n        core = t.strip('.,;:()[]{}\"\\'')\n        if not core:\n            continue\n        if pattern1.match(core) or pattern2.match(core):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with common discourse markers (However, Moreover, In addition, Therefore, When, While, Although, Because, Thus, Meanwhile)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # Split into sentence-like chunks\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    markers = {'however','moreover','furthermore','therefore','additionally','consequently','meanwhile','thus','hence','in','when','while','although','because','there','nonetheless','nevertheless'}\n    count = 0\n    for s in sents:\n        m = re.match(r\"^\\s*['\\\"\\(\\[]*([A-Za-z\\-]+)\", s)\n        if m:\n            first = m.group(1).lower()\n            if first in markers:\n                count += 1\n    return float(count) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are subordinating conjunctions or relative pronouns (because, although, that, which, who, etc.) as a proxy for clause complexity'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    subs = {'although','because','since','unless','while','when','where','which','that','who','whom','whose','after','before','until','whereas','if','provided','assuming'}\n    count = sum(1 for t in tokens if t in subs)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of titlecase tokens that occur mid-sentence (not the first token of a sentence) \u2014 proxy for proper nouns/section titles embedded in text'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.split(r'([.!?])', text) if s.strip()]\n    # Build sequence of sentences robustly: split by punctuation but keep order\n    # Simpler: split on punctuation marks for sentences\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    total_tokens = 0\n    mid_titlecase = 0\n    for s in sents:\n        words = re.findall(r\"\\b[\\w'-]+\\b\", s)\n        if not words:\n            continue\n        for i, w in enumerate(words):\n            total_tokens += 1\n            if i == 0:\n                continue\n            # Titlecase heuristic: first char uppercase, others lowercase\n            if len(w) > 1 and w[0].isupper() and w[1:].islower():\n                mid_titlecase += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(mid_titlecase) / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that match parenthetical citation-like patterns (year in parentheses, [number], or \"et al\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"\\b\\S+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = 0\n    # Patterns: (Smith, 2020), (2020), [1], et al\n    paren_years = re.findall(r'\\([^\\)]*\\d{4}[^\\)]*\\)', text)\n    bracket_nums = re.findall(r'\\[\\s*\\d+\\s*\\]', text)\n    # Count occurrences of 'et al' (case-insensitive)\n    et_al = sum(1 for _ in re.finditer(r'\\bet\\s+al\\b', text, flags=re.IGNORECASE))\n    count += len(paren_years) + len(bracket_nums) + et_al\n    # Normalize by token count (cap at token count)\n    return float(min(count, len(tokens))) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of formal transition words (moreover, furthermore, however, therefore, consequently, thus, hence) normalized by token count'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    transitions = {'moreover','furthermore','however','therefore','consequently','thus','hence','nonetheless','nevertheless','additionally','alternatively'}\n    count = sum(1 for t in tokens if t in transitions)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hedging/epistemic modal words (may, might, could, seem, appear, suggest, possibly, likely)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    hedges = {'may', 'might', 'could', 'would', 'seem', 'seems', 'seemed', 'appear', 'appears', 'appeared', 'suggest', 'suggests', 'suggested', 'possibly', 'perhaps', 'likely', 'probable', 'probabilistic'}\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a common discourse marker (In, However, Moreover, Therefore, If, While, Although, Imagine, Because, As)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences by punctuation followed by whitespace (keeps trailing punctuation)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    markers = {'in', 'however', 'moreover', 'therefore', 'if', 'while', 'although', 'imagine', 'because', 'as', 'thus', 'consequently', 'meanwhile'}\n    count = 0\n    for s in sentences:\n        # get first token of sentence\n        m = re.search(r\"\\b[\\w']+\", s)\n        if m:\n            first = m.group(0).lower()\n            if first in markers:\n                count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Average number of semicolons and colons per sentence (semicolon/colon density), normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    sent_count = len(sentences) if sentences else 1\n    punct_count = text.count(';') + text.count(':')\n    return float(punct_count) / sent_count\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are second-person pronouns (you, your, yours, yourself) indicating direct address or imaginative prompts'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    second = {'you', 'your', 'yours', 'yourself', 'yourselves'}\n    count = sum(1 for t in tokens if t in second)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words; measures sentence-length variability, robust to single sentence'\n    import re, math\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    lengths = [len(re.findall(r\"\\b[\\w']+\\b\", s)) for s in sents]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    sd = math.sqrt(var)\n    return sd / mean\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in -ly (adverbial morphology), a proxy for adverb/adverbial style usage'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 3)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?) as a measure of rhetorical/questioning style'\n    import re\n    if not text:\n        return 0.0\n    # Count sentences and question sentences robustly\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.endswith('?') or '?' in s)\n    return float(q_count) / len(sentences)\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in -ly (a proxy for adverb/adverbial style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 3)\n    return float(ly_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Semicolon density: semicolons per sentence (sentences counted as .!? with minimum 1)'\n    if not text:\n        return 0.0\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    if sentence_count < 1:\n        sentence_count = 1\n    semicolons = text.count(';')\n    return float(semicolons) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Colon density: colons (:) per sentence (useful for formal explanatory lists and definitions)'\n    if not text:\n        return 0.0\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    if sentence_count < 1:\n        sentence_count = 1\n    colons = text.count(':')\n    return float(colons) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in -ing with length>4 (proxy for gerund/continuous nominalizations)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing') and len(t) > 4)\n    return float(ing_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Median clause length in words, where clauses are split on common intra-sentence delimiters ; , : \u2014 -'\n    import re\n    if not text:\n        return 0.0\n    clauses = re.split(r'[;,:\\u2014\\u2013\\-]', text)\n    lengths = []\n    for c in clauses:\n        words = re.findall(r'\\w+', c)\n        if words:\n            lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    lengths.sort()\n    n = len(lengths)\n    mid = n // 2\n    if n % 2 == 1:\n        return float(lengths[mid])\n    else:\n        return float((lengths[mid - 1] + lengths[mid]) / 2.0)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens in Titlecase (initial capital + lowercase letters) as a proxy for formal named/technical terms'\n    import re\n    if not text:\n        return 0.0\n    title_tokens = re.findall(r'\\b[A-Z][a-z]+\\b', text)\n    total_tokens = re.findall(r'\\w+', text)\n    if not total_tokens:\n        return 0.0\n    return float(len(title_tokens)) / len(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Auxiliary/modal verb ratio: fraction of tokens that are auxiliaries/modals (is, are, was, have, will, can, etc.)'\n    import re\n    if not text:\n        return 0.0\n    auxiliaries = {'is','are','was','were','be','been','being','has','have','had','do','does','did','will','would','shall','should','can','could','may','might','must'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    aux_count = sum(1 for t in tokens if t in auxiliaries)\n    return float(aux_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Hapax rate: fraction of word tokens that appear only once (unique/rare-token indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    freqs = Counter(tokens)\n    hapax = sum(1 for t, c in freqs.items() if c == 1)\n    return float(hapax) / len(tokens)\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of tokens that occur only once (lexical variety proxy)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    counts = Counter(tokens)\n    hapaxes = sum(1 for c in counts.values() if c == 1)\n    return float(hapaxes) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Gerund/continuous form fraction: fraction of tokens ending in \"ing\" (style/verb-as-noun usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return float(ing_count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Question sentence fraction: fraction of sentences that are questions (rhetorical/questioning tone)'\n    if not text:\n        return 0.0\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    if sentence_count <= 0:\n        return 0.0\n    question_count = text.count('?')\n    return float(question_count) / float(sentence_count)\n\n\n", "def feature(text: str) -> float:\n    'Uncommon word fraction: fraction of tokens not in a small common-word list (lexical sophistication proxy)'\n    import re\n    if not text:\n        return 0.0\n    common = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there','their','what','so','up','out','if','about','who','get','which','go','me','when','make','can','like','time','no','just','him','know','take','people','into','year','your','good','some','could','them','see','other','than','then','now','look','only','come','its','over','think','also','back','after','use','two','how','our','work','first','well','way','even','new','want','because','any','these','give','day','most','us'}\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    uncommon = sum(1 for t in tokens if t not in common)\n    return float(uncommon) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Unique punctuation types used (count of different punctuation characters present)'\n    if not text:\n        return 0.0\n    puncts = {c for c in text if not c.isalnum() and not c.isspace()}\n    return float(len(puncts))\n\n\n", "def feature(text: str) -> float:\n    'Ordinal/token fraction: fraction of tokens that are ordinals (first, second, 3rd, etc.) indicating list/structured discourse'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not words:\n        return 0.0\n    ordinals_words = {'first','second','third','fourth','fifth','sixth','seventh','eighth','ninth','tenth','eleventh','twelfth'}\n    numeric_ordinals = re.findall(r'\\b\\d+(?:st|nd|rd|th)\\b', text.lower())\n    ord_word_count = sum(1 for w in words if w in ordinals_words)\n    ord_count = ord_word_count + len(numeric_ordinals)\n    return float(ord_count) / float(len(words))\n\n\n", "def feature(text: str) -> float:\n    'Weasel/weighing words fraction: fraction of casual/weasel words like \"very, really, just\" (less precise style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    weasel = {'very','really','just','quite','extremely','literally','basically','actually','somewhat','perhaps','maybe','fairly','relatively','often','generally','sortof','kindof','mostly'}\n    count = sum(1 for t in tokens if t in weasel)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of repeated adjacent bigrams (duplicate bigrams / total bigrams), proxy for formulaic repetition'\n    import re\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    total = len(bigrams)\n    unique = len(set(bigrams))\n    repeats = total - unique\n    return float(repeats) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are the definite article \"the\" (indicator of formal/definitive phrasing)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    the_count = sum(1 for t in tokens if t == 'the')\n    return float(the_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average count of colons and semicolons per sentence ( (\":\" + \";\") / max(1, sentence_count) )'\n    import re\n    punct_count = text.count(':') + text.count(';')\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    return float(punct_count) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very long (>30 words), measuring long explanatory sentence tendency'\n    import re\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    long_count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) > 30:\n            long_count += 1\n    return float(long_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit (years, statistics, enumerations)'\n    import re\n    raw_tokens = re.findall(r'\\S+', text)\n    if not raw_tokens:\n        return 0.0\n    num_tokens = sum(1 for t in raw_tokens if any(c.isdigit() for c in t))\n    return float(num_tokens) / len(raw_tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing a hyphen (\"-\"), capturing use of compound adjectives and technical terms'\n    import re\n    raw_tokens = re.findall(r'\\S+', text)\n    if not raw_tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in raw_tokens if '-' in t)\n    return float(hyphen_count) / len(raw_tokens)\n\n", "def feature(text: str) -> float:\n    'Normalized character-level entropy (0-1) as a proxy for character variety and lexical richness'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    counts = Counter(text)\n    total = sum(counts.values())\n    if total == 0 or len(counts) <= 1:\n        return 0.0\n    entropy = -sum((c/total) * math.log2((c/total)) for c in counts.values())\n    max_entropy = math.log2(len(counts))\n    if max_entropy <= 0:\n        return 0.0\n    return float(entropy / max_entropy)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words, robustly measuring sentence variability'\n    import re\n    import math\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std / mean)\n", "def feature(text: str) -> float:\n    'Density of explicit quotation marks (double quotes + single quotes used as quotation marks, excluding apostrophes inside words), normalized by token count.'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    token_count = max(1, len(words))\n    double_q = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    # Count single quotes that are likely quotation marks (not embedded apostrophes inside words)\n    single_q_matches = re.findall(r\"(?<![A-Za-z0-9])'|'(?![A-Za-z0-9])\", text)\n    single_q = len(single_q_matches)\n    total_q = double_q + single_q\n    return float(total_q) / token_count\n\n", "def feature(text: str) -> float:\n    'Fraction of whitespace-delimited tokens that contain an internal hyphen (\"-\"), capturing hyphenated compounds like \"fast-paced\".'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_count = 0\n    for t in tokens:\n        if '-' in t and any(ch.isalpha() for ch in t):\n            hyphen_count += 1\n    return float(hyphen_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are four-digit year-like numbers (e.g., 1998, 2005), normalized by token count.'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    years = re.findall(r'\\b\\d{4}\\b', text)\n    return float(len(years)) / len(words)\n\n", "def feature(text: str) -> float:\n    'Density of academic/discourse transition words (however, moreover, therefore, thus, furthermore, consequently, hence), normalized by token count.'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    transitions = {'however', 'moreover', 'therefore', 'thus', 'furthermore', 'consequently', 'hence', 'nevertheless', 'nonetheless'}\n    count = sum(1 for w in words if w in transitions)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (semicolon count divided by sentence count, sentences >= 1).'\n    if not text:\n        return 0.0\n    semicolons = text.count(';')\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    return float(semicolons) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Average number of colons per sentence (colon count divided by sentence count, sentences >= 1), capturing list/introduction style.'\n    if not text:\n        return 0.0\n    colons = text.count(':')\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    return float(colons) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total token count (lowercased), a simple lexical diversity measure.'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words; high values indicate variable sentence sizing'\n    import re, math\n    if not text:\n        return 0.0\n    # Split on sentence end punctuation sequences\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', p)) for p in parts]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(var)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Proxy for explicit passive constructions including an auxiliary (was/were/is/are/has/had/been/being) near an -ed token, normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|being|has|had|have)\\b(?:\\s+\\w+){0,3}\\s+\\w+ed\\b', re.IGNORECASE)\n    matches = len(pattern.findall(text))\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(matches) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain a parenthetical four-digit year like (1999), a marker of citation-style academic writing'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\(\\s*\\d{4}\\s*\\)', text)\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(years)) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Ratio of quoted segments (single-quoted or double-quoted quoted-word/phrase patterns) to sentence count'\n    import re\n    if not text:\n        return 0.0\n    double = re.findall(r'\"\\s*[^\"]+?\\s*\"', text)\n    single = re.findall(r\"'\\s*[^']+?\\s*'\", text)\n    total_quoted = len(double) + len(single)\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(total_quoted) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Density of common function words (a small stopword subset) as fraction of all tokens; helps indicate connective/flow usage'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','of','to','a','in','is','that','it','for','on','with','as','are','was','be','by','this','an','or','which','from','at','not','but','have','has','had','will','may','might','can','could'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    sw_count = sum(1 for t in tokens if t in stopwords)\n    return float(sw_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Semicolon occurrences per sentence; semicolons often indicate clause complexity and academic style'\n    import re\n    if not text:\n        return 0.0\n    semi_count = text.count(';')\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(semi_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Indicator (1.0/0.0) for presence of an explicit section header or title-like prefix (e.g., \"Compare and Contrast:\", \"Introduction:\") near start of text'\n    import re\n    if not text:\n        return 0.0\n    # Look for a header line ending with ':' within the first 400 characters or any \"compare and contrast\" phrase\n    head_segment = text[:400]\n    if re.search(r'(?m)^[A-Za-z0-9 \\-]{1,60}:\\s*$', head_segment) or re.search(r'compare and contrast', text, re.IGNORECASE):\n        return 1.0\n    # also detect short uppercase heading followed by newline and content\n    if re.search(r'(?m)^[A-Z][A-Z0-9 ,\\-]{2,60}\\n', head_segment):\n        return 1.0\n    return 0.0\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (can, could, may, might, should, would, must) indicating hedging or instruction'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'should', 'would', 'must', 'shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like 4-digit years (1000-2099) or bracketed numeric citations like [1], indicating historical/academic framing'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+|\\[.+?\\]|\\d+', text)\n    word_count = max(1, len(re.findall(r'\\w+', text)))\n    years = re.findall(r'\\b(1[0-9]{3}|20[0-9]{2})\\b', text)\n    bracket_cites = re.findall(r'\\[\\s*\\d+\\s*\\]', text)\n    return float(len(years) + len(bracket_cites)) / float(word_count)\n\n\n", "def feature(text: str) -> float:\n    'Average number of colons/semicolons per sentence (indicator of clause/sub-clause complexity and academic style)'\n    if not text:\n        return 0.0\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    sep_count = text.count(':') + text.count(';')\n    return float(sep_count) / float(sentence_count)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of capitalized words that appear mid-sentence (likely proper nouns or domain-specific terms) \u2014 ignores words at sentence starts'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    total_words = len(words)\n    if total_words == 0:\n        return 0.0\n    mid_caps = 0\n    # iterate over capitalized word matches and check preceding context for sentence boundary\n    for m in re.finditer(r'\\b([A-Z][a-z]{1,}|[A-Z]{2,})\\b', text):\n        start = m.start()\n        # look back a short window to find last non-space character\n        context = text[:start].rstrip()\n        if not context:\n            # beginning of text -> sentence start\n            continue\n        last = context[-1]\n        if last in '.!?':\n            # likely sentence start\n            continue\n        # otherwise count as mid-sentence capitalized token\n        mid_caps += 1\n    return float(mid_caps) / float(total_words)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (proxy for function-word density and formality)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','a','an','in','on','at','of','and','or','but','to','for','with','from','by','as',\n        'is','are','was','were','be','been','being','that','which','who','whom','this','these','those','it','its'\n    }\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (ending with \"?\") \u2014 low in expository/encyclopedic prose'\n    if not text:\n        return 0.0\n    sentence_end_count = text.count('.') + text.count('!') + text.count('?')\n    if sentence_end_count == 0:\n        return 0.0\n    question_count = text.count('?')\n    return float(question_count) / float(sentence_end_count)\n", "def feature(text: str) -> float:\n    'Discourse-marker density: frequency of common academic/connective phrases (however, therefore, moreover, on the other hand) per sentence'\n    import re\n    if not text:\n        return 0.0\n    lc = text.lower()\n    markers = ['however', 'therefore', 'moreover', 'furthermore', 'in addition', 'on the other hand', 'conversely', 'nevertheless', 'as a result', 'in contrast']\n    count = 0\n    for m in markers:\n        # count word/phrase occurrences\n        count += len(re.findall(r'\\b' + re.escape(m) + r'\\b', lc))\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(count) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio: fraction of tokens that are first-person pronouns (I, we, my, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for t in tokens if t.lower() in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Citation-like density: counts of year-in-parentheses (e.g., (1999)), \"et al\", \"ibid\" per sentence (proxy for scholarly style)'\n    import re\n    if not text:\n        return 0.0\n    years = len(re.findall(r'\\(\\s*\\d{4}\\s*\\)', text))\n    etal = len(re.findall(r'\\bet al\\b', text, flags=re.IGNORECASE))\n    ibid = len(re.findall(r'\\bibid\\b', text, flags=re.IGNORECASE))\n    see = len(re.findall(r'\\bsee also\\b', text, flags=re.IGNORECASE))\n    total = years + etal + ibid + see\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(total) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Hyphenated-word ratio: fraction of whitespace-delimited tokens that contain an internal hyphen (e.g., long-term)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t and len(t.strip('-')) > 0)\n    return float(hyphen_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Function-word entropy (normalized): diversity of common function-word usage (higher = more even distribution among function words)'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    func_words = ['the','a','an','in','on','at','by','for','with','to','of','and','or','but','if','when','while','as','that','this','these','those']\n    freqs = {}\n    total = 0\n    for t in tokens:\n        if t in func_words:\n            freqs[t] = freqs.get(t, 0) + 1\n            total += 1\n    if total == 0:\n        return 0.0\n    # compute normalized entropy (divide by log(n) to get range 0..1)\n    n = len(freqs) if freqs else 1\n    ent = 0.0\n    for v in freqs.values():\n        p = v / total\n        ent -= p * (math.log(p) if p > 0 else 0.0)\n    norm = math.log(n) if n > 1 else 1.0\n    return float(ent / norm)\n\n", "def feature(text: str) -> float:\n    'Semicolon density per sentence: number of semicolons normalized by sentence count (proxy for clause-complex academic style)'\n    if not text:\n        return 0.0\n    semis = text.count(';')\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(semis) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, we, me, us, my, our, mine, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','we','me','us','my','our','mine','ours','myself','ourselves'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (semicolons indicate clause-level complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        # treat whole text as one sentence\n        sentences = [text.strip()]\n    semicolon_count = text.count(';')\n    return float(semicolon_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Average length (in characters) of the first word of each sentence (captures sentence openings like \"The\" vs \"Furthermore\")'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    first_word_lengths = []\n    for s in sentences:\n        m = re.search(r\"[A-Za-z']+\", s)\n        if m:\n            first_word_lengths.append(len(m.group(0)))\n        else:\n            first_word_lengths.append(0)\n    if not first_word_lengths:\n        return 0.0\n    return float(sum(first_word_lengths)) / len(first_word_lengths)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are very long words (length > 12), an extreme lexical-sophistication marker'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z]+\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 12)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Normalized lexical entropy over word frequencies (entropy/log2(V)) where V is number of unique word types; returns 0 for trivial texts'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    freqs = {}\n    for t in tokens:\n        freqs[t] = freqs.get(t, 0) + 1\n    V = len(freqs)\n    if V <= 1:\n        return 0.0\n    entropy = 0.0\n    for c in freqs.values():\n        p = c / n\n        entropy -= p * math.log2(p)\n    # normalize by maximum entropy log2(V)\n    return entropy / math.log2(V)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of the frequencies of the top-10 most common tokens (lowercased): measures repetition patterns'\n    import re, collections, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    counts = collections.Counter(tokens)\n    most_common = [c for _, c in counts.most_common(10)]\n    if not most_common:\n        return 0.0\n    mean = sum(most_common) / len(most_common)\n    if mean == 0:\n        return 0.0\n    variance = sum((c - mean) ** 2 for c in most_common) / len(most_common)\n    std = math.sqrt(variance)\n    return std / mean\n\n", "def feature(text: str) -> float:\n    'Distinct punctuation variety ratio: number of distinct punctuation characters divided by total punctuation characters (0 if none)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are uppercase abbreviations/acronyms (2-4 letters, all uppercase), indicative of technical writing'\n    import re\n    if not text:\n        return 0.0\n    acronyms = re.findall(r'\\b[A-Z]{2,4}\\b', text)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(len(acronyms)) / len(tokens)\n", "def feature(text: str) -> float:\n    'Density of 4-digit year-like numbers (1000-2100) per sentence to capture historical/academic references'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(1[0-9]{3}|20[0-9]{2}|2100)\\b', text)\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(years)) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Average number of colons and semicolons per sentence (captures clause-listing and formal punctuation use)'\n    if not text:\n        return 0.0\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    punct_count = text.count(':') + text.count(';')\n    return float(punct_count) / float(sent_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (small curated set) as a coarse lexical density proxy'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','of','to','a','in','is','that','it','for','on','as','with','by','an','be','are','was','were','this','which'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    sw_count = sum(1 for t in tokens if t in stopwords)\n    return float(sw_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that share the most common sentence-initial word (measures repetitive sentence starts)'\n    import re\n    if not text:\n        return 0.0\n    # Split sentences roughly\n    raw_sents = re.split(r'[.!?]+', text)\n    starts = []\n    for s in raw_sents:\n        s = s.strip()\n        if not s:\n            continue\n        m = re.match(r'\\W*([A-Za-z0-9\\'-]+)', s)\n        if m:\n            starts.append(m.group(1).lower())\n    if not starts:\n        return 0.0\n    from collections import Counter\n    most_common_count = Counter(starts).most_common(1)[0][1]\n    return float(most_common_count) / float(len(starts))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very long (more than 30 words), indicating high sentence complexity'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    long_count = 0\n    for s in sents:\n        words = re.findall(r'\\w+', s)\n        if len(words) > 30:\n            long_count += 1\n    return float(long_count) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in suffixes common to nominal-verb derivations (ize/ise/ate), indicating academic verb formation'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ize', 'ise', 'ate')\n    count = sum(1 for t in tokens if any(t.endswith(suf) and len(t) > len(suf) + 1 for suf in suffixes))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Normalized count of existential \"there is/are/was/were\" patterns per sentence as a marker of formal/expository framing'\n    import re\n    if not text:\n        return 0.0\n    pattern = re.compile(r'\\bthere\\s+(is|are|was|were)\\b', re.IGNORECASE)\n    matches = len(pattern.findall(text))\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(matches) / float(sent_count)\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), 0 if no sentence delimiters'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence end punctuation, but keep robust for no punctuation\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback: use entire text as one sentence if there are words\n        words = re.findall(r'\\w+', text)\n        return float(len(words)) if words else 0.0\n    words_per_sent = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not words_per_sent:\n        return 0.0\n    return float(sum(words_per_sent)) / float(len(words_per_sent))\n\n", "def feature(text: str) -> float:\n    'Adverb density: fraction of word tokens ending with \"ly\" (case-insensitive)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.lower().endswith('ly'))\n    return float(ly_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio: fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Quotation density: fraction of characters that are quotation marks or curly quotes (indicates dialogue or quoted material)'\n    if not text:\n        return 0.0\n    quote_chars = {'\"', \"\u201c\", \"\u201d\", \"\u2018\", \"\u2019\"}\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    count = sum(1 for c in text if c in quote_chars)\n    return float(count) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentence_count), 0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    sentence_boundaries = re.split(r'[.!?]+', text)\n    sentences = [s for s in sentence_boundaries if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Repeated token fraction: fraction of tokens that appear more than once (measure of redundancy/repetition)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    c = Counter(tokens)\n    repeated = sum(1 for t in tokens if c[t] > 1)\n    return float(repeated) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Numeric token fraction: fraction of word tokens that contain at least one digit (dates, numbers, measurements)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(num_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipses density: fraction of word tokens that are or contain an ellipsis (\"...\") \u2014 indicates trailing or dramatic prose'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    ell_count = sum(1 for t in tokens if '...' in t)\n    return float(ell_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Comma density: number of commas per word'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    num_words = len(words)\n    if num_words == 0:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count) / float(num_words)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that look like adverbs (ending in \"ly\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    advs = re.findall(r'\\b\\w+ly\\b', text.lower())\n    return float(len(advs)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Hapax ratio: fraction of word types that occur exactly once (hapax legomena / tokens)'\n    import re\n    from collections import Counter\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    freqs = Counter(tokens)\n    hapaxes = sum(1 for _, v in freqs.items() if v == 1)\n    # normalize by token count to keep scale comparable\n    return float(hapaxes) / float(total)\n\n", "def feature(text: str) -> float:\n    'Sensory-word density: fraction of tokens that are common sensory/imagery words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sensory = {'see','saw','look','listen','heard','sound','smell','taste','touch','feel',\n               'warm','cold','hot','bright','dark','glow','glowing','shimmer','shimmering',\n               'sparkle','salt','sea','ocean','wind','breeze','horizon','sun','sunset',\n               'sunrise','orange','turquoise','lighthouse','storm','rain','wave','rainy'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a pronoun (I, we, they, he, she, it, you, my, our, their)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    pronouns = {'i','we','they','he','she','it','you','my','our','their','his','her','its','me','us','them'}\n    starts = 0\n    for s in sentences:\n        # strip leading punctuation and spaces\n        s0 = re.sub(r'^[\\s\"\u201c\u201d\\'`(\\[{-]+', '', s)\n        first_word_match = re.match(r'\\b(\\w+)\\b', s0.lower() if s0 else '')\n        if first_word_match and first_word_match.group(1) in pronouns:\n            starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proxy for passive constructions: occurrences of auxiliary + past-participial \"-ed\" divided by sentence count'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    text_l = text.lower()\n    # match auxiliaries followed by a word ending in ed (simple heuristic)\n    matches = re.findall(r'\\b(?:was|were|is|are|been|being|be|had|has|have)\\s+\\w+ed\\b', text_l)\n    # sentence count\n    sentences = re.split(r'[.!?]+', text)\n    sentence_count = sum(1 for s in sentences if s.strip())\n    if sentence_count == 0:\n        return 0.0\n    return float(len(matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Modal verb density: fraction of tokens that are modal verbs (would, could, should, might, must, may, will, shall)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    modals = {'would','could','should','might','must','may','will','shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(total)\n", "def feature(text: str) -> float:\n    'Density of double-quote characters (indicator of direct speech/dialogue) per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_words = max(1, len(words))\n    quote_chars = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    return float(quote_chars) / total_words\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count (lexical diversity)'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if not words:\n        return 0.0\n    types = len(set(words))\n    return float(types) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ing\" (present participles/gerunds/progressive aspect)'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) > 3 and w.endswith('ing'))\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per sentence (indicates trailing thoughts/dramatic pauses)'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(ellipses) / sentences\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in \"ly\" (simple heuristic for adverb usage / descriptive style)'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) > 2 and w.endswith('ly'))\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Average length of punctuation clusters (sequences of non-alphanumeric, non-space chars)'\n    import re\n    if not text:\n        return 0.0\n    clusters = re.findall(r'[^A-Za-z0-9\\s]+', text)\n    if not clusters:\n        return 0.0\n    avg_len = sum(len(c) for c in clusters) / len(clusters)\n    return float(avg_len)\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation characters that are not followed by a space (punctuation spacing irregularity)'\n    import re\n    if not text:\n        return 0.0\n    punct_positions = [m.end() for m in re.finditer(r'[^A-Za-z0-9\\s]', text)]\n    if not punct_positions:\n        return 0.0\n    count_no_space = 0\n    for pos in punct_positions:\n        if pos < len(text) and text[pos] not in (' ', '\\n', '\\t', '\\r'):\n            count_no_space += 1\n    return float(count_no_space) / len(punct_positions)\n", "def feature(text: str) -> float:\n    'Proportion of words that are adverbs ending with \"ly\" (indicative of descriptive/flowery prose)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if w.endswith('ly') and len(w) > 3)\n    return float(ly_count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Adjacent-word alliteration ratio: fraction of adjacent word pairs whose initial letter is identical'\n    import re\n    if not text:\n        return 0.0\n    words = [w for w in re.findall(r\"\\b[a-zA-Z]\\w*\\b\", text) if len(w) > 0]\n    n = len(words)\n    if n < 2:\n        return 0.0\n    same_init = 0\n    for a, b in zip(words, words[1:]):\n        if a[0].lower() == b[0].lower():\n            same_init += 1\n    return float(same_init) / (n - 1)\n\n", "def feature(text: str) -> float:\n    'Density of subordinating conjunctions (although, because, while, as, since, though, unless, whereas, after, before, when) per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    subs = {'although','because','while','as','since','though','unless','whereas','after','before','when','if','until'}\n    count = sum(1 for w in words if w in subs)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a stock opening phrase (e.g., \"as the\", \"in a\", \"in the\", \"all across\", \"the old\")'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starters = ('as the', 'in a', 'in the', 'all across', 'the old', 'the angel', 'as she', 'as he', 'once upon', 'in a distant')\n    count = 0\n    for s in sentences:\n        s_low = s.lower()\n        for st in starters:\n            if s_low.startswith(st):\n                count += 1\n                break\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of words that are not in a small common-function-word set (a rough uncommon-word / lexical richness proxy)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    common = {\n        'the','of','and','to','a','in','that','is','it','for','on','with','as','was','he','she','they',\n        'at','by','an','be','have','has','had','not','are','from','or','which','but','this','his','her',\n        'their','were','will','would','I','you','we','our','my','me'\n    }\n    uncommon = sum(1 for w in words if w not in common)\n    return float(uncommon) / len(words)\n\n", "def feature(text: str) -> float:\n    'Density of em-dash or en-dash characters (\u2014, \u2013) per word, useful for detecting dramatic dash-heavy prose'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    word_count = len(words)\n    dashes = text.count('\u2014') + text.count('\u2013') + text.count('--')\n    if word_count == 0:\n        return float(dashes)\n    return float(dashes) / word_count\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio: fraction of words that are first-person pronouns (I, me, my, we, us, our)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Scene/setting noun density: fraction of words that match a curated set of setting/imagery nouns (sky, horizon, cliff, castle, stars, galaxy, town, hill, war, plague, ramparts)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    settings = {'sky','horizon','cliff','castle','stars','galaxy','town','hill','war','plague','plagues','ramparts','chest','heart','sun','dawn','dusk','edge','town','alien'}\n    count = sum(1 for w in words if w in settings)\n    return float(count) / len(words)\n", "def feature(text: str) -> float:\n    'Normalized count of ellipses (\"...\") occurrences per 100 words (captures poetic/hesitant style)'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    # word count robustly\n    import re\n    words = re.findall(r\"\\w+(?:'\\w+)?\", text)\n    wc = max(1, len(words))\n    return float(ellipses) / float(wc) * 100.0\n\n", "def feature(text: str) -> float:\n    'Density of simile-like constructions using the word \"like\" (counts \" like \" occurrences) per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\w+(?:'\\w+)?\", text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    like_matches = len(re.findall(r'\\blike\\b', text, flags=re.IGNORECASE))\n    return float(like_matches) / float(wc)\n\n", "def feature(text: str) -> float:\n    'Maximum unigram relative frequency: frequency of the most common word divided by total words (repetition indicator)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\w+(?:'\\w+)?\", text.lower())\n    if not words:\n        return 0.0\n    counts = Counter(words)\n    most_common = counts.most_common(1)[0][1]\n    return float(most_common) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\w+(?:'\\w+)?\", text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of sensory/descriptive words (small curated lexicon of sight/sound/smell/touch/taste terms) per word'\n    import re\n    if not text:\n        return 0.0\n    sensory = {'see','saw','sight','hear','heard','sound','smell','scent','taste','feel','felt','bright','dark','dusty','rust','silent','quiet','loud','bleak','serene','cold','warm','hot','wet','dry','gloomy','shimmer','shimmered'}\n    words = re.findall(r\"\\w+(?:'\\w+)?\", text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in sensory)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of scene/setting nouns from a curated list (town, castle, diner, hill, motorway, mirror, booth, ramparts, road) per word'\n    import re\n    if not text:\n        return 0.0\n    setting = {'town','castle','diner','hill','motorway','mirror','booth','ramparts','road','cars','trucks','countryside','landscape','home','castle','restaurant','diner','bridge','street'}\n    words = re.findall(r\"\\w+(?:'\\w+)?\", text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in setting)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of common irregular past-tense verbs (was, were, went, ran, saw, felt, said, took, came, began, had, made, knew, left) per word'\n    import re\n    if not text:\n        return 0.0\n    irregular = {'was','were','went','ran','saw','felt','said','took','came','began','had','made','knew','left','lost','found','brought','did','came','met','read'}\n    words = re.findall(r\"\\w+(?:'\\w+)?\", text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in irregular)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<= 4 words) \u2014 narrative passages often have lower short-sentence fraction'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    short = 0\n    for s in parts:\n        words = re.findall(r\"\\w+(?:'\\w+)?\", s)\n        if len(words) <= 4 and len(words) > 0:\n            short += 1\n    return float(short) / float(len(parts))\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the word \"as\" (case-insensitive)'\n    import re\n    if not text or text.strip() == '':\n        return 0.0\n    # Split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts_with_as = 0\n    for s in sentences:\n        first_word_match = re.match(r\"^\\s*([A-Za-z']+)\", s)\n        if first_word_match and first_word_match.group(1).lower() == 'as':\n            starts_with_as += 1\n    return float(starts_with_as) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of word tokens ending with \"ing\" (gerund/progressive marker) among all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text)\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if len(t) > 3 and t.lower().endswith('ing'))\n    return float(ing_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of sensory vocabulary (see, hear, feel, taste, smell, touch, blink, listen, gaze) per word'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    sensory = {'see', 'saw', 'seen', 'look', 'looked', 'look', 'looked', 'see', 'hear', 'heard', 'listen', 'listened', 'feel', 'felt', 'touch', 'touched', 'smell', 'smelled', 'taste', 'tasted', 'blink', 'blinked', 'gaze', 'gazed', 'stare', 'stared', 'watch', 'watched'}\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Indicator-like score (0.0 or 1.0) if a short title-like line exists (line with 2-10 words and mostly Titlecase)'\n    if not text:\n        return 0.0\n    lines = [l.strip() for l in text.splitlines() if l.strip()]\n    for line in lines:\n        # ignore lines that look like sentences with punctuation\n        if any(p in line for p in '.:?!'):\n            continue\n        parts = line.split()\n        if 2 <= len(parts) <= 10:\n            titlecase_count = sum(1 for w in parts if w.istitle())\n            if titlecase_count / float(len(parts)) >= 0.6:\n                return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Relative pronoun density (that, which, who, whom, whose) per word as a proxy for relative/complex clauses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    rel = {'that', 'which', 'who', 'whom', 'whose'}\n    count = sum(1 for t in tokens if t in rel)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique lowercased word types divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = set(tokens)\n    return float(len(unique)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a subordinating conjunction (as, while, when, if, although, because, since, though, after, before)'\n    import re\n    if not text:\n        return 0.0\n    subs = {'as', 'while', 'when', 'if', 'although', 'because', 'since', 'though', 'after', 'before', 'until'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        m = re.match(r\"^\\s*([A-Za-z']+)\", s)\n        if m and m.group(1).lower() in subs:\n            starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of occurrences of the document-first bigram (first two words) among all bigrams (repetition of opening phrase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    first_bigram = (tokens[0], tokens[1])\n    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    if not bigrams:\n        return 0.0\n    count = sum(1 for b in bigrams if b == first_bigram)\n    return float(count) / float(len(bigrams))\n", "def feature(text: str) -> float:\n    'Repeated trigram fraction: proportion of word-trigrams that occur more than once (0 if none)'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 3:\n        return 0.0\n    trigrams = [' '.join(words[i:i+3]) for i in range(len(words)-2)]\n    from collections import Counter\n    c = Counter(trigrams)\n    repeated_count = sum(count for trigram, count in c.items() if count > 1)\n    # measure repeated trigram occurrences relative to total trigrams\n    return float(repeated_count) / float(len(trigrams))\n\n", "def feature(text: str) -> float:\n    'Title-like short-line proportion: fraction of lines that are <=6 words and appear Title-Cased (useful for headings)'\n    import re\n    lines = [line.strip() for line in text.splitlines() if line.strip() != '']\n    if not lines:\n        return 0.0\n    title_like = 0\n    for line in lines:\n        words = re.findall(r\"[A-Za-z']+\", line)\n        if 0 < len(words) <= 6:\n            # consider a word title-cased if its first letter is uppercase\n            if all(w[0].isupper() for w in words if w):\n                title_like += 1\n    return float(title_like) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'First-person pronoun density: fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours','myself','ourselves'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Sentence length variance: variance of sentence lengths in words (0 if fewer than 2 sentences)'\n    import re, math\n    # split on sentence-ending punctuation (keep robustness)\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip() != '']\n    if len(sentences) < 2:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if len(lengths) < 2:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Content-word density: fraction of tokens that are not common stopwords (simple stopword list)'\n    import re\n    stopwords = {\n        'the','a','an','and','or','but','if','then','else','when','while','of','in','on','at','to','for',\n        'with','by','from','is','was','were','be','been','are','that','this','these','those','it','its','as'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    content = sum(1 for t in tokens if t not in stopwords)\n    return float(content) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Passive/perfect snapshot: fraction of sentences containing patterns like \"was .*ed\", \"were .*ed\", \"has .*ed\" as a proxy for passive or perfect constructions'\n    import re\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip() != '']\n    if not sentences:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|has|have|had|been)\\s+\\w+ed\\b', re.IGNORECASE)\n    matched = sum(1 for s in sentences if pattern.search(s))\n    return float(matched) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Paragraph length variance normalized: variance of paragraph lengths (in words) divided by (mean+1) to stabilize; 0 if <=1 paragraphs'\n    import re, math\n    # paragraphs separated by one or more blank lines\n    paras = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip() != '']\n    if len(paras) <= 1:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', p)) for p in paras]\n    mean = sum(lengths) / len(lengths)\n    var = sum((L - mean) ** 2 for L in lengths) / len(lengths)\n    return float(var) / (mean + 1.0)\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total words (lexical diversity)'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    uniq = len(set(words))\n    return float(uniq) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of semicolons and colons per word (formal/complex sentence marker)'\n    import re\n    punct_count = text.count(';') + text.count(':')\n    words = re.findall(r'\\w+', text)\n    denom = max(1, len(words))\n    return float(punct_count) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Average words per clause, where clauses are split on commas, semicolons, and colons'\n    import re\n    clauses = re.split(r'[,:;]', text)\n    counts = [len(re.findall(r'\\w+', c)) for c in clauses if c.strip()]\n    if not counts:\n        return 0.0\n    return float(sum(counts)) / float(len(counts))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a common conjunction/adverb (and, but, so, then, however, also, yet, or)'\n    import re\n    starts = []\n    # Split on sentence-ending punctuation (keep robustness for no punctuation)\n    raw_sents = re.split(r'[.!?]+', text)\n    for s in raw_sents:\n        s = s.strip()\n        if not s:\n            continue\n        m = re.match(r'\\b([A-Za-z]+)\\b', s)\n        if m:\n            starts.append(m.group(1).lower())\n    if not starts:\n        return 0.0\n    conj = {'and', 'but', 'so', 'then', 'however', 'also', 'yet', 'or'}\n    count = sum(1 for w in starts if w in conj)\n    return float(count) / float(len(starts))\n\n", "def feature(text: str) -> float:\n    'Fraction of interior words (not first in sentence) that are Titlecase (capitalized like proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    interior_words = 0\n    titlecase_interior = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        words = re.findall(r'\\b\\w+\\b', s)\n        if len(words) <= 1:\n            continue\n        for w in words[1:]:\n            interior_words += 1\n            if w[0].isupper() and (len(w) == 1 or w[1:].islower()):\n                titlecase_interior += 1\n    if interior_words == 0:\n        return 0.0\n    return float(titlecase_interior) / float(interior_words)\n\n", "def feature(text: str) -> float:\n    'Density of emphatic repeated punctuation sequences (runs of !!! or ???) per word'\n    import re\n    runs = re.findall(r'!{2,}', text) + re.findall(r'\\?{2,}', text)\n    run_count = len(runs)\n    words = re.findall(r'\\w+', text)\n    denom = max(1, len(words))\n    return float(run_count) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Density of emotive/evocative words from a small affect lexicon per word'\n    import re\n    lex = {'blood', 'dark', 'bleak', 'sinister', 'silence', 'plague', 'fear', 'love', 'hate', 'cry', 'scream', 'warning', 'caution', 'angry', 'happy', 'sad', 'bleed', 'ghost'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in lex)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the same first word as the previous sentence (repetitive sentence openings)'\n    import re\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sents) < 2:\n        return 0.0\n    first_words = []\n    for s in sents:\n        m = re.match(r'\\b([A-Za-z]+)\\b', s)\n        first_words.append(m.group(1).lower() if m else '')\n    repeats = sum(1 for i in range(1, len(first_words)) if first_words[i] == first_words[i-1] and first_words[i] != '')\n    return float(repeats) / float(len(first_words) - 1)\n", "def feature(text: str) -> float:\n    'Fraction of modal verbs (can/could/may/might/must/shall/should/will/would) among tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Density of words ending with common adjective suffixes (ous, ful, less, ive, able, al, ic)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ous', 'ful', 'less', 'ive', 'able', 'al', 'ic')\n    count = 0\n    for t in tokens:\n        for s in suffixes:\n            if t.endswith(s) and len(t) > len(s) + 1:\n                count += 1\n                break\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Number of newline characters per word (captures poetic/line-broken formatting)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text)\n    word_count = len(tokens)\n    if word_count == 0:\n        return 0.0\n    newline_count = text.count('\\n')\n    return float(newline_count) / float(word_count)\n\n\n", "def feature(text: str) -> float:\n    'Density of em-dashes or double-hyphens (\u2014 or --) per word (captures parenthetical/dash-heavy style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text)\n    word_count = len(tokens)\n    if word_count == 0:\n        return 0.0\n    count = text.count('\u2014') + text.count('--')\n    return float(count) / float(word_count)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sensory verbs (see, saw, hear, heard, feel, felt, smell, taste, looked, listened) among tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    sensory = {'see', 'saw', 'seen', 'hear', 'heard', 'listen', 'listened', 'feel', 'felt', 'smell', 'smelled', 'taste', 'tasted', 'look', 'looked', 'touch', 'touched'}\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Density of common academic/expository keywords (significant, contributed, factors, research, analysis, therefore, thus)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    academic = {'significant', 'significantly', 'contributed', 'contribute', 'industrialization', 'industrial', 'decades', 'analysis', 'factors', 'movement', 'study', 'research', 'evidence', 'therefore', 'thus', 'however', 'resulted'}\n    count = sum(1 for t in tokens if t in academic)\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Proportion of punctuation characters that occur in repeated sequences (e.g., !!, --, ..., ??) \u2014 captures emphasis or hesitancy'\n    import re\n    if not text:\n        return 0.0\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    repeated = len(re.findall(r'([^\\w\\s])\\1+', text))\n    return float(repeated) / float(total_punct)\n", "def feature(text: str) -> float:\n    'Estimated fraction of sentences showing a passive-voice-like pattern (was/were/... + past-participle ending in -ed)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    passive_pattern = re.compile(r'\\b(?:was|were|is|are|been|be|had been|has been|have been|was being|were being)\\s+\\w+ed\\b', re.I)\n    passive_count = sum(1 for s in sentences if passive_pattern.search(s))\n    return float(passive_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Binary-ish indicator (0.0 or 1.0) that the text begins with a short title/heading line (like a book/story title)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines()]\n    # find first non-empty line and next non-empty line\n    first_idx = None\n    for i, ln in enumerate(lines):\n        if ln.strip():\n            first_idx = i\n            break\n    if first_idx is None:\n        return 0.0\n    first_line = lines[first_idx].strip()\n    # find next non-empty line after first\n    second_line = ''\n    for ln in lines[first_idx+1:]:\n        if ln.strip():\n            second_line = ln.strip()\n            break\n    # title-like if first line short (<7 words) and second line exists and begins with an uppercase letter (start of paragraph)\n    words_first = re.findall(r'\\w+', first_line)\n    if len(words_first) <= 6 and second_line and second_line[0].isupper():\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common dialogue/speech verbs (said, asked, replied, whispered, shouted, murmured)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    speech_verbs = {'said', 'asked', 'replied', 'whispered', 'shouted', 'murmured', 'cried', 'answered', 'yelled'}\n    count = sum(1 for t in tokens if t in speech_verbs)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are hapax legomena (occur exactly once in the text)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    freqs = Counter(tokens)\n    hapax = sum(1 for t in tokens if freqs[t] == 1)\n    return float(hapax) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of em-dash or double-hyphen usage: count of \"\u2014\" or \"--\" per word (captures dramatic punctuation)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = max(1, len(words))\n    dash_count = text.count('\u2014') + text.count('--')\n    return float(dash_count) / word_count\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are sensory words (see/hear/feel/smell/taste and common variants and sensory adjectives)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    sensory = {'see', 'saw', 'seen', 'look', 'looked', 'hear', 'heard', 'listen', 'listened', 'smell', 'smelled', 'taste', 'tasted', 'feel', 'felt', 'touch', 'touched', 'loud', 'quiet', 'silent', 'soft', 'bright', 'dark', 'acrid', 'sweet'}\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating conjunction (and, but, or, so, yet, for, nor) \u2014 indicates sentence-initial conjunction usage'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    conj = {'and', 'but', 'or', 'so', 'yet', 'for', 'nor'}\n    starts = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s.lower())\n        if words and words[0] in conj:\n            starts += 1\n    return float(starts) / len(sentences)\n", "def feature(text: str) -> float:\n    'Density of the auxiliary \"had\" (common in past-perfect narrative) as tokens per word'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    return words.count('had') / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (i, me, we, us, my, our, mine, ours)'\n    import re\n    pronouns = {'i','me','we','us','my','our','mine','ours'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    return sum(1 for w in words if w in pronouns) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending with common adjective-like suffixes (e.g., -ous, -ive, -able, -less, -ful, -ic, -al, -y) among tokens of length>=4'\n    import re\n    suffixes = ('ous','ive','able','less','ful','ic','al','y','ent','ant')\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    long_words = [w for w in words if len(w) >= 4]\n    if not long_words:\n        return 0.0\n    count = 0\n    for w in long_words:\n        for s in suffixes:\n            if w.endswith(s):\n                count += 1\n                break\n    return count / float(len(long_words))\n\n", "def feature(text: str) -> float:\n    'Density of semicolons and colons per word (counts of \";\" and \":\" divided by word count)'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    punct_count = text.count(';') + text.count(':')\n    return punct_count / float(wc)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens equal to the coordinating conjunction \"and\" (captures list/compound style)'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    return words.count('and') / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average number of words per clause, where clauses are split on .,!?;: (returns 0 for no clauses)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    clauses = [c.strip() for c in re.split(r'[.!?;:]+', text) if c.strip()]\n    if not clauses:\n        return 0.0\n    lens = []\n    for c in clauses:\n        words = re.findall(r'\\b\\w+\\b', c)\n        lens.append(len(words))\n    if not lens:\n        return 0.0\n    return sum(lens) / float(len(lens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are third-person possessive pronouns (their, theirs, his, her, hers, its)'\n    import re\n    poss = {'their','theirs','his','her','hers','its'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    return sum(1 for w in words if w in poss) / float(len(words))\n", "def feature(text: str) -> float:\n    'Density of the definite article \"the\" per token (may capture generic/over-descriptive phrasing)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    the_count = sum(1 for w in words if w == 'the')\n    return float(the_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with common literary/complex suffixes (ous,ive,able,ic,al,ant,ent,ful,less)'\n    import re\n    if not text:\n        return 0.0\n    suffixes = ('ous','able','ible','ive','ic','al','ant','ent','ful','less')\n    words = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        for s in suffixes:\n            if w.endswith(s) and len(w) > len(s) + 1:\n                count += 1\n                break\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating conjunction or typical clause-starter (and, but, so, then, also, however, thus)'\n    import re\n    if not text:\n        return 0.0\n    sent_split = re.split(r'(?<=[.!?])\\s+|\\n{2,}', text.strip())\n    sents = [s.strip() for s in sent_split if s and s.strip()]\n    if not sents:\n        return 0.0\n    starters = {'and','but','so','then','also','however','thus','yet'}\n    count = 0\n    for s in sents:\n        words = re.findall(r'\\w+', s.lower())\n        if words and words[0] in starters:\n            count += 1\n    return float(count) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per word (captures listy/compound sentence style)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = len(words)\n    semi = text.count(';')\n    return float(semi) / float(word_count if word_count > 0 else 1)\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words (lower = unusually uniform sentence length)'\n    import re, math\n    if not text:\n        return 0.0\n    sents = re.split(r'(?<=[.!?])\\s+|\\n{2,}', text.strip())\n    lengths = []\n    for s in sents:\n        s = s.strip()\n        if not s:\n            continue\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n", "def feature(text: str) -> float:\n    'Fraction of occurrences of \" by <agent>\" patterns per word (proxy for passive voice constructions that explicitly name the agent)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = len(words)\n    matches = re.findall(r'\\bby\\s+\\w+', text.lower())\n    return float(len(matches)) / float(word_count if word_count > 0 else 1)\n\n", "def feature(text: str) -> float:\n    'Fraction of commas that are followed by a lowercase letter (comma+continuation vs comma+sentence break/titlecase) \u2014 signals clause continuation style'\n    import re\n    if not text:\n        return 0.0\n    total_commas = text.count(',')\n    if total_commas == 0:\n        return 0.0\n    # find patterns like ', ' followed by a lowercase letter\n    cont = len(re.findall(r',\\s+[a-z]', text))\n    return float(cont) / float(total_commas)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<=5 words) \u2014 short-sentence usage frequency'\n    import re\n    if not text:\n        return 0.0\n    sents = re.split(r'(?<=[.!?])\\s+|\\n{2,}', text.strip())\n    sents = [s.strip() for s in sents if s and s.strip()]\n    if not sents:\n        return 0.0\n    short_count = 0\n    for s in sents:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 5:\n            short_count += 1\n    return float(short_count) / float(len(sents))\n", "def feature(text: str) -> float:\n    'Lexical diversity: number of unique lowercased word tokens divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain at least one comma (indicates syntactic complexity and clause embedding)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    count = 0\n    for s in sents:\n        if ',' in s:\n            count += 1\n    return float(count) / len(sents)\n\n", "def feature(text: str) -> float:\n    'Binary indicator (0.0 or 1.0) for the exact phrase \"once upon a time\" (case-insensitive) appearing in the text'\n    if not text:\n        return 0.0\n    return 1.0 if 'once upon a time' in text.lower() else 0.0\n\n", "def feature(text: str) -> float:\n    'Variance of word lengths (measures uniformity vs varied word-length usage); returns 0.0 if insufficient words'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text)\n    n = len(words)\n    if n <= 1:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences whose first token is an adverbial-looking word ending with \"ly\" (sentence-initial adverb usage)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    count = 0\n    for s in sents:\n        words = re.findall(r\"\\b[^\\s]+\\b\", s)\n        if not words:\n            continue\n        first = re.sub(r'^[^A-Za-z]+|[^A-Za-z]+$', '', words[0]).lower()\n        if first.endswith('ly') and len(first) > 2:\n            count += 1\n    return float(count) / len(sents)\n", "def feature(text: str) -> float:\n    'Density of subordinating conjunctions (because, although, while, since, unless, etc.) per sentence'\n    import re\n    if not text:\n        return 0.0\n    conj_pattern = r'\\b(because|although|though|while|since|whereas|unless|after|before|provided|if|when|whenever)\\b'\n    matches = re.findall(conj_pattern, text, flags=re.IGNORECASE)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain digits (captures numeric specificity, measurements, years, weights)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[^\\s]+\\b', text)\n    if not words:\n        return 0.0\n    num = sum(1 for w in words if re.search(r'\\d', w))\n    return float(num) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Pronoun diversity: number of distinct pronoun forms divided by total pronoun occurrences (0 if no pronouns)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','we','you','he','she','they','it','me','him','her','us','them','my','your','his','hers','their','its','our','mine','yours','theirs','herself','himself','themselves','yourself','yourselves'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    pronoun_tokens = [t for t in tokens if t in pronouns]\n    if not pronoun_tokens:\n        return 0.0\n    unique = len(set(pronoun_tokens))\n    return float(unique) / float(len(pronoun_tokens))\n\n", "def feature(text: str) -> float:\n    'Simile-like phrasing density (counts \"like X\" and \"as ... as\" patterns per sentence)'\n    import re\n    if not text:\n        return 0.0\n    like_matches = re.findall(r'\\blike\\s+\\w+', text, flags=re.IGNORECASE)\n    as_matches = re.findall(r'\\bas\\s+\\w+(?:\\s+\\w+){0,6}\\s+as\\b', text, flags=re.IGNORECASE)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(like_matches) + len(as_matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Proportion of repeated trisets (3-word sequences) \u2014 measures phrase repetition/redundancy'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"\\b[\\w']+\\b\", text)]\n    if len(tokens) < 3:\n        return 0.0\n    trigrams = [' '.join(tokens[i:i+3]) for i in range(len(tokens)-2)]\n    total = len(trigrams)\n    from collections import Counter\n    c = Counter(trigrams)\n    repeated = sum(1 for k,v in c.items() if v > 1)\n    return float(repeated) / float(total)\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that end with \"ing\" (proxy for progressive aspect / gerunds)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) >= 4 and t.endswith('ing'))\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Density of past-progressive constructions (occurrences of \"was|were\" followed by an -ing word) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text)\n    denom = max(1, len(tokens))\n    matches = re.findall(r'\\b(?:was|were)\\s+[A-Za-z]+ing\\b', text, flags=re.I)\n    return float(len(matches)) / denom\n\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that are long (more than 20 words) \u2014 indicates long, descriptive sentences'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence delimiters\n    raw_sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in raw_sentences if s and s.strip()]\n    if not sentences:\n        return 0.0\n    long_count = 0\n    for s in sentences:\n        words = re.findall(r\"\\w+\", s)\n        if len(words) > 20:\n            long_count += 1\n    return float(long_count) / len(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Maximum single-token frequency divided by total tokens (measures repetition / token burstiness)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    max_freq = max(counts.values())\n    return float(max_freq) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are sensory/perceptual verbs (see, hear, feel, smell, taste, look, watch, listen, observe variants)'\n    import re\n    if not text:\n        return 0.0\n    sensory = {'see','saw','seen','see','look','looked','looked','gaze','gazed','stare','stared',\n               'hear','heard','listen','listened','listen','feel','felt','touch','touched',\n               'smell','smelled','smelt','smell','taste','tasted','observe','observed','watch','watched'}\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (would, could, should, might, must, may, can, will, shall)'\n    import re\n    if not text:\n        return 0.0\n    modals = {'would','could','should','might','must','may','can','will','shall'}\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Average number of characters between commas (mean segment length when splitting on commas); 0 if no commas'\n    if not text:\n        return 0.0\n    if ',' not in text:\n        return 0.0\n    segments = [seg.strip() for seg in text.split(',')]\n    lengths = [len(seg) for seg in segments if seg is not None]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are sensory words (see, look, hear, feel, smell, taste, touch and variants)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    senses = {'see','saw','seen','look','looked','gaze','gazed','watch','watched',\n              'hear','heard','listen','listened','feel','felt','touch','touched',\n              'smell','smelt','smelled','taste','tasted','observe','observed','perceive','perceived'}\n    count = sum(1 for w in words if w in senses)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen (captures hyphenated compounds like Martian-born)'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of words that look like descriptive adjectives by common suffixes (y, ous, ive, ent, ant, al, ful, less, able, ary)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[A-Za-z]{3,}\\b', text.lower())\n    if not words:\n        return 0.0\n    suffixes = ('y','ous','ive','ent','ant','al','ful','less','able','ary')\n    count = 0\n    for w in words:\n        for s in suffixes:\n            if w.endswith(s):\n                count += 1\n                break\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proxy for passive-voice patterns: count of (was|were|is|are|be|been|being) followed by an -ed word per sentence'\n    import re\n    if not text:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being)\\s+\\w+ed\\b', flags=re.I)\n    matches = len(pattern.findall(text))\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(matches) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Number of distinct sensory modalities mentioned (see/hear/feel/smell/taste/touch) normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    senses_map = {\n        'sight': r'\\b(?:see|saw|seen|look|gaze|watch|observe)\\b',\n        'hearing': r'\\b(?:hear|heard|listen|listened)\\b',\n        'touch': r'\\b(?:feel|felt|touch|touched)\\b',\n        'smell': r'\\b(?:smell|smelled|smelt)\\b',\n        'taste': r'\\b(?:taste|tasted)\\b'\n    }\n    found = 0\n    lower = text.lower()\n    for pat in senses_map.values():\n        if re.search(pat, lower):\n            found += 1\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(found) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Proportion of the text taken up by the opening clause (words before the first comma or sentence end)'\n    import re\n    if not text:\n        return 0.0\n    total_words = len(re.findall(r'\\b\\w+\\b', text))\n    if total_words == 0:\n        return 0.0\n    m = re.search(r'^[^,\\.!\\?]+', text.strip())\n    opening = m.group(0) if m else ''\n    opening_words = len(re.findall(r'\\b\\w+\\b', opening))\n    return float(opening_words) / float(total_words)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain any digit (captures explicit durations, numbers, or measurements)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that contain a hyphen (captures hyphenated compounds like \"7-month\")'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the word \"the\" (case-insensitive), indicating a definite-article narrative opening'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences by sentence-ending punctuation, preserve non-empty\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts_with_the = 0\n    for s in sentences:\n        m = re.search(r'\\b\\w+\\b', s)\n        if m and m.group(0).lower() == 'the':\n            starts_with_the += 1\n    return float(starts_with_the) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Density of common past auxiliaries (\"was\",\"were\",\"had\") per token as a proxy for past-tense narrative framing'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    auxiliaries = {'was', 'were', 'had'}\n    count = sum(1 for w in words if w in auxiliaries)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that match a small emotion word lexicon (surprised, envy, powerful, catastrophic, etc.)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    lex = {'surprise','surprised','surprising','envy','envious','jealous','powerful','power','catastrophic','catastrophe','fear','afraid','terrified','anxious','sorrow','sad','anger','angry','joy','happy'}\n    count = sum(1 for w in words if w in lex)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences containing temporal adverbs/markers (after, today, then, later, yesterday, years, months)'\n    import re\n    if not text:\n        return 0.0\n    markers = {'after','today','then','later','yesterday','tomorrow','now','previously','before','years','year','months','month'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    hit = 0\n    for s in sentences:\n        words = set(re.findall(r'\\w+', s.lower()))\n        if words & markers:\n            hit += 1\n    return float(hit) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Average adjacent-sentence Jaccard similarity (word-level). Returns 0.0 if fewer than 2 sentences.'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences and create sets of words\n    raw_sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(raw_sentences) < 2:\n        return 0.0\n    sets = [set(re.findall(r'\\w+', s.lower())) for s in raw_sentences]\n    sims = []\n    for a, b in zip(sets, sets[1:]):\n        if not a and not b:\n            sims.append(0.0)\n            continue\n        inter = a & b\n        union = a | b\n        if not union:\n            sims.append(0.0)\n        else:\n            sims.append(len(inter) / float(len(union)))\n    if not sims:\n        return 0.0\n    return sum(sims) / float(len(sims))\n\n", "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths in words (0.0 if fewer than 2 sentences), captures sentence-length variability'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if len(lengths) < 2:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    var = sum((L - mean) ** 2 for L in lengths) / float(len(lengths))\n    return math.sqrt(var)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the definite article \"the\" (case-insensitive)'\n    sentences = re.split(r'[.!?]+', text)\n    sent_tokens = []\n    for s in sentences:\n        s = s.strip()\n        if s:\n            tokens = re.findall(r'\\w+', s, flags=re.UNICODE)\n            sent_tokens.append(tokens)\n    if not sent_tokens:\n        return 0.0\n    starts = 0\n    for tokens in sent_tokens:\n        if tokens and tokens[0].lower() == 'the':\n            starts += 1\n    return float(starts) / float(len(sent_tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of word trigrams that are repeated at least once (repetition density of short n-grams)'\n    words = [w.lower() for w in re.findall(r'\\w+', text)]\n    if len(words) < 3:\n        return 0.0\n    trigrams = [' '.join(words[i:i+3]) for i in range(len(words)-2)]\n    total = len(trigrams)\n    seen = {}\n    for t in trigrams:\n        seen[t] = seen.get(t, 0) + 1\n    repeated = sum(1 for t in trigrams if seen.get(t, 0) > 1)\n    return float(repeated) / float(total) if total > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Approximate character-level entropy (bits) of the text, using all characters except whitespace'\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    freq = {}\n    for c in chars:\n        freq[c] = freq.get(c, 0) + 1\n    total = float(len(chars))\n    import math\n    entropy = 0.0\n    for v in freq.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    return float(entropy)\n\n", "def feature(text: str) -> float:\n    'Ratio of first sentence length (words) to average sentence length (first_sentence_words/(avg_sentence_words+eps))'\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    first_len = 0\n    if sentences:\n        first_len = len(re.findall(r'\\w+', sentences[0]))\n        avg_len = sum(len(re.findall(r'\\w+', s)) for s in sentences) / float(len(sentences)) if sentences else len(words)\n    else:\n        # treat whole text as one sentence\n        first_len = len(words)\n        avg_len = len(words)\n    eps = 1e-9\n    return float(first_len) / (float(avg_len) + eps)\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that begin with a dash or em-dash (common in dialogue formatting)'\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    dash_lines = 0\n    for ln in lines:\n        s = ln.lstrip()\n        if s.startswith('-') or s.startswith('\u2014'):\n            dash_lines += 1\n    return float(dash_lines) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, our, us), case-insensitive'\n    words = re.findall(r\"\\w+\", text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours', \"i'm\", \"im\", \"i've\", \"i'd\", \"i'll\"}\n    count = 0\n    for w in words:\n        if w in first_person:\n            count += 1\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that are common color words (proxy for vivid/colorful imagery)'\n    colors = {'red','blue','green','yellow','black','white','brown','grey','gray','pink','orange','purple','violet','indigo','gold','silver','crimson','scarlet','beige','teal','cyan','magenta'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in colors)\n    return float(count) / float(len(words))\n", "def feature(text: str) -> float:\n    'Fraction of lines that look like short title lines (1-6 words, each word title-cased), e.g. \"The Colors Fade\"'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip() != '']\n    if not lines:\n        return 0.0\n    title_like = 0\n    for ln in lines:\n        tokens = re.findall(r'\\b\\w+\\b', ln)\n        if 1 <= len(tokens) <= 6:\n            alpha_tokens = [t for t in tokens if any(c.isalpha() for c in t)]\n            if alpha_tokens and all(t[0].isupper() for t in alpha_tokens):\n                title_like += 1\n    return float(title_like) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    fp = {'i','me','my','mine','myself','we','us','our','ours','ourselves'}\n    count = sum(1 for w in words if w in fp)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Normalized frequency of internal-thought markers like \"I thought\" or \"thought to myself\" (counts / words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    n_words = max(1, len(words))\n    # match pronoun ... thought within a short window and \"thought to myself\"\n    pattern = re.compile(r'\\b(?:i|we|he|she|they|you|it|someone|somebody)\\b(?:\\s+\\w+){0,4}\\s+thought\\b', re.I)\n    m1 = len(pattern.findall(text))\n    m2 = len(re.findall(r'\\bthought\\s+to\\s+myself\\b', text, re.I))\n    total = m1 + m2\n    return float(total) / float(n_words)\n\n", "def feature(text: str) -> float:\n    'Ratio of common past-tense or narrative auxiliaries (was, were, had, did, said, felt, seemed, became) per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    n = max(1, len(words))\n    pattern = re.compile(r'\\b(?:was|were|had|did|said|felt|seemed|became|came|went|were)\\b', re.I)\n    matches = len(pattern.findall(text))\n    return float(matches) / float(n)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with coordinating conjunctions (and, but, so, yet, or) \u2014 indicates sentence chaining style'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    conj = {'and','but','so','yet','or'}\n    starts = 0\n    for s in sentences:\n        parts = re.findall(r'\\w+', s)\n        if parts:\n            if parts[0].lower() in conj:\n                starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per word (captures clause-complex prose typical of literary style)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    n = max(1, len(words))\n    sc = text.count(';')\n    return float(sc) / float(n)\n\n", "def feature(text: str) -> float:\n    'Normalized count of simile-like patterns (\"like a/an X\", \"as X as\") per word, indicating figurative language'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    n = max(1, len(words))\n    like_matches = len(re.findall(r'\\blike an? \\w+\\b', text, re.I))\n    as_matches = len(re.findall(r'\\bas \\s+\\w+\\s+as\\b', text, re.I))\n    return float(like_matches + as_matches) / float(n)\n\n", "def feature(text: str) -> float:\n    'Fraction of alphabetic tokens ending with \"y\" and length>=3 (captures descriptive adjectives such as \"distant\", \"ordinary\" ends-with-y pattern)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'[A-Za-z]+', text)\n    if not tokens:\n        return 0.0\n    y_count = sum(1 for t in tokens if len(t) >= 3 and t.lower().endswith('y'))\n    return float(y_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a common preposition (in, on, at, during, within, under, above, across, into, among, between, along, beneath, inside, outside, over)'\n    import re\n    preps = {'in','on','at','during','within','under','above','across','into','among','between','along','beneath','inside','outside','over'}\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s and s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.match(r'\\s*([A-Za-z]+)', s)\n        if m:\n            if m.group(1).lower() in preps:\n                count += 1\n    return float(count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are past-tense auxiliaries like \"had\", \"was\", \"were\", \"did\" (simple proxy for past narration)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    aux = {'had','was','were','did'}\n    count = sum(1 for t in tokens if t in aux)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that end with common adjective suffixes (ous, ive, al, ic, ent, ant, able, ible, ary, ory, ish) indicating descriptive style'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ous','ive','al','ic','ent','ant','able','ible','ary','ory','ish')\n    count = 0\n    for t in tokens:\n        if len(t) > 3:\n            for s in suffixes:\n                if t.endswith(s):\n                    count += 1\n                    break\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain at least one comma (indicator of multi-clause, descriptive sentence structure)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s and s.strip()]\n    if not sentences:\n        return 0.0\n    count = sum(1 for s in sentences if ',' in s)\n    return float(count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with the bigram \"in a\" (case-insensitive) capturing common narrative openings'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s and s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        if s.lstrip().lower().startswith('in a ' ) or s.lstrip().lower().startswith('in an '):\n            count += 1\n    return float(count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are Titlecase (first letter uppercase, rest lowercase) \u2014 proxy for many named entities / invented names'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) > 1 and t.istitle() and not t.isupper():\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are not in a small common-word list (proxy for lexical novelty / rarer vocabulary)'\n    import re\n    if not text:\n        return 0.0\n    common = {'the','a','an','and','or','but','to','of','in','on','is','are','was','were','be','been','have','has','had','he','she','it','they','we','you','i','that','this','with','for','as','at','by','from','not','their','his','her','its','which'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t not in common)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are third-person pronouns (he/she/they and variants) as a narrative-person indicator'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    third = {'he','she','they','him','her','them','his','their','hers','theirs'}\n    count = sum(1 for t in tokens if t in third)\n    return float(count) / total\n\n", "def feature(text: str) -> float:\n    'Density of past-tense auxiliary/content indicators (was, were, had, did) per word as a past-narrative signal'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    past_words = {'was','were','had','did','felt','thought','saw','said','knew','came','went','became'}\n    count = sum(1 for t in tokens if t in past_words)\n    return float(count) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are relative pronouns (that, which, who, whom, whose) indicating clause density'\n    import re\n    tokens = re.findall(r'\\w+', (text or '').lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    rel = {'that','which','who','whom','whose'}\n    count = sum(1 for t in tokens if t in rel)\n    return float(count) / total\n\n", "def feature(text: str) -> float:\n    'Density of em-dash or double-hyphen tokens used as dialogue or interrupted-speech markers per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total = len(words)\n    if total == 0:\n        return 0.0\n    dash_count = text.count('\u2014') + text.count('\u2013') + text.count('--')\n    return float(dash_count) / total\n\n", "def feature(text: str) -> float:\n    'Proxy for past-perfect usage: fraction of \"had\" followed by a past-participle-looking token (had + Xed/en/been) per word'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    matches = re.findall(r'\\bhad\\s+(?:\\w+ed|\\w+en|\\bbeen\\b)', text, flags=re.IGNORECASE)\n    return float(len(matches)) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are uncommon relative to a small common-word list (content-word density / lexical richness proxy)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    common = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at',\n        'this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there','their'\n    }\n    uncommon_count = sum(1 for t in tokens if t not in common)\n    return float(uncommon_count) / total\n", "def feature(text: str) -> float:\n    'Average number of colon characters (:) per sentence, 0.0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = len(sentences)\n    colons = text.count(':')\n    if num_sent == 0:\n        return 0.0\n    return float(colons) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Average count of coordinating conjunctions (and, but, or, so, for, nor, yet) per sentence'\n    import re\n    if not text:\n        return 0.0\n    conj = {'and','but','or','so','for','nor','yet'}\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    total = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s.lower())\n        total += sum(1 for w in words if w in conj)\n    return float(total) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are an auxiliary (was/were/is/are) immediately followed by an -ing token (heuristic for progressive constructions)'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    auxiliaries = {'was','were','is','are','am','been','be','being'}\n    count = 0\n    for i in range(len(words)-1):\n        if words[i] in auxiliaries and words[i+1].endswith('ing') and len(words[i+1])>3:\n            count += 1\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of unique consecutive word bigrams to total bigrams (higher => more varied phrasing), 0.0 if not enough tokens'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    unique = len(set(bigrams))\n    total = len(bigrams)\n    return float(unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Density of repeated punctuation sequences (like \"!!\", \"??\", \"--\", \"...\") per word, counts sequences of 2+ identical punctuation characters'\n    import re\n    if not text:\n        return 0.0\n    repeats = re.findall(r'([^\\w\\s])\\1{1,}', text)  # captures any punctuation repeated at least twice\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(repeats)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of modal verbs (can, could, would, should, might, may, must) per word as a proxy for hypothetical/conditional tone'\n    import re\n    if not text:\n        return 0.0\n    modals = {'can','could','would','should','might','may','must'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in modals)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Ratio of the first sentence length (in words) to the average sentence length (>=0.0), 0.0 if no sentences'\n    import re\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    lens = [len(re.findall(r'\\w+', s)) for s in sents]\n    if not lens:\n        return 0.0\n    first = lens[0]\n    avg = sum(lens) / float(len(lens))\n    if avg == 0:\n        return 0.0\n    return float(first) / avg\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique lowercased word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"\\b[\\w'-]+\\b\", text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return unique / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" tokens divided by number of words (0 if no words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    word_count = len(words)\n    ellipses = text.count('...')\n    return ellipses / float(word_count) if word_count > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Em-dash and dash usage density: count of em/en-dash or double-hyphen tokens per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    word_count = max(1, len(words))\n    # count various dash forms\n    dash_count = text.count('\u2014') + text.count('\u2013') + text.count('--')\n    # also count isolated \" - \" occurrences as a dash usage heuristic\n    dash_count += len(re.findall(r'\\s-\\s', text))\n    return dash_count / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a first-person pronoun (I, I\\'m, I\\'ve, I\\'d, etc.)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = re.split(r'[.!?]+\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    first_person_starts = 0\n    for s in sentences:\n        s2 = s.lstrip(' \"\\'(')  # remove leading quotes/parentheses/spaces\n        # take first token\n        m = re.match(r\"^([A-Za-z][\\w']*)\", s2)\n        if m:\n            tok = m.group(1).lower()\n            if tok in {'i', \"i'm\", \"i've\", \"i'd\", \"i'll\", \"im\", \"ive\", \"id\", \"ill\"}:\n                first_person_starts += 1\n    return first_person_starts / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per sentence (0 if no sentences)'\n    if not text:\n        return 0.0\n    import re\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+\\s+', text.strip()) if s.strip()]))\n    semi_count = text.count(';')\n    return semi_count / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Common stopword density: fraction of tokens that are among a small common stopword list'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','a','to','of','in','that','it','is','was','he','she','they','i','you','for','on','with','as','his','her','at','by','an','be','this','have','had','but','from'}\n    tokens = [t.lower().strip(\"`'\\\".,:;()[]{}\") for t in re.findall(r\"\\b[\\w'-]+\\b\", text)]\n    if not tokens:\n        return 0.0\n    stop_count = sum(1 for t in tokens if t in stopwords)\n    return stop_count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average length of consecutive punctuation runs (e.g., \"...\", \"!!!\", \"\u2014\") in characters'\n    import re\n    if not text:\n        return 0.0\n    runs = re.findall(r'[^A-Za-z0-9\\s]+', text)\n    if not runs:\n        return 0.0\n    lengths = [len(r) for r in runs]\n    return sum(lengths) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain hyphens, slashes, or underscores (indicates hyphenation or compound tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-/]+\\b\", text)\n    if not tokens:\n        return 0.0\n    special = sum(1 for t in tokens if ('-' in t) or ('/' in t) or ('_' in t))\n    return special / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that end with \"ing\" (present participles / progressive aspect)'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    ing_count = 0\n    for t in tokens:\n        tl = t.lower()\n        if len(tl) > 3 and tl.endswith('ing'):\n            ing_count += 1\n    return float(ing_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word types that occur exactly once (unique-once token rate)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    freq = {}\n    for t in tokens:\n        freq[t] = freq.get(t, 0) + 1\n    hapaxes = sum(1 for v in freq.values() if v == 1)\n    types = len(freq)\n    if types == 0:\n        return 0.0\n    return float(hapaxes) / types\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences normalized by word count'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    ellipses = text.count('...')\n    return float(ellipses) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Repeated-token ratio: proportion of tokens that are repetitions (total repeated occurrences / total tokens)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    freq = {}\n    for t in tokens:\n        freq[t] = freq.get(t, 0) + 1\n    repeated_occurrences = sum((c - 1) for c in freq.values() if c > 1)\n    return float(repeated_occurrences) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Sensory-verb ratio: fraction of tokens that are common sensory/perceptual verbs (saw, heard, felt, looked, noticed, smelled, tasted, listened)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    sensory = {'saw', 'heard', 'felt', 'looked', 'noticed', 'smelled', 'tasted', 'listened', 'gazed', 'glanced', 'observed', 'sensed'}\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Dialogue-verb density: fraction of tokens that are common speech verbs (said, asked, replied, whispered, shouted, cried, answered, exclaimed)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    speech = {'said', 'asked', 'replied', 'whispered', 'murmured', 'shouted', 'cried', 'answered', 'exclaimed'}\n    count = sum(1 for t in tokens if t in speech)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Adjective-suffix proxy: fraction of tokens ending with common adjective suffixes (ful, ous, ive, able, ic, al, y, ish)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ful', 'ous', 'ive', 'able', 'ible', 'ic', 'al', 'y', 'ish')\n    count = 0\n    for t in tokens:\n        if len(t) > 3:\n            for suf in suffixes:\n                if t.endswith(suf):\n                    count += 1\n                    break\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Normalized sentence-length variance: variance of sentence lengths (words) divided by (mean length + 1)'\n    import re\n    # split into sentences\n    sents = re.split(r'(?<=[.!?])\\s+', text.strip())\n    lengths = []\n    for s in sents:\n        words = re.findall(r'\\w+', s)\n        if words:\n            lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(var) / (mean + 1.0)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    words = re.findall(r'\\w+', text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    first_person = re.findall(r'\\b(?:i|me|my|mine|we|us|our|ours)\\b', text, flags=re.IGNORECASE)\n    return float(len(first_person)) / float(wc)\n\n", "def feature(text: str) -> float:\n    'Ratio of distinct interior capitalized word-forms (proper names inside sentences) to total tokens'\n    import re\n    words = re.findall(r'\\w+', text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    # all capitalized words of at least 3 letters\n    cap_all = re.findall(r'\\b[A-Z][a-z]{2,}\\b', text)\n    # sentence-start capitalized words (after start or after .!? and whitespace)\n    sentence_starts = re.findall(r'(?:^|[\\.!\\?]\\s+)([A-Z][a-z]{2,})', text)\n    interior_unique = set([w for w in cap_all]) - set([w for w in sentence_starts])\n    return float(len(interior_unique)) / float(wc)\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per token (captures trailing/thoughtful pauses and truncated text)'\n    import re\n    words = re.findall(r'\\w+', text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    ellipses = len(re.findall(r'\\.\\.\\.+', text))\n    return float(ellipses) / float(wc)\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation that appears as repeated runs (e.g., \"!!!\", \"??\", \"...\") capturing emphatic/fragmented style'\n    import re\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total_punct = len(puncts)\n    if total_punct == 0:\n        return 0.0\n    runs = re.findall(r'([^\\w\\s])\\1{1,}', text)  # matches sequences where the same punctuation repeats at least twice\n    return float(len(runs)) / float(total_punct)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens longer than 12 characters (captures very long/compound words)'\n    import re\n    words = re.findall(r'\\w+', text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) > 12)\n    return float(long_words) / float(wc)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that begin with a first-person pronoun (I, we, my) \u2014 indicates introspective / autobiographical openings'\n    import re\n    # split into sentence-like chunks using punctuation boundaries; keep chunks even if no terminal punctuation\n    chunks = re.split(r'(?<=[\\.!\\?])\\s+|\\n+', text)\n    if not chunks:\n        return 0.0\n    starts = 0\n    total = 0\n    for ch in chunks:\n        s = ch.strip()\n        if not s:\n            continue\n        total += 1\n        first_word_match = re.match(r'^\\W*([A-Za-z]+)', s)\n        if first_word_match:\n            fw = first_word_match.group(1).lower()\n            if fw in ('i', 'we', 'my', 'me', 'our', 'us'):\n                starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts) / float(total)\n\n", "def feature(text: str) -> float:\n    'Density of newline characters (line breaks) per token, capturing paragraphing style typical in fiction excerpts'\n    import re\n    words = re.findall(r'\\w+', text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    newlines = text.count('\\n')\n    return float(newlines) / float(wc)\n", "def feature(text: str) -> float:\n    'Fraction of modal verbs (can/could/may/might/shall/should/will/would/must/ought) among tokens'\n    import re\n    modals = {'can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would', 'must', 'ought'}\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in modals)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per word (0 if no words)'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = len(words)\n    if word_count == 0:\n        return 0.0\n    semicolons = text.count(';')\n    return float(semicolons) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Hyphen and em-dash density: count of \"-\" or \"\u2014\" characters divided by word count'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = len(words)\n    if word_count == 0:\n        return 0.0\n    hyphen_count = text.count('-') + text.count('\u2014')\n    return float(hyphen_count) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Numeric token fraction: proportion of tokens that contain at least one digit'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    num_count = sum(1 for w in words if re.search(r'\\d', w))\n    return float(num_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Citation-like pattern density: counts of \"et al\" and 4-digit years per word'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = len(words)\n    if word_count == 0:\n        return 0.0\n    etal = len(re.findall(r'\\bet\\s+al\\b', text, flags=re.I))\n    years = len(re.findall(r'\\b(?:17|18|19|20)\\d{2}\\b', text))\n    return float(etal + years) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Average length of contiguous punctuation clusters (e.g., \"...\", \"?!\", \"--\") in characters (0 if none)'\n    import re\n    clusters = re.findall(r'[^A-Za-z0-9\\s]+', text)\n    if not clusters:\n        return 0.0\n    lengths = [len(c) for c in clusters]\n    return float(sum(lengths)) / float(len(lengths))\n", "def feature(text: str) -> float:\n    'Density of relative/connecting pronouns (which, that, who, whom, whose) per token - hints at explanatory/relative-clause style'\n    import re\n    tokens = re.findall(r\"[A-Za-z]+\", text)\n    if not tokens:\n        return 0.0\n    rel = {'which', 'that', 'who', 'whom', 'whose'}\n    count = sum(1 for t in tokens if t.lower() in rel)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that contain a digit (numbers, years, ordinals) to capture numeric mentions'\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens (split on whitespace) that contain a hyphen or dash (hyphenation or compound words)'\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if '-' in t or '\u2013' in t or '\u2014' in t:\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain quoted text (detects dialogue-heavy passages); sentences split on .!?'\n    import re\n    parts = re.split(r'[.!?]+', text)\n    sentences = [p.strip() for p in parts if p.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', \"''\", \"\u2018\", \"\u2019\")\n    count = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            count += 1\n    return float(count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences normalized by number of words (captures trailing/thoughtful style)'\n    dots = text.count('...')\n    words = len(text.split())\n    if words == 0:\n        return 0.0\n    return float(dots) / float(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common sensory verbs (see/hear/smell/taste/feel/watch/listen variants)'\n    import re\n    sensory = {\n        'see','saw','seen','seeing',\n        'hear','heard','hearing','listen','listened','listening',\n        'smell','smelt','smelled','smelling',\n        'taste','tasted','tasting',\n        'feel','felt','feeling','touch','touched','touching',\n        'watch','watched','watching','glance','glanced','gazed','gaze'\n    }\n    tokens = re.findall(r\"[A-Za-z]+\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.lower() in sensory)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Hapax ratio: number of word types that occur exactly once divided by total token count (a different lexical-richness signal than TTR)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    counts = {}\n    for t in tokens:\n        counts[t] = counts.get(t, 0) + 1\n    hapax_types = sum(1 for v in counts.values() if v == 1)\n    return float(hapax_types) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (can/could/may/might/must/shall/should/will/would/ought) indicating modality and stance'\n    import re\n    tokens = re.findall(r\"[A-Za-z]+\", text)\n    if not tokens:\n        return 0.0\n    modals = {'can','could','may','might','must','shall','should','will','would','ought'}\n    count = sum(1 for t in tokens if t.lower() in modals)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Density of typographic (non-ASCII) quotation/apostrophe/dash characters per word (curly quotes, em-dash, ellipsis, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    nwords = max(1, len(tokens))\n    special_chars = ['\\u2018', '\\u2019', '\\u201c', '\\u201d', '\\u2013', '\\u2014', '\\u2026']\n    count = sum(text.count(ch) for ch in special_chars)\n    return count / float(nwords)\n\n", "def feature(text: str) -> float:\n    'Density of modal verbs (can/could/may/might/must/shall/should/will/would) as a style/hedging indicator'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not words:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for w in words if w in modals)\n    return count / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of bigrams where both tokens are capitalized (proxy for multi-word named entities like \"Jeremiah Smith\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    cap_bigrams = sum(1 for a,b in bigrams if a and b and a[0].isupper() and b[0].isupper())\n    return cap_bigrams / float(len(bigrams))\n\n", "def feature(text: str) -> float:\n    'Bigram type-token ratio: unique lowercased word bigrams divided by total bigrams (lexical variety at the phrase level)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if len(words) < 2:\n        return 0.0\n    bigrams = [' '.join((words[i], words[i+1])) for i in range(len(words)-1)]\n    unique = len(set(bigrams))\n    return unique / float(len(bigrams))\n\n", "def feature(text: str) -> float:\n    'Adverb density approximated by fraction of tokens ending with \"ly\" (common stylistic marker)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) > 2 and w.endswith('ly'))\n    return count / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Relative narrator perspective score: first-person pronoun fraction over (first + third person pronouns) to indicate viewpoint'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not words:\n        return 0.0\n    first = {'i','me','my','mine','we','us','our','ours'}\n    third = {'he','she','they','him','her','them','his','hers','their','theirs','himself','herself','themselves'}\n    first_count = sum(1 for w in words if w in first)\n    third_count = sum(1 for w in words if w in third)\n    denom = first_count + third_count\n    if denom == 0:\n        return 0.0\n    return first_count / float(denom)\n\n", "def feature(text: str) -> float:\n    'Density of tokens with common adjective suffixes (ous, ive, al, ic, ful, less, ish, ant, ent, ary) to approximate descriptive/adjectival richness'\n    import re\n    if not text:\n        return 0.0\n    suffixes = ('ous','ive','able','al','ic','ful','less','ish','ant','ent','ary')\n    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if len(w) > 4 and any(w.endswith(s) for s in suffixes):\n            count += 1\n    return count / float(len(words))\n", "def feature(text: str) -> float:\n    'Average length in words of quoted segments (0.0 if no quoted segments)'\n    import re\n    if not text:\n        return 0.0\n    # capture content between common quote characters\n    segments = re.findall(r'[\"\u201c\u201d](.*?)[\"\u201c\u201d]', text, flags=re.DOTALL)\n    if not segments:\n        # try single quotes as fallback\n        segments = re.findall(r\"[\u2018\u2019'](.*?)[\u2018\u2019']\", text, flags=re.DOTALL)\n    if not segments:\n        return 0.0\n    word_counts = []\n    for seg in segments:\n        words = re.findall(r\"\\b[\\w']+\\b\", seg)\n        word_counts.append(len(words))\n    return float(sum(word_counts)) / float(len(word_counts))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are interior titlecase words (Titlecase not at sentence start), proxy for character names'\n    import re\n    if not text:\n        return 0.0\n    # find sentence starts\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    sentence_starts = set()\n    for s in sentences:\n        m = re.match(r'\\s*([A-Z][a-z]+)\\b', s)\n        if m:\n            sentence_starts.add(m.group(1))\n    # find titlecase tokens (Capitalized followed by lowercase letters)\n    title_tokens = re.findall(r'\\b([A-Z][a-z]+)\\b', text)\n    # exclude sentence-start tokens\n    interior_title = [t for t in title_tokens if t not in sentence_starts]\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    return float(len(interior_title)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are third-person pronouns (he, she, they, him, her, them, his, hers, their, theirs)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'he','she','they','him','her','them','his','hers','their','theirs','himself','herself','themselves'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with common adjective suffixes (-y, -ous, -ive, -ful, -less, -ic, -al) as a rough proxy for descriptive/adjectival density'\n    import re\n    if not text:\n        return 0.0\n    suffixes = ('y','ous','ive','ful','less','ic','al','able','ible')\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        for s in suffixes:\n            if t.endswith(s) and len(t) > len(s) + 1:\n                count += 1\n                break\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of the auxiliary \"had\" per token (proxy for past perfect / narrative layering)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    had_count = len(re.findall(r'\\bhad\\b', text.lower()))\n    return float(had_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of quoted segments that are short (<=5 words), indicating short dialogue exchanges'\n    import re\n    if not text:\n        return 0.0\n    segments = re.findall(r'[\"\u201c\u201d](.*?)[\"\u201c\u201d]', text, flags=re.DOTALL)\n    if not segments:\n        segments = re.findall(r\"[\u2018\u2019'](.*?)[\u2018\u2019']\", text, flags=re.DOTALL)\n    if not segments:\n        return 0.0\n    short = 0\n    for seg in segments:\n        words = re.findall(r\"\\b[\\w']+\\b\", seg)\n        if len(words) <= 5:\n            short += 1\n    return float(short) / float(len(segments))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in -ly (simple adverb proxy) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 3)\n    return float(ly_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ly\" (proxy for adverb/adjectival style) per word'\n    try:\n        import re\n        tokens = re.findall(r'\\w+', text.lower())\n        if not tokens:\n            return 0.0\n        ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n        return float(ly_count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Density of \"was|were\" followed by -ing (was running, were walking) per word'\n    try:\n        import re\n        if not text:\n            return 0.0\n        matches = re.findall(r'\\b(?:was|were)\\b\\s+\\w+ing\\b', text, flags=re.IGNORECASE)\n        tokens = re.findall(r'\\w+', text)\n        if not tokens:\n            return 0.0\n        return float(len(matches)) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Binary-ish score (0.0 or 1.0) indicating presence of a short title line followed by a blank line'\n    try:\n        if not text:\n            return 0.0\n        lines = text.splitlines()\n        # find first non-empty line\n        first_idx = None\n        for i, ln in enumerate(lines):\n            if ln.strip():\n                first_idx = i\n                break\n        if first_idx is None:\n            return 0.0\n        first_line = lines[first_idx].strip()\n        # check next line exists and is blank (title separated by blank)\n        next_blank = (first_idx + 1 < len(lines) and lines[first_idx + 1].strip() == '')\n        # require short number of words and title-cased majority\n        words = [w for w in first_line.split() if any(c.isalpha() for c in w)]\n        if not words:\n            return 0.0\n        cap_count = sum(1 for w in words if w[0].isupper())\n        if next_blank and len(words) <= 8 and cap_count >= max(1, int(0.6 * len(words))):\n            return 1.0\n        return 0.0\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Hapax rate: fraction of word tokens that are hapaxes (occur exactly once)'\n    try:\n        import re\n        tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n        n = len(tokens)\n        if n == 0:\n            return 0.0\n        freq = {}\n        for t in tokens:\n            freq[t] = freq.get(t, 0) + 1\n        hapaxes = sum(1 for v in freq.values() if v == 1)\n        # return hapaxes (types occurring once) over total tokens\n        return float(hapaxes) / float(n)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Density of discourse/connective words (however, therefore, moreover, although, because, since, while, whereas) per word'\n    try:\n        import re\n        if not text:\n            return 0.0\n        tokens = re.findall(r'\\w+', text.lower())\n        if not tokens:\n            return 0.0\n        connectives = {'however', 'therefore', 'moreover', 'furthermore', 'consequently',\n                       'nevertheless', 'meanwhile', 'although', 'though', 'while', 'whereas',\n                       'because', 'since', 'thus', 'hence'}\n        count = sum(1 for t in tokens if t in connectives)\n        return float(count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Average clause length (words) when splitting on commas, colons and semicolons; 0 if no clauses'\n    try:\n        import re\n        if not text:\n            return 0.0\n        # split on clause separators\n        clauses = re.split(r'[,:;]+', text)\n        # clean clauses and count words\n        lengths = []\n        for c in clauses:\n            words = re.findall(r'\\w+', c)\n            if words:\n                lengths.append(len(words))\n        if not lengths:\n            return 0.0\n        return float(sum(lengths)) / float(len(lengths))\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Punctuation repetition score: number of repeated punctuation sequences of length>=3 (e.g., \"...\", \"!!!\") per word'\n    try:\n        import re\n        if not text:\n            return 0.0\n        tokens = re.findall(r'\\w+', text)\n        words = len(tokens)\n        repeats = re.findall(r'([!?.\u2026]{3,})', text)\n        if words == 0:\n            return float(len(repeats))\n        return float(len(repeats)) / float(words)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Density of multi-word capitalized sequences (2+ consecutive Titlecase words), proxy for multi-word proper names'\n    try:\n        import re\n        if not text:\n            return 0.0\n        # find sequences like \"John Locke\" or \"New York City\" (at least two Titlecase words)\n        matches = re.findall(r'\\b(?:[A-Z][a-z]+(?:\\s+|-))+[A-Z][a-z]+\\b', text)\n        tokens = re.findall(r'\\w+', text)\n        if not tokens:\n            return float(len(matches))\n        return float(len(matches)) / float(len(tokens))\n    except Exception:\n        return 0.0\n", "def feature(text: str) -> float:\n    'Ratio of explicit past auxiliary verbs (was/were) to combined past+present auxiliaries (was/were vs is/are); 0 if none'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    past = len(re.findall(r'\\bwas\\b', lower)) + len(re.findall(r'\\bwere\\b', lower))\n    present = len(re.findall(r'\\bis\\b', lower)) + len(re.findall(r'\\bare\\b', lower))\n    denom = past + present\n    if denom == 0:\n        return 0.0\n    return float(past) / denom\n\n", "def feature(text: str) -> float:\n    'Hapax legomena rate: fraction of word types that occur only once in the document'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    from collections import Counter\n    counts = Counter(words)\n    hapaxes = sum(1 for w, c in counts.items() if c == 1)\n    # rate relative to total tokens to emphasize uniqueness in token-level distribution\n    return float(hapaxes) / len(words)\n\n", "def feature(text: str) -> float:\n    'Frequency of the verb \"said\" per token as a proxy for explicit dialogue attribution'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    said_count = sum(1 for w in words if w == 'said')\n    return float(said_count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Average number of words per clause, approximating clause length by splitting on commas, semicolons, colons, and em-dash'\n    import re\n    if not text:\n        return 0.0\n    # split on clause delimiters\n    clauses = [c.strip() for c in re.split(r'[;:,\u2014\\u2014]+', text) if c.strip()]\n    if not clauses:\n        return 0.0\n    clause_word_counts = [len(re.findall(r'\\w+', c)) for c in clauses]\n    # avoid zero-length clauses\n    clause_word_counts = [c for c in clause_word_counts if c > 0]\n    if not clause_word_counts:\n        return 0.0\n    return float(sum(clause_word_counts)) / len(clause_word_counts)\n\n", "def feature(text: str) -> float:\n    'Density of sensory verbs (see/hear/touch/taste/smell/feel variants) per token to capture vivid descriptive narration'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    sensory = {'see','saw','seen','look','looked','feel','felt','hear','heard','smell','smelled','taste','tasted','touch','touched','listen','listened','gaze','gazed','watch','watched'}\n    count = sum(1 for w in words if w in sensory)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Density of interior capitalized tokens (capitalized words that are not the first word of a sentence) as a proxy for named entities and title-casing'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences, then check words after the first in each sentence\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    total_words = 0\n    cap_interior = 0\n    for s in sentences:\n        words = re.findall(r'\\w+|\\S', s)  # keep tokens but we'll inspect word tokens\n        # extract pure word tokens preserving order\n        pure_words = [w for w in re.findall(r'\\w+', s)]\n        total_words += len(pure_words)\n        for idx, w in enumerate(pure_words):\n            if idx == 0:\n                continue\n            if w[0].isupper():\n                cap_interior += 1\n    if total_words == 0:\n        return 0.0\n    return float(cap_interior) / total_words\n\n", "def feature(text: str) -> float:\n    'Average number of quoted/dialogue segments per sentence (0 if no sentences)'\n    import re\n    if not text:\n        return 0.0\n    # find quoted segments using straight and curly double quotes\n    quoted = re.findall(r'[\u201c\"\u201d](.+?)[\u201c\"\u201d]', text, flags=re.DOTALL)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    return float(len(quoted)) / float(len(sentences))\n\n\n", "def feature(text: str) -> float:\n    'Density of common narrative clich\u00e9s/stock phrases per token (heuristic list)'\n    import re\n    if not text:\n        return 0.0\n    phrases = [\n        'woke with a start', 'heart pounding', 'sunny morning', 'carolers singing',\n        'had heard about', 'not fair', 'flipped her', 'shimmering', 'walked into the',\n        'confused for a moment', 'took a peculiar turn'\n    ]\n    lower = text.lower()\n    word_count = max(1, len(re.findall(r'\\w+', text)))\n    hits = 0\n    for p in phrases:\n        # count overlapping occurrences\n        hits += len(re.findall(re.escape(p), lower))\n    return float(hits) / float(word_count)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of repeated word bigrams (indicator of formulaic/repetitive phrasing)'\n    import re\n    from collections import Counter\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = ['%s_%s' % (tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    counts = Counter(bigrams)\n    repeated = sum(1 for b, c in counts.items() if c > 1)\n    return float(repeated) / float(len(bigrams))\n\n\n", "def feature(text: str) -> float:\n    'Normalized standard deviation of sentence lengths (std / mean) to capture variation in sentence rhythm'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences if s.strip()]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    std = math.sqrt(var)\n    return std / mean\n\n\n", "def feature(text: str) -> float:\n    'Density of sensory vocabulary (see/hear/smell/taste/touch and common sensory adjectives) per token'\n    import re\n    if not text:\n        return 0.0\n    sensory = {\n        'see','saw','seen','looked','heard','hear','smell','taste','tasted','touch','felt',\n        'bright','dark','glowing','shimmer','shimmering','loud','quiet','soft','warm','cold',\n        'roaring','whispered','silence','sweet','bitter','sour'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    hits = sum(1 for t in tokens if t in sensory)\n    return float(hits) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Density of modal verbs (can/could/may/might/must/shall/should/will/would) per token'\n    import re\n    if not text:\n        return 0.0\n    modals = {'can','could','may','might','must','shall','should','will','would'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    hits = sum(1 for t in tokens if t in modals)\n    return float(hits) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Average number of interior punctuation characters per sentence (excluding terminal . ! ?), normalized by sentences'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_punct = 0\n    for s in sentences:\n        # count punctuation characters excluding sentence terminators\n        total_punct += sum(1 for c in s if not c.isalnum() and not c.isspace() and c not in '.!?')\n    return float(total_punct) / float(len(sentences))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a pronoun (I, we, he, she, they, it) \u2014 indicator of narrative focalization'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    pronouns = {'i','we','he','she','they','it'}\n    starts = 0\n    for s in sentences:\n        m = re.match(r\"^\\s*([A-Za-z]+)\", s)\n        if m and m.group(1).lower() in pronouns:\n            starts += 1\n    return float(starts) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Density of blank/empty lines (number of blank lines divided by total lines)'\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    blank = sum(1 for ln in lines if not ln.strip())\n    return float(blank) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that begin like dialogue (start with a quotation mark, curly quote, dash, or em-dash)'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    count = 0\n    for ln in lines:\n        s = ln.lstrip()\n        if s.startswith('\"') or s.startswith('\u201c') or s.startswith(\"'\") or s.startswith('\u2014') or s.startswith('-') or s.startswith('\u2018'):\n            count += 1\n    return float(count) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with a first-person pronoun (I, we, me, my, mine, our, us) (0 if no sentences)'\n    if not text:\n        return 0.0\n    import re\n    # crude sentence split\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    fp = set(['i','we','me','my','mine','our','ours','us'])\n    starts = 0\n    for s in parts:\n        words = re.findall(r\"\\b[\\w']+\\b\", s)\n        if not words:\n            continue\n        first = words[0].lower()\n        if first in fp:\n            starts += 1\n    return float(starts) / float(len(parts))\n\n", "def feature(text: str) -> float:\n    'Average number of common speech verbs (said/replied/asked/whispered/muttered/cried/shouted/yelled) per sentence'\n    if not text:\n        return 0.0\n    import re\n    verbs = r'\\b(?:said|replied|asked|whispered|muttered|cried|shouted|yelled)\\b'\n    sentence_count = max(1, len([p for p in re.split(r'[.!?]+', text) if p.strip()]))\n    matches = re.findall(verbs, text, flags=re.I)\n    return float(len(matches)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of interior sentence tokens (not the first token of a sentence) that are Titlecased (Token.istitle())'\n    if not text:\n        return 0.0\n    import re\n    sentences = [s.strip() for s in re.split(r'([.!?]+)', text) if s.strip()]\n    # combine fragments into full sentences when split preserves punctuation separately\n    merged = []\n    i = 0\n    while i < len(sentences):\n        if i+1 < len(sentences) and re.match(r'^[.!?]+$', sentences[i+1]):\n            merged.append((sentences[i] + sentences[i+1]).strip())\n            i += 2\n        else:\n            merged.append(sentences[i])\n            i += 1\n    total_interior = 0\n    title_interior = 0\n    for s in merged:\n        words = re.findall(r\"\\b[\\w']+\\b\", s)\n        if len(words) <= 1:\n            continue\n        for w in words[1:]:\n            total_interior += 1\n            if w.istitle():\n                title_interior += 1\n    if total_interior == 0:\n        return 0.0\n    return float(title_interior) / float(total_interior)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with common adjective suffixes (ous, ive, able, al, ful, less, ic, ish, y)'\n    if not text:\n        return 0.0\n    import re\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    matches = 0\n    for t in tokens:\n        if re.search(r'(ous|ive|able|al|ful|less|ic|ish|y)$', t):\n            matches += 1\n    return float(matches) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with the word \"the\" (case-insensitive) \u2014 captures formulaic narrative openings'\n    if not text:\n        return 0.0\n    import re\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    count = 0\n    for s in parts:\n        m = re.match(r\"^\\s*([A-Za-z']+)\", s)\n        if m and m.group(1).lower() == 'the':\n            count += 1\n    return float(count) / float(len(parts))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that are very short (3 or fewer words) \u2014 indicates snappy dialogue or short bursts'\n    if not text:\n        return 0.0\n    import re\n    sentences = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r\"\\b[\\w']+\\b\", s)\n        if len(words) <= 3:\n            short += 1\n    return float(short) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Fraction of hapax legomena (tokens that occur exactly once) among all word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    freqs = Counter(tokens)\n    hapaxes = sum(1 for v in freqs.values() if v == 1)\n    return float(hapaxes) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of detected full names (consecutive Titlecase tokens, e.g., \"John Smith\") per word'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    n = len(tokens)\n    if n < 2:\n        return 0.0\n    def is_title(tok):\n        return len(tok) > 1 and tok[0].isupper() and tok[1:].islower()\n    pairs = 0\n    for i in range(n - 1):\n        if is_title(tokens[i]) and is_title(tokens[i + 1]):\n            pairs += 1\n    return float(pairs) / max(1.0, n)\n\n", "def feature(text: str) -> float:\n    'Fraction of words that occur inside quotation marks (ASCII or curly quotes), 0 if none'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total = len(words)\n    if total == 0:\n        return 0.0\n    # capture contents inside both straight and curly double quotes\n    quoted = re.findall(r'[\"\u201c\u201d](.+?)[\"\u201c\u201d]', text, flags=re.DOTALL)\n    quoted_words = 0\n    for q in quoted:\n        quoted_words += len(re.findall(r'\\w+', q))\n    return float(quoted_words) / total\n\n", "def feature(text: str) -> float:\n    'Density of common English stopwords (small curated list) among tokens'\n    import re\n    if not text:\n        return 0.0\n    STOP = {'the','and','of','to','a','in','that','is','it','was','he','she','they','i','you','for','on','with','as','at','by','an','be','this','which','or','from'}\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    stop_count = sum(1 for t in tokens if t in STOP)\n    return float(stop_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average length of consecutive punctuation runs (e.g., \"...\" counts as run length 3); 0 if no punctuation'\n    import re\n    if not text:\n        return 0.0\n    runs = re.findall(r'([^\\w\\s]+)', text)\n    if not runs:\n        return 0.0\n    lengths = [len(r) for r in runs]\n    return float(sum(lengths)) / len(lengths)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter (after trimming leading whitespace/quotes), indicates capitalization irregularity'\n    import re\n    if not text:\n        return 0.0\n    # split by sentence-ending punctuation followed by whitespace (keeps trailing punctuation as boundary)\n    parts = [p.strip() for p in re.split(r'[.!?]+\\s*', text) if p.strip()]\n    if not parts:\n        return 0.0\n    lower_start = 0\n    total = 0\n    for p in parts:\n        # remove leading quotes/brackets\n        p2 = re.sub(r'^[\\\"\u201c\u201d\\'\\(\\[\\s]+', '', p)\n        if not p2:\n            continue\n        total += 1\n        first = p2[0]\n        if first.isalpha() and first.islower():\n            lower_start += 1\n    if total == 0:\n        return 0.0\n    return float(lower_start) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with common adjective suffixes (ous, ful, ive, less, able, al, ic, ish, ary, ant, ent)'\n    import re\n    if not text:\n        return 0.0\n    SUFF = ('ous','ful','ive','less','able','al','ic','ish','ary','ant','ent')\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) > 4 and any(t.endswith(s) for s in SUFF):\n            count += 1\n    return float(count) / len(tokens)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common speech/dialogue verbs (said, asked, shouted, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    speech_verbs = {'said','asked','replied','whispered','muttered','shouted','exclaimed','cried',\n                    'sighed','yelled','answered','answered','beckoned','laughed','snapped','observed'}\n    count = sum(1 for t in tokens if t in speech_verbs)\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per 100 words (scaled) to capture trailing/thoughtful prose'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    words = len(re.findall(r'\\w+', text))\n    if words == 0:\n        return 0.0\n    return ellipses * 100.0 / float(words)\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronoun count to third-person pronoun count (first/third), 0 if none'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    first = {'i','me','my','mine','we','us','our','ours'}\n    third = {'he','him','his','she','her','hers','they','them','their','theirs','it','its'}\n    fcount = sum(1 for t in tokens if t in first)\n    tcount = sum(1 for t in tokens if t in third)\n    if fcount == 0 and tcount == 0:\n        return 0.0\n    # return ratio with smoothing to avoid huge numbers\n    return fcount / (tcount + 1e-6)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating/subordinating conjunction (and, but, so, however, then, although)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    if not sentences:\n        return 0.0\n    conj = {'and','but','so','then','however','although','because','when','while','yet','then','also'}\n    starts = 0\n    total = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        total += 1\n        m = re.findall(r\"\\w+\", s)\n        if m:\n            first = m[0].lower()\n            if first in conj:\n                starts += 1\n    if total == 0:\n        return 0.0\n    return starts / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain direct-quote markers (double quotes or typographic quotes or em-dash) indicating dialogue'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    if not sentences:\n        return 0.0\n    quote_marks = ('\"', '\u201c', '\u201d', '\u2014')\n    has = 0\n    total = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        total += 1\n        if any(ch in s for ch in quote_marks):\n            has += 1\n    if total == 0:\n        return 0.0\n    return has / float(total)\n\n", "def feature(text: str) -> float:\n    'Density of past-progressive phrases like \"was running\"/\"were going\" per 1000 tokens (captures narrative progressive usage)'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    matches = re.findall(r'\\b(?:was|were)\\s+\\w+ing\\b', lowered)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    return len(matches) * 1000.0 / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of paragraphs that are short (<= 2 sentences), capturing short-paragraph fiction style'\n    import re\n    if not text:\n        return 0.0\n    paragraphs = [p for p in re.split(r'\\n\\s*\\n', text.strip()) if p.strip()]\n    if not paragraphs:\n        return 0.0\n    short = 0\n    for p in paragraphs:\n        # count sentences crudely\n        sentences = re.findall(r'[^.!?]+[.!?]?', p)\n        # filter out empty-like\n        sentence_count = sum(1 for s in sentences if s.strip())\n        if sentence_count <= 2:\n            short += 1\n    return short / float(len(paragraphs))\n\n", "def feature(text: str) -> float:\n    'Average length of consecutive Titlecase-word sequences (e.g., multiword names) normalized by total token count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    runs = []\n    current = 0\n    for w in words:\n        if len(w) > 1 and w[0].isupper() and w[1:].islower():\n            current += 1\n        else:\n            if current > 0:\n                runs.append(current)\n            current = 0\n    if current > 0:\n        runs.append(current)\n    if not runs:\n        return 0.0\n    avg_run = sum(runs) / float(len(runs))\n    return avg_run / float(len(words))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that match a short list of common irregular past-tense verbs (approx. irregular past usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    irregular_past = {'was','were','had','went','came','said','took','made','got','saw','knew','thought','told','left','felt','kept','found','gave','began','brought','ran','fell','flew','chose','spoke','slept','stood','wrote','read','shook','rode','built','brought','hid','held'}\n    count = sum(1 for t in tokens if t in irregular_past)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (words per sentence); 0.0 if fewer than 2 sentences'\n    import re, math\n    if not text:\n        return 0.0\n    # Split on sentence end punctuation groups\n    raw_sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in raw_sentences if s.strip()]\n    if len(sentences) < 2:\n        return 0.0\n    lengths = [len(re.findall(r\"\\b[\\w']+\\b\", s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return math.sqrt(var)\n\n", "def feature(text: str) -> float:\n    'Density of semicolons and colons per word (indicator of clause complexity / formal style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    words = len(tokens)\n    if words == 0:\n        return 0.0\n    punct_count = text.count(';') + text.count(':')\n    return float(punct_count) / words\n\n", "def feature(text: str) -> float:\n    'Density of subordinate/conjunctive connectors (because, although, while, since, however, despite, unless, whereas)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    connectors = {'because','although','though','since','while','however','despite','unless','whereas','whilst','provided','rather','whereupon'}\n    count = sum(1 for t in tokens if t in connectors)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing 3+ repeated characters in a row (elongation like \"sooo\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1', re.IGNORECASE)\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain direct-quote characters or curly quotes (indicator of dialogue presence)'\n    import re\n    if not text:\n        return 0.0\n    # naive sentence split\n    raw_sentences = re.split(r'(?<=[.!?])\\s+', text)\n    sentences = [s for s in raw_sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = ('\"', \"\u201c\", \"\u201d\", \"\u2018\", \"\u2019\", \"\u00bb\", \"\u00ab\")\n    has_quote = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars) or s.strip().startswith('\u2014') or s.strip().startswith('-'):\n            has_quote += 1\n    return float(has_quote) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of auxiliary+ -ing bigrams (was/is/are/am/were + token ending in \"ing\") \u2014 proxy for progressive aspect usage'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    aux = {'am','is','are','was','were','be','being','been'}\n    bigram_matches = 0\n    bigram_total = max(1, len(tokens)-1)\n    for a, b in zip(tokens, tokens[1:]):\n        if a in aux and b.endswith('ing'):\n            bigram_matches += 1\n    return float(bigram_matches) / bigram_total\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per word (captures trailing/pausing stylistic markers)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = max(1, len(words))\n    ellipses = text.count('...')\n    return float(ellipses) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (3 or fewer words) - indicates terse or dramatic human-like phrasing'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[^.!?]+[.!?]?', text)\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        if len(words) <= 3 and len(words) > 0:\n            short += 1\n    return float(short) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are coordinating conjunctions (and, but, or, so, yet, for, nor) - captures colloquial linking style'\n    import re\n    if not text:\n        return 0.0\n    conj = {'and', 'but', 'or', 'so', 'yet', 'for', 'nor'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in conj)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of sensory/observation verbs and nouns (see, looked, heard, smelled, felt, watched, listened, observed, etc.) per token'\n    import re\n    if not text:\n        return 0.0\n    sens = {'see','saw','seen','look','looked','glance','glanced','hear','heard','listen','listened','smell','smelled','taste','tasted','feel','felt','watch','watched','observe','observed','touch','touched','peek','peeked','notice','noticed','gaze','gazed'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sens)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Stopword density: fraction of tokens that are common function words (the, of, and, to, a, in, that, is, it, was, for, on, with, as)'\n    import re\n    if not text:\n        return 0.0\n    stops = {'the','of','and','to','a','in','that','is','it','was','for','on','with','as','an','by','at','from','this','these','those','be','have','has','had'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stops)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that end with an exclamation or question mark (captures emotive or interrogative sentence endings)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[^.!?]+[.!?]?', text)\n    if not sentences:\n        return 0.0\n    special = 0\n    for s in sentences:\n        s = s.rstrip()\n        if s.endswith('!') or s.endswith('?'):\n            special += 1\n    return float(special) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (words per sentence); 0 for <2 sentences'\n    import re, math\n    if not text:\n        return 0.0\n    # split on sentence-ending punctuation, keep non-empty\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sentences) < 2:\n        return 0.0\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that appear inside explicit quoted dialogue segments (double or substantial single quotes)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    total_words = len(tokens)\n    inside_words = 0\n    # double quotes\n    for seg in re.findall(r'\"([^\"]+)\"', text):\n        inside_words += len(re.findall(r'\\w+', seg))\n    # single-quoted segments that contain at least one space (to avoid capturing contractions)\n    for seg in re.findall(r\"'([^']+)'\", text):\n        if ' ' in seg:\n            inside_words += len(re.findall(r'\\w+', seg))\n    return float(inside_words) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are adverbs ending in \"ly\" (simple heuristic for adverb usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ly'))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of unique bigrams that repeat (count of distinct bigrams seen >1 divided by total bigrams)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = ['%s %s' % (tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    freq = {}\n    for b in bigrams:\n        freq[b] = freq.get(b, 0) + 1\n    repeated = sum(1 for v in freq.values() if v > 1)\n    return float(repeated) / float(len(bigrams))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating/subordinating conjunction or common connective (and, but, so, then, because, however, etc.)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences by punctuation followed by whitespace (keeps last fragment too)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    connectors = {'and', 'but', 'so', 'then', 'also', 'however', 'because', 'when', 'while', 'though', 'yet', 'therefore'}\n    starts = 0\n    total = 0\n    for s in sentences:\n        total += 1\n        first_tokens = re.findall(r'\\w+', s)\n        if not first_tokens:\n            continue\n        if first_tokens[0].lower() in connectors:\n            starts += 1\n    return float(starts) / float(total) if total else 0.0\n\n", "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word types that occur exactly once (lexical uniqueness measure)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    freq = {}\n    for t in tokens:\n        freq[t] = freq.get(t, 0) + 1\n    hapax = sum(1 for v in freq.values() if v == 1)\n    return float(hapax) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like time expressions (e.g., \"5:01\", \"12:30\", tokens containing \":\" or standalone AM/PM)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    time_re = re.compile(r'\\b\\d{1,2}:\\d{2}\\b')\n    ampm_re = re.compile(r'(?i)\\b(?:am|pm)\\b')\n    count = 0\n    for t in tokens:\n        if time_re.search(t) or ampm_re.search(t) or (':' in t and any(ch.isdigit() for ch in t)):\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain an ellipsis (\"...\" or \"\u2026\")'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    count = sum(1 for s in sents if '...' in s or '\u2026' in s)\n    return float(count) / len(sents)\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens ending with \"ly\" (simple proxy for adverb/adverbial style)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.endswith('ly'))\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that are common dialogue/speech verbs (said, asked, shouted, whispered, etc.)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    speech_verbs = {'said', 'asked', 'shouted', 'whispered', 'muttered', 'replied', 'cried', 'yelled', 'screamed', 'retorted', 'sighed', 'sobbed', 'laughed', 'murmured'}\n    count = sum(1 for w in words if w in speech_verbs)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Binary-ish feature: 1.0 if the fraction of simple sensory words is very low (< 0.02), else 0.0'\n    import re\n    if not text:\n        return 1.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 1.0\n    sensory = {'see','saw','seen','see','hear','heard','felt','feel','smell','smelled','taste','tasted','look','listen','touch','sight','sound','felt','glow'}\n    frac = sum(1 for w in words if w in sensory) / len(words)\n    return 1.0 if frac < 0.02 else 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a pronoun (I, we, he, she, they, it, you) - rough indicator of narrative/character focus'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    pronouns = {'i','we','he','she','they','it','you','me','us','him','her','them'}\n    count = 0\n    for s in sents:\n        m = re.match(r'\\s*([A-Za-z]\\w+)', s)\n        if m:\n            first = m.group(1).lower()\n            if first in pronouns:\n                count += 1\n    return float(count) / len(sents)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word types / total word tokens) as a simple lexical diversity measure'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    types = set(words)\n    return float(len(types)) / float(len(words))\n", "def feature(text: str) -> float:\n    'Ellipsis density: count of sequences of 3+ dots normalized by number of sentences (0 if none)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = re.findall(r'\\.{3,}', text)\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(ellipses)) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Adverb density: fraction of word-tokens that end with \"ly\" (heuristic for descriptive/adverbial style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n    return float(ly_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Gerund/progressive ratio: fraction of tokens ending with \"ing\" (narrative/progressive aspect proxy)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing') and len(t) > 3)\n    return float(ing_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'First-person pronoun ratio: fraction of pronoun tokens that are first-person (I, we, me, my, etc.)'\n    import re\n    if not text:\n        return 0.0\n    pronouns_first = {'i','we','me','us','our','ours','my','mine','myself','ourselves'}\n    pronouns_all = pronouns_first.union({'you','your','yours','yourself','yourselves','he','she','it','they','them','their','theirs','him','her','himself','herself','itself','themselves'})\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    total_pronouns = sum(1 for t in tokens if t in pronouns_all)\n    if total_pronouns == 0:\n        return 0.0\n    first_count = sum(1 for t in tokens if t in pronouns_first)\n    return float(first_count) / float(total_pronouns)\n\n", "def feature(text: str) -> float:\n    'Definite article frequency: fraction of tokens that are \"the\" (stylistic preference indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    the_count = sum(1 for t in tokens if t == 'the')\n    return float(the_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Hapax proportion: fraction of word types that occur exactly once (lexical novelty / repetition measure)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    counts = Counter(words)\n    hapaxes = sum(1 for w, c in counts.items() if c == 1)\n    return float(hapaxes) / float(len(counts))\n\n", "def feature(text: str) -> float:\n    'Dialogue sentence fraction: fraction of sentences containing quotation marks or common dialogue markers (\u2014 or leading -)'\n    import re\n    if not text:\n        return 0.0\n    # split into rough sentences\n    sentences = re.findall(r'[^.!?]+[.!?]?', text)\n    if not sentences:\n        return 0.0\n    def is_dialogue(s: str) -> bool:\n        if '\"' in s or '\u201c' in s or '\u201d' in s:\n            return True\n        if '\u2014' in s or s.strip().startswith('-'):\n            return True\n        return False\n    dialogue_count = sum(1 for s in sentences if is_dialogue(s))\n    return float(dialogue_count) / float(len(sentences))\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that are adverbs ending in \"-ly\" (proxy for descriptive/flowery style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text, flags=re.IGNORECASE)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 2 and t.lower().endswith('ly'))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common color words (yellow, gray, brown, red, blue, black, white, green, murky, etc.)'\n    import re\n    colors = {'yellow','gray','grey','brown','red','blue','black','white','green','murky','bright','dark','pale','ivory','crimson','silver','golden','gold','tan','beige'}\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in colors)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a coordinating or narrative conjunction (And, But, So, Then)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences heuristically\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    starts = 0\n    for s in sents:\n        m = re.match(r\"^([A-Za-z']+)\", s)\n        if m and m.group(1).lower() in {'and','but','so','then','also','however','meanwhile'}:\n            starts += 1\n    return float(starts) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Average number of words per clause separated by commas (mean clause length), 0 if no commas'\n    import re, statistics\n    if not text:\n        return 0.0\n    # Split on commas to find clauses\n    clauses = [c.strip() for c in text.split(',') if c.strip()]\n    if not clauses:\n        return 0.0\n    lengths = [len(re.findall(r'\\b[\\w\\'-]+\\b', c)) for c in clauses]\n    if not lengths:\n        return 0.0\n    # return mean clause length\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that participate in a simple passive-voice pattern like \"was Xed\" or \"were Xed\" (proxy for passive voice)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text, flags=re.IGNORECASE)\n    if not tokens:\n        return 0.0\n    # count passive-like bigrams using regex across text\n    matches = re.findall(r'\\b(?:was|were|is|are|been|being|be)\\s+[A-Za-z]+ed\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total token count (lexical variety)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"\\b[\\w'-]+\\b\", text)]\n    if not tokens:\n        return 0.0\n    unique = set(tokens)\n    return float(len(unique)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are temporal narrative markers (first, then, after, before, later, suddenly, meanwhile)'\n    import re\n    markers = {'first','then','after','before','later','suddenly','meanwhile','finally','earlier','previously','next'}\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in markers)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of simile patterns like \"like a\", \"like the\", \"as a\", \"as the\", or \"as if\" per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    patterns = re.findall(r'\\blike (?:a|the)\\b|\\bas (?:a|the)\\b|\\bas if\\b', text, flags=re.IGNORECASE)\n    return float(len(patterns)) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of word tokens ending with \"ing\" (proxy for progressive/descriptive phrasing)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total number of tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / n\n\n\n", "def feature(text: str) -> float:\n    'Diversity of sentence starting words: distinct first words of sentences divided by number of sentences'\n    import re\n    if not text:\n        return 0.0\n    # Split on sentence terminators while preserving non-empty pieces\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    starts = []\n    for p in parts:\n        m = re.search(r'\\b\\w+\\b', p)\n        if m:\n            starts.append(m.group(0).lower())\n    if not starts:\n        return 0.0\n    distinct = len(set(starts))\n    return float(distinct) / len(starts)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<= 5 words) \u2014 captures choppy, vignette-like sentence usage'\n    import re\n    if not text:\n        return 0.0\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    short = 0\n    total = 0\n    for p in parts:\n        words = re.findall(r'\\b\\w+\\b', p)\n        if len(words) == 0:\n            continue\n        total += 1\n        if len(words) <= 5:\n            short += 1\n    if total == 0:\n        return 0.0\n    return float(short) / total\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences containing explicit dialogue markers (double quotes or leading dash), proxy for dialogue-heavy writing'\n    import re\n    if not text:\n        return 0.0\n    parts = [p for p in re.split(r'(?<=[.!?])\\s+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    dialog = 0\n    for p in parts:\n        s = p.strip()\n        if '\"' in s or '\u201c' in s or '\u201d' in s or s.startswith('-') or re.search(r'^\\s*-\\s*\\w', p):\n            dialog += 1\n    return float(dialog) / len(parts)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens equal to \"like\" (proxy for simile/comparative figurative language)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t == 'like')\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Punctuation-spacing irregularity: fraction of punctuation characters (.,!?;:) that are immediately followed by a non-space character (likely spacing errors)'\n    import re\n    if not text:\n        return 0.0\n    puncts = re.findall(r'[.,!?;:]', text)\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    # punctuation followed by non-space and not end-of-text\n    irregular = len(re.findall(r'[.,!?;:](?=\\S)', text))\n    return float(irregular) / total\n", "def feature(text: str) -> float:\n    'Normalized count of explicit simile markers (e.g., \"like a\", \"as if\", \"as though\", \"as ... as\") per token'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    patterns = [r'\\blike\\s+(a|an|the)\\b', r'\\bas if\\b', r'\\bas though\\b', r'\\bas\\s+\\w+\\s+as\\b']\n    tokens = re.findall(r'\\b\\w+\\b', lower)\n    if not tokens:\n        return 0.0\n    matches = 0\n    for p in patterns:\n        matches += len(re.findall(p, lower))\n    return float(matches) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain explicit quotation marks (double or Unicode quote characters)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences (simple heuristic)\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', '\u201e', '\u201f')\n    qcount = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            qcount += 1\n    return float(qcount) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Median sentence length in words (returns 0.0 for texts with no sentences)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    lengths.sort()\n    n = len(lengths)\n    mid = n // 2\n    if n % 2 == 1:\n        return float(lengths[mid])\n    else:\n        return (lengths[mid - 1] + lengths[mid]) / 2.0\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (0.0 if no sentences)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    num_commas = text.count(',')\n    return float(num_commas) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that match a small emotion/affect lexicon (e.g., stunned, ugh, happy, sad, afraid)'\n    import re\n    if not text:\n        return 0.0\n    emotions = {'happy','sad','angry','afraid','fearful','joy','joyful','sorrow','excited','stunned','afraid','lonely','relieved','anxious','love','hate','peaceful','terrified','upset','embarrassed','ashamed','guilty','content','bored','ugh','disgust','disgusted','frustrated','nostalgic','melancholy'}\n    tokens = re.findall(r\"\\b[a-z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in emotions)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a conjunction/discourse marker (e.g., and, but, so, however)'\n    import re\n    if not text:\n        return 0.0\n    markers = {'and','but','so','or','yet','for','nor','however','then','still','thus','therefore'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'\\b\\w+\\b', s)\n        if m and m.group(0).lower() in markers:\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of adjacent word pairs that are exact repetitions (e.g., \"very very\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    repeats = 0\n    for i in range(1, len(tokens)):\n        if tokens[i] == tokens[i-1]:\n            repeats += 1\n    return float(repeats) / (len(tokens) - 1)\n", "def feature(text: str) -> float:\n    'Density of explicit dialogue markers: count of quotation marks and em-dashes per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    num_sent = max(1, len(sentences))\n    quote_marks = text.count('\"') + text.count('\u201c') + text.count('\u201d') + text.count(\"''\")\n    emdash_marks = text.count('\u2014') + text.count('--') + text.count('\u2013')\n    markers = quote_marks + emdash_marks\n    return float(markers) / num_sent\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word forms / total word tokens) using simple alphanumeric+apostrophe tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower().strip(\"'\") for t in re.findall(r\"[A-Za-z0-9']+\", text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are past-tense auxiliaries or markers (was, were, had, did, etc.)'\n    import re\n    if not text:\n        return 0.0\n    auxs = {'was', 'were', 'had', 'did', \"wasn't\", \"weren't\", \"hadn't\", \"didn't\", 'has', 'have', 'had'}\n    tokens = [t.lower().strip(\"'\") for t in re.findall(r\"[A-Za-z0-9']+\", text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in auxs)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that match a small curated sensory/adjectival vocabulary (colors, textures, shine, emotion words)'\n    import re\n    if not text:\n        return 0.0\n    sensory = {'dark', 'dim', 'bright', 'glistening', 'gleaming', 'polished', 'rusty', 'smooth', 'soft', 'hard', 'cold', 'warm', 'shiny', 'glossy', 'ivory', 'stunned', 'beautiful', 'beautifully', 'polished', 'rusted', 'thirsty'}\n    tokens = [t.lower().strip(\"'\") for t in re.findall(r\"[A-Za-z0-9']+\", text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of alphabetic tokens ending with \"s\" (proxy for third-person singular present or plural-heavy text), excluding common small words'\n    import re\n    if not text:\n        return 0.0\n    exclude = {'is', 'as', 'was', 'this', 'his', 'its', 'lets', 'lets'}  # common short tokens to ignore\n    tokens = [t.lower().strip(\"'\") for t in re.findall(r\"[A-Za-z']+\", text)]\n    alpha_tokens = [t for t in tokens if any(c.isalpha() for c in t)]\n    if not alpha_tokens:\n        return 0.0\n    count = 0\n    for t in alpha_tokens:\n        if len(t) > 2 and t.endswith('s') and t not in exclude:\n            count += 1\n    return float(count) / len(alpha_tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a coordinating/conjunctive word (and, but, so, then, however) \u2014 stylistic starter usage'\n    import re\n    if not text:\n        return 0.0\n    starters = {'and', 'but', 'so', 'then', 'also', 'however', 'yet', 'because', 'still'}\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        first_words = re.findall(r\"[A-Za-z']+\", s)\n        if first_words:\n            if first_words[0].lower().strip(\"'\") in starters:\n                count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Lexical repetition score: frequency of the most common token divided by total tokens (1 = single repeated word)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = [t.lower().strip(\"'\") for t in re.findall(r\"[A-Za-z0-9']+\", text)]\n    if not tokens:\n        return 0.0\n    freqs = Counter(tokens)\n    most_common = freqs.most_common(1)[0][1]\n    return float(most_common) / len(tokens)\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a subordinating conjunction (as, when, while, although, because, since, if, though, after, before)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences using punctuation\n    raw_sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in raw_sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    starters = {'as', 'when', 'while', 'although', 'because', 'since', 'if', 'though', 'after', 'before', 'once', 'unless'}\n    count = 0\n    for s in sentences:\n        m = re.match(r\"\\s*([A-Za-z']+)\", s)\n        if m and m.group(1).lower() in starters:\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by number of sentences; uses 1 as min denominator to avoid divide-by-zero)'\n    if not text:\n        return 0.0\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    commas = text.count(',')\n    denom = max(1, sentences)\n    return float(commas) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Lexical diversity estimated as unique token count divided by total token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are intensifiers (very, extremely, totally, utterly, completely, absolutely, really, so, too, quite, highly)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    intensifiers = {'very', 'extremely', 'totally', 'utterly', 'completely', 'absolutely', 'really', 'so', 'too', 'quite', 'highly', 'really'}\n    count = sum(1 for t in tokens if t in intensifiers)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are forms of the verb \"to be\" or related auxiliaries (is, are, am, was, were, be, being, been, has, have, had)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    be_forms = {'is', 'are', 'am', 'was', 'were', 'be', 'being', 'been', 'has', 'have', 'had'}\n    count = sum(1 for t in tokens if t in be_forms)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing a hyphen (hyphenated compounds) as a simplistic style marker'\n    import re\n    if not text:\n        return 0.0\n    # tokenization on whitespace to preserve hyphenated tokens\n    tokens = [t for t in re.findall(r'\\S+', text) if t.strip()]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average clause length measured as mean number of words between commas; fallback to mean sentence length if no commas present'\n    import re\n    if not text:\n        return 0.0\n    # clauses split by commas\n    raw_clauses = [c.strip() for c in text.split(',') if c.strip()]\n    word_pattern = re.compile(r\"\\b[\\w']+\\b\")\n    if raw_clauses:\n        lengths = [len(word_pattern.findall(c)) for c in raw_clauses]\n        # avoid zero-length clauses\n        lengths = [l for l in lengths if l > 0]\n        if not lengths:\n            return 0.0\n        return float(sum(lengths)) / len(lengths)\n    # fallback: sentences\n    raw_sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not raw_sentences:\n        # fallback to entire text word count\n        total_words = len(word_pattern.findall(text))\n        return float(total_words)\n    lengths = [len(word_pattern.findall(s)) for s in raw_sentences]\n    lengths = [l for l in lengths if l > 0]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / len(lengths)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that belong to a small narrative genre vocabulary (sci-fi/horror words like \"galaxy\",\"ship\",\"alien\",\"void\",\"corpse\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    genre_words = {'galaxy','ship','alien','void','captain','orbit','planet','surface','readings','gelatinous',\n                   'mortician','corpse','slumber','woke','body','glowing','game','void','orb','sci','science','engine',\n                   'ship','star','world','alien','creature','spaceship','colony','astronaut','synthetic','android','cyber'}\n    count = sum(1 for t in tokens if t in genre_words)\n    return float(count) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens (lexical variety)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Median sentence length in words (split on .!?), returns 0 for empty or no sentences'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence-like chunks\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', p)) for p in parts]\n    lengths = [l for l in lengths if l >= 0]\n    if not lengths:\n        return 0.0\n    lengths.sort()\n    n = len(lengths)\n    if n % 2 == 1:\n        median = float(lengths[n//2])\n    else:\n        median = (lengths[n//2 - 1] + lengths[n//2]) / 2.0\n    return median\n\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that mix past-tense \"-ed\" tokens with present/future auxiliaries (proxy for tense inconsistency)'\n    import re\n    if not text:\n        return 0.0\n    # crude sentence split\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    present_pattern = re.compile(r'\\b(is|are|am|have|has|do|does|will|shall|isn\\'t|aren\\'t|doesn\\'t|don\\'t)\\b', re.I)\n    past_ed_pattern = re.compile(r'\\b\\w+ed\\b', re.I)\n    mixed = 0\n    for s in sentences:\n        if past_ed_pattern.search(s) and present_pattern.search(s):\n            mixed += 1\n    return float(mixed) / len(sentences)\n\n\n", "def feature(text: str) -> float:\n    'Proportion of punctuation occurrences that are preceded by a space (typographic spacing errors), 0 if no punctuation'\n    import re\n    if not text:\n        return 0.0\n    # count occurrences like \" .\", \" ,\", \" !\", etc.\n    bad_before = len(re.findall(r'\\s[.,;:!?]', text))\n    total_punct = sum(1 for c in text if c in '.,;:!?')\n    if total_punct == 0:\n        return 0.0\n    return float(bad_before) / total_punct\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are non-stopwords (rough proxy for content-word density / information density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    stopwords = {'the','a','an','and','or','but','if','then','in','on','at','by','for','of','to','with','from','as',\n                 'that','this','these','those','is','are','was','were','be','been','being','have','has','had','do','does','did',\n                 'i','you','he','she','it','they','we','me','him','her','them','our','us','my','your','yours','its','his','hers'}\n    content = sum(1 for t in tokens if t not in stopwords)\n    return float(content) / len(tokens)\n\n\n", "def feature(text: str) -> float:\n    'Average length of contiguous punctuation runs (e.g., \"...\" or \"--\" counted by run length), 0 if no punctuation'\n    import re\n    if not text:\n        return 0.0\n    runs = re.findall(r'([^\\w\\s]+)', text)\n    if not runs:\n        return 0.0\n    lengths = [len(r) for r in runs]\n    return float(sum(lengths)) / len(lengths)\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") measured as ellipsis occurrences per sentence (0 if no sentences)'\n    try:\n        import re\n        if not text:\n            return 0.0\n        ellipses = len(re.findall(r'\\.\\.\\.+', text))\n        # count sentences as occurrences of .!? (fallback minimum 1)\n        sentences = max(1, len(re.findall(r'[.!?]', text)))\n        return float(ellipses) / sentences\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain an uppercase letter after the first character (camelCase or internal capitals)'\n    try:\n        tokens = text.split()\n        if not tokens:\n            return 0.0\n        def has_internal_caps(t):\n            # ignore purely punctuation tokens\n            if len(t) < 2:\n                return False\n            for c in t[1:]:\n                if c.isupper():\n                    return True\n            return False\n        cnt = sum(1 for t in tokens if has_internal_caps(t))\n        return float(cnt) / len(tokens)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that appear to be title/heading lines (majority of words start with uppercase)'\n    try:\n        lines = [ln for ln in text.splitlines() if ln.strip()]\n        if not lines:\n            return 0.0\n        def is_title_line(ln):\n            words = [w.strip('()\"\\'[]{},:;.') for w in ln.split() if w.strip('()\"\\'[]{},:;.')]\n            if not words:\n                return False\n            cap = sum(1 for w in words if w and w[0].isupper())\n            return cap / len(words) >= 0.5\n        title_lines = sum(1 for ln in lines if is_title_line(ln))\n        return float(title_lines) / len(lines)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words (0 if insufficient data)'\n    try:\n        import re, math\n        if not text or not text.strip():\n            return 0.0\n        # split on sentence enders but keep potential empty fragments removed\n        sentences = [s.strip() for s in re.split(r'(?<=[\\.\\!\\?])\\s+', text) if s.strip()]\n        lengths = [len(s.split()) for s in sentences if len(s.split()) > 0]\n        if len(lengths) < 2:\n            return 0.0\n        mean = sum(lengths) / len(lengths)\n        var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n        std = math.sqrt(var)\n        return std / mean if mean != 0 else 0.0\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (small curated list)'\n    try:\n        stops = {'the','and','of','in','to','a','was','is','for','on','with','as','by','that','it','at','from','this','be','are','an','has','or'}\n        tokens = [t.strip(\".,;:()\\\"'[]{}\").lower() for t in text.split() if t.strip(\".,;:()\\\"'[]{}\")]\n        if not tokens:\n            return 0.0\n        cnt = sum(1 for t in tokens if t in stops)\n        return float(cnt) / len(tokens)\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation characters that belong to repeated punctuation runs (e.g., \"!!\", \"...\", \"??\")'\n    try:\n        import re\n        if not text:\n            return 0.0\n        # find all punctuation characters\n        puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n        total = len(puncts)\n        if total == 0:\n            return 0.0\n        # find runs of the same punctuation char of length >=2\n        runs = re.findall(r'([^\\w\\s])\\1+', text)\n        # runs returns each matched character once per run; estimate total chars in runs by scanning\n        run_chars = 0\n        for m in re.finditer(r'([^\\w\\s])\\1+', text):\n            run_chars += len(m.group(0))\n        return float(run_chars) / total\n    except Exception:\n        return 0.0\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique tokens divided by total tokens (lexical diversity)'\n    try:\n        tokens = [t.strip('.,;:()\\\"\\'[]{}').lower() for t in text.split() if t.strip('.,;:()\\\"\\'[]{}')]\n        if not tokens:\n            return 0.0\n        unique = len(set(tokens))\n        return float(unique) / len(tokens)\n    except Exception:\n        return 0.0\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like a short title (short line, initial caps on words)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    title_lines = 0\n    for ln in lines:\n        words = ln.split()\n        if 1 <= len(words) <= 6:\n            all_initial_caps = True\n            for w in words:\n                cleaned = re.sub(r'^[^A-Za-z]+|[^A-Za-z]+$', '', w)\n                if not cleaned:\n                    all_initial_caps = False\n                    break\n                if not cleaned[0].isupper():\n                    all_initial_caps = False\n                    break\n            if all_initial_caps:\n                title_lines += 1\n    return float(title_lines) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: lexical diversity (unique tokens divided by total tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count)'\n    if not text:\n        return 0.0\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences <= 0:\n        return 0.0\n    commas = text.count(',')\n    return float(commas) / sentences\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common stopwords (small, robust stopword set)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','a','an','and','to','of','in','on','for','with','is','was','were','are','be','have','had','that','this','it','i','you','he','she','they','we','my','your','as','at','by','from'}\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    stop_count = sum(1 for t in tokens if t in stopwords)\n    return float(stop_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (can/could/may/might/must/shall/should/will/would)'\n    import re\n    if not text:\n        return 0.0\n    modals = {'can','could','may','might','must','shall','should','will','would'}\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    mod_count = sum(1 for t in tokens if t in modals)\n    return float(mod_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain dialogue markers (quotes or long dash)'\n    import re\n    if not text:\n        return 0.0\n    # Split into loose sentence-like chunks\n    sentences = re.findall(r'[^.!?]+[.!?]?', text, flags=re.MULTILINE)\n    if not sentences:\n        return 0.0\n    dialogue_markers = ('\"', '\u201c', '\u201d', '\u2014', '--')\n    dialogue_count = 0\n    for s in sentences:\n        if any(m in s for m in dialogue_markers):\n            dialogue_count += 1\n    return float(dialogue_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Ratio of gerund tokens (ending in \"ing\") to past-tense-looking tokens (ending in \"ed\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    gerund = sum(1 for t in tokens if t.endswith('ing'))\n    past = sum(1 for t in tokens if t.endswith('ed'))\n    if gerund == 0 and past == 0:\n        return 0.0\n    return float(gerund) / (past + 1e-6)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating conjunction (and, but, or, so, then, also)'\n    import re\n    if not text:\n        return 0.0\n    conj_set = {'and','but','or','so','then','also'}\n    sentences = re.findall(r'[^.!?]+[.!?]?', text, flags=re.MULTILINE)\n    if not sentences:\n        return 0.0\n    starts_with_conj = 0\n    for s in sentences:\n        m = re.search(r'^[\\s\"\u201c\u201d\\'(.-]*([A-Za-z]+)', s)\n        if m:\n            first = m.group(1).lower()\n            if first in conj_set:\n                starts_with_conj += 1\n    return float(starts_with_conj) / len(sentences)\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are numeric or contain digits (numeric density)'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num) / max(1, len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are long (>25 words) \u2014 indicator of complex, formal sentence structure'\n    if not text:\n        return 0.0\n    # split into sentence-like segments\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    long_count = sum(1 for s in sentences if len(s.split()) > 25)\n    return float(long_count) / max(1, len(sentences))\n\n", "def feature(text: str) -> float:\n    'Density of parenthetical acronym occurrences like \"(ABC)\" per word (common in technical/academic prose)'\n    if not text:\n        return 0.0\n    # count simple parenthetical all-caps acronyms\n    acr_paren = re.findall(r'\\(\\s*[A-Z]{2,6}s?\\s*(?:,[A-Z]{2,6}s?\\s*)*\\)', text)\n    words = text.split()\n    return float(len(acr_paren)) / max(1, len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are formal discourse/connective markers (however, therefore, moreover, etc.)'\n    if not text:\n        return 0.0\n    connectors = {'however','therefore','moreover','furthermore','consequently','additionally','likewise','nevertheless','nonetheless','subsequently','whereas','thus','hence','despite','alternatively'}\n    tokens = re.findall(r\"\\w+\", text.lower())\n    count = sum(1 for t in tokens if t in connectors)\n    return float(count) / max(1, len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are all-caps acronyms (length >=2), e.g., \"EU\", \"EEC\", \"ACOs\" (indicates formal/technical writing)'\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[A-Za-z\\.]{2,}\\b', text)\n    acr = sum(1 for t in tokens if any(c.isalpha() for c in t) and t.replace('.','').isupper())\n    all_tokens = text.split()\n    return float(acr) / max(1, len(all_tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of words that appear inside parentheses (measures how parenthetical information is used, e.g., expansions, citations)'\n    if not text:\n        return 0.0\n    inner_texts = re.findall(r'\\((.*?)\\)', text)\n    inner_words = sum(len(re.findall(r'\\w+', s)) for s in inner_texts)\n    total_words = len(re.findall(r'\\w+', text))\n    return float(inner_words) / max(1, total_words)\n\n", "def feature(text: str) -> float:\n    'Ratio of short title-like lines (1\u20134 words) in Title Case to total lines (heuristic for section headers)'\n    if not text:\n        return 0.0\n    lines = [l.strip() for l in text.splitlines() if l.strip()]\n    if not lines:\n        # also check if single-line text could still be a header-like phrase (treat entire text as one line)\n        lines = [text.strip()] if text.strip() else []\n    if not lines:\n        return 0.0\n    def is_title_like(line):\n        words = [w for w in re.findall(r\"[A-Za-z']+\", line)]\n        if not (1 <= len(words) <= 4):\n            return False\n        # each word starts with uppercase and rest lowercase (Title Case)\n        return all(w[0].isupper() and (len(w) == 1 or w[1:].islower()) for w in words)\n    count = sum(1 for l in lines if is_title_like(l))\n    return float(count) / max(1, len(lines))\n", "def feature(text: str) -> float:\n    'Average sentence length in tokens (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # sentences split at punctuation followed by whitespace\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip())\n    token_re = re.compile(r'\\w+')\n    lengths = []\n    for s in sentences:\n        tokens = token_re.findall(s)\n        if tokens:\n            lengths.append(len(tokens))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons divided by total characters (captures formal clause linking typical of scholarly prose)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float(text.count(';')) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Mid-sentence capitalized token ratio: fraction of tokens that are capitalized but are not the first token of a sentence (named entities within sentences)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip())\n    cap_count = 0\n    total_interior = 0\n    token_re = re.compile(r\"[A-Za-z][\\w'-]*\")\n    for s in sentences:\n        tokens = token_re.findall(s)\n        if not tokens:\n            continue\n        # consider tokens after the first token in the sentence\n        for t in tokens[1:]:\n            total_interior += 1\n            if t[0].isupper():\n                cap_count += 1\n    if total_interior == 0:\n        return 0.0\n    return float(cap_count) / float(total_interior)\n\n", "def feature(text: str) -> float:\n    'Hyphenated token ratio: fraction of whitespace-separated tokens that contain a hyphen (compound technical terms)'\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t and any(ch.isalpha() for ch in t))\n    return float(hyphen_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Discourse marker sentence-start ratio: fraction of sentences that begin with a formal connective (e.g., \"However\", \"Moreover\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    markers = {'however','moreover','furthermore','additionally','consequently','therefore','nonetheless','similarly','in addition','alternatively','notably','conversely','nevertheless','importantly','subsequently','accordingly'}\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip())\n    if not sentences:\n        return 0.0\n    count = 0\n    total = 0\n    for s in sentences:\n        s_strip = s.strip()\n        if not s_strip:\n            continue\n        total += 1\n        # get first one or two words\n        first = s_strip.split()[:2]\n        first_phrase = ' '.join(w.lower().strip(',:;()[]') for w in first)\n        # check both one-word and two-word markers\n        if first_phrase.split()[0] in markers or first_phrase in markers:\n            count += 1\n    if total == 0:\n        return 0.0\n    return float(count) / float(total)\n\n", "def feature(text: str) -> float:\n    'Citation-like pattern density: counts occurrences of \"et al\" and 4-digit years (1900-2099) normalized by word count'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = max(1, len(words))\n    etal = len(re.findall(r'\\bet al\\b', text, flags=re.IGNORECASE))\n    years = len(re.findall(r'\\b(?:19|20)\\d{2}\\b', text))\n    return float(etal + years) / float(word_count)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences ending with a period (vs other terminal punctuation) \u2014 scholarly text tends to favor declarative sentences'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # find sentence-ending punctuation sequences\n    ends = re.findall(r'([\\.!?])', text)\n    if not ends:\n        return 0.0\n    period_count = sum(1 for e in ends if e == '.')\n    return float(period_count) / float(len(ends))\n\n", "def feature(text: str) -> float:\n    'Occurrence ratio of \" by \" phrases normalized by sentence count (captures agentive \"by\" phrases often in passive constructions)'\n    if not text or not text.strip():\n        return 0.0\n    import re\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip())\n    sent_count = max(1, sum(1 for s in sentences if s.strip()))\n    by_count = len(re.findall(r'\\sby\\s', text, flags=re.IGNORECASE))\n    return float(by_count) / float(sent_count)\n", "def feature(text: str) -> float:\n    'Density of short-to-medium acronyms (all-caps tokens of length 2\u20136) as a fraction of word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    acr = re.findall(r'\\b[A-Z]{2,6}\\b', text)\n    return float(len(acr)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for sentence complexity / clause density)'\n    import re\n    if not text:\n        return 0.0\n    sentence_count = max(1.0, float(len(re.findall(r'[.!?]', text))))\n    comma_count = float(text.count(','))\n    return comma_count / sentence_count\n\n", "def feature(text: str) -> float:\n    'Average number of colons per sentence (colons often mark definitions, lists, or clause structures in formal writing)'\n    import re\n    if not text:\n        return 0.0\n    sentence_count = max(1.0, float(len(re.findall(r'[.!?]', text))))\n    colon_count = float(text.count(':'))\n    return colon_count / sentence_count\n\n", "def feature(text: str) -> float:\n    'Frequency of \"binomial\"/taxonomic-like two-word patterns (Capitalized + lowercase following word), normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    # captures patterns like \"Homo sapiens\" or other Capitalized + lowercase sequences\n    pairs = re.findall(r'\\b[A-Z][a-z]+\\s+[a-z]{2,}\\b', text)\n    return float(len(pairs)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain hyphens (hyphenated technical compounds like \"single-stranded\", \"well-being\")'\n    import re\n    if not text:\n        return 0.0\n    toks = text.split()\n    if not toks:\n        return 0.0\n    hy = sum(1 for t in toks if '-' in t)\n    return float(hy) / float(len(toks))\n\n", "def feature(text: str) -> float:\n    'Density of dotted abbreviations and Latin abbreviations (e.g., \"e.g.\", \"i.e.\", \"U.S.\", \"et al.\") per word token'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    # common short dotted forms and sequences of single-letter-dot patterns\n    dotted = re.findall(r'\\b(?:e\\.g\\.|i\\.e\\.|et al\\.|et al|[A-Za-z](?:\\.[A-Za-z])+\\.)\\b', text, flags=re.IGNORECASE)\n    return float(len(dotted)) / float(len(words))\n", "def feature(text: str) -> float:\n    'Density of parenthetical years/citation-style years (e.g., (2001)) per sentence, proxy for academic citations'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[.!?]', text)\n    num_sent = max(1, len(sentences))\n    # detect 4-digit years in parentheses (1900-2099)\n    year_paren_matches = re.findall(r'\\(\\s*(?:19|20)\\d{2}\\b[^\\)]*\\)', text)\n    return float(len(year_paren_matches)) / float(num_sent)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (clauseiness / sentence complexity)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Ratio of adjacent stopword bigrams to all bigrams (measures heavy function-word sequences)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','a','an','in','on','and','or','of','to','for','with','by','is','are','was','were','that','this','it','as','at','from','be','have','has','had','not'}\n    tokens = re.findall(r\"\\w+'?\\w*|\\w+\", text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = sum(1 for i in range(len(tokens)-1) if tokens[i] in stop and tokens[i+1] in stop)\n    return float(bigrams) / float(max(1, len(tokens)-1))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with a lowercase alphabetic character (formatting/typo signal)'\n    import re\n    if not text:\n        return 0.0\n    # split by sentence terminators but keep segments\n    segments = re.split(r'(?<=[.!?])\\s+', text)\n    starts = 0\n    total = 0\n    for seg in segments:\n        seg = seg.strip()\n        if not seg:\n            continue\n        # find first alphabetic character\n        m = re.search(r'[A-Za-z]', seg)\n        if m:\n            total += 1\n            ch = seg[m.start()]\n            if ch.islower():\n                starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts) / float(total)\n\n", "def feature(text: str) -> float:\n    'Frequency of the phrase \"et al.\" per word (academic citation marker density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    matches = re.findall(r'\\bet al\\.?', text, flags=re.I)\n    return float(len(matches)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of repeated punctuation sequences of length >=3 (e.g., \"...\", \"!!!\") per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    repeats = re.findall(r'([^\\w\\s])\\1\\1+', text)\n    return float(len(repeats)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of tokens containing technical symbols (hyphens, slashes, percent, degree, micro, \u00b1 etc.)'\n    s = text or ''\n    toks = s.split()\n    if not toks:\n        return 0.0\n    symbols = set('-/%%\u00b5\u03bc\u00b0\u00b1\u2013\u2014')\n    # count tokens that include any of these special technical characters\n    cnt = 0\n    for t in toks:\n        if any((ch in symbols) for ch in t):\n            cnt += 1\n    result = float(cnt) / float(len(toks))\n    return result\n\n", "def feature(text: str) -> float:\n    'Frequency of single-capital-letter-with-dot patterns (e.g., \"E. coli\", \"T. cruzi\") per token'\n    import re\n    s = text or ''\n    tokens = re.findall(r'\\b\\w+\\b', s)\n    if not tokens:\n        return 0.0\n    # find occurrences like 'E.' possibly followed by space\n    matches = re.findall(r'\\b[A-Z]\\.', s)\n    result = float(len(matches)) / float(len(tokens))\n    return result\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that begin with enumerators (digits, bullets, or dashes) indicating procedural steps'\n    import re\n    s = text or ''\n    lines = [ln for ln in s.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    starts = 0\n    for ln in lines:\n        if re.match(r'^\\s*(?:\\d+[\\.\\)]|[-*\u2022])\\s*', ln):\n            starts += 1\n    result = float(starts) / float(len(lines))\n    return result\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence, a proxy for syntactic complexity'\n    s = text or ''\n    num_commas = s.count(',')\n    num_sent = s.count('.') + s.count('!') + s.count('?')\n    denom = max(1, num_sent)\n    result = float(num_commas) / float(denom)\n    return result\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that look like enzyme/biochemical names ending in \"-ase\"'\n    import re\n    s = text or ''\n    words = re.findall(r'\\b\\w+\\b', s.lower())\n    if not words:\n        return 0.0\n    cnt = sum(1 for w in words if w.endswith('ase'))\n    result = float(cnt) / float(len(words))\n    return result\n\n", "def feature(text: str) -> float:\n    'Density of bracketed numeric citations (e.g., [1], [2,3]) per sentence'\n    import re\n    s = text or ''\n    matches = re.findall(r'\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]', s)\n    num_sent = max(1, s.count('.') + s.count('!') + s.count('?'))\n    result = float(len(matches)) / float(num_sent)\n    return result\n\n", "def feature(text: str) -> float:\n    'Repeated bigram ratio: 1 - (unique bigrams / total bigrams), measures templated repetition'\n    import re\n    s = text or ''\n    words = re.findall(r'\\b\\w+\\b', s.lower())\n    if len(words) < 2:\n        return 0.0\n    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n    total = len(bigrams)\n    unique = len(set(bigrams))\n    result = 1.0 - (float(unique) / float(total)) if total > 0 else 0.0\n    return result\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas indicate clause/subordinate density)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    # Count sentence separators (., !, ?). If none, treat as one sentence.\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(commas) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Passive-voice pattern density: occurrences of \"is/are/was/were/... by\" per sentence'\n    import re\n    if not text:\n        return 0.0\n    pattern = re.compile(r'\\b(?:is|are|was|were|be|been|being|am)\\b(?:\\s+\\w+){0,5}\\s+\\bby\\b', re.I)\n    matches = len(pattern.findall(text))\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(matches) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are parenthetical abbreviations like \"e.g.\", \"i.e.\", \"et al.\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+[\\w\\.\\']*\\b', text.lower())\n    if not tokens:\n        return 0.0\n    abbrev_pattern = re.compile(r'\\b(e\\.g\\.|eg|i\\.e\\.|ie|et al\\.?)\\b', re.I)\n    matches = len(abbrev_pattern.findall(text.lower()))\n    return float(matches) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of hyphenated tokens (contain \"-\") to all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are Greek-letter names (alpha, beta, gamma, ...) often in scientific text'\n    import re\n    if not text:\n        return 0.0\n    greek = {'alpha','beta','gamma','delta','epsilon','zeta','eta','theta','iota','kappa','lambda','mu','nu','xi','omicron','pi','rho','sigma','tau','upsilon','phi','chi','psi','omega'}\n    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in greek)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of parenthetical expressions that contain alphabetic characters (e.g., (see Fig. 2), (E. coli))'\n    import re\n    if not text:\n        return 0.0\n    parens = re.findall(r'\\([^)]*\\)', text)\n    if not parens:\n        return 0.0\n    has_letters = sum(1 for p in parens if re.search(r'[A-Za-z]', p))\n    return float(has_letters) / float(len(parens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with common academic sentence-openers (\"The\", \"This\", \"In\", \"For\", \"To\")'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    starters = {'the','this','in','for','to','these','those','we','our'}\n    count = 0\n    for s in sents:\n        m = re.match(r'\\s*([A-Za-z]+)', s)\n        if m and m.group(1).lower() in starters:\n            count += 1\n    return float(count) / float(len(sents))\n", "def feature(text: str) -> float:\n    'Estimate of passive-voice constructions: occurrences of be-forms followed by an -ed token per sentence'\n    import re\n    if not text:\n        return 0.0\n    be_forms = r'\\b(?:is|are|was|were|be|been|being|am)\\b'\n    # look for patterns like 'is produced', 'were observed', etc.\n    matches = re.findall(be_forms + r'\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    # normalize by number of sentences (approx)\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(matches)) / sentences\n\n", "def feature(text: str) -> float:\n    'Density of \"et al.\" style citations (occurrences of \"et al\" or \"et al.\") per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w.'\u2019]+\\b\", text)\n    if not tokens:\n        return 0.0\n    matches = re.findall(r'\\bet\\s+al\\.?\\,?', text, flags=re.IGNORECASE)\n    return float(len(matches)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of repeated lowercased bigrams (number of bigrams that occur more than once divided by total bigrams)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if len(words) < 2:\n        return 0.0\n    bigrams = ['%s %s' % (words[i], words[i+1]) for i in range(len(words)-1)]\n    from collections import Counter\n    counts = Counter(bigrams)\n    repeated = sum(1 for b, c in counts.items() if c > 1)\n    return float(repeated) / max(1, len(bigrams))\n\n", "def feature(text: str) -> float:\n    'Density of hedging/softening verbs and adjectives (may, might, suggest, likely, possible, indicate, etc.) per token'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','suggest','suggests','indicate','indicates','possible','possibly','likely','appear','appears','tend','tends','may be','is likely','is possible','we propose'}\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = 0\n    txt_lower = text.lower()\n    # count multiword hedges first (simple)\n    for h in ['may be','is likely','is possible','we propose']:\n        if h in txt_lower:\n            count += txt_lower.count(h)\n    # count single-token hedges\n    for t in tokens:\n        if t in hedges:\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of acronym-definition patterns where an all-caps token appears inside parentheses (e.g., \"polymerase chain reaction (PCR)\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    # find patterns like '(PCR)' or '(RNA)'\n    matches = re.findall(r'\\(([A-Z]{2,6})\\)', text)\n    # also look for definition pattern: words before '(' then all-caps inside\n    # count distinct matches\n    return float(len(matches)) / len(tokens)\n", "def feature(text: str) -> float:\n    'Ratio of detected Latin binomial-like phrases (Capitalized lowercase + lowercase, e.g., \"Homo sapiens\") to token count'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = len(words)\n    if word_count == 0:\n        return 0.0\n    matches = re.findall(r'\\b[A-Z][a-z]{2,}\\s+[a-z]{2,}\\b', text)\n    # each match covers two words; normalize by token count\n    return float(len(matches) * 2) / word_count\n\n", "def feature(text: str) -> float:\n    'Frequency of explicit figure/table mentions (Figure, Fig., Table) per sentence, indicating formal scientific formatting'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    mentions = re.findall(r'\\b(?:fig\\.|figure|table|fig)\\b', text, flags=re.I)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    return float(len(mentions)) / max(1, len(sentences))\n\n", "def feature(text: str) -> float:\n    'Density of \" by \" agent phrases per sentence (common in passive constructions like \"was discovered by\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    by_matches = re.findall(r'\\bby\\b', text, flags=re.I)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    return float(len(by_matches)) / max(1, len(sentences))\n\n", "def feature(text: str) -> float:\n    'Frequency of complex \"of the\"/\"of a\"/\"of an\" noun-of-noun constructs per sentence, common in dense academic prose'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    patterns = re.findall(r'\\bof the\\b|\\bof a\\b|\\bof an\\b', text, flags=re.I)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    return float(len(patterns)) / max(1, len(sentences))\n\n", "def feature(text: str) -> float:\n    'Variance of sentence lengths (in tokens); high variance can indicate mixed rhetorical structures vs. uniform generated prose'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    raw_sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(raw_sentences) <= 1:\n        return 0.0\n    lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in raw_sentences]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Density of bracketed numeric citations (e.g., [1], [2,3]) per sentence, typical in formal academic writing'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    matches = re.findall(r'\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]', text)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    return float(len(matches)) / max(1, len(sentences))\n\n", "def feature(text: str) -> float:\n    'Average number of colons per sentence (colons often introduce lists, definitions, or section-like clauses in technical writing)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    colon_count = text.count(':')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    return float(colon_count) / max(1, len(sentences))\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (semicolons indicate clause-heavy academic style)'\n    import re\n    if not text:\n        return 0.0\n    semicolons = text.count(';')\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(semicolons) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are measurement units or numeric tokens immediately followed by units (ml, mg, \u00b0C, %, etc.)'\n    import re\n    if not text:\n        return 0.0\n    units = {'ml','l','mg','g','kg','\u00b5l','ul','mm','cm','m','mol','mM','M','kda','kb','bp','kbp','%','\u00b0c','\u00b0f'}\n    # count explicit unit tokens\n    tokens = re.findall(r\"\\b[\\w%\u00b0\u00b5\u03bc]+(?:\\b|$)\", text.lower())\n    if not tokens:\n        return 0.0\n    unit_count = 0\n    # detect patterns like '10ml' or '10 ml' and standalone unit tokens\n    unit_pattern = re.compile(r'\\d+(?:[\\.,]\\d+)?\\s*(%|\u00b0c|\u00b0f|ml|l|mg|g|kg|\u00b5l|ul|mm|cm|m|mol|mmol|mM|M|kda|kb|bp|kbp)\\b', re.I)\n    for m in unit_pattern.finditer(text):\n        unit_count += 1\n    for t in tokens:\n        if t.strip('.,;:()[]') in units:\n            unit_count += 1\n    # normalize by token count\n    return float(unit_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Frequency of \"et al\" citation mentions normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r'\\bet al\\.?\\b', text, flags=re.I)\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(matches)) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Density of explicit numeric expressions (integers, decimals, scientific notation, ranges) per word token'\n    import re\n    if not text:\n        return 0.0\n    nums = re.findall(r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?(?:[eE][-+]?\\d+)?\\b|\\b\\d+\\s*[-\u2013]\\s*\\d+\\b', text)\n    word_count = max(1, len(re.findall(r'\\w+', text)))\n    return float(len(nums)) / word_count\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that are Title-Case style headings (most words start with uppercase followed by lowercase)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_title_case_line(ln: str) -> bool:\n        words = re.findall(r\"[A-Za-z][A-Za-z']*\", ln)\n        if not words:\n            return False\n        title_like = 0\n        for w in words:\n            if len(w) > 1 and w[0].isupper() and w[1:].islower():\n                title_like += 1\n            elif len(w) == 1 and w.isupper():\n                title_like += 1\n        return (title_like / len(words)) >= 0.6\n    title_lines = sum(1 for ln in lines if is_title_case_line(ln))\n    return float(title_lines) / len(lines)\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (indicative of complex, academic clause-joining)'\n    if not text:\n        return 0.0\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences <= 0:\n        sentences = 1\n    return float(text.count(';')) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences <= 0:\n        return float(len(words))\n    return float(len(words)) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: uniqueness of word usage (unique word count / total word count)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (clause density indicator)'\n    if not text:\n        return 0.0\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences <= 0:\n        sentences = 1\n    return float(text.count(',')) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated (contain - or \u2013 or \u2014), common in technical/compound terms'\n    import re\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphen_tokens = sum(1 for t in tokens if '-' in t or '\u2013' in t or '\u2014' in t)\n    return float(hyphen_tokens) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are uppercase acronyms (all-caps tokens length>=2)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    acronyms = 0\n    for t in tokens:\n        if len(t) >= 2 and t.isalpha() and t.upper() == t and not t.islower():\n            acronyms += 1\n    return float(acronyms) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Parenthetical-year density: number of parenthetical groups that contain a 4-digit year per sentence'\n    import re\n    if not text:\n        return 0.0\n    paren_with_year = len(re.findall(r'\\([^)]*(?:18|19|20)\\d{2}[^)]*\\)', text))\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences <= 0:\n        sentences = 1\n    return float(paren_with_year) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Bibliographic-author-patterns per sentence (patterns like \"Lastname, F.\" or \"Lastname, First\")'\n    import re\n    if not text:\n        return 0.0\n    # matches patterns like \"Smith, J.\" or \"Smith, John\"\n    patterns = re.findall(r'\\b[A-Z][a-z]+,\\s+(?:[A-Z]\\.|[A-Z][a-z]+)\\b', text)\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences <= 0:\n        sentences = 1\n    return float(len(patterns)) / float(sentences)\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (captures clause density / syntactic complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    comma_count = text.count(',')\n    if not sentences:\n        # if no clear sentence split, normalize by word count instead\n        words = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text)\n        return float(comma_count) / max(1.0, len(words))\n    return float(comma_count) / max(1.0, len(sentences))\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with -ly (adverb/adjective forms) relative to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text)\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.lower().endswith('ly'))\n    return float(ly_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending with -ing (gerunds/participles) as a proxy for nominalization and progressive aspect'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text)\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.lower().endswith('ing'))\n    return float(ing_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain digits (numerical density, tables/technical style indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\S+\\b\", text)\n    if not tokens:\n        return 0.0\n    digit_count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(digit_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are long (>20 words) indicating complex sentence structure'\n    import re\n    if not text:\n        return 0.0\n    raw_sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not raw_sentences:\n        # if no clear sentences, treat whole text as one sentence\n        raw_sentences = [text.strip()]\n    long_count = 0\n    for s in raw_sentences:\n        words = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", s)\n        if len(words) > 20:\n            long_count += 1\n    return float(long_count) / float(len(raw_sentences))\n\n", "def feature(text: str) -> float:\n    'Simple passive-voice indicator: occurrences of auxiliary verbs followed by past participle (-ed or -en) per sentence'\n    import re\n    if not text:\n        return 0.0\n    # look for patterns like \"was eaten\", \"has been eaten\", \"is called\"\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|has been|have been|had been|has|have|had)\\b\\s+\\w+(?:ed|en)\\b', flags=re.I)\n    occurrences = len(pattern.findall(text))\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # normalize by words if no clear sentences\n        words = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text)\n        denom = max(1.0, len(words))\n    else:\n        denom = max(1.0, len(sentences))\n    return float(occurrences) / float(denom)\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total tokens (lexical diversity)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of modal verbs (could, would, should, may, might, must, can) per token'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'could', 'would', 'should', 'may', 'might', 'must', 'can'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that look like a section heading (single/titleword followed by newline)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l.rstrip() for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    heading_like = 0\n    for line in lines:\n        # Consider a heading if it is short (<=4 words), begins with a capital letter,\n        # and mostly alphabetic (few punctuation) \u2014 typical \"Introduction\", \"Conclusion\"\n        words = re.findall(r'\\w+', line)\n        if 1 <= len(words) <= 4 and words[0][0].isupper():\n            # strong heading clue: line ends without punctuation or is very short\n            heading_like += 1\n    return float(heading_like) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, we, our, us) to total tokens'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    pronouns = {'i', 'me', 'my', 'mine', 'we', 'our', 'us', 'ours'}\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of words in segments between commas (clause/chunk length), robust to no commas'\n    import re\n    if not text:\n        return 0.0\n    # Break text into comma-separated chunks, but consider sentence boundaries too\n    chunks = []\n    for sentence in re.split(r'[.!?]+', text):\n        sentence = sentence.strip()\n        if not sentence:\n            continue\n        parts = [p.strip() for p in sentence.split(',') if p.strip() != '']\n        if parts:\n            chunks.extend(parts)\n    if not chunks:\n        # fallback to average words per sentence\n        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n        if not sentences:\n            return 0.0\n        counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n        return float(sum(counts)) / len(counts) if counts else 0.0\n    counts = [len(re.findall(r'\\w+', c)) for c in chunks]\n    counts = [c for c in counts if c > 0]\n    if not counts:\n        return 0.0\n    return float(sum(counts)) / len(counts)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a discourse/transition word (however, but, moreover, therefore, thus, meanwhile)'\n    import re\n    if not text:\n        return 0.0\n    transitions = {'however', 'but', 'moreover', 'therefore', 'thus', 'meanwhile', 'consequently', 'nevertheless', 'furthermore', 'nonetheless'}\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        # fallback: split on punctuation\n        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        # find first word token\n        m = re.search(r'\\b\\w+\\b', s.lower())\n        if m and m.group(0) in transitions:\n            count += 1\n    return float(count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Ratio of adjacent repeated word bigrams (e.g., \"the the\") to total bigrams, catches simple duplication errors'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = zip(tokens, tokens[1:])\n    total = 0\n    repeats = 0\n    for a, b in bigrams:\n        total += 1\n        if a == b:\n            repeats += 1\n    return float(repeats) / total if total else 0.0\n", "def feature(text: str) -> float:\n    'Newline density: fraction of characters that are newline characters (indicates paragraph/section structure)'\n    if not text:\n        return 0.0\n    return text.count('\\n') / float(len(text))\n\n", "def feature(text: str) -> float:\n    'Ratio of mid-sentence capitalized tokens (start with uppercase, not acronyms) to total tokens; catches proper nouns or inconsistent capitalization'\n    import re\n    tokens = list(re.finditer(r'\\b\\w+\\b', text))\n    if not tokens:\n        return 0.0\n    mid_caps = 0\n    for ti, m in enumerate(tokens):\n        tok = m.group(0)\n        start = m.start()\n        # previous character (if any)\n        prev_char = text[start-1] if start > 0 else ''\n        # consider it mid-sentence if previous char is not a sentence terminator or newline (approx.)\n        if tok and tok[0].isupper() and not tok.isupper() and prev_char not in ('.', '!', '?', '\\n', '\\r'):\n            mid_caps += 1\n    return mid_caps / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are all-uppercase acronyms (length >= 2), e.g., \"IBM\", \"USA\"'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    acronyms = sum(1 for t in tokens if len(t) >= 2 and t.isupper() and any(c.isalpha() for c in t))\n    return acronyms / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of colons and semicolons per sentence (captures list/heading and clause usage)'\n    import re\n    if not text:\n        return 0.0\n    colon_sem = text.count(':') + text.count(';')\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return colon_sem / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a digit (detects enumerations, dates, numeric lists, or coded labels)'\n    import re\n    tokens = re.findall(r'\\b\\S+\\b', text)\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return digit_tokens / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), robust to empty input'\n    import re\n    if not text:\n        return 0.0\n    # Rough sentence splits: keep segments separated by .!? characters\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(s.split()) for s in sentences]\n    return float(sum(lengths)) / len(lengths)\n\n", "def feature(text: str) -> float:\n    'Punctuation variety score: number of distinct punctuation characters divided by total punctuation occurrences (higher = more varied punctuation use)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    unique = len(set(puncts))\n    total = len(puncts)\n    return unique / float(total)\n\n", "def feature(text: str) -> float:\n    'Normalized word-length standard deviation (stddev / mean word length), measures variability in word lengths'\n    import re, math\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / float(len(lengths))\n    if mean == 0:\n        return 0.0\n    variance = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    stddev = math.sqrt(variance)\n    return stddev / mean\n", "def feature(text: str) -> float:\n    'Density of time expressions like \"7:00\", \"07:30 am\" per token (captures logs/diaries)'\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    times = re.findall(r'\\b\\d{1,2}:\\d{2}\\b(?:\\s?(?:am|pm))?|\\b\\d{1,2}\\s?(?:am|pm)\\b', text, flags=re.I)\n    return float(len(times)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (semicolon usage / sentence count)'\n    import re\n    if not text:\n        return 0.0\n    semicolons = text.count(';')\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return semicolons / sentence_count\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+'?\\w*\\b\", text)\n    if not tokens:\n        return 0.0\n    first_person = set(['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours', \"i'm\", \"i've\", \"i'd\", \"i'll\"])\n    count = sum(1 for t in tokens if t.lower() in first_person)\n    return count / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Colon frequency per sentence (captures labels, headings, and descriptive timestamps)'\n    import re\n    if not text:\n        return 0.0\n    colon_count = text.count(':')\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return colon_count / sentence_count\n\n", "def feature(text: str) -> float:\n    'Fraction of mid-sentence capitalized tokens (capitalized tokens not at apparent sentence start) suggesting named entities'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    cap_mid = 0\n    # Build a simple list of sentence-start token indices by checking preceding token end punctuation.\n    sentence_boundaries = set()\n    for i, tok in enumerate(tokens):\n        if i == 0:\n            sentence_boundaries.add(0)\n        else:\n            prev = tokens[i-1]\n            if re.search(r'[.!?]$', prev):\n                sentence_boundaries.add(i)\n    for i, tok in enumerate(tokens):\n        # consider alpha-starting tokens\n        if i not in sentence_boundaries and tok[0].isupper() and any(c.isalpha() for c in tok):\n            cap_mid += 1\n    return cap_mid / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain any digit (numerical token density)'\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if re.search(r'\\d', t))\n    return num / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of explicit enumerative list markers (numeric lists like \"1.\", \"2)\", bullet-like leading dashes) per sentence'\n    import re\n    if not text:\n        return 0.0\n    # markers: digit+., digit+), line-start dash/bullet\n    markers = re.findall(r'(?m)^\\s*[-*\u2022]\\s+|\\b\\d+\\.\\s|\\b\\d+\\)\\s', text)\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return len(markers) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Density of common abbreviations (e.g., e.g., i.e., etc., Mr., Dr.) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    abbrev_matches = re.findall(r'\\b(?:e\\.g|i\\.e|etc|vs|Mr|Mrs|Ms|Dr|Prof|Sr|Jr)\\.', text, flags=re.I)\n    return len(abbrev_matches) / len(tokens)\n", "def feature(text: str) -> float:\n    'Frequency of common author-year citation patterns (e.g., \"(Smith, 2019)\" or \"Smith et al., 2019\") per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    patterns = [\n        r'\\([A-Za-z][A-Za-z0-9\\-\\s\\.]+,\\s*\\d{4}\\)',   # (Lastname, 2019) or (Lastname et al., 2019)\n        r'\\b[A-Z][A-Za-z\\-]+ et al\\.,?\\s*\\d{4}\\b',   # Lastname et al., 2019\n        r'\\[\\s*[A-Za-z][A-Za-z\\-\\s\\.]*\\d{4}\\s*\\]'    # [Lastname 2019] etc.\n    ]\n    combined = '|'.join('(?:%s)' % p for p in patterns)\n    matches = re.findall(combined, text)\n    return float(len(matches)) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of quotation characters that are curly (typographic) quotes versus all quotes (captures formal editing with \u201c \u201d or \u2018 \u2019)'\n    if not text:\n        return 0.0\n    curly = sum(text.count(c) for c in '\u201c\u201d\u2018\u2019')\n    ascii_quotes = text.count('\"') + text.count(\"'\")\n    total = curly + ascii_quotes\n    return float(curly) / float(total) if total else 0.0\n\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per token to capture trailing/elliptical styles'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    ellipses = text.count('...')\n    return float(ellipses) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that look like Roman numerals (e.g., \"XVI\", \"XVII\") after stripping surrounding punctuation'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    roman_re = re.compile(r'^(?i:M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))$')\n    def clean(t):\n        return t.strip('.,;:()[]{}\"\\'')\n    count = 0\n    for t in tokens:\n        ct = clean(t)\n        if ct and roman_re.match(ct):\n            count += 1\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Proportion of mid-sentence tokens (not the first word of a sentence) that start with an uppercase letter (captures proper nouns and formatted headings within sentences)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences naively\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+', text)\n    total_mid = 0\n    mid_caps = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        words = s.split()\n        if len(words) <= 1:\n            continue\n        for w in words[1:]:\n            total_mid += 1\n            # consider uppercase if first alphabetical char is uppercase\n            for ch in w:\n                if ch.isalpha():\n                    if ch.isupper():\n                        mid_caps += 1\n                    break\n    return float(mid_caps) / float(total_mid) if total_mid else 0.0\n\n\n", "def feature(text: str) -> float:\n    'Count of four-digit year mentions (1000-2099) per token (useful for historical/academic texts with dates and citations)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    years = re.findall(r'\\b(?:1[0-9]{3}|20[0-9]{2})\\b', text)\n    return float(len(years)) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that contain internal periods or multiple periods (captures abbreviations like \"U.S.\" or dotted abbreviations)'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if '.' in t:\n            # strip trailing punctuation but keep internal dots\n            stripped = t.strip('.,;:()[]{}\"\\'')\n            if stripped.count('.') > 1 or ('.' in stripped and not stripped.endswith('.')):\n                count += 1\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in tokens (captures variability in sentence complexity and style)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[\\.\\?\\!])\\s+', text)\n    lengths = []\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        lengths.append(len(s.split()))\n    if not lengths:\n        return 0.0\n    mean = float(sum(lengths)) / float(len(lengths))\n    if mean == 0.0 or len(lengths) == 1:\n        return 0.0\n    var = sum((L - mean) ** 2 for L in lengths) / float(len(lengths))\n    std = math.sqrt(var)\n    return std / mean\n", "def feature(text: str) -> float:\n    'Ratio of tokens that contain any digit (years, chapter numbers, statistics) to total tokens'\n    tokens = text.split()\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count) / float(n)\n\n", "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per token, capturing trailing summaries or omitted text fragments'\n    tokens = text.split()\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    ellipses = text.count('...')\n    return float(ellipses) / float(n)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are long (>20 words) as a proxy for syntactic complexity and academic style'\n    import re\n    # split into sentences by punctuation marks, keep non-empty\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    long_count = 0\n    for s in sents:\n        words = s.split()\n        if len(words) > 20:\n            long_count += 1\n    return float(long_count) / float(len(sents))\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-like pattern rate: occurrences of \"was/were/is/are/been/being\" followed by a past-participle-like token (ends with -ed)'\n    import re\n    tokens = text.split()\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|being|be)\\b\\s+\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    return float(len(matches)) / float(n)\n\n", "def feature(text: str) -> float:\n    'Density of double-quote characters (including curly quotes) per token, indicating quoted material or titles'\n    tokens = text.split()\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    quote_chars = ['\"', '\u201c', '\u201d']\n    count = sum(text.count(ch) for ch in quote_chars)\n    return float(count) / float(n)\n\n", "def feature(text: str) -> float:\n    'Indicator (0.0 or 1.0) of an explicit chapter/section mention (e.g., \"Chapter 5\" or \"chapter\")'\n    import re\n    if not text:\n        return 0.0\n    if re.search(r'\\bchapter\\b\\s*\\d+', text, re.IGNORECASE) or re.search(r'\\bchapter\\b', text, re.IGNORECASE):\n        return 1.0\n    return 0.0\n\n", "def feature(text: str) -> float:\n    'Rate of abbreviation patterns (e.g., \"e.g.\", \"i.e.\", \"U.S.\", \"U.K.\") per token, detecting formal examples and references'\n    import re\n    tokens = text.split()\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    # patterns like U.S. or A.B.C. and common Latin abbreviations\n    patterns = re.findall(r'\\b(?:[A-Za-z]\\.){2,}\\b', text)  # U.S., U.S.A.\n    patterns += re.findall(r'\\b(?:e\\.g\\.|i\\.e\\.|etc\\.)', text, re.IGNORECASE)\n    return float(len(patterns)) / float(n)\n", "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per sentence (captures clause-complexity style)'\n    import re\n    semis = text.count(';')\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(semis) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Ratio of titlecase tokens (Word with initial capital and rest lower) excluding the very first token'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    if len(words) <= 1:\n        return 0.0\n    titlecase_count = 0\n    for w in words[1:]:\n        if len(w) > 1 and w[0].isupper() and w[1:].islower():\n            titlecase_count += 1\n    return float(titlecase_count) / float(len(words) - 1)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated (contain -, \u2013 or \u2014), indicating compound/adjective usage'\n    import re\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphenated = sum(1 for t in tokens if '-' in t or '\u2013' in t or '\u2014' in t)\n    return float(hyphenated) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of smart/curly quotes to all quote-like characters (smart vs straight quotes)'\n    smart = text.count('\u2018') + text.count('\u2019') + text.count('\u201c') + text.count('\u201d')\n    straight = text.count('\"') + text.count(\"'\")\n    total = smart + straight\n    if total == 0:\n        return 0.0\n    return float(smart) / float(total)\n\n", "def feature(text: str) -> float:\n    'Average run length of consecutive common stopwords (captures function-word sequencing)'\n    import re\n    stopwords = {'the','and','of','to','in','a','is','that','it','for','was','with','as','by','on','at','from','be','this','which','or'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    runs = []\n    current = 0\n    for t in tokens:\n        if t in stopwords:\n            current += 1\n        else:\n            if current > 0:\n                runs.append(current)\n                current = 0\n    if current > 0:\n        runs.append(current)\n    if not runs:\n        return 0.0\n    return float(sum(runs)) / float(len(runs))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens with formal/academic suffixes (e.g., -ize, -ology, -ism) as a proxy for register'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ize', 'ise', 'ology', 'graphy', 'ism', 'ist', 'ary', 'ical', 'ative', 'atory', 'ent', 'ant')\n    count = sum(1 for t in tokens if len(t) > 4 and any(t.endswith(s) for s in suffixes))\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Approximate passive voice indicator: ratio of be-verbs followed shortly by an -ed word'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    token_count = max(1, len(tokens))\n    pattern = re.compile(r'\\b(?:is|are|was|were|be|been|being|am)\\b(?:\\s+\\w+){0,2}\\s+\\w+ed\\b', flags=re.IGNORECASE)\n    matches = len(pattern.findall(text))\n    return float(matches) / float(token_count)\n", "def feature(text: str) -> float:\n    'Fraction of short title-like lines: lines of 1-6 words where each word starts with an uppercase letter (heuristic for headings)'\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_title_like(line):\n        if any(line.endswith(p) for p in '.:!?'):\n            return False\n        words = line.split()\n        if not (1 <= len(words) <= 6):\n            return False\n        for w in words:\n            if not w[0].isupper():\n                return False\n        return True\n    title_lines = sum(1 for ln in lines if is_title_like(ln))\n    return float(title_lines) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences per token (captures trailing elisions and informal continuations)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    tokens = len(re.findall(r'\\w+', text))\n    if tokens == 0:\n        return float(ellipses)\n    return float(ellipses) / tokens\n\n", "def feature(text: str) -> float:\n    'Fraction of smart/curly quotes and apostrophes among all quote-like characters (\u2019 \u2018\u2018 \u201c \u201d vs straight \\' and \")'\n    if not text:\n        return 0.0\n    curly = text.count('\u2019') + text.count('\u2018') + text.count('\u201c') + text.count('\u201d')\n    straight = text.count(\"'\") + text.count('\"')\n    total = curly + straight\n    if total == 0:\n        return 0.0\n    return float(curly) / total\n\n", "def feature(text: str) -> float:\n    'Ratio of hyphenated tokens to total tokens (detects compound adjectives, intra-word hyphenation common in edited human text)'\n    import re\n    if not text:\n        return 0.0\n    hyphenated = re.findall(r'\\b\\w+(?:[-\u2013\u2014]\\w+)+\\b', text)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(len(hyphenated))\n    return float(len(hyphenated)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Adverb density: fraction of tokens ending in \"ly\" (approximate measure of adverbial/modifying style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.endswith('ly'))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Entropy of punctuation distribution across common punctuation types (.,;:!?()\\\"\\'-); higher entropy => more varied punctuation use'\n    import math, re\n    if not text:\n        return 0.0\n    categories = {\n        '.': text.count('.'),\n        ',': text.count(','),\n        ';': text.count(';'),\n        ':': text.count(':'),\n        '?': text.count('?'),\n        '!': text.count('!'),\n        'paren': text.count('(') + text.count(')'),\n        'quote': text.count('\"') + text.count(\"'\") + text.count('\u201c') + text.count('\u201d') + text.count('\u2019') + text.count('\u2018'),\n        'hyphen': text.count('-') + text.count('\u2013') + text.count('\u2014')\n    }\n    total = sum(categories.values())\n    if total == 0:\n        return 0.0\n    entropy = 0.0\n    for v in categories.values():\n        if v > 0:\n            p = v / total\n            entropy -= p * math.log(p)\n    # return raw entropy (can be normalized downstream if desired)\n    return float(entropy)\n\n", "def feature(text: str) -> float:\n    'Crude passive-voice indicator: count of \"be\"-forms followed within up to two tokens by an -ed word, normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|am)\\b(?:\\s+\\w+){0,2}\\s+\\w+ed\\b', lowered)\n    sent_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(matches)) / sent_count\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are capitalized but not sentence-initial (captures named-entity mid-sentence density)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    mid_caps = 0\n    total = 0\n    for s in sentences:\n        words = re.findall(r\"\\b[\\w\u2019'-]+\\b\", s, re.UNICODE)\n        if not words:\n            continue\n        for w in words[1:]:\n            total += 1\n            # exclude lone \"I\"\n            if len(w) > 1 and w[0].isupper():\n                mid_caps += 1\n    if total == 0:\n        return 0.0\n    return float(mid_caps) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen or en-dash (captures ranges, compounds like \"XVI-XVII\" or \"state-of-the-art\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w\u2019'-]+\\b\", text, re.UNICODE)\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if ('-' in t or '\u2013' in t or '\u2014' in t))\n    return float(hyphen_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of passive-voice cues (forms of \"be\" followed by -ed word) per token as a noisy indicator of passive constructions'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w\u2019'-]+\\b\", text, re.UNICODE)\n    token_count = max(1, len(tokens))\n    # look for patterns like \"was created\", \"is followed\"\n    matches = re.findall(r'\\b(?:is|was|were|are|be|been|am|seems|appears|has been|have been|had been|will be)\\s+[A-Za-z]+ed\\b', text, flags=re.I)\n    return float(len(matches)) / token_count\n\n", "def feature(text: str) -> float:\n    'Diversity of punctuation: unique punctuation character types divided by total punctuation occurrences (0 if no punctuation)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    unique = len(set(puncts))\n    total = len(puncts)\n    return float(unique) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like abbreviations with internal periods (e.g., \"U.S.\", \"e.g.\", \"et al.\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\S+\", text)\n    if not tokens:\n        return 0.0\n    abbrev = 0\n    for t in tokens:\n        # token contains at least one internal dot (not trailing sentence dot)\n        if re.search(r'\\w\\.\\w', t):\n            abbrev += 1\n    return float(abbrev) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that contain a digit (numeral density, catches centuries, dates, enumerations)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w\u2019'-]+\\b\", text, re.UNICODE)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (captures typographic apostrophes, accented letters often in edited human text)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / total\n", "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: auxiliaries (was/were/is/are/been/being) followed by a past-participle-like token (/ed|en|wn|t endings)'\n    import re\n    if not text:\n        return 0.0\n    # look for patterns like \"was created\", \"were established\", \"is driven\", \"has been cited\"\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|has|had|have)(?:\\s+been)?\\s+[A-Za-z-]+(?:ed|en|wn|t)\\b', re.I)\n    matches = pattern.findall(text)\n    words = re.findall(r'\\b\\w+\\b', text)\n    return float(len(matches)) / len(words) if words else 0.0\n\n", "def feature(text: str) -> float:\n    'Ratio of modal and hedging words (may,might,could,may,appears,suggests,seems,likely,possibly,perhaps) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    hedge_words = {'may','might','could','would','should','can','must','ought',\n                   'suggest','suggests','suggests','suggested','appear','appears','appeared',\n                   'seem','seems','seemed','tend','tends','tended','likely','possibly','perhaps',\n                   'indicate','indicates','indicated','arguably'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedge_words)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit (captures numeric-heavy prose like reports, dates, measures)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like Roman numerals (I V X L C D M), possibly hyphenated (e.g., \"XVI-XVII\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[^\\s]+\\b', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'^[IVXLCDMivxlcdm]+(?:-[IVXLCDMivxlcdm]+)?$')\n    count = sum(1 for t in tokens if pattern.match(t))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of em-dash style usage: count of em-dash or double-hyphen occurrences normalized by text length'\n    import re\n    if not text:\n        return 0.0\n    em_count = text.count('\u2014') + text.count('--')\n    length = len(text)\n    return float(em_count) / length if length > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Average number of word tokens inside parentheses normalized by total token count (0 if no parentheses)'\n    import re\n    if not text:\n        return 0.0\n    parens = re.findall(r'\\((.*?)\\)', text, re.S)\n    if not parens:\n        return 0.0\n    total_inside = sum(len(re.findall(r'\\b\\w+\\b', p)) for p in parens)\n    num = len(parens)\n    total_tokens = len(re.findall(r'\\b\\w+\\b', text))\n    if total_tokens == 0:\n        return 0.0\n    average_inside = float(total_inside) / num\n    return average_inside / total_tokens\n", "def feature(text: str) -> float:\n    'Semicolon density: semicolons per 1000 characters (captures clause-linking complexity)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    semis = text.count(';')\n    # scale to per-1000 characters for numeric stability across lengths\n    return float(semis) / float(total_chars) * 1000.0\n\n", "def feature(text: str) -> float:\n    'Nominalization ratio: fraction of tokens ending with typical noun-forming suffixes (-tion, -ment, -ness, -ity, -ence, -ance)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9\u2019']+\", text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ( 'tion','sion','ment','ness','ity','ence','ance' )\n    count = 0\n    for t in tokens:\n        for s in suffixes:\n            if t.endswith(s):\n                count += 1\n                break\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Roman numeral / century pattern density: fraction of tokens that look like Roman numerals (e.g., XVI, XIX or ranges like XVI-XVII)'\n    import re\n    if not text:\n        return 0.0\n    # uppercase for matching Roman letters reliably\n    upper = text.upper()\n    # match single roman tokens or hyphenated ranges like XVI-XVII\n    matches = re.findall(r'\\b[M|D|C|L|X|V|I]+(?:-[M|D|C|L|X|V|I]+)*\\b', upper)\n    tokens = re.findall(r\"[A-Za-z0-9\u2019']+\", text)\n    if not tokens:\n        return 0.0\n    return float(len(matches)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Hedging/adverbial uncertainty ratio: fraction of tokens that are common hedging words (perhaps, possibly, likely, seems, appears, arguably, may, might, could)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9\u2019']+\", text.lower())\n    if not tokens:\n        return 0.0\n    hedges = {'perhaps','possibly','likely','seems','appears','arguably','undeniably','may','might','could','tends','tend','suggests','suggest'}\n    count = 0\n    for t in tokens:\n        if t in hedges:\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Definitional phrase ratio: occurrences of \"is a\", \"is an\", \"is the\", \"is defined as\", \"refers to\" per token (captures definitional/expository style)'\n    import re\n    if not text:\n        return 0.0\n    patterns = [r'\\bis a\\b', r'\\bis an\\b', r'\\bis the\\b', r'\\bis defined as\\b', r'\\brefers to\\b', r'\\bcan be defined as\\b', r'\\bmeans that\\b', r'\\bis known as\\b']\n    count = 0\n    lower = text.lower()\n    for p in patterns:\n        count += len(re.findall(p, lower))\n    tokens = re.findall(r\"[A-Za-z0-9\u2019']+\", text)\n    if not tokens:\n        return 0.0\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Digit token ratio: fraction of tokens that contain at least one digit (dates, enumerations, measurements)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9\u2019']+\", text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(ch.isdigit() for ch in t):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Sentence-initial pronoun ratio: fraction of sentences that start with a pronoun (I, we, you, he, she, it, they) \u2014 signals personal/narrative tone'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    pronouns = {'i','we','you','he','she','it','they','we\\'re','i\\'m','they\\'re','weve','theyve'}\n    count = 0\n    for s in sents:\n        # find first token-like sequence\n        m = re.search(r\"[A-Za-z0-9\u2019']+\", s)\n        if m:\n            first = m.group(0).lower()\n            if first in pronouns:\n                count += 1\n    return float(count) / float(len(sents))\n", "def feature(text: str) -> float:\n    'Ratio of words that look like nominalizations (common formal suffixes like -tion, -ment, -ity, -ness)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    suffixes = ('tion', 'sion', 'ment', 'ity', 'ness', 'ence', 'ance', 'alism', 'ization')\n    count = 0\n    for w in words:\n        for s in suffixes:\n            if w.endswith(s) and len(w) > len(s) + 2:\n                count += 1\n                break\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of semicolons and colons per character (indicator of formal/complex sentence structure)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    punct_count = text.count(';') + text.count(':')\n    return float(punct_count) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, we, my, our, me, us, mine, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'we', 'me', 'us', 'my', 'our', 'mine', 'ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of hedging/modality words (may, might, could, suggest, appears, perhaps, possibly) to tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    hedges = {'may', 'might', 'could', 'seem', 'seems', 'appear', 'appears', 'suggest', 'suggests', 'tend', 'tends', 'perhaps', 'possibly', 'indicate', 'indicates'}\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: fraction of occurrences like \"was|were|is|are|has|have|had\" followed by a past-participle-like token (-ed)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    # Look for sequences: auxiliary + word ending in ed (approximate past-participle)\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|has|have|had)\\b\\s+\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    return float(len(matches)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters used normalized by total punctuation count'\n    import re, string\n    if not text:\n        return 0.0\n    puncts = [c for c in text if c in string.punctuation]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Density of short all-caps abbreviations (2-5 letters) which often appear in formal/official texts'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[A-Z]{2,5}\\b', text)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(tokens)) / float(len(words))\n", "def feature(text: str) -> float:\n    'Fraction of lines that look like short Title-Case headings (e.g., \"Case Summary\")'\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_title_case_line(ln: str) -> bool:\n        parts = ln.split()\n        if len(parts) < 1 or len(parts) > 6:\n            return False\n        # require most words to start with uppercase and have at least one lowercase letter after\n        good = 0\n        for p in parts:\n            if len(p) >= 2 and p[0].isupper() and any(ch.islower() for ch in p[1:]):\n                good += 1\n        return good >= max(1, len(parts) - 1)\n    count = sum(1 for ln in lines if is_title_case_line(ln))\n    return float(count) / float(len(lines))\n\n\n", "def feature(text: str) -> float:\n    'Approximate passive voice score: count of \"was/were/been/being\" + past-participle occurrences per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    # pattern: auxiliaries followed by a word ending in \"ed\" or \"en\" (simple heuristic) or \"by <agent>\"\n    passive_matches = len(re.findall(r'\\b(?:was|were|is|are|been|being|had been|has been|was being|were being)\\b\\s+\\w+(?:ed|en)\\b', text, flags=re.IGNORECASE))\n    by_matches = len(re.findall(r'\\bby\\s+[A-Z0-9a-z_\\-]+\\b', text, flags=re.IGNORECASE))\n    score = (passive_matches + 0.5 * by_matches) / float(sent_count)\n    return float(score)\n\n\n", "def feature(text: str) -> float:\n    'Density of multiword TitleCase sequences (likely named entities) per token'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    # count occurrences of consecutive TitleCase words, e.g., \"Christopher Lee\", \"Tulon Company\"\n    multi_title = len(re.findall(r'\\b[A-Z][a-z]+\\s+[A-Z][a-z]+\\b', text))\n    return float(multi_title) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that match a curated list of legal/journalistic/domain lexical cues'\n    import re\n    if not text:\n        return 0.0\n    cue_words = {'case','offense','offence','detention','protocol','employee','terminated','terminated','company','communications','directed','film','plaintiff','defendant','interview','authorized','supervisory','charge','charges','production','terminated','terminated','terminated'}\n    tokens = [t.lower() for t in re.findall(r'\\b[A-Za-z]+\\b', text)]\n    if not tokens:\n        return 0.0\n    matches = sum(1 for t in tokens if t in cue_words)\n    return float(matches) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Variance of sentence lengths in words (higher indicates mixed sentence structure)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sentences) < 2:\n        return 0.0\n    lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    return float(var)\n\n\n", "def feature(text: str) -> float:\n    'Average number of colons or semicolons per sentence (indicator of formal/explanatory structure)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    colons = text.count(':') + text.count(';')\n    return float(colons) / float(sent_count)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen (hyphenated compounds density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[\\w\\-]+\\b', text)\n    if not tokens:\n        return 0.0\n    hyphens = sum(1 for t in tokens if '-' in t)\n    return float(hyphens) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Type-token ratio (unique word count divided by total word count) as a measure of lexical richness'\n    import re\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\b[a-zA-Z]+\\b', text)]\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n", "def feature(text: str) -> float:\n    'Comma density: fraction of characters that are commas (complex, clause-heavy prose)'\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count) / max(1.0, float(len(text)))\n\n", "def feature(text: str) -> float:\n    'Passive-voice-like pattern density: counts \"be\" verbs followed by -ed forms per word'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    pattern = re.compile(r'\\b(?:is|are|was|were|be|been|being|am)\\s+\\w+ed\\b', flags=re.I)\n    matches = pattern.findall(text)\n    return float(len(matches)) / max(1.0, float(len(tokens)))\n\n", "def feature(text: str) -> float:\n    'In-text citation/parenthetical density: fraction of tokens that look like short citations (e.g., (Smith), (Smith, 2020))'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    # look for parenthetical groups that start with capital letter and are reasonably short\n    cites = re.findall(r'\\([A-Z][A-Za-z\\-\\.,&\\s]{0,40}?\\d{0,4}?\\)', text)\n    return float(len(cites)) / max(1.0, float(len(tokens)))\n\n", "def feature(text: str) -> float:\n    'Possessive vs contraction ratio: possessive \\'s occurrences divided by (possessive + common contractions)'\n    import re\n    if not text:\n        return 0.0\n    poss = len(re.findall(r\"\\b\\w+'s\\b\", text, flags=re.I))\n    # common contraction endings (including n't, 're, 've, 'll, 'd, 'm)\n    contraction = len(re.findall(r\"\\b\\w+(?:n't|'re|'ve|'ll|'d|'m)\\b\", text, flags=re.I))\n    denom = poss + contraction\n    if denom == 0:\n        return 0.0\n    return float(poss) / float(denom)\n\n", "def feature(text: str) -> float:\n    'Fraction of quotes that are typographic/smart quotes (\u2019 \u2018 \u201c \u201d) vs straight quotes'\n    if not text:\n        return 0.0\n    smart_chars = '\u2019\u2018\u201c\u201d'\n    smart_count = sum(text.count(ch) for ch in smart_chars)\n    straight_count = text.count('\"') + text.count(\"'\")\n    total = smart_count + straight_count\n    if total == 0:\n        return 0.0\n    return float(smart_count) / float(total)\n\n", "def feature(text: str) -> float:\n    'Hyphenated-word ratio: fraction of whitespace-separated tokens that contain a hyphen'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t and any(c.isalnum() for c in t.replace('-', '')))\n    return float(hyphen_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Hapax ratio: fraction of word types that occur only once (vocabulary richness indicator)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\b\\w+\\b', text)]\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    hapaxes = sum(1 for w, c in freqs.items() if c == 1)\n    return float(hapaxes) / len(words)\n\n", "def feature(text: str) -> float:\n    'Passive voice heuristic: occurrences of auxiliary + past-participial \"-ed\" pattern per sentence'\n    import re\n    if not text:\n        return 0.0\n    # count passive-like patterns\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|be|am|been|been|has been|have been|had been)\\b\\s+\\w+ed\\b', re.IGNORECASE)\n    passive_count = len(pattern.findall(text))\n    # sentence count fallback\n    sentence_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(passive_count) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Capitalized mid-sentence word ratio: proportion of words that are capitalized but not at sentence start (named entities/proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences preserving terminals\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    cap_mid = 0\n    mid_total = 0\n    for s in sentences:\n        if not s.strip():\n            continue\n        tokens = re.findall(r'\\b\\w+\\b', s)\n        if not tokens:\n            continue\n        # consider tokens after the first in sentence\n        for t in tokens[1:]:\n            mid_total += 1\n            if t[0].isupper() and not (t == 'I'):\n                cap_mid += 1\n    if mid_total == 0:\n        return 0.0\n    return float(cap_mid) / mid_total\n\n", "def feature(text: str) -> float:\n    'Unicode apostrophe density: fraction of characters that are right-single-quotation-marks (\u2019), often in human-edited prose'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    u_apost = text.count('\u2019')\n    return float(u_apost) / total_chars\n\n", "def feature(text: str) -> float:\n    'Sentence-length coefficient of variation: stddev(sentence_lengths)/mean(sentence_lengths) using word counts (measures variability)'\n    import re, math\n    if not text:\n        return 0.0\n    # split into sentences by .!? and count words per sentence\n    raw_sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    lens = []\n    for s in raw_sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        lens.append(len(words))\n    if not lens:\n        return 0.0\n    mean = sum(lens) / len(lens)\n    if mean == 0:\n        return 0.0\n    # population stddev\n    var = sum((l - mean) ** 2 for l in lens) / len(lens)\n    std = math.sqrt(var)\n    return float(std) / mean\n\n", "def feature(text: str) -> float:\n    'Title-case bigram ratio: fraction of adjacent token pairs where both tokens are capitalized (proxy for named entities like \"Martin Scorsese\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if len(tokens) < 2:\n        return 0.0\n    bigram_hits = 0\n    total_bigrams = 0\n    # exclude sentence starts to reduce false positives\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    for s in sentences:\n        toks = re.findall(r'\\b\\w+\\b', s)\n        if len(toks) < 2:\n            continue\n        for i in range(len(toks) - 1):\n            t1, t2 = toks[i], toks[i+1]\n            total_bigrams += 1\n            if t1[0].isupper() and t2[0].isupper() and t1.lower() != t1 and t2.lower() != t2:\n                bigram_hits += 1\n    if total_bigrams == 0:\n        return 0.0\n    return float(bigram_hits) / total_bigrams\n", "def feature(text: str) -> float:\n    'Type-token ratio (distinct word types divided by total tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    return float(len(set(tokens))) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of quotation characters that are curly/typographic quotes (indicative of typographic polish)'\n    if not text:\n        return 0.0\n    curly = text.count('\u201c') + text.count('\u201d') + text.count('\u2018') + text.count('\u2019')\n    straight = text.count('\"') + text.count(\"'\")\n    total = curly + straight\n    if total == 0:\n        return 0.0\n    return float(curly) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (smart quotes, em-dashes, accented letters)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / total_chars\n\n", "def feature(text: str) -> float:\n    'Ratio of words ending in -ly (heuristic for adverb/adverbial style)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if w.endswith('ly') and len(w) > 2)\n    return float(ly_count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Proxy for passive or auxiliary + -ed constructions: count of auxiliaries followed by -ed token normalized by words'\n    import re\n    if not text:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|be|being|had|has|have)\\s+\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(matches)) / len(words)\n\n", "def feature(text: str) -> float:\n    'Colon density: fraction of characters that are colons (often present in lists, subtitles, bibliographic notes)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    return float(text.count(':')) / total\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a conjunction or discourse marker (and, but, or, so, however, because, although)'\n    import re\n    if not text:\n        return 0.0\n    markers = {'and', 'but', 'or', 'so', 'also', 'however', 'because', 'although', 'therefore', 'thus', 'however'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    total = 0\n    for s in sentences:\n        words = re.findall(r\"\\b[\\w']+\\b\", s.lower())\n        if not words:\n            continue\n        total += 1\n        if words[0] in markers:\n            starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts) / total\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation characters'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / len(puncts)\n", "def feature(text: str) -> float:\n    'Semicolon density: fraction of characters that are semicolons'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    semis = text.count(';')\n    return float(semis) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Count of parenthetical citations that contain semicolons (e.g., \"(Author; Author)\") normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    parenthetical_with_semicolon = re.findall(r'\\([^)]*;[^)]*\\)', text)\n    token_count = max(1, len(re.findall(r'\\w+', text)))\n    return float(len(parenthetical_with_semicolon)) / float(token_count)\n\n", "def feature(text: str) -> float:\n    'Ratio of legal/modal words (shall, must, may, should, ought, authorized, right, permit, detain) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    keywords = {'shall','must','may','should','ought','authorized','authorize','right','entitled','permit','detain','detention'}\n    count = sum(1 for t in tokens if t in keywords)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens ending in \"ing\" (gerund/continuous form indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing'))\n    return float(ing_count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Variance of sentence lengths (in words) - higher values indicate more variation in sentence structure'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences if len(re.findall(r'\\w+', s)) > 0]\n    if len(counts) < 2:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    return float(var)\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters used divided by total punctuation count'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    total = len(puncts)\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice score: count of \"was/were/is/are/been\" followed by a past-participle-like token (ending with \"ed\") normalized by tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    # look for patterns like \"was <word>ed\", \"were <word>ed\", \"has been <word>ed\", etc.\n    matches = re.findall(r'\\b(?:was|were|is|are|has been|have been|had been|been)\\b\\s+\\w+ed\\b', text.lower())\n    return float(len(matches)) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (words per sentence) to capture structural variability'\n    if not text:\n        return 0.0\n    import re, math\n    sentences = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        counts.append(len(words))\n    if len(counts) <= 1:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / (len(counts) - 1)\n    return float(math.sqrt(var))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are likely adverbs (words ending in \"ly\")'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not words:\n        return 0.0\n    advs = sum(1 for w in words if len(w) > 2 and w.endswith('ly'))\n    return advs / len(words)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that contain a hyphen (e.g., multi-word compounds or ranges)'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphened = sum(1 for t in tokens if '-' in t or '\u2013' in t or '\u2014' in t)\n    return hyphened / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences containing a simple passive-voice cue (e.g., \"was Xed\", \"is Xed\")'\n    if not text:\n        return 0.0\n    import re\n    # find sentence chunks\n    sentences = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text.lower()) if s.strip()]\n    if not sentences:\n        return 0.0\n    passive_pattern = re.compile(r'\\b(?:is|are|was|were|be|been|being|has|have|had)\\b\\s+\\w+ed\\b')\n    passive_count = sum(1 for s in sentences if passive_pattern.search(s))\n    return passive_count / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Density of quotation characters (single and double quotes, including curly quotes) per character'\n    if not text:\n        return 0.0\n    quotes = set(['\"', '\\'', '\u201c', '\u201d', '\u2018', '\u2019'])\n    total = len(text)\n    if total == 0:\n        return 0.0\n    qcount = sum(1 for c in text if c in quotes)\n    return qcount / total\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters used divided by total punctuation tokens (higher = more varied punctuation)'\n    if not text:\n        return 0.0\n    import re\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return distinct / len(puncts)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a pronoun or demonstrative (I, we, you, he, she, they, it, this, that)'\n    if not text:\n        return 0.0\n    import re\n    starts = re.findall(r'(?:^|[.!?]\\s+)(\\w+)', text)\n    if not starts:\n        return 0.0\n    pronouns = set(['i','we','you','he','she','they','it','this','that','these','those','my','our','their','his','her'])\n    starts_lower = [s.lower() for s in starts if s]\n    if not starts_lower:\n        return 0.0\n    match_count = sum(1 for s in starts_lower if s in pronouns)\n    return match_count / len(starts_lower)\n", "def feature(text: str) -> float:\n    'Normalized count of ellipses (\"...\") as an indicator of truncation or informal excerpts'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    ell = text.count('...')\n    return float(ell) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Proportion of adverbs approximated by tokens ending with \"ly\" (case-insensitive)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[^\\s]+\\b\", text)\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if w.lower().endswith('ly'))\n    return float(ly_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of modal verbs (may/might/must/shall/should/can/could/will/would) per token'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    modals = re.findall(r'\\b(?:may|might|must|shall|should|can|could|will|would)\\b', text.lower())\n    return float(len(modals)) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Proportion of hyphenated words (token containing a hyphen) among all word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[^\\s]+\\b', text)\n    if not words:\n        return 0.0\n    hyph = sum(1 for w in words if '-' in w and re.search(r'\\w-\\w', w))\n    return float(hyph) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Standard deviation of clause lengths (words per clause) where clauses split on , ; : and sentence-conjunctions'\n    import re, math\n    if not text:\n        return 0.0\n    # Split on common clause delimiters and conjunction words that often join clauses\n    clauses = re.split(r'[,:;]|\\band\\b|\\bbut\\b|\\bor\\b', text, flags=re.IGNORECASE)\n    clause_word_counts = [len(re.findall(r'\\b\\w+\\b', c)) for c in clauses if c and re.search(r'\\w', c)]\n    if not clause_word_counts:\n        return 0.0\n    mean = sum(clause_word_counts) / float(len(clause_word_counts))\n    if len(clause_word_counts) == 1:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in clause_word_counts) / float(len(clause_word_counts))\n    return float(math.sqrt(var))\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice score: counts \"be\" forms followed by -ed words and occurrences of \"by\" after such verbs per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    if not sentences:\n        return 0.0\n    passive_matches = 0\n    for s in sentences:\n        # be + verb-ed\n        passive_matches += len(re.findall(r'\\b(?:is|are|was|were|be|been|being|am)\\b\\s+\\w+ed\\b', s, flags=re.IGNORECASE))\n        # patterns like \"was X by\"\n        passive_matches += len(re.findall(r'\\b(?:was|were|is|are)\\b\\s+\\w+(?:ed|en)\\b\\s+by\\b', s, flags=re.IGNORECASE))\n    return float(passive_matches) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ratio of first sentence length to average sentence length (first sentence length / mean sentence length)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s and re.search(r'\\w', s)]\n    if not sentences:\n        return 0.0\n    sent_word_counts = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences]\n    if not sent_word_counts:\n        return 0.0\n    first_len = float(sent_word_counts[0])\n    avg_len = float(sum(sent_word_counts)) / float(len(sent_word_counts))\n    if avg_len == 0.0:\n        return 0.0\n    return first_len / avg_len\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice indicator: proportion of \"be\" forms followed by past-participle-like tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    token_count = max(1, len(tokens))\n    # look for common \"be + past-participle\" patterns (e.g., \"was produced\", \"is shown\", \"were written\", \"has been done\")\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|am)\\s+\\w+(?:ed|en)\\b', text, flags=re.IGNORECASE)\n    # also include \"has been <verb>\" or \"have been <verb>\"\n    matches += re.findall(r'\\b(?:has|have|had)\\s+been\\s+\\w+(?:ed|en)\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / token_count\n\n", "def feature(text: str) -> float:\n    'Density of citation-like tokens (year in parentheses, bracketed reference numbers, \"et al.\", or author-year patterns) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    token_count = max(1, len(tokens))\n    count = 0\n    # (YYYY)\n    count += len(re.findall(r'\\(\\s*\\d{4}\\s*\\)', text))\n    # [1], [12]\n    count += len(re.findall(r'\\[\\s*\\d+\\s*\\]', text))\n    # \"et al.\"\n    count += len(re.findall(r'\\bet al\\b\\.?', text, flags=re.IGNORECASE))\n    # Author, YYYY  (simple heuristic: Capitalizedword, 2004)\n    count += len(re.findall(r'\\b[A-Z][a-z]+,\\s*\\d{4}\\b', text))\n    return float(count) / token_count\n\n", "def feature(text: str) -> float:\n    'Colon density: fraction of characters that are colons (:)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    colons = text.count(':')\n    return float(colons) / total_chars\n\n", "def feature(text: str) -> float:\n    'Fraction of very short sentences (<= 4 words), indicating terse or list-like prose'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences preserving possible trailing punctuation\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short_count = 0\n    for s in sentences:\n        wc = len(re.findall(r'\\w+', s))\n        if 1 <= wc <= 4:\n            short_count += 1\n    return float(short_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, we, my, our, me, us, mine, ours) among tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i', 'we', 'me', 'us', 'my', 'our', 'mine', 'ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Standard deviation of word lengths (characters) to capture variability in token length'\n    import math, re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    n = len(words)\n    if n == 0:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    return math.sqrt(var)\n\n", "def feature(text: str) -> float:\n    'Ratio of modal verbs (can, could, may, might, shall, should, will, would, must) among tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would', 'must'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Immediate repetition ratio: fraction of adjacent token pairs that are identical (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(len(tokens) - 1) if tokens[i] == tokens[i + 1])\n    return float(repeats) / max(1, len(tokens) - 1)\n", "def feature(text: str) -> float:\n    'Density of internal capitalized tokens (probable proper-nouns) excluding sentence-initial tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    total_tokens = 0\n    proper_tokens = 0\n    for s in sentences:\n        tokens = [t for t in re.findall(r'\\S+', s)]\n        if not tokens:\n            continue\n        # ignore first token of the sentence (may be capitalized by sentence start)\n        for t in tokens[1:]:\n            cleaned = t.strip('\"\\',:;()[]{}')\n            if not cleaned:\n                continue\n            total_tokens += 1\n            # consider as proper noun if starts with uppercase letter and has at least one lowercase letter following\n            if cleaned[0].isupper() and any(c.islower() for c in cleaned[1:]):\n                proper_tokens += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(proper_tokens) / float(total_tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that match year-like patterns (e.g., 1999, 2020) which often indicate formal/historical text'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    years = re.findall(r'\\b(?:19|20)\\d{2}\\b', text)\n    return float(len(years)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas per sentence) \u2014 captures clause complexity'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_counts = [s.count(',') for s in sentences]\n    return float(sum(comma_counts)) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Proportion of first-person pronouns (I, me, my, we, us, our) among tokens \u2014 low in formal/academic texts'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with a discourse-transition word (however, moreover, therefore, etc.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    transitions = {'however','moreover','furthermore','therefore','consequently','additionally',\n                   'nevertheless','nonetheless','meanwhile','similarly','alternatively','ultimately',\n                   'firstly','secondly','finally','conversely','in addition','for example','for instance'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        # grab first one or two words to handle multiword transitions like 'in addition'\n        words = re.findall(r'\\b\\w+\\b', s.lower())\n        if not words:\n            continue\n        first = words[0]\n        first_two = ' '.join(words[:2]) if len(words) >= 2 else first\n        if first in transitions or first_two in transitions:\n            starts += 1\n    return float(starts) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ratio of modal verbs (can, could, may, might, must, shall, should, will, would) among tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can','could','may','might','must','shall','should','will','would'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of hedge/modality words (may, might, could, appears, suggests, likely, etc.)'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','can','would','should','possibly','likely',\n              'suggests','suggest','appears','appear','seems','seem','tends','tend','probable','probably'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of first-person pronouns (I, we, me, us, my, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    first_person = {'i','we','me','us','our','ours','my','mine','ourselves','myself'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of subordinating conjunctions (because, although, since, whereas, unless, while, though, after, before)'\n    import re\n    if not text:\n        return 0.0\n    subs = {'because','although','since','whereas','unless','while','though','after','before','once','provided','where','whilst'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in subs)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are -ing forms (gerunds/continuous verbs), approximated by token suffix'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[a-zA-Z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) > 3 and t.endswith('ing'):\n            # Avoid counting trailing \"'ing\" weird tokens by ensuring at least one letter before\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of common academic connective/transition phrases (e.g., \"in order to\", \"as a result\", \"on the other hand\")'\n    import re\n    if not text:\n        return 0.0\n    phrases = ['in order to','as a result','on the other hand','in terms of','due to','with respect to',\n               'in contrast','as well as','for example','the purpose of','the aim of','it is important to']\n    lower = text.lower()\n    tokens = re.findall(r'\\w+', lower)\n    if not tokens:\n        return 0.0\n    count = 0\n    for p in phrases:\n        # Count non-overlapping occurrences\n        count += lower.count(p)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation characters divided by total punctuation occurrences (0 if none)'\n    if not text:\n        return 0.0\n    puncts = set('.,;:!?-()[]\\'\"{}<>/\\\\@#$%^&*_+=|~`')\n    total = 0\n    present = set()\n    for c in text:\n        if c in puncts:\n            total += 1\n            present.add(c)\n    if total == 0:\n        return 0.0\n    return float(len(present)) / float(total)\n\n", "def feature(text: str) -> float:\n    'Estimated average clauses per sentence: (sentences + commas + semicolons + colons) / max(1, sentences)'\n    if not text:\n        return 0.0\n    # sentence delimiters\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)  # treat as at least one sentence\n    clause_count = sentence_count + text.count(',') + text.count(';') + text.count(':')\n    return float(clause_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Normalized count of citation-like patterns: numeric bracket refs [1], parenthetical author-year (Smith, 2020), and \"et al.\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    bracket_refs = re.findall(r'\\[\\s*\\d+\\s*\\]', text)\n    author_year = re.findall(r'\\([A-Za-z][^)]{0,80}?,\\s*\\d{4}\\)', text)\n    et_al = re.findall(r'\\bet al\\.', text.lower())\n    total = len(bracket_refs) + len(author_year) + len(et_al)\n    return float(total) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (typographic quotes, em-dash, accented chars)'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / total if total else 0.0\n\n", "def feature(text: str) -> float:\n    'Yule\\'s K lexical diversity measure (higher = more repetition), returns 0 for tiny texts'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    N = len(tokens)\n    if N <= 1:\n        return 0.0\n    freqs = Counter(tokens)\n    M2 = sum(f * f for f in freqs.values())\n    # Yule's K scaled by 10^4\n    return 10000.0 * (M2 - N) / (N * N)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending with common adjective suffixes (e.g., -ous, -able, -ive)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ous', 'able', 'ive', 'al', 'ic', 'ful', 'less', 'ish', 'ent', 'ant')\n    count = 0\n    for t in tokens:\n        for s in suffixes:\n            if t.endswith(s):\n                count += 1\n                break\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating/subordinating conjunction (informal marker)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences by punctuation followed by whitespace, but be lenient\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    if not sentences:\n        return 0.0\n    conj = {'and', 'but', 'or', 'so', 'for', 'nor', 'yet', 'however', 'although', 'because', 'since', 'while', 'though'}\n    starts = 0\n    total = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        m = re.search(r\"\\b\\w[\\w']*\\b\", s)\n        if not m:\n            continue\n        total += 1\n        first = m.group(0).lower()\n        # treat sentences that start with quotation marks or parentheses by stripping leading non-word\n        if first in conj:\n            starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts) / total\n\n", "def feature(text: str) -> float:\n    'Density of dash characters (hyphen, en-dash, em-dash) per character, capturing parenthetical asides'\n    if not text:\n        return 0.0\n    dash_chars = {'-', '\u2013', '\u2014'}\n    total = len(text)\n    dcount = sum(1 for c in text if c in dash_chars)\n    return float(dcount) / total if total else 0.0\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that are the definite article \"the\" (signals referential/formal prose)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    the_count = sum(1 for t in tokens if t == 'the')\n    return float(the_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Punctuation variety: unique punctuation characters divided by total punctuation count (0 if no punctuation)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    unique = len(set(puncts))\n    return float(unique) / total\n\n", "def feature(text: str) -> float:\n    'Newline density: fraction of characters that are newline characters (paragraphing signal)'\n    if not text:\n        return 0.0\n    total = len(text)\n    n = text.count('\\n')\n    return float(n) / total if total else 0.0\n", "def feature(text: str) -> float:\n    'Ratio of punctuation tokens that are preceded by a space (e.g., \"word .\"), which flags odd spacing'\n    import re\n    if not text:\n        return 0.0\n    spaced_before = len(re.findall(r'\\s[.,;:!?]', text))\n    total_punct = len(re.findall(r'[.,;:!?]', text))\n    if total_punct == 0:\n        return 0.0\n    return float(spaced_before) / float(total_punct)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences starting with the most common sentence-initial word (measure of repetitive openings)'\n    import re\n    if not text:\n        return 0.0\n    # find words that start sentences (either at start or after .!? and whitespace)\n    starts = re.findall(r'(?:^|[.!?]\\s+)(\\w+)', text)\n    if not starts:\n        return 0.0\n    counts = {}\n    for w in starts:\n        wlow = w.lower()\n        counts[wlow] = counts.get(wlow, 0) + 1\n    max_count = max(counts.values())\n    return float(max_count) / float(len(starts))\n\n", "def feature(text: str) -> float:\n    'Approximate average syllables per word using contiguous vowel-group counts per token (vowels: aeiouy)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    vowel_group = re.compile(r'[aeiouyAEIOUY]+')\n    total = 0\n    for w in words:\n        groups = len(vowel_group.findall(w))\n        # treat 0 vowel groups as 1 to avoid zero-syllable tokens like \"rhythms\" being 0\n        total += groups if groups > 0 else 1\n    return float(total) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that belong to multi-word Title-Case spans (>=2 words) not at sentence starts (likely titles)'\n    import re\n    if not text:\n        return 0.0\n    # find multi-word Title-Case spans\n    pattern = re.compile(r'\\b(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,})\\b')\n    matches = list(pattern.finditer(text))\n    if not matches:\n        return 0.0\n    total_words = 0\n    title_words = 0\n    # total token count for normalization\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    total_words = len(tokens) if tokens else 0\n    if total_words == 0:\n        return 0.0\n    for m in matches:\n        # determine whether the match is at sentence start by checking previous non-space character\n        start = m.start()\n        prev_idx = start - 1\n        prev_char = None\n        while prev_idx >= 0:\n            if not text[prev_idx].isspace():\n                prev_char = text[prev_idx]\n                break\n            prev_idx -= 1\n        # if prev_char is None (start of text) or a sentence terminator, skip (sentence-initial)\n        if prev_char is None or prev_char in '.!?':\n            continue\n        # count words in this match\n        title_words += len(re.findall(r'\\b[A-Z][a-z]+\\b', m.group()))\n    return float(title_words) / float(total_words)\n\n", "def feature(text: str) -> float:\n    'Normalized Shannon entropy of the lowercased word frequency distribution (0..1), higher = more lexical unpredictability'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    freqs = {}\n    for t in tokens:\n        freqs[t] = freqs.get(t, 0) + 1\n    total = float(len(tokens))\n    entropy = 0.0\n    for cnt in freqs.values():\n        p = cnt / total\n        entropy -= p * math.log2(p)\n    distinct = len(freqs)\n    # normalize by log2(distinct) to bring into [0,1]; if distinct<=1, entropy=0\n    if distinct <= 1:\n        return 0.0\n    return entropy / math.log2(distinct)\n\n", "def feature(text: str) -> float:\n    'Density of academic/register words from a small curated list (fraction of tokens matching academic vocabulary)'\n    import re\n    if not text:\n        return 0.0\n    academic = {\n        'analysis','argument','study','research','method','evidence','significant','impact',\n        'socio','audience','published','published','theory','results','contribute','development',\n        'problem','issue','analysis','discussion','citation','source','purpose','objective','conclusion'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if t in academic:\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation characters (0 if none)'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = set(puncts)\n    return float(len(distinct)) / float(len(puncts))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, us, our, mine, ours)'\n    import re\n    if not text:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, mine, we, us, our, ours) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = re.findall(r'\\b(?:i|me|my|mine|we|us|our|ours)\\b', text, flags=re.IGNORECASE)\n    return float(len(first_person)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are possessive apostrophes (e.g., \"John\\'s\", \"company\u2019s\") as opposed to contractions'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+[\u2019\\']s\\b', text, flags=re.IGNORECASE)\n    all_tokens = re.findall(r'\\b\\w+\\b', text)\n    if not all_tokens:\n        return 0.0\n    return float(len(tokens)) / float(len(all_tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of hyphenated words (e.g., \"case-study\", \"paper-based\") to total tokens'\n    import re\n    if not text:\n        return 0.0\n    hyphenated = re.findall(r'\\b[A-Za-z0-9]+-[A-Za-z0-9\\-]+\\b', text)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(len(hyphenated)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences normalized by text length (chars)'\n    if not text:\n        return 0.0\n    count = text.count('...')\n    # normalize per 1000 chars to keep values reasonable for long/short texts\n    denom = max(1.0, float(len(text)))\n    return float(count) / denom\n\n", "def feature(text: str) -> float:\n    'Ratio of common filler/colloquial words (e.g., actually, basically, literally, like, just) to tokens'\n    import re\n    if not text:\n        return 0.0\n    fillers = {'actually','basically','literally','honestly','really','very','just','right','okay','ok','like','sorta','kindof','kinda'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in fillers)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of 4-digit year-like tokens (e.g., \"1999\", \"2020\") to total tokens \u2014 indicator of historical/academic texts'\n    import re\n    if not text:\n        return 0.0\n    year_tokens = re.findall(r'\\b\\d{4}\\b', text)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(len(year_tokens)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Vowel-to-consonant character ratio (vowels / (consonants + 1)) to capture phonetic/lexical patterns'\n    import re\n    if not text:\n        return 0.0\n    letters = re.findall(r'[A-Za-z]', text)\n    if not letters:\n        return 0.0\n    vowels = sum(1 for c in letters if c.lower() in 'aeiou')\n    consonants = len(letters) - vowels\n    return float(vowels) / float(consonants + 1)\n", "def feature(text: str) -> float:\n    'Colon density: fraction of characters that are colons (\":\"), common in formal lists/clauses'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float(text.count(':')) / total_chars\n\n", "def feature(text: str) -> float:\n    'Hyphenated-token ratio: fraction of tokens that contain internal hyphens (compound words)'\n    import re\n    if not text:\n        return 0.0\n    hyphenated = re.findall(r'\\b\\w+(?:-\\w+)+\\b', text)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(len(hyphenated)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Consecutive Title-Case pairs ratio: proportion of times two adjacent tokens both start with an uppercase letter (multiword proper names)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if len(tokens) < 2:\n        return 0.0\n    pairs = 0\n    for i in range(1, len(tokens)):\n        a, b = tokens[i-1], tokens[i]\n        if a[0].isupper() and b[0].isupper():\n            # prefer TitleCase rather than all-caps acronyms for proper-name detection\n            if (len(a) == 1 or (len(a) > 1 and a[1].islower() or a.isupper())) and (len(b) == 1 or (len(b) > 1 and b[1].islower() or b.isupper())):\n                pairs += 1\n    return float(pairs) / (len(tokens) - 1)\n\n", "def feature(text: str) -> float:\n    '\"et al.\" ratio: frequency of the scholarly citation marker \"et al.\" (case-insensitive) normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    matches = re.findall(r'\\bet\\s+al\\.?\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Adjective-suffix signal: fraction of tokens ending with common adjective suffixes (e.g., -ous, -al, -ive) as a proxy for descriptive/formal style'\n    import re\n    if not text:\n        return 0.0\n    suffixes = ('ous', 'al', 'able', 'ible', 'ive', 'ic', 'ary', 'ical', 'ent', 'ant')\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        for s in suffixes:\n            if t.endswith(s) and len(t) > len(s) + 1:\n                count += 1\n                break\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Acronym ratio: fraction of tokens that are acronyms (all-caps sequences or dotted acronyms like U.S.A.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\S+\\b', text)\n    word_tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    # Find acronyms like \"NASA\" or \"U.S.\" or \"U.S.A.\"\n    acronyms = re.findall(r'\\b(?:[A-Z]{2,}|(?:[A-Z]\\.){2,})\\b', text)\n    # Normalize by token count using word-like tokens when available\n    denom = len(word_tokens) if word_tokens else len(tokens)\n    return float(len(acronyms)) / max(1, denom)\n\n", "def feature(text: str) -> float:\n    'Parenthetical-citation density: fraction of tokens that match common parenthetical citation patterns like \"(Smith, 2010)\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    # Patterns: (Surname, 2010), (Surname et al., 2010), (Surname et al.)\n    pattern = re.compile(r'\\(\\s*[A-Z][A-Za-z\\-]{2,}(?:\\s+et\\s+al\\.?)?(?:,\\s*\\d{3,4})?\\s*\\)')\n    matches = pattern.findall(text)\n    return float(len(matches)) / len(tokens)\n", "def feature(text: str) -> float:\n    'Normalized frequency of ellipses (\"...\") per sentence, capturing trailing/hesitant style or omitted text'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    # fallback to sequences of three or more dots\n    import re\n    ellipses += len(re.findall(r'\\.{3,}', text)) - len(re.findall(r'\\.\\.\\.', text))\n    sentences = max(1, len([s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]))\n    return float(ellipses) / float(sentences)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating/discourse word (And, But, So, Also, However) \u2014 signals conversational or rhetorical structure'\n    import re\n    if not text:\n        return 0.0\n    parts = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not parts:\n        return 0.0\n    starts = 0\n    for s in parts:\n        m = re.match(r'^\\W*([A-Za-z\\'\u2019]+)', s)\n        if m:\n            w = m.group(1).lower()\n            if w in {'and', 'but', 'so', 'also', 'however', 'thus', 'then', 'therefore', 'meanwhile'}:\n                starts += 1\n    return float(starts) / float(len(parts))\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences whose first alphanumeric character is lowercase (indicates fragments, formatting oddities, or stylistic choices)'\n    import re\n    if not text:\n        return 0.0\n    parts = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not parts:\n        return 0.0\n    lower_start = 0\n    for s in parts:\n        m = re.search(r'[A-Za-z]', s)\n        if m and m.group(0).islower():\n            lower_start += 1\n    return float(lower_start) / float(len(parts))\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that fall within quoted spans (both straight and typographic quotes) as a measure of quoted content density'\n    import re\n    if not text:\n        return 0.0\n    total_len = max(1, len(text))\n    quoted = 0\n    # capture straight double quotes\n    for m in re.finditer(r'\"([^\"]*)\"', text):\n        quoted += len(m.group(1))\n    # capture smart quotes \u201c...\u201d\n    for m in re.finditer(r'[\u201c\u201d]([^\u201c\u201d]*)[\u201c\u201d]', text):\n        quoted += len(m.group(1))\n    # single quotes around titles/quotes too\n    for m in re.finditer(r\"'([^']*)'\", text):\n        quoted += len(m.group(1))\n    return float(quoted) / float(total_len)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that are very short (<5 words), indicating telegraphic style or frequent sentence fragments'\n    import re\n    if not text:\n        return 0.0\n    parts = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not parts:\n        return 0.0\n    short = 0\n    for s in parts:\n        words = [w for w in s.split() if w.strip()]\n        if len(words) < 5:\n            short += 1\n    return float(short) / float(len(parts))\n", "def feature(text: str) -> float:\n    'Fraction of sentence-ending punctuation that are question marks or exclamation points'\n    import re\n    if not text:\n        return 0.0\n    ends = re.findall(r'[.!?]', text)\n    if not ends:\n        return 0.0\n    q_e = sum(1 for c in ends if c in ('?', '!'))\n    return float(q_e) / len(ends)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing a curly (typographic) apostrophe U+2019 (e.g., doctor\u2019s) among all tokens'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    curly = sum(1 for t in tokens if '\\u2019' in t)\n    return float(curly) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that participate in multi-word Title-Case sequences (consecutive capitalized words length >= 2)'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    is_cap = [1 if (w[0].isupper() and (len(w)==1 or w[1:].islower())) else 0 for w in words]\n    in_sequence = 0\n    i = 0\n    n = len(is_cap)\n    while i < n:\n        if is_cap[i]:\n            j = i\n            while j + 1 < n and is_cap[j+1]:\n                j += 1\n            seq_len = j - i + 1\n            if seq_len >= 2:\n                in_sequence += seq_len\n            i = j + 1\n        else:\n            i += 1\n    return float(in_sequence) / len(words)\n\n", "def feature(text: str) -> float:\n    'Average number of clause-connectors (commas, semicolons, colons) per sentence'\n    import re\n    if not text:\n        return 0.0\n    connectors = text.count(',') + text.count(';') + text.count(':')\n    sentence_count = len(re.findall(r'[.!?]', text))\n    if sentence_count <= 0:\n        # if no standard sentence punctuation, treat entire text as one sentence\n        sentence_count = 1\n    return float(connectors) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of double-quote characters that are typographic/curly (\u201c or \u201d) versus all double-quote characters'\n    if not text:\n        return 0.0\n    straight = text.count('\"')\n    left_curly = text.count('\\u201c')\n    right_curly = text.count('\\u201d')\n    total = straight + left_curly + right_curly\n    if total == 0:\n        return 0.0\n    return float(left_curly + right_curly) / total\n\n", "def feature(text: str) -> float:\n    'Variety of punctuation characters: distinct punctuation types divided by total punctuation characters'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Standard deviation of stopword fraction across sentences (measures consistency of function-word use)'\n    import re, math\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','to','of','a','an','that','it','for','on','with','as','by','from','this','these','those','be','are','was','were','but','or','which','their','its','at','from'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # if no sentence separators, treat entire text as one sentence\n        sentences = [text.strip()]\n    fractions = []\n    for s in sentences:\n        words = re.findall(r'\\b[a-zA-Z]+\\b', s.lower())\n        if not words:\n            continue\n        sw = sum(1 for w in words if w in stopwords)\n        fractions.append(sw / float(len(words)))\n    if not fractions:\n        return 0.0\n    mean = sum(fractions) / len(fractions)\n    variance = sum((x - mean) ** 2 for x in fractions) / len(fractions)\n    return math.sqrt(variance)\n", "def feature(text: str) -> float:\n    'Density of quotation characters (double and curly quotes) as fraction of characters'\n    if not text:\n        return 0.0\n    quote_chars = ['\"', '\u201c', '\u201d', '\u00ab', '\u00bb']\n    count = sum(text.count(q) for q in quote_chars)\n    return float(count) / max(1.0, len(text))\n\n", "def feature(text: str) -> float:\n    'Normalized count of parenthetical or bracket-style citations like (Author, 1999), (1999) or [1]'\n    import re\n    if not text:\n        return 0.0\n    patterns = [\n        r'\\([A-Z][A-Za-z\\-\\s\\.&]+,\\s*\\d{4}\\)',  # (Smith, 2010)\n        r'\\(\\s*\\d{4}\\s*\\)',                    # (2010)\n        r'\\[\\s*\\d+\\s*\\]',                      # [1]\n        r'\\([A-Za-z\\.]+\\s+et\\s+al\\.,\\s*\\d{4}\\)'# (Smith et al., 2010)\n    ]\n    matches = 0\n    for p in patterns:\n        matches += len(re.findall(p, text))\n    words = len(re.findall(r'\\w+', text))\n    return float(matches) / max(1.0, words)\n\n", "def feature(text: str) -> float:\n    'Density of common scholarly abbreviations (e.g., e.g., i.e., et al., cf., etc.) per token'\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    abbrs = ['e.g.', 'i.e.', 'et al.', 'cf.', 'vs.', 'fig.', 'figs.', 'etc.', 'resp.']\n    count = sum(lowered.count(a) for a in abbrs)\n    tokens = max(1, len([t for t in __import__('re').findall(r'\\w+', text)]))\n    return float(count) / tokens\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens ending with academic/technical suffixes (al, ive, ous, ism, ist, logy, graphy, ence)'\n    import re\n    if not text:\n        return 0.0\n    suffixes = ('al', 'ive', 'ous', 'ism', 'ist', 'logy', 'graphy', 'ence', 'ance', 'ology', 'ic')\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) > 4 and any(t.endswith(s) for s in suffixes):\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are percent signs (%) as a marker of quantitative reporting'\n    if not text:\n        return 0.0\n    return float(text.count('%')) / max(1.0, len(text))\n\n", "def feature(text: str) -> float:\n    'Ratio of relative pronouns (which, that, who, whom, whose, where, when) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    rel = {'which', 'that', 'who', 'whom', 'whose', 'where', 'when'}\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in rel)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of certainty/assertion words (clearly, definitely, obviously, certainly, always, never, undoubtedly) to tokens'\n    import re\n    if not text:\n        return 0.0\n    cert = {'clearly', 'definitely', 'obviously', 'certainly', 'always', 'never', 'undoubtedly', 'surely'}\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in cert)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that appear to be possessives (ending with \\'s, \u2019s, or s\\')'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\S+\", text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        tl = t.lower().rstrip('.,;:!?')\n        if tl.endswith(\"'s\") or tl.endswith(\"\u2019s\") or tl.endswith(\"s'\"):\n            count += 1\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentence terminal punctuation that are ? or ! (questions/exclamations vs total sentences)'\n    if not text:\n        return 0.0\n    q = text.count('?')\n    e = text.count('!')\n    total = text.count('.') + q + e\n    if total <= 0:\n        return 0.0\n    return float(q + e) / float(total)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that are very short (<= 5 words)'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence-ending punctuation\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short_count = 0\n    for s in sentences:\n        words = s.split()\n        if 0 < len(words) <= 5:\n            short_count += 1\n    return float(short_count) / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Density of quotation characters (single or double, straight or typographic) relative to text length'\n    if not text:\n        return 0.0\n    quote_chars = '\"\\'\u201c\u201d\u2018\u2019'\n    count = sum(text.count(q) for q in quote_chars)\n    L = len(text)\n    if L == 0:\n        return 0.0\n    return float(count) / float(L)\n\n", "def feature(text: str) -> float:\n    'Proportion of bigram occurrences that appear more than once (bigram repetition rate)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = list(zip(tokens, tokens[1:]))\n    total = len(bigrams)\n    counts = Counter(bigrams)\n    repeated_occurrences = sum(cnt for cnt in counts.values() if cnt > 1)\n    return float(repeated_occurrences) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that start with list/outline markers (bullets or numbered list indicators)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    pattern = re.compile(r'^\\s*(?:[\\-\\*\\u2022]|\\d{1,3}[\\.\\)\\]])\\s+')\n    list_lines = sum(1 for l in lines if pattern.match(l))\n    return float(list_lines) / float(len(lines))\n", "def feature(text: str) -> float:\n    'Ratio of the definite article \"the\" to total tokens (lowercased)'\n    if not text:\n        return 0.0\n    toks = text.split()\n    if not toks:\n        return 0.0\n    the_count = sum(1 for t in toks if t.lower().strip(\".,;:!?\\\"'()[]{}\") == 'the')\n    return the_count / len(toks)\n\n", "def feature(text: str) -> float:\n    'Ratio of common prepositions to total tokens (in, on, at, by, for, to, from, with, about, among, between, into, through, over, under)'\n    if not text:\n        return 0.0\n    preps = {'in','on','at','by','for','to','from','with','about','among','between','into','through','over','under','against','during','without','within','toward','towards'}\n    toks = text.split()\n    if not toks:\n        return 0.0\n    count = sum(1 for t in toks if t.lower().strip(\".,;:!?\\\"'()[]{}\") in preps)\n    return count / len(toks)\n\n", "def feature(text: str) -> float:\n    'Ratio of coordinating conjunctions (and, but, or, nor, for, so, yet) to total tokens'\n    if not text:\n        return 0.0\n    conjs = {'and','but','or','nor','for','so','yet'}\n    toks = text.split()\n    if not toks:\n        return 0.0\n    count = sum(1 for t in toks if t.lower().strip(\".,;:!?\\\"'()[]{}\") in conjs)\n    return count / len(toks)\n\n", "def feature(text: str) -> float:\n    'Fraction of adjacent sentences that start with the same word (sentence-start repetition)'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence terminators followed by whitespace; keep fragments sensible\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if len(sents) < 2:\n        return 0.0\n    starts = []\n    for s in sents:\n        parts = s.split()\n        starts.append(parts[0].lower().strip(\".,:;\\\"'()[]{}\") if parts else '')\n    repeats = 0\n    pairs = 0\n    for i in range(1, len(starts)):\n        pairs += 1\n        if starts[i] and starts[i] == starts[i-1]:\n            repeats += 1\n    return repeats / pairs if pairs > 0 else 0.0\n\n", "def feature(text: str) -> float:\n    'Colon character density: fraction of characters that are colons'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    return text.count(':') / total\n\n", "def feature(text: str) -> float:\n    'Enumeration pattern ratio: frequency of inline/list markers like \"1.\", \"2.\", \"a)\", \"b)\" per token'\n    import re\n    if not text:\n        return 0.0\n    toks = text.split()\n    if not toks:\n        return 0.0\n    # patterns: digits followed by dot, single letters followed by ')' or dot, parenthesized numbers/letters\n    pat = re.compile(r'^(?:\\d+\\.$|\\d+\\)|[a-zA-Z]\\.$|[a-zA-Z]\\)|\\(\\d+\\)|\\([a-zA-Z]\\))$')\n    matches = sum(1 for t in toks if pat.match(t.strip()))\n    # also count inline markers that may be attached to punctuation, e.g., \"1.\" inside text\n    inline = len(re.findall(r'\\b\\d+\\.', text)) + len(re.findall(r'\\b[a-zA-Z]\\)', text))\n    # combine but avoid double counting by taking max of token matches and inline counts normalized\n    count = max(matches, inline)\n    return count / len(toks)\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (mean words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    lengths = []\n    for s in sents:\n        words = [w for w in s.split() if w.strip()]\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    return sum(lengths) / len(lengths)\n\n", "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters present normalized (0-1)'\n    if not text:\n        return 0.0\n    puncs = set(c for c in text if not c.isalnum() and not c.isspace())\n    # normalize by a reasonable upper bound (10 distinct punctuation marks)\n    denom = 10.0\n    return min(len(puncs) / denom, 1.0)\n", "def feature(text: str) -> float:\n    'Punctuation diversity: distinct punctuation characters divided by total punctuation characters'\n    import re\n    if not text:\n        return 0.0\n    punct = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not punct:\n        return 0.0\n    distinct = len(set(punct))\n    total = len(punct)\n    return float(distinct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Quote density: fraction of characters that are single or double quotation marks (straight or typographic)'\n    if not text:\n        return 0.0\n    quote_chars = ['\"', \"'\", '\u201c', '\u201d', '\u2018', '\u2019']\n    count = sum(text.count(q) for q in quote_chars)\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float(count) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Coordinating conjunction ratio: fraction of tokens that are common coordinating conjunctions (and, but, or, nor, for, so, yet)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    conj = {'and', 'but', 'or', 'nor', 'for', 'so', 'yet'}\n    count = sum(1 for t in tokens if t in conj)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Enumerated list indicator: occurrences of numbered/lettered/list markers per sentence (detects outlines/lists)'\n    import re\n    if not text:\n        return 0.0\n    # count sentences approximately\n    sentences = [s for s in re.split(r'[\\.!?]+', text) if s.strip()]\n    sentence_count = max(1, len(sentences))\n    # match common list markers at start of line or after newline/leading spaces\n    markers = re.findall(r'(?m)^\\s*(?:\\d+[\\.\\)]|[a-zA-Z][\\.\\)]|\\([ivxlcdm]+\\))', text)\n    return float(len(markers)) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Repetitive bigram ratio: fraction of bigrams that are repeated (1 - unique_bigrams/total_bigrams)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+|\\S', text.lower())  # keep punctuation as tokens too\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    total = len(bigrams)\n    unique = len(set(bigrams))\n    return float(total - unique) / float(total)\n\n", "def feature(text: str) -> float:\n    'Numeric word ratio: fraction of tokens that are number words (one, two, three, dozen, hundred, thousand, million, etc.)'\n    import re\n    if not text:\n        return 0.0\n    number_words = {'zero','one','two','three','four','five','six','seven','eight','nine','ten',\n                    'eleven','twelve','thirteen','fourteen','fifteen','sixteen','seventeen','eighteen','nineteen','twenty',\n                    'dozen','hundred','thousand','million','billion','trillion'}\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in number_words)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Hyphenated compound ratio: fraction of tokens that contain an internal hyphen (captures academic/formal compounds)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_count = 0\n    for t in tokens:\n        # consider token a hyphenated compound if it contains at least one hyphen with alphanumeric on both sides\n        if re.search(r'[A-Za-z0-9]+-[A-Za-z0-9]+', t):\n            hyphen_count += 1\n    return float(hyphen_count) / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen (compound adjective/noun usage indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9\u2019'-]+\", text)\n    if not tokens:\n        return 0.0\n    hyphenated = sum(1 for t in tokens if '-' in t)\n    return float(hyphenated) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that are long (>20 words), measuring sentence-length extremity'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence candidates\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    long_count = 0\n    for s in sentences:\n        words = re.findall(r\"[A-Za-z0-9\u2019'-]+\", s)\n        if len(words) > 20:\n            long_count += 1\n    return float(long_count) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens ending with \"ed\" (heuristic past-tense/past-participle marker)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9\u2019'-]+\", text)\n    if not tokens:\n        return 0.0\n    ed_count = 0\n    for t in tokens:\n        tl = t.lower()\n        # avoid counting very short words like \"ed\" or names; require length >=4\n        if len(tl) >= 4 and tl.endswith('ed'):\n            ed_count += 1\n    return float(ed_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with \"the\" (case-insensitive), capturing definite-article sentence openings'\n    import re\n    if not text:\n        return 0.0\n    # find sentences preserving content\n    raw_sentences = re.findall(r'[^.!?]+[.!?]?', text)\n    sentences = [s.strip() for s in raw_sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    starts_the = 0\n    for s in sentences:\n        # get first word\n        m = re.search(r\"[A-Za-z0-9\u2019'-]+\", s)\n        if m and m.group(0).lower() == 'the':\n            starts_the += 1\n    return float(starts_the) / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Density of common discourse/connective markers (however, moreover, therefore, thus, furthermore, etc.)'\n    import re\n    if not text:\n        return 0.0\n    markers = {'however','moreover','therefore','thus','furthermore','nevertheless','additionally','consequently','meanwhile','accordingly','hence','nonetheless','alternatively','likewise','similarly','subsequently','notwithstanding'}\n    tokens = re.findall(r\"[A-Za-z0-9\u2019'-]+\", text.lower())\n    if not tokens:\n        return 0.0\n    marker_count = sum(1 for t in tokens if t in markers)\n    return float(marker_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Stopword diversity: distinct stopwords divided by total stopword occurrences (low values indicate repetition)'\n    import re\n    if not text:\n        return 0.0\n    # small common-English stopword list\n    stopwords = {'the','a','an','and','or','but','if','while','of','in','on','for','to','with','by','is','are','was','were','be','been','being','that','this','these','those','as','at','from','it','its','he','she','they','we','you','I','his','her','their','our','my','your','not','no','so','than','then'}\n    tokens = re.findall(r\"[A-Za-z0-9\u2019'-]+\", text)\n    if not tokens:\n        return 0.0\n    sw_tokens = [t.lower() for t in tokens if t.lower() in stopwords]\n    if not sw_tokens:\n        return 0.0\n    distinct = len(set(sw_tokens))\n    total = len(sw_tokens)\n    return float(distinct) / total\n\n", "def feature(text: str) -> float:\n    'Average Jaccard overlap of word sets between consecutive sentences (measures local repetitiveness/cohesion)'\n    import re\n    if not text:\n        return 0.0\n    raw_sentences = re.findall(r'[^.!?]+[.!?]?', text)\n    sentences = [s.strip() for s in raw_sentences if s.strip()]\n    if len(sentences) < 2:\n        return 0.0\n    def wordset(s):\n        return set(w.lower() for w in re.findall(r\"[A-Za-z0-9\u2019'-]+\", s))\n    overlaps = []\n    prev_set = wordset(sentences[0])\n    for s in sentences[1:]:\n        cur_set = wordset(s)\n        if not prev_set and not cur_set:\n            j = 0.0\n        else:\n            inter = len(prev_set & cur_set)\n            union = len(prev_set | cur_set)\n            j = float(inter) / union if union > 0 else 0.0\n        overlaps.append(j)\n        prev_set = cur_set\n    if not overlaps:\n        return 0.0\n    return float(sum(overlaps)) / len(overlaps)\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (captures clause density in sentence-level units)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    return float(comma_count) / len(sentences)\n", "def feature(text: str) -> float:\n    'Density of ellipses (occurrences of three or more consecutive dots) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    return float(ellipses) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that are uppercase acronyms (2+ consecutive ASCII uppercase letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    acr = 0\n    for t in tokens:\n        if len(t) >= 2 and re.fullmatch(r'[A-Z]{2,}', t):\n            acr += 1\n    return float(acr) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences whose first word repeats the first word of the previous sentence (start-word repetition)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sents) < 2:\n        return 0.0\n    first_words = []\n    for s in sents:\n        m = re.search(r'\\b[^\\d\\W_]+\\b', s)\n        first_words.append(m.group(0).lower() if m else '')\n    repeats = 0\n    total = 0\n    for i in range(1, len(first_words)):\n        total += 1\n        if first_words[i] and first_words[i] == first_words[i-1]:\n            repeats += 1\n    return float(repeats) / float(total) if total else 0.0\n\n", "def feature(text: str) -> float:\n    'Proportion of punctuation characters that are comma or semicolon (clause punctuation concentration)'\n    if not text:\n        return 0.0\n    punctuation = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not punctuation:\n        return 0.0\n    cs = sum(1 for c in punctuation if c in {',', ';'})\n    return float(cs) / float(len(punctuation))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are very short fragments (fewer than 3 words)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    short = 0\n    for s in sents:\n        words = re.findall(r'\\b[^\\d\\W_]+\\b', s)\n        if len(words) < 3:\n            short += 1\n    return float(short) / float(len(sents))\n", "def feature(text: str) -> float:\n    'Ratio of ampersand tokens (&) to total word tokens (captures \"X & Y\" citation style)'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    amp_count = text.count('&')\n    return float(amp_count) / float(max(1, len(words)))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that look like proper nouns (capitalized inside sentences, not sentence-starts)'\n    import re\n    if not text:\n        return 0.0\n    tokens = list(re.finditer(r'\\b\\w+\\b', text))\n    if not tokens:\n        return 0.0\n    proper = 0\n    for m in tokens:\n        token = m.group(0)\n        # skip all-uppercase (acronyms) and tokens starting with digit\n        if not token or token[0].isdigit() or token.isupper():\n            continue\n        if not token[0].isupper():\n            continue\n        # determine if token is at sentence start by looking for last non-space character\n        start = m.start()\n        j = start - 1\n        while j >= 0 and text[j].isspace():\n            j -= 1\n        if j < 0:\n            # start of text -> likely sentence start, skip\n            continue\n        if text[j] in '.!?':\n            # preceded by sentence-ending punctuation -> sentence start, skip\n            continue\n        proper += 1\n    return float(proper) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common literary/academic keywords (novel, chapter, study, research, literature, author, data, theory, analysis)'\n    import re\n    if not text:\n        return 0.0\n    kws = {'novel', 'chapter', 'author', 'literature', 'literary', 'study', 'studying', 'research', 'data', 'analysis', 'theory', 'biochemistry', 'prize', 'nobel'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in kws)\n    return float(count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Density of ellipsis occurrences (\"...\") normalized by text length'\n    if not text:\n        return 0.0\n    ell_count = text.count('...')\n    return float(ell_count) / float(len(text))\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that end with -ing (gerund/participle forms), excluding short tokens to reduce false positives like \"king\"'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    ing_count = sum(1 for w in words if len(w) > 4 and w.endswith('ing'))\n    return float(ing_count) / float(len(words))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence-like punctuation count)'\n    if not text:\n        return 0.0\n    import re\n    comma_count = text.count(',')\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    return float(comma_count) / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that match a common citation/author-pair pattern like \"Surname & Surname\"'\n    import re\n    if not text:\n        return 0.0\n    # match patterns like \"Carter & Porges\" or \"Smith & Jones\"\n    matches = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?\\s*&\\s*[A-Z][a-z]+\\b', text)\n    words = re.findall(r'\\w+', text)\n    return float(len(matches)) / float(max(1, len(words)))\n\n", "def feature(text: str) -> float:\n    'Average sentence length in words (word count divided by sentence-like punctuation count)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    return float(len(words)) / float(sentence_count)\n", "def feature(text: str) -> float:\n    'Normalized variance of sentence lengths (variance / (mean+1)) to capture sentence-length variability'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(var) / (mean + 1.0)\n\n", "def feature(text: str) -> float:\n    'Approximate fraction of sentences that contain a simple passive-voice pattern (heuristic)'\n    import re\n    if not text:\n        return 0.0\n    # Break into sentences\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    passive_re = re.compile(r'\\b(?:is|are|was|were|be|been|being|am)\\b\\s+\\w+(?:ed\\b|en\\b)', re.IGNORECASE)\n    passive_count = sum(1 for s in sentences if passive_re.search(s))\n    return passive_count / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are hedging/modality/uncertainty words (may, might, could, perhaps, seem, appear, likely, possible, tends)'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    tokens = re.findall(r\"\\b[a-z']+\\b\", lowered)\n    if not tokens:\n        return 0.0\n    hedges = {'may', 'might', 'could', 'can', 'seem', 'seems', 'seemed', 'appear', 'appears', 'appeared',\n              'perhaps', 'possibly', 'likely', 'tend', 'tends', 'tended', 'possible', 'suggest', 'suggests', 'suggested'}\n    count = 0\n    for t in tokens:\n        if t in hedges:\n            count += 1\n        # handle simple multiword hedge like 'tend to'\n    # count occurrences of 'tend to' as an extra heuristic\n    count += len(re.findall(r'\\btend\\s+to\\b', lowered))\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are colons (\":\"), indicating labeled lists, ratios or formal structures'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float(text.count(':')) / total_chars\n\n", "def feature(text: str) -> float:\n    'Density of quotation marks (double or curly quotes) per character \u2014 indicates quoting or reported speech'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    quote_chars = ['\"', '\u201c', '\u201d']\n    count = sum(text.count(q) for q in quote_chars)\n    return float(count) / total_chars\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that end with technical/derivational suffixes (ize/ise/ology/ism/ist/ally/ence/ance/etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ize', 'ise', 'ology', 'graphy', 'phobia', 'ism', 'ist', 'ally', 'ence', 'ance', 'hood', 'ship', 'scope', 'ology')\n    count = sum(1 for t in tokens if t.endswith(suffixes))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Normalized Shannon entropy of punctuation-character distribution (0=no variety, 1=max variety observed)'\n    import math\n    if not text:\n        return 0.0\n    # collect punctuation characters (non-alnum, non-space)\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    freq = {}\n    for p in puncts:\n        freq[p] = freq.get(p, 0) + 1\n    total = float(len(puncts))\n    probs = [v / total for v in freq.values()]\n    # entropy in bits\n    ent = -sum(p * math.log2(p) for p in probs if p > 0)\n    # normalize by log2(number of distinct punctuation types)\n    n_types = len(probs)\n    if n_types <= 1:\n        return 0.0\n    norm = ent / math.log2(n_types)\n    return float(norm)\n", "def feature(text: str) -> float:\n    'Proportion of word tokens that are -ing gerunds/continuous verbs (heuristic: tokens ending with \"ing\")'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return float(ing_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Density of modal verbs (may, might, should, could, would, can, will, shall) as a proportion of tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'may','might','should','could','would','can','will','shall','must'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that start like list items or bullet points (e.g., \"-\", \"*\", \"1.\", \"a)\", etc.)'\n    import re\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    marker_re = re.compile(r'^\\s*(?:[-*\u2022]|[0-9]+[.)]|[A-Za-z][\\.)])\\s+')\n    count = sum(1 for L in lines if marker_re.search(L))\n    return float(count) / len(lines)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that end with common adjective suffixes (heuristic for descriptive vocabulary)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ous','ive','able','al','ic','ful','less','ary','ent','ant','ish')\n    count = sum(1 for t in tokens if len(t) > 4 and any(t.endswith(s) for s in suffixes))\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a function word or preposition/article (heuristic for clause-first vs content-first starts)'\n    import re\n    if not text:\n        return 0.0\n    raw_sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not raw_sentences:\n        return 0.0\n    starts = 0\n    function_starts = {'the','a','an','this','that','these','those','in','on','for','to','of','by','with','and','but','when','if','while','during','after','before','as','at'}\n    for s in raw_sentences:\n        m = re.match(r'^\\s*([A-Za-z\\'-]+)', s)\n        if m:\n            first = m.group(1).lower()\n            if first in function_starts:\n                starts += 1\n    return float(starts) / len(raw_sentences)\n\n", "def feature(text: str) -> float:\n    'Adverb density: proportion of tokens ending in \"ly\" (simple heuristic for adverbial style/hedging)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if len(t) > 3 and t.endswith('ly'))\n    return float(ly_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Approximate passive voice indicator: number of \"be\" + past-participle (words ending in -ed) occurrences per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|am)\\s+[A-Za-z-]+ed\\b', text, flags=re.I)\n    return float(len(matches)) / float(sent_count)\n", "def feature(text: str) -> float:\n    'Ratio of capitalized tokens appearing mid-sentence (proxy for proper nouns / named entities)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences, then tokens per sentence\n    sentences = [s.strip() for s in re.split(r'([.!?])', text)]\n    # build real sentence chunks (join fragments)\n    chunks = []\n    cur = ''\n    for part in sentences:\n        if not part:\n            continue\n        cur += part\n        if re.search(r'[.!?]\\s*$', part):\n            chunks.append(cur.strip())\n            cur = ''\n        elif len(part) > 0 and part[-1] in '.!?':\n            chunks.append(cur.strip())\n            cur = ''\n    if cur:\n        chunks.append(cur.strip())\n    if not chunks:\n        chunks = [text]\n    mid_caps = 0\n    total_tokens = 0\n    for s in chunks:\n        tokens = re.findall(r'\\b\\w+\\b', s)\n        for i, t in enumerate(tokens):\n            total_tokens += 1\n            if i == 0:\n                continue\n            # capitalized but not all-caps (to avoid acronyms)\n            if t[0].isupper() and not t.isupper():\n                mid_caps += 1\n    if total_tokens == 0:\n        return 0.0\n    return mid_caps / total_tokens\n\n", "def feature(text: str) -> float:\n    'Fraction of parenthetical groups that resemble citations (contain 4-digit year or \"et al\")'\n    import re\n    if not text:\n        return 0.0\n    paren_groups = re.findall(r'\\(([^)]*)\\)', text)\n    if not paren_groups:\n        return 0.0\n    matches = 0\n    for g in paren_groups:\n        if re.search(r'\\b\\d{4}\\b', g) or re.search(r'\\bet al\\b', g, re.IGNORECASE) or re.search(r'\\bibid\\.?\\b', g, re.IGNORECASE):\n            matches += 1\n    return matches / len(paren_groups)\n\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice marker density: matches of \"was/were/is/are ... -ed/-en\" per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|being|be)\\s+\\w+(?:ed|en)\\b', re.IGNORECASE)\n    matches = sum(len(pattern.findall(s)) for s in sentences)\n    return matches / len(sentences)\n\n", "def feature(text: str) -> float:\n    'Normalized standard deviation of word lengths (std / (mean+1)), robust to empty input'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return std / (mean + 1.0)\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens containing hyphens or slashes (compound terms, ranges, and technical constructs)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t or '/' in t)\n    return count / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens that are acronyms (two or more uppercase letters, all letters) as indicator of scientific/organizational names'\n    import re\n    if not text:\n        return 0.0\n    # find candidate tokens (strip common trailing punctuation)\n    tokens = re.findall(r'\\b[A-Za-z]{2,}\\b', text)\n    if not tokens:\n        return 0.0\n    acr = sum(1 for t in tokens if t.isupper())\n    return acr / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with an article (\"the\", \"a\", \"an\") \u2014 common in formal expository prose'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.match(r'^\\s*(\\w+)', s)\n        if m and m.group(1).lower() in {'the', 'a', 'an'}:\n            count += 1\n    return count / len(sentences)\n", "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters used divided by total characters (captures stylistic richness)'\n    import re\n    if not text:\n        return 0.0\n    punct_chars = set('. , ; : ! ? - \u2014 ( ) [ ] { } \" \\' / \\\\ @ # $ % ^ & * _ ` ~ < > = +'.split())\n    # flatten to single-char set\n    punct_chars = set(''.join(punct_chars))\n    found = set(c for c in text if c in punct_chars)\n    # normalize by length to avoid bias from long texts\n    denom = max(1, len(text))\n    return float(len(found)) / denom\n\n", "def feature(text: str) -> float:\n    'Hedging/qualifier word ratio (words like \"may\", \"might\", \"perhaps\", \"generally\", \"seems\") per token'\n    import re\n    if not text:\n        return 0.0\n    hedges = r'\\b(may|might|could|perhaps|seems?|appears|generally|often|typically|suggests?|likely|unlikely|possibly)\\b'\n    matches = re.findall(hedges, text, flags=re.IGNORECASE)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    return float(len(matches)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Ratio of year-like four-digit numbers (1000-2100) to tokens (captures citations / academic dates)'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(1[0-9]{3}|20[0-9]{2}|2100)\\b', text)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    return float(len(years)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: count of forms like \"was|were|is|are ... <verb>ed\" normalized by tokens'\n    import re\n    if not text:\n        return 0.0\n    # simple heuristic: auxiliary verb followed by a past-participle-looking token\n    pattern = r'\\b(?:was|were|is|are|been|being|be|had been|has been|have been|will be|was being|were being)\\b\\s+\\w+ed\\b'\n    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    return float(len(matches)) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Concentration of repetition: frequency of the most common token divided by total tokens (high value => repetitive focus)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    freqs = Counter(tokens)\n    most_common = freqs.most_common(1)[0][1]\n    return float(most_common) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Adverb usage: fraction of tokens that look like adverbs by ending with \"ly\" (stylistic cue)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.lower().endswith('ly') and len(t) > 2)\n    return float(ly_count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that look like adverbs (ending in \"ly\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and any(c.isalpha() for c in t))\n    return float(ly_count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Colon density: fraction of characters that are colon characters (\":\"), useful for definitions/lists'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float(text.count(':')) / float(total_chars)\n\n\n", "def feature(text: str) -> float:\n    'Fraction of tokens that are common subordinating conjunctions (because, although, since, while, if, etc.)'\n    import re\n    if not text:\n        return 0.0\n    subs = {'because','although','though','since','unless','while','whereas','after','before','once','until','if','when','whenever','where','as'}\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in subs)\n    return float(count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Definition-pattern frequency: counts of \"X is a/an/the Y\" style patterns per sentence'\n    import re\n    if not text:\n        return 0.0\n    s = text.lower()\n    matches = re.findall(r'\\b\\w+\\s+is\\s+(?:a|an|the)\\s+\\w+', s)\n    sentences = len(re.findall(r'[.!?]', text))\n    return float(len(matches)) / max(1.0, float(sentences))\n\n\n", "def feature(text: str) -> float:\n    'Enumeration/list indicator: fraction of sentences containing list markers (first, second, next, 1., (1), finally, etc.)'\n    import re\n    if not text:\n        return 0.0\n    markers = r'\\b(?:first|firstly|second|secondly|third|next|then|finally|subsequently)\\b'\n    numeric_list = r'\\b\\d+\\.'  # \"1.\"\n    parenthesized_num = r'\\(\\d+\\)'\n    sentences = re.findall(r'[^.!?]+', text)\n    if not sentences:\n        return 0.0\n    count = 0\n    for sent in sentences:\n        if re.search(markers, sent, flags=re.IGNORECASE) or re.search(numeric_list, sent) or re.search(parenthesized_num, sent):\n            count += 1\n    return float(count) / float(len(sentences))\n\n\n", "def feature(text: str) -> float:\n    'Hyphenated word ratio: fraction of tokens that contain a hyphen (compound/technical style indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_count) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    \"Possessive ratio: fraction of tokens that are possessive forms (ending with 's or ')\"\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    lower_tokens = [t.lower() for t in tokens]\n    poss = 0\n    for t in lower_tokens:\n        if t.endswith(\"'s\") or t.endswith(\"\u2019s\") or t.endswith(\"'\"):\n            # ensure not a lone apostrophe or just punctuation\n            if any(c.isalpha() for c in t):\n                poss += 1\n    return float(poss) / float(len(tokens))\n\n\n", "def feature(text: str) -> float:\n    'Mean word length excluding a small set of common stopwords (captures lexical sophistication of content words)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','a','an','and','or','but','if','in','on','at','to','for','with','by','of','is','are','was','were','be','being','been','that','this','as','from','it','its','which','who','he','she','they','we','you','i','me','my','his','her','their','our','than','then'}\n    words = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    content = [w for w in words if w not in stopwords and any(c.isalpha() for c in w)]\n    if not content:\n        return 0.0\n    return float(sum(len(w) for w in content)) / float(len(content))\n", "def feature(text: str) -> float:\n    'Density of ellipses (ASCII \"...\" or unicode \"\u2026\") as a fraction of characters, capturing excerpting or trailing ellipses'\n    if not text:\n        return 0.0\n    ellipses = text.count('...') + text.count('\u2026')\n    return float(ellipses) / max(1, len(text))\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that are hedging/modal/qualifying terms (e.g., may, might, could, perhaps, seems), indicating tentative academic tone'\n    import re\n    hedge = {'may','might','could','would','should','perhaps','possibly','probably','seems','seem','appears','appears','likely','suggests','arguably','seeming'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedge)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of total word tokens that appear inside quoted spans (straight or curly double quotes), capturing titles/quoted material density'\n    import re\n    total_words = len(re.findall(r'\\w+', text))\n    if total_words == 0:\n        return 0.0\n    quoted_words = 0\n    # curly quotes\n    for span in re.findall(r'\u201c([^\u201d]+)\u201d', text):\n        quoted_words += len(re.findall(r'\\w+', span))\n    # straight quotes\n    for span in re.findall(r'\"([^\"]+)\"', text):\n        quoted_words += len(re.findall(r'\\w+', span))\n    return float(quoted_words) / total_words\n\n", "def feature(text: str) -> float:\n    'Ratio of ordinal/enumerative words (first, second, next, finally, then, etc.) to tokens; picks out structured/outline prose'\n    import re\n    ordinals = {'first','second','third','fourth','fifth','next','then','finally','initially','subsequently','ultimately','previously','nextly'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in ordinals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Colon character density (\":\" per character), often high in explanatory or list-heavy academic prose'\n    if not text:\n        return 0.0\n    return float(text.count(':')) / len(text)\n\n", "def feature(text: str) -> float:\n    'Proportion of sentences that begin with a coordinating conjunction (and, but, or, so, yet, for, nor), indicating informal/connected sentence starts'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like segments\n    parts = re.split(r'[.!?]+', text)\n    starts = 0\n    total = 0\n    conj = {'and','but','or','so','yet','for','nor'}\n    for p in parts:\n        s = p.strip()\n        if not s:\n            continue\n        tokens = re.findall(r'\\w+', s)\n        if not tokens:\n            continue\n        total += 1\n        if tokens[0].lower() in conj:\n            starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts) / total\n", "def feature(text: str) -> float:\n    'Proportion of apostrophe uses that look like possessives (e.g., owner\\'s or goods\u2019 ) among all apostrophe-like characters'\n    if not text:\n        return 0.0\n    # curly and straight apostrophes\n    apost_count = text.count(\"'\") + text.count(\"\u2019\")\n    if apost_count == 0:\n        return 0.0\n    possessive_patterns = re.findall(r\"\\b\\w+(?:'|\u2019)\\s?s\\b\", text)  # captures patterns like word's (with optional stray space)\n    # also capture plural possessive like words' or words\u2019 (word + s + apostrophe)\n    possessive_patterns += re.findall(r\"\\b\\w+s(?:'|\u2019)\\b\", text)\n    possessive_count = len(possessive_patterns)\n    return float(possessive_count) / apost_count\n\n", "def feature(text: str) -> float:\n    'Ratio of modal verbs (may, might, could, would, should, can, must, shall) to total token count'\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'may','might','could','would','should','can','must','shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of adjacent bigrams where both tokens are Titlecase (capitalized then lowercase), indicating multi-token named entities'\n    if not text:\n        return 0.0\n    matches = list(re.finditer(r'\\b\\w+\\b', text))\n    if len(matches) < 2:\n        return 0.0\n    bigram_count = 0\n    total_possible = max(1, len(matches) - 1)\n    for i in range(len(matches) - 1):\n        w1 = matches[i].group(0)\n        w2 = matches[i+1].group(0)\n        # check Titlecase pattern: first char uppercase, rest lowercase letters (or mixed but not ALL CAPS)\n        def is_titlecase(w, start_idx):\n            if not w:\n                return False\n            if not w[0].isupper():\n                return False\n            # exclude all caps (acronyms)\n            if w.isupper():\n                return False\n            # exclude tokens that begin a sentence (check char before token)\n            prev_char_idx = start_idx - 1\n            if prev_char_idx >= 0:\n                prev_nonspace = None\n                j = prev_char_idx\n                while j >= 0 and text[j].isspace():\n                    j -= 1\n                if j >= 0:\n                    prev_nonspace = text[j]\n                if prev_nonspace in '.!?':\n                    return False\n            return True\n        if is_titlecase(w1, matches[i].start()) and is_titlecase(w2, matches[i+1].start()):\n            bigram_count += 1\n    return float(bigram_count) / total_possible\n\n", "def feature(text: str) -> float:\n    'Fraction of punctuation characters that are non-ASCII (unicode punctuation like em-dash, en-dash, ellipsis char, smart quotes)'\n    if not text:\n        return 0.0\n    punct_chars = [c for c in text if (not c.isalnum() and not c.isspace())]\n    if not punct_chars:\n        return 0.0\n    unicode_punct = sum(1 for c in punct_chars if ord(c) > 127)\n    return float(unicode_punct) / len(punct_chars)\n\n", "def feature(text: str) -> float:\n    'Density of quoted phrases per sentence: number of substrings enclosed in quotes divided by sentence count'\n    if not text:\n        return 0.0\n    # match content between common quote characters (non-greedy)\n    quoted = re.findall(r'[\"\u201c\u201d\u00ab\u00bb\u2018\u2019\\']([^\"\u201c\u201d\u00ab\u00bb\u2018\u2019\\']{1,500}?)[\"\u201c\u201d\u00ab\u00bb\u2018\u2019\\']', text)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(quoted)) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: count of be-forms followed by an -ed token per sentence'\n    if not text:\n        return 0.0\n    # be-forms followed by a past-participle-like token ending with ed\n    passive_matches = re.findall(r'\\b(?:was|were|is|are|be|been|being)\\s+[A-Za-z]+ed\\b', text, flags=re.IGNORECASE)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(passive_matches)) / sentence_count\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a quotation mark (capturing title/quote-starting sentences)'\n    if not text:\n        return 0.0\n    # split into sentences heuristically\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = set('\"\\'\u201c\u201d\u2018\u2019\u00ab\u00bb')\n    starts_with_quote = sum(1 for s in sentences if s and s[0] in quote_chars)\n    return float(starts_with_quote) / len(sentences)\n", "def feature(text: str) -> float:\n    'Heuristic passive-voice score: occurrences of a \"be\" form followed by an -ed token per sentence'\n    import re\n    if not text:\n        return 0.0\n    # look for common \"be\" auxiliaries followed by a past-participial-looking word\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|wasn\\'t|weren\\'t|has been|have been|had been)\\b\\s+\\b\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    # normalize by number of sentences (to be robust across lengths)\n    sent_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(matches) / sent_count)\n\n", "def feature(text: str) -> float:\n    'Ratio of hedging/qualifying words (may, might, seem, suggest, perhaps, appear, likely, tends) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','perhaps','seem','seems','seemed','appear','appears','appeared',\n              'suggest','suggests','suggested','likely','tends','tend','arguably','possibly','apparently'}\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count / len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of tokens matching common literary-criticism stems (novel, character, theme, plot, symbolism, narrator, author, allegor)'\n    import re\n    if not text:\n        return 0.0\n    stems = ['novel','character','theme','plot','symbol','narrat','author','allegor','motif','irony','dialogue','prose','chapter','story','fiction','trag','comedy']\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        for s in stems:\n            if s in t:\n                count += 1\n                break\n    return float(count / len(tokens))\n\n", "def feature(text: str) -> float:\n    'Normalized Shannon token-entropy (entropy of token distribution divided by log2(V) to range ~0-1)'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    freq = {}\n    for t in tokens:\n        freq[t] = freq.get(t, 0) + 1\n    V = len(freq)\n    if V <= 1:\n        return 0.0\n    total = len(tokens)\n    entropy = 0.0\n    for c in freq.values():\n        p = c / total\n        entropy -= p * math.log2(p)\n    # normalize by maximum entropy log2(V)\n    return float(entropy / math.log2(V))\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are typographic quotation marks or em-dashes (smart quotes \u201c \u201d \u2018 \u2019 and \u2014)'\n    if not text:\n        return 0.0\n    special = {'\\u2018','\\u2019','\\u201C','\\u201D','\\u2014'}\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    count = sum(1 for c in text if c in special)\n    return float(count / total_chars)\n\n", "def feature(text: str) -> float:\n    'Colon density: fraction of characters that are colons (\":\"), indicating lists / explanatory clauses'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    return float(text.count(':') / total)\n\n", "def feature(text: str) -> float:\n    'Ratio of first-person pronoun tokens (I, me, my, mine, we, our, us) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    first_person = {'i','me','my','mine','we','our','ours','us'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count / len(tokens))\n", "def feature(text: str) -> float:\n    'Ratio of tokens that appear to be past-tense verbs (heuristic: alphabetic tokens ending with \"ed\" and length>3)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    past_ed = sum(1 for t in tokens if t.endswith('ed') and len(t) > 3)\n    return past_ed / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord>127) to capture typographic cues like curly quotes, em-dashes, diacritics'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return non_ascii / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of quote-like characters that are \u201ccurly\u201d/typographic quotes (\u2019 \u201c \u201d \u2018) versus all quote/apostrophe characters'\n    if not text:\n        return 0.0\n    smart_set = {'\\u2019', '\\u2018', '\\u201c', '\\u201d'}  # \u2019 \u2018 \u201c \u201d\n    all_quote_set = {\"'\", '\"', '\\u2019', '\\u2018', '\\u201c', '\\u201d'}\n    total_quotes = sum(1 for c in text if c in all_quote_set)\n    if total_quotes == 0:\n        return 0.0\n    smart_count = sum(1 for c in text if c in smart_set)\n    return smart_count / float(total_quotes)\n\n", "def feature(text: str) -> float:\n    'Clause density: average number of clause-delimiting punctuation marks (commas, semicolons, colons) per sentence'\n    import re\n    if not text:\n        return 0.0\n    clause_marks = text.count(',') + text.count(';') + text.count(':')\n    sentence_count = len(re.findall(r'[.!?]', text))\n    sentence_count = max(1, sentence_count)  # avoid divide by zero; treat whole text as one sentence if none\n    return clause_marks / float(sentence_count)\n\n", "def feature(text: str) -> float:\n    'Proportion of tokens that end with common adjective suffixes (heuristic for descriptive/qualitative language)'\n    import re\n    suffixes = ('ous', 'ive', 'able', 'al', 'ic', 'ish', 'ary', 'ent', 'ant')\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        for s in suffixes:\n            if len(t) > len(s) + 1 and t.endswith(s):\n                count += 1\n                break\n    return count / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating/conjunctive adverb (And, But, So, Then, However, Thus, Also) signaling narrative or rhetorical style'\n    import re\n    if not text:\n        return 0.0\n    starts = {'and', 'but', 'so', 'then', 'however', 'thus', 'also'}\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'\\b\\w+\\b', s)\n        if m and m.group(0).lower() in starts:\n            count += 1\n    return count / float(len(sentences))\n\n", "def feature(text: str) -> float:\n    'Ratio of hyphenated tokens (tokens containing \"-\") to all word tokens, capturing compounds like self-immolation or case-study'\n    import re\n    if not text:\n        return 0.0\n    # token pattern includes hyphens inside words\n    tokens = re.findall(r'\\b[\\w-]+\\b', text)\n    if not tokens:\n        return 0.0\n    hyph = sum(1 for t in tokens if '-' in t)\n    return hyph / float(len(tokens))\n", "def feature(text: str) -> float:\n    'Fraction of quote characters that are typographic curly quotes (smart quotes)'\n    if not text:\n        return 0.0\n    # count curly and straight quotes (both single and double)\n    curly = text.count('\u201c') + text.count('\u201d') + text.count('\u2018') + text.count('\u2019')\n    straight = text.count('\"') + text.count(\"'\")\n    total = curly + straight\n    if total == 0:\n        return 0.0\n    return float(curly) / float(total)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that contain a quoted span that looks like a title (starts with a capitalized word and is short)'\n    import re\n    if not text:\n        return 0.0\n    # extract sentences\n    sentences = re.findall(r'[^.!?]+[.!?]?', text, flags=re.S)\n    if not sentences:\n        return 0.0\n    # find all quoted spans and mark sentences that contain a likely title\n    quoted_spans = []\n    for m in re.finditer(r'[\u201c\"\\'\\u2018\\u201C](.+?)[\u201d\"\\'\\u2019\\u201D]', text, flags=re.S):\n        inner = m.group(1).strip()\n        # limit length and word count to likely title length\n        words = re.findall(r'\\w+', inner)\n        if words and len(inner) <= 60 and 1 <= len(words) <= 6 and words[0][0].isupper():\n            # record span indices\n            quoted_spans.append((m.start(), m.end(), inner))\n    if not quoted_spans:\n        return 0.0\n    # assign each sentence if it contains any title-like quoted span\n    sentence_contains = 0\n    for s in sentences:\n        s_start = text.find(s)\n        if s_start == -1:\n            continue\n        s_end = s_start + len(s)\n        for (qs, qe, _) in quoted_spans:\n            if qs >= s_start and qe <= s_end:\n                sentence_contains += 1\n                break\n    return float(sentence_contains) / max(1.0, float(len(sentences)))\n\n", "def feature(text: str) -> float:\n    'Proportion of word tokens that are common literary/documentary terms (e.g., \"novel\", \"play\", \"author\")'\n    import re\n    if not text:\n        return 0.0\n    literary_terms = {\n        'novel','play','story','author','written','published','poem','chapter','fiction',\n        'nonfiction','narrative','protagonist','antagonist','allegory','short','essay',\n        'novella','character','plot','scene','drama','act'\n    }\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in literary_terms)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Ratio of hedging or modal/uncertainty verbs and adverbs (e.g., may, might, could, perhaps, likely)'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','perhaps','possibly','likely','seem','seems','appear','appears',\n              'suggest','suggests','tend','tends','often','sometimes','probable','probably','unlikely'}\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of word tokens that are 4-digit years roughly in the range 1000-2100'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(1\\d{3}|20\\d{2}|2100)\\b', text)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(len(years)) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Density of em-dash or en-dash characters as a fraction of total characters'\n    if not text:\n        return 0.0\n    dash_count = text.count('\u2014') + text.count('\u2013')\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float(dash_count) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Fraction of characters in the text that are non-ASCII (useful to detect typographic quotes, accents, curly apostrophes)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / float(total)\n\n", "def feature(text: str) -> float:\n    'Concentration of word bigrams: frequency of the most common bigram divided by total bigrams'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    # build bigrams\n    bigrams = {}\n    total = 0\n    for a, b in zip(tokens, tokens[1:]):\n        key = a + '\\t' + b\n        bigrams[key] = bigrams.get(key, 0) + 1\n        total += 1\n    if total == 0:\n        return 0.0\n    max_freq = max(bigrams.values()) if bigrams else 0\n    return float(max_freq) / float(total)\n", "def feature(text: str) -> float:\n    'Ratio of hedging/epistemic verbs (seem, appear, suggest, indicate variants) to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    hedges = {'seem','seems','seemed','appear','appears','appeared','suggest','suggests','suggested','indicate','indicates','indicated','imply','implies','implied'}\n    count = sum(1 for w in words if w in hedges)\n    return float(count) / len(words)\n\n", "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that contain an internal hyphen (hyphenation density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # consider as hyphenated if contains hyphen and at least one alphanumeric on both sides\n        if '-' in t and re.search(r'[A-Za-z0-9]', t.replace('-', '')):\n            count += 1\n    return float(count) / len(tokens)\n\n", "def feature(text: str) -> float:\n    'Fraction of sentences that are long (more than 25 word tokens) as a proxy for syntactic complexity'\n    import re\n    if not text:\n        return 0.0\n    # crude sentence split\n    raw_sents = re.findall(r'[^.!?]+[.!?]?', text)\n    sents = [s.strip() for s in raw_sents if s and s.strip()]\n    if not sents:\n        return 0.0\n    long_count = 0\n    for s in sents:\n        wcount = len(re.findall(r'\\w+', s))\n        if wcount > 25:\n            long_count += 1\n    return float(long_count) / len(sents)\n\n", "def feature(text: str) -> float:\n    'Density of explicit quotation marks (\", \u201c, \u201d, \u00ab, \u00bb) per word token'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    quote_chars = ['\"', '\u201c', '\u201d', '\u00ab', '\u00bb']\n    qcount = sum(text.count(ch) for ch in quote_chars)\n    return float(qcount) / len(words)\n\n", "def feature(text: str) -> float:\n    'Ratio of immediately repeated consecutive words (e.g., \"very very\") to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    repeats = 0\n    for i in range(1, len(words)):\n        if words[i] == words[i-1]:\n            repeats += 1\n    return float(repeats) / len(words)\n\n", "def feature(text: str) -> float:\n    'Character-level density of em-dash (\u2014) characters (fraction of all characters that are em-dashes)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    count = text.count('\u2014')\n    return float(count) / total_chars\n\n", "def feature(text: str) -> float:\n    'Ratio of coordinating conjunction tokens (and, but, or, nor, for, so, yet) to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    coordinators = {'and','but','or','nor','for','so','yet'}\n    count = sum(1 for w in words if w in coordinators)\n    return float(count) / len(words)\n", "def feature(text: str) -> float:\n    'Proportion of non-ASCII characters in the text (typographic punctuation, dashes, ellipses, accented letters)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / float(total_chars)\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that look like short Title-Case headings (<=40 chars, each word capitalized), useful for sectioned documents'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    heading_count = 0\n    for ln in lines:\n        s = ln.strip()\n        if len(s) > 40 or len(s.split()) == 0 or len(s.split()) > 6:\n            continue\n        words = s.split()\n        all_title = True\n        for w in words:\n            if not w[0].isalpha():\n                continue\n            if not w[0].isupper():\n                all_title = False\n                break\n        # avoid counting typical sentence fragments with punctuation\n        if all_title and not any(p in s for p in '.!?;'):\n            heading_count += 1\n    return float(heading_count) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Fraction of lines that end with a colon (often used for headings or labels like \"The Description:\" )'\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    colon_lines = sum(1 for ln in lines if ln.strip().endswith(':'))\n    return float(colon_lines) / float(len(lines))\n\n", "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas indicate clause complexity in formal writing)'\n    if not text:\n        return 0.0\n    num_commas = text.count(',')\n    num_sentences = text.count('.') + text.count('!') + text.count('?')\n    if num_sentences == 0:\n        # if no clear sentence terminators, use 1 to avoid division by zero\n        num_sentences = 1\n    return float(num_commas) / float(num_sentences)\n\n", "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (semicolon use often correlates with formal, human-edited prose)'\n    if not text:\n        return 0.0\n    semis = text.count(';')\n    num_sentences = text.count('.') + text.count('!') + text.count('?')\n    if num_sentences == 0:\n        num_sentences = 1\n    return float(semis) / float(num_sentences)\n\n", "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total tokens (lexical diversity)'\n    if not text:\n        return 0.0\n    re_mod = __import__('re')\n    tokens = re_mod.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n", "def feature(text: str) -> float:\n    'Fraction of non-empty lines that are short (<=30 chars) and do not end with terminal punctuation \u2014 likely headings or labels'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines()]\n    if not lines:\n        return 0.0\n    short_nonpunct = 0\n    total = 0\n    for ln in lines:\n        s = ln.strip()\n        if not s:\n            continue\n        total += 1\n        if len(s) <= 30 and not s.endswith(('.', '!', '?', ':', ';')):\n            short_nonpunct += 1\n    if total == 0:\n        return 0.0\n    return float(short_nonpunct) / float(total)\n\n", "def feature(text: str) -> float:\n    'Ratio of explicit possessive markers (\\'s or \u2019s) to total word tokens; common in case summaries and narrative descriptions'\n    if not text:\n        return 0.0\n    re_mod = __import__('re')\n    tokens = re_mod.findall(r'\\w+', text)\n    token_count = len(tokens)\n    if token_count == 0:\n        return 0.0\n    possessive_count = text.count(\"'s\") + text.count(\"\u2019s\")\n    return float(possessive_count) / float(token_count)\n"]}