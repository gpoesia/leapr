{
  "used_features": [
    "def feature(image: np.ndarray) -> float:\n    'Normalized contrast between center region and periphery (center mean minus periphery mean, divided by overall std)'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape[:2]\n    ch, cw = max(1, h // 4), max(1, w // 4)\n    center = img[ch:3*ch, cw:3*cw]\n    # periphery = everything except center region\n    mask = np.ones_like(img, dtype=bool)\n    mask[ch:3*ch, cw:3*cw] = False\n    periphery = img[mask]\n    center_mean = float(np.mean(center)) if center.size else 0.0\n    periphery_mean = float(np.mean(periphery)) if periphery.size else 0.0\n    overall_std = float(np.std(img)) + eps\n    result = (center_mean - periphery_mean) / overall_std\n    return float(np.nan_to_num(result))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean edge magnitude computed from image gradients'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    try:\n        gy, gx = np.gradient(img)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    result = np.mean(mag) if mag.size else 0.0\n    return float(result + eps)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Horizontal symmetry score: normalized correlation between left half and mirrored right half (-1..1)'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    mid = w // 2\n    if w < 2:\n        return 0.0\n    left = img[:, :mid]\n    right = img[:, -mid:]\n    right_mirror = np.fliplr(right)\n    A = left.ravel()\n    B = right_mirror.ravel()\n    if A.size == 0 or B.size == 0:\n        return 0.0\n    A_mean = A.mean()\n    B_mean = B.mean()\n    num = np.sum((A - A_mean) * (B - B_mean))\n    den = np.sqrt(np.sum((A - A_mean)**2) * np.sum((B - B_mean)**2)) + eps\n    result = num / den\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels above the 90th percentile intensity (bright pixel fraction)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    thresh = float(np.percentile(arr, 90))\n    frac = float(np.count_nonzero(arr > thresh)) / float(arr.size)\n    return float(frac)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity variance: variance of mean intensities across concentric rings (10 bins)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    dist = np.hypot(ys - cy, xs - cx)\n    maxd = float(dist.max()) if dist.size else 1.0\n    if maxd <= 0:\n        return 0.0\n    nbins = 10\n    bins = np.linspace(0, maxd, nbins + 1)\n    means = []\n    for i in range(nbins):\n        mask = (dist >= bins[i]) & (dist < bins[i+1])\n        if np.any(mask):\n            means.append(np.mean(img[mask]))\n    if len(means) <= 1:\n        return 0.0\n    result = float(np.var(means) / (np.mean(means)**2 + eps))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance of the brightest pixel from image center (0=center, 1=corner)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    idx = np.argmax(img)\n    h, w = img.shape\n    y, x = divmod(int(idx), w)\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    dist = np.hypot(y - cy, x - cx)\n    max_dist = np.hypot(max(cy, h - 1 - cy), max(cx, w - 1 - cx))\n    result = float(dist / (max_dist + eps))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gini coefficient of pixel intensities (measure of sparsity/inequality, 0..1)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    # Make non-negative by shifting if necessary\n    arr_min = arr.min()\n    if arr_min < 0:\n        arr = arr - arr_min\n    sum_x = arr.sum()\n    n = arr.size\n    if sum_x == 0 or n == 0:\n        return 0.0\n    arr_sorted = np.sort(arr)\n    idx = np.arange(1, n + 1)\n    gini = (2.0 * np.sum(idx * arr_sorted) / (n * sum_x)) - (n + 1.0) / n\n    gini = float(np.clip(gini, 0.0, 1.0))\n    return float(gini)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Shannon entropy of intensity histogram (32 bins), higher for more complex textures'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    bins = 32\n    hist, _ = np.histogram(arr, bins=bins, density=False)\n    p = hist.astype(float) / (hist.sum() + 1e-12)\n    p_nonzero = p[p > 0]\n    if p_nonzero.size == 0:\n        return 0.0\n    entropy = -np.sum(p_nonzero * np.log2(p_nonzero))\n    return float(entropy)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Standard deviation of the four quadrant mean intensities (captures off-center objects)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    hy, wx = h // 2, w // 2\n    q1 = img[:hy, :wx]\n    q2 = img[:hy, wx:]\n    q3 = img[hy:, :wx]\n    q4 = img[hy:, wx:]\n    means = []\n    for q in (q1, q2, q3, q4):\n        means.append(float(np.mean(q)) if q.size else 0.0)\n    overall_mean = float(np.mean(img)) if img.size else 0.0\n    result = float(np.std(means) / (abs(overall_mean) + eps))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical gradient asymmetry: normalized difference between top-half and bottom-half mean vertical gradients'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    try:\n        gy, gx = np.gradient(img)\n    except Exception:\n        return 0.0\n    h = img.shape[0]\n    mid = h // 2\n    top = gy[:mid, :]\n    bottom = gy[mid:, :]\n    if top.size == 0 or bottom.size == 0:\n        return 0.0\n    top_mean = float(np.mean(top))\n    bottom_mean = float(np.mean(bottom))\n    denom = (abs(top_mean) + abs(bottom_mean) + eps)\n    result = (top_mean - bottom_mean) / denom\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity skewness (third standardized moment) of pixel values'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    mu = arr.mean()\n    sigma = arr.std()\n    if sigma <= 0:\n        return 0.0\n    skew = np.mean((arr - mu)**3) / (sigma**3 + eps)\n    return float(np.clip(skew, -1e6, 1e6))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of high-frequency energy to total energy from 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    try:\n        F = np.fft.fftshift(np.fft.fft2(img))\n        mag = np.abs(F)\n    except Exception:\n        return 0.0\n    cy, cx = h // 2, w // 2\n    # low-pass radius: 10% of max radius, at least 1\n    maxr = np.hypot(cy, cx) or 1.0\n    r0 = max(1.0, 0.1 * maxr)\n    ys = np.arange(h)[:, None] - cy\n    xs = np.arange(w)[None, :] - cx\n    dist = np.hypot(ys, xs)\n    low_mask = dist <= r0\n    total = mag.sum() + eps\n    high = mag[~low_mask].sum()\n    return float(high / total)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of local intensity peaks per 1000 pixels (sparse local maxima count)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    if h < 3 or w < 3:\n        return 0.0\n    # pad and compare to 8 neighbors\n    p = np.pad(img, 1, mode='constant', constant_values=np.min(img))\n    center = p[1:-1, 1:-1]\n    neighs = [\n        p[0:-2, 0:-2], p[0:-2, 1:-1], p[0:-2, 2:],\n        p[1:-1, 0:-2],               p[1:-1, 2:],\n        p[2:,   0:-2], p[2:,   1:-1], p[2:,   2:]\n    ]\n    greater = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        greater &= (center > n)\n    # require peaks to be above mean+std to ignore noise\n    thresh = img.mean() + img.std()\n    peaks = greater & (center > thresh)\n    count = int(np.count_nonzero(peaks))\n    density = count / float(max(1, h * w)) * 1000.0\n    return float(density)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bounding-box area fraction of very bright pixels (>95th percentile)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    flat = np.ravel(arr)\n    if flat.size == 0:\n        return 0.0\n    thresh = float(np.percentile(flat, 95))\n    mask = arr > thresh\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.where(mask)\n    h, w = arr.shape\n    y0, y1 = ys.min(), ys.max()\n    x0, x1 = xs.min(), xs.max()\n    bbox_area = (y1 - y0 + 1) * (x1 - x0 + 1)\n    frac = bbox_area / float(h * w)\n    return float(frac)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Balance of gradient orientation energy (1.0 = equal vertical/horizontal energy)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    try:\n        gy, gx = np.gradient(img)\n    except Exception:\n        return 0.0\n    vert_energy = float(np.sum(np.abs(gx)))\n    horz_energy = float(np.sum(np.abs(gy)))\n    total = vert_energy + horz_energy + eps\n    balance = 1.0 - abs(vert_energy - horz_energy) / total\n    return float(np.clip(balance, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of border pixels darker than the image median (border width 5% min 1)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    bw = max(1, int(round(0.05 * min(h, w))))\n    mask = np.zeros_like(arr, dtype=bool)\n    mask[:bw, :] = True\n    mask[-bw:, :] = True\n    mask[:, :bw] = True\n    mask[:, -bw:] = True\n    border = arr[mask]\n    if border.size == 0:\n        return 0.0\n    med = float(np.median(arr))\n    frac = float(np.count_nonzero(border < med)) / float(border.size)\n    return float(frac)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels within 1% range of median intensity (flatness indicator)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    mn, mx = arr.min(), arr.max()\n    span = max(eps, mx - mn)\n    med = float(np.median(arr))\n    tol = 0.01 * span\n    frac = float(np.count_nonzero(np.abs(arr - med) <= tol)) / float(arr.size)\n    return float(frac)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity moment of inertia normalized by max radius^2 (0=centered bright, 1=peripheral)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    r2 = (ys - cy)**2 + (xs - cx)**2\n    weight = img\n    total_weight = float(weight.sum())\n    if total_weight <= 0:\n        return 0.0\n    moi = float((weight * r2).sum() / (total_weight + eps))\n    max_r2 = float(r2.max()) if r2.size else 1.0\n    return float(np.clip(moi / (max_r2 + eps), 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted horizontal centroid offset normalized to image half-width (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    xs = np.arange(w)[None, :]\n    total = float(img.sum())\n    if total == 0 or w <= 1:\n        return 0.0\n    centroid_x = float((img * xs).sum() / total)\n    cx = (w - 1) / 2.0\n    dist = abs(centroid_x - cx)\n    max_dist = cx if cx > 0 else 1.0\n    norm = dist / (max_dist + eps)\n    return float(np.clip(norm, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Spectral entropy of FFT magnitude (bits), higher for more complex frequency content'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    try:\n        F = np.fft.fft2(arr)\n        mag = np.abs(F).ravel()\n    except Exception:\n        return 0.0\n    mag_sum = mag.sum()\n    if mag_sum <= 0:\n        return 0.0\n    p = mag / (mag_sum + eps)\n    p_nonzero = p[p > 0]\n    if p_nonzero.size == 0:\n        return 0.0\n    entropy = -np.sum(p_nonzero * np.log2(p_nonzero))\n    return float(entropy)\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity skewness (third standardized moment), indicating asymmetry of brightness'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    mu = arr.mean()\n    sigma = arr.std()\n    if sigma <= 0:\n        return 0.0\n    skew = np.mean(((arr - mu) / (sigma + eps)) ** 3)\n    return float(np.clip(skew, -10.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels that are strong edges (gradient magnitude > mean+std)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(img)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thresh = mag.mean() + mag.std()\n    strong = np.count_nonzero(mag > thresh)\n    frac = strong / float(max(1, mag.size))\n    return float(frac)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant gradient orientation concentration (0=uniform, 1=all aligned)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(img)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= 0:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # range [-pi, pi]\n    cx = np.sum(mag * np.cos(theta))\n    cy = np.sum(mag * np.sin(theta))\n    R = np.hypot(cx, cy) / (mag.sum() + eps)\n    return float(np.clip(R, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Coarseness: mean local variance over medium-size blocks normalized by overall variance'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    overall_var = arr.var()\n    if overall_var <= 0:\n        return 0.0\n    # choose block size based on smaller dimension\n    block = max(1, min(h, w) // 8)\n    bh = block\n    bw = block\n    # pad to multiple of block\n    pad_h = (-h) % bh\n    pad_w = (-w) % bw\n    if pad_h or pad_w:\n        arr_p = np.pad(arr, ((0, pad_h), (0, pad_w)), mode='reflect')\n    else:\n        arr_p = arr\n    H, W = arr_p.shape\n    arr_blocks = arr_p.reshape(H // bh, bh, W // bw, bw)\n    # compute variance within each block\n    block_vars = arr_blocks.transpose(0,2,1,3).reshape(-1, bh*bw).var(axis=1)\n    mean_block_var = float(np.mean(block_vars)) if block_vars.size else 0.0\n    result = mean_block_var / (overall_var + eps)\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Otsu-based normalized separation: (mean_high - mean_low) / (overall_mean+eps)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    # map to 256 bins based on data range\n    amin, amax = arr.min(), arr.max()\n    if amax <= amin:\n        return 0.0\n    nbins = 256\n    hist, edges = np.histogram(arr, bins=nbins, range=(amin, amax))\n    hist = hist.astype(float)\n    total = hist.sum()\n    if total <= 0:\n        return 0.0\n    probs = hist / total\n    bin_centers = (edges[:-1] + edges[1:]) / 2.0\n    cumulative = np.cumsum(probs)\n    cumulative_mean = np.cumsum(probs * bin_centers)\n    global_mean = cumulative_mean[-1]\n    # compute between-class variance and best threshold\n    sigma_b = (global_mean * cumulative - cumulative_mean) ** 2 / (cumulative * (1 - cumulative) + eps)\n    idx = np.nanargmax(sigma_b)\n    thresh = bin_centers[idx]\n    low = arr[arr <= thresh]\n    high = arr[arr > thresh]\n    if low.size == 0 or high.size == 0:\n        return 0.0\n    mean_low = low.mean()\n    mean_high = high.mean()\n    overall_mean = arr.mean()\n    result = (mean_high - mean_low) / (abs(overall_mean) + eps)\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio score of the foreground bounding box (1.0 = square, lower = elongated)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # foreground defined as pixels > median to be robust\n    med = np.median(arr)\n    mask = arr > med\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.where(mask)\n    ymin, ymax = ys.min(), ys.max()\n    xmin, xmax = xs.min(), xs.max()\n    bh = max(1, ymax - ymin + 1)\n    bw = max(1, xmax - xmin + 1)\n    ratio = min(bw / float(bh + eps), bh / float(bw + eps))\n    # normalize ratio to [0,1], where 1 is square\n    result = float(np.clip(ratio, 0.0, 1.0))\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized centroid offset: distance between intensity centroid and image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    total = arr.sum()\n    if total == 0:\n        return 0.0\n    cy = float((arr * ys).sum() / (total + eps))\n    cx = float((arr * xs).sum() / (total + eps))\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    maxd = np.hypot(center_x, center_y) + eps\n    result = dist / maxd\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels that match vertical mirror within tolerance (1.0 = perfect symmetry)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = arr[:, :mid]\n    right = arr[:, -mid:]\n    right_mirror = np.fliplr(right)\n    # align shapes if odd width\n    if left.shape != right_mirror.shape:\n        min_c = min(left.shape[1], right_mirror.shape[1])\n        left = left[:, :min_c]\n        right_mirror = right_mirror[:, :min_c]\n    diff = np.abs(left - right_mirror)\n    tol = max(np.std(arr) * 0.2, 1e-6)\n    matched = np.count_nonzero(diff <= tol)\n    total = diff.size\n    if total == 0:\n        return 0.0\n    frac = matched / float(total)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio via discrete Laplacian (sum|L| / sum|I|)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    I = np.nan_to_num(img.astype(float))\n    h, w = I.shape\n    if h < 3 or w < 3:\n        # fallback to gradient-based high freq estimate\n        try:\n            gy, gx = np.gradient(I)\n            hf = np.sum(np.hypot(gx, gy))\n        except Exception:\n            return 0.0\n        denom = np.sum(np.abs(I)) + eps\n        return float(hf / denom)\n    # pad with reflect to compute Laplacian\n    P = np.pad(I, 1, mode='reflect')\n    center = P[1:-1, 1:-1]\n    up = P[0:-2, 1:-1]\n    down = P[2:, 1:-1]\n    left = P[1:-1, 0:-2]\n    right = P[1:-1, 2:]\n    lap = 4 * center - (up + down + left + right)\n    hf_energy = np.sum(np.abs(lap))\n    denom = np.sum(np.abs(I)) + eps\n    ratio = hf_energy / denom\n    return float(ratio)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast spread: (90th percentile - 10th percentile) normalized by mean intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    p90 = np.percentile(arr, 90)\n    p10 = np.percentile(arr, 10)\n    mean = arr.mean()\n    result = (p90 - p10) / (abs(mean) + eps)\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry score: normalized correlation between top half and mirrored bottom half (-1..1)'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    mid = h // 2\n    if h < 2:\n        return 0.0\n    top = img[:mid, :]\n    bottom = img[-mid:, :]\n    bottom_mirror = np.flipud(bottom)\n    A = top.ravel()\n    B = bottom_mirror.ravel()\n    if A.size == 0 or B.size == 0:\n        return 0.0\n    A_mean = A.mean()\n    B_mean = B.mean()\n    num = np.sum((A - A_mean) * (B - B_mean))\n    den = np.sqrt(np.sum((A - A_mean) ** 2) * np.sum((B - B_mean) ** 2)) + eps\n    result = num / den\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Top-bottom brightness bias normalized by image std (positive => top brighter)'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 2:\n        return 0.0\n    mid = h // 2\n    top_mean = float(np.mean(arr[:mid, :])) if arr[:mid, :].size else 0.0\n    bot_mean = float(np.mean(arr[-mid:, :])) if arr[-mid:, :].size else 0.0\n    overall_std = float(np.std(arr)) + eps\n    result = (top_mean - bot_mean) / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of center-region pixels that are significantly brighter than image median'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    ch, cw = max(1, h // 4), max(1, w // 4)\n    center = arr[ch:3*ch, cw:3*cw]\n    if center.size == 0:\n        return 0.0\n    med = float(np.median(arr))\n    std = float(np.std(arr))\n    thr = med + 0.5 * std\n    frac = float(np.count_nonzero(center > thr)) / float(center.size)\n    return float(frac)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of local intensity peaks (local maxima stronger than neighbors and > mean+0.5*std)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    padded = np.pad(arr, ((1, 1), (1, 1)), mode='constant', constant_values=-np.inf)\n    C = padded[1:-1, 1:-1]\n    N = padded[:-2, 1:-1]\n    S = padded[2:, 1:-1]\n    W = padded[1:-1, :-2]\n    E = padded[1:-1, 2:]\n    NW = padded[:-2, :-2]\n    NE = padded[:-2, 2:]\n    SW = padded[2:, :-2]\n    SE = padded[2:, 2:]\n    is_peak = (C > N) & (C > S) & (C > W) & (C > E) & (C > NW) & (C > NE) & (C > SW) & (C > SE)\n    mean = float(np.mean(arr))\n    std = float(np.std(arr))\n    thr = mean + 0.5 * std\n    mask = is_peak & (C > thr)\n    count = int(np.count_nonzero(mask))\n    area = float(h * w)\n    result = count / (area + 1e-12)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bounding-box occupancy: proportion of nonzero pixels inside their tight bbox (compactness)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = img.mean(axis=2)\n    else:\n        arr = img\n    arr = np.nan_to_num(arr.astype(float))\n    nz = arr != 0\n    if not np.any(nz):\n        return 0.0\n    ys, xs = np.where(nz)\n    y0, y1 = ys.min(), ys.max()\n    x0, x1 = xs.min(), xs.max()\n    bbox_area = float((y1 - y0 + 1) * (x1 - x0 + 1))\n    count = float(np.count_nonzero(nz))\n    result = count / (bbox_area + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Coarseness ratio: mean long-distance difference / mean adjacent difference (distance 2 / distance 1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 1 or w < 1:\n        return 0.0\n    # adjacent diffs (vertical and horizontal)\n    dh = np.abs(arr[1:, :] - arr[:-1, :]).ravel()\n    dw = np.abs(arr[:, 1:] - arr[:, :-1]).ravel()\n    fine = np.concatenate([dh, dw]) if (dh.size + dw.size) > 0 else np.array([0.0])\n    # distance-2 diffs\n    dh2 = np.abs(arr[2:, :] - arr[:-2, :]).ravel() if h > 2 else np.array([])\n    dw2 = np.abs(arr[:, 2:] - arr[:, :-2]).ravel() if w > 2 else np.array([])\n    coarse = np.concatenate([dh2, dw2]) if (dh2.size + dw2.size) > 0 else np.array([0.0])\n    fine_mean = float(np.mean(fine)) if fine.size else 0.0\n    coarse_mean = float(np.mean(coarse)) if coarse.size else 0.0\n    result = coarse_mean / (fine_mean + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Inter-percentile contrast: (90th - 10th percentile) normalized by median'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = img.mean(axis=2)\n    else:\n        arr = img\n    arr = np.ravel(np.nan_to_num(arr.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    p90 = float(np.percentile(arr, 90))\n    p10 = float(np.percentile(arr, 10))\n    p50 = float(np.percentile(arr, 50))\n    result = (p90 - p10) / (abs(p50) + 1e-12)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation entropy (16 bins), weighted by gradient magnitude (higher = more varied directions)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(img)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total = mag.sum()\n    if total <= 0:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # [-pi, pi]\n    bins = 16\n    hist, _ = np.histogram(theta.ravel(), bins=bins, range=(-np.pi, np.pi), weights=mag.ravel())\n    p = hist.astype(float) / (hist.sum() + 1e-12)\n    p_nonzero = p[p > 0]\n    if p_nonzero.size == 0:\n        return 0.0\n    entropy = -np.sum(p_nonzero * np.log2(p_nonzero))\n    # normalize by max entropy = log2(bins)\n    result = entropy / (np.log2(bins) + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial monotonicity: Pearson correlation between ring radius and mean ring intensity (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    dist = np.hypot(ys - cy, xs - cx)\n    maxd = float(dist.max()) if dist.size else 1.0\n    if maxd <= 0:\n        return 0.0\n    nbins = min(20, int(max(4, round(maxd))))\n    bins = np.linspace(0, maxd, nbins + 1)\n    ring_means = []\n    ring_r = []\n    for i in range(nbins):\n        mask = (dist >= bins[i]) & (dist < bins[i + 1])\n        if np.any(mask):\n            ring_means.append(np.mean(img[mask]))\n            ring_r.append(0.5 * (bins[i] + bins[i + 1]))\n    if len(ring_means) <= 1:\n        return 0.0\n    r = np.array(ring_r)\n    m = np.array(ring_means)\n    rm = (r - r.mean())\n    mm = (m - m.mean())\n    denom = (np.sqrt((rm ** 2).sum() * (mm ** 2).sum()) + eps)\n    corr = float(np.sum(rm * mm) / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric (Hasler & S\u00fcsstrunk); returns 0 for grayscale inputs'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    # ensure float channels\n    arr = np.nan_to_num(img.astype(float))\n    R = arr[:, :, 0].ravel()\n    G = arr[:, :, 1].ravel()\n    B = arr[:, :, 2].ravel()\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    std_root = np.sqrt(std_rg ** 2 + std_yb ** 2)\n    mean_root = np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    result = std_root + 0.3 * mean_root\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized aspect ratio of the bright-region bounding box (1.0 = square, 0.0 = very elongated)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    thr = img.mean() + 0.5 * img.std()\n    mask = img > thr\n    if not mask.any():\n        return 0.0\n    ys, xs = np.where(mask)\n    miny, maxy = ys.min(), ys.max()\n    minx, maxx = xs.min(), xs.max()\n    bbox_h = max(1, maxy - miny + 1)\n    bbox_w = max(1, maxx - minx + 1)\n    small = min(bbox_h, bbox_w)\n    large = max(bbox_h, bbox_w)\n    ratio = float(small) / float(large + eps)\n    return float(np.clip(ratio, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Solidity approximation of the largest bright component (area / bounding-box-area)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    thr = arr.mean() + 0.5 * arr.std()\n    mask = arr > thr\n    if not mask.any():\n        return 0.0\n    visited = np.zeros(mask.shape, dtype=bool)\n    best_area = 0\n    best_bbox_area = 1\n    # 4-neighbor flood fill for each component\n    for y in range(h):\n        for x in range(w):\n            if mask[y, x] and not visited[y, x]:\n                stack = [(y, x)]\n                visited[y, x] = True\n                ys_min = y; ys_max = y; xs_min = x; xs_max = x\n                area = 0\n                while stack:\n                    cy, cx = stack.pop()\n                    area += 1\n                    if cy < ys_min: ys_min = cy\n                    if cy > ys_max: ys_max = cy\n                    if cx < xs_min: xs_min = cx\n                    if cx > xs_max: xs_max = cx\n                    # neighbors\n                    if cy > 0 and mask[cy-1, cx] and not visited[cy-1, cx]:\n                        visited[cy-1, cx] = True; stack.append((cy-1, cx))\n                    if cy+1 < h and mask[cy+1, cx] and not visited[cy+1, cx]:\n                        visited[cy+1, cx] = True; stack.append((cy+1, cx))\n                    if cx > 0 and mask[cy, cx-1] and not visited[cy, cx-1]:\n                        visited[cy, cx-1] = True; stack.append((cy, cx-1))\n                    if cx+1 < w and mask[cy, cx+1] and not visited[cy, cx+1]:\n                        visited[cy, cx+1] = True; stack.append((cy, cx+1))\n                bbox_h = max(1, ys_max - ys_min + 1)\n                bbox_w = max(1, xs_max - xs_min + 1)\n                bbox_area = bbox_h * bbox_w\n                if area > best_area:\n                    best_area = area\n                    best_bbox_area = bbox_area\n    solidity = float(best_area) / float(best_bbox_area + eps)\n    return float(np.clip(solidity, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Absolute horizontal symmetry (0.0 asymmetric, 1.0 perfectly symmetric)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if w < 2:\n        return 1.0\n    half = w // 2\n    left = arr[:, :half]\n    if w % 2 == 0:\n        right = arr[:, half:]\n    else:\n        right = arr[:, half+1:]\n    # make same shape by trimming larger side\n    minw = min(left.shape[1], right.shape[1])\n    left = left[:, :minw].ravel()\n    right = right[:, :minw][:, ::-1].ravel()  # mirror right\n    if left.size == 0:\n        return 1.0\n    la = left - left.mean()\n    ra = right - right.mean()\n    num = np.sum(la * ra)\n    den = np.sqrt((la*la).sum() * (ra*ra).sum()) + eps\n    corr = float(num / den)\n    return float(np.clip(abs(corr), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density per 1000 pixels using gradient magnitude thresholding'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(arr)\n    mag = np.hypot(gx, gy)\n    thr = mag.mean() + mag.std()\n    count = int(np.count_nonzero(mag > thr))\n    density = count / float(max(1, h * w)) * 1000.0\n    return float(density)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Prominence of the darkest pixel relative to its local neighborhood (positive = pronounced dark spot)'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    idx = np.argmin(arr)\n    y, x = np.unravel_index(idx, arr.shape)\n    r = 2  # neighborhood radius => 5x5\n    y0 = max(0, y - r); y1 = min(h, y + r + 1)\n    x0 = max(0, x - r); x1 = min(w, x + r + 1)\n    local = arr[y0:y1, x0:x1]\n    if local.size == 0:\n        return 0.0\n    local_mean = float(np.mean(local))\n    overall_std = float(np.std(arr)) + eps\n    prominence = (local_mean - float(arr[y, x])) / overall_std\n    return float(prominence)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average corner-like response: mean(|Ix * Iy|) normalized by mean gradient energy (0..1 approx)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    if arr.size == 0:\n        return 0.0\n    gy, gx = np.gradient(arr)\n    prod = np.abs(gx * gy)\n    energy = (gx * gx + gy * gy)\n    mean_prod = float(np.mean(prod))\n    mean_energy = float(np.mean(energy)) + eps\n    score = mean_prod / mean_energy\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Texture anisotropy: normalized difference between average row-variance and column-variance (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    if arr.size == 0:\n        return 0.0\n    row_var = float(np.mean(np.var(arr, axis=1)))\n    col_var = float(np.mean(np.var(arr, axis=0)))\n    diff = abs(row_var - col_var)\n    denom = row_var + col_var + eps\n    anis = diff / denom\n    return float(np.clip(anis, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Pearson-like brightness skewness (3*(mean-median)/std), clipped to [-1,1]'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float)).ravel()\n    if arr.size == 0:\n        return 0.0\n    mean = float(np.mean(arr))\n    med = float(np.median(arr))\n    std = float(np.std(arr))\n    if std <= 0:\n        return 0.0\n    skew = 3.0 * (mean - med) / std\n    return float(np.clip(skew, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Maximum radial sharpness: largest adjacent-ring mean jump normalized by overall std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    dist = np.hypot(ys - cy, xs - cx)\n    maxd = float(dist.max()) if dist.size else 1.0\n    if maxd <= 0:\n        return 0.0\n    nbins = min(30, max(4, int(round(maxd))))\n    bins = np.linspace(0, maxd, nbins + 1)\n    ring_means = []\n    for i in range(nbins):\n        mask = (dist >= bins[i]) & (dist < bins[i + 1])\n        if np.any(mask):\n            ring_means.append(np.mean(img[mask]))\n    if len(ring_means) <= 1:\n        return 0.0\n    rm = np.array(ring_means)\n    diffs = np.abs(np.diff(rm))\n    maxjump = float(diffs.max())\n    overall_std = float(np.std(img)) + eps\n    result = maxjump / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted centroid offset normalized by max radius (0=centered, 1=at edge)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    total = float(arr.sum())\n    if total == 0:\n        return 0.0\n    cy = float(((ys * arr).sum()) / (total + eps))\n    cx = float(((xs * arr).sum()) / (total + eps))\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    maxr = np.hypot(center_y, center_x) + eps\n    offset = dist / maxr\n    return float(np.clip(offset, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local variance in a kxk window (k=3 or 5) normalized by global variance'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    # convert to grayscale if color\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    k = 3 if min(h, w) >= 3 else 1\n    if min(h, w) >= 5:\n        k = 5\n    pad = k // 2\n    P = np.pad(arr, pad_width=pad, mode='reflect')\n    S = P.cumsum(axis=0).cumsum(axis=1)\n    # window sum using integral image: sums for all centers\n    sum_ = S[k:, k:] - S[:-k, k:] - S[k:, :-k] + S[:-k, :-k]\n    P2 = np.pad(arr * arr, pad_width=pad, mode='reflect')\n    S2 = P2.cumsum(axis=0).cumsum(axis=1)\n    sum2 = S2[k:, k:] - S2[:-k, k:] - S2[k:, :-k] + S2[:-k, :-k]\n    area = float(k * k)\n    local_mean = sum_ / area\n    local_mean_sq = sum2 / area\n    local_var = local_mean_sq - (local_mean * local_mean)\n    # clamp tiny negatives\n    local_var = np.maximum(local_var, 0.0)\n    mean_local_var = float(np.mean(local_var))\n    global_var = float(np.var(arr)) + eps\n    result = mean_local_var / global_var\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels classified as edges by local gradient (75th percentile threshold)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total = mag.size\n    if total == 0:\n        return 0.0\n    thr = float(np.percentile(mag.ravel(), 75))\n    cnt = int(np.count_nonzero(mag > thr))\n    result = float(cnt) / float(total + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian normalized by mean absolute intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    if arr.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n        gxx = np.gradient(gx, axis=1)\n        gyy = np.gradient(gy, axis=0)\n        lap = gxx + gyy\n    except Exception:\n        return 0.0\n    mean_abs_lap = float(np.mean(np.abs(lap)))\n    mean_abs_img = float(np.mean(np.abs(arr))) + eps\n    result = mean_abs_lap / mean_abs_img\n    return float(np.clip(result, 0.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity skewness (third standardized moment) clipped to [-5,5]'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2)).ravel()\n    else:\n        arr = np.nan_to_num(img.astype(float)).ravel()\n    if arr.size < 2:\n        return 0.0\n    mu = float(arr.mean())\n    std = float(arr.std())\n    if std <= 0:\n        return 0.0\n    m3 = float(((arr - mu) ** 3).mean())\n    skew = m3 / (std ** 3)\n    skew = max(min(skew, 5.0), -5.0)\n    return float(skew)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels within a narrow band around the median (flatness)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    lo = float(np.nanmin(arr))\n    hi = float(np.nanmax(arr))\n    if hi - lo <= 0:\n        return 1.0\n    med = float(np.median(arr))\n    delta = 0.05 * (hi - lo)\n    mask = np.abs(arr - med) <= delta\n    frac = float(np.count_nonzero(mask)) / float(arr.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast of the brightest quadrant vs others normalized by image std'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    hm = h // 2\n    wm = w // 2\n    q1 = arr[:hm, :wm]\n    q2 = arr[:hm, wm:]\n    q3 = arr[hm:, :wm]\n    q4 = arr[hm:, wm:]\n    means = [float(np.mean(q)) if q.size else 0.0 for q in (q1, q2, q3, q4)]\n    overall_std = float(np.std(arr)) + eps\n    sorted_means = sorted(means, reverse=True)\n    top = sorted_means[0]\n    second = sorted_means[1] if len(sorted_means) > 1 else 0.0\n    result = (top - second) / overall_std\n    return float(np.clip(result, 0.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Centroid offset of bright region (threshold = mean+0.5*std) normalized by image diagonal'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    mu = float(arr.mean())\n    sd = float(arr.std())\n    thr = mu + 0.5 * sd\n    mask = arr > thr\n    if not np.any(mask):\n        return 0.0\n    weights = (arr - thr) * mask\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    total_w = float(weights.sum()) + eps\n    cy = float((weights * ys).sum()) / total_w\n    cx = float((weights * xs).sum()) / total_w\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    maxd = np.hypot(center_y, center_x) + eps\n    result = dist / maxd\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness measure for RGB images (0 for grayscale) normalized by max intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    imgf = np.nan_to_num(img.astype(float))\n    R = imgf[..., 0]\n    G = imgf[..., 1]\n    B = imgf[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(rg))\n    mean_yb = float(np.mean(yb))\n    # Has commonly used form\n    colorfulness = np.hypot(std_rg, std_yb) + 0.3 * np.hypot(mean_rg, mean_yb)\n    # normalize by typical max intensity (use 1/255 scaling if image appears in 0..255)\n    maxv = float(np.nanmax(imgf)) + eps\n    result = colorfulness / (maxv + eps)\n    return float(np.clip(result, 0.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Kurtosis-based tailness of intensity distribution (tanh-compressed)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2)).ravel()\n    else:\n        arr = np.nan_to_num(img.astype(float)).ravel()\n    if arr.size < 4:\n        return 0.0\n    mu = float(arr.mean())\n    sd = float(arr.std())\n    if sd <= 0:\n        return 0.0\n    kurt = float(((arr - mu) ** 4).mean()) / (sd ** 4) - 3.0\n    # compress range to (-1,1) to be stable\n    result = float(np.tanh(kurt / 10.0))\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of vertical-edge energy to total edge energy (weighted by gradient magnitude)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total = float(mag.sum()) + eps\n    if total <= 0:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    # vertical directions are near +/- pi/2\n    ang_diff = np.abs(np.abs(theta) - (np.pi / 2.0))\n    mask_vert = ang_diff <= (np.pi / 8.0)  # within 22.5 degrees of vertical\n    vert_energy = float(np.sum(mag[mask_vert]))\n    result = vert_energy / total\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Shannon entropy of intensity histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    flat = arr.ravel()\n    if flat.size == 0:\n        return 0.0\n    bins = 64\n    hist, _ = np.histogram(flat, bins=bins, range=(flat.min(), flat.max()) if flat.min() != flat.max() else (0.0, 1.0))\n    total = hist.sum()\n    if total <= 0:\n        return 0.0\n    p = hist.astype(float) / (total + eps)\n    p_nonzero = p[p > 0]\n    entropy = -np.sum(p_nonzero * np.log2(p_nonzero))\n    result = entropy / (np.log2(bins) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric (Hasler-S\u00fcsstrunk); 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim < 3 or img.shape[2] < 3:\n        return 0.0\n    arr = np.nan_to_num(img.astype(float))\n    R = arr[..., 0]\n    G = arr[..., 1]\n    B = arr[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(rg))\n    mean_yb = float(np.mean(yb))\n    colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n    return float(colorfulness)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels brighter than mean+std (foreground fill ratio)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    m = float(np.mean(arr))\n    s = float(np.std(arr))\n    thresh = m + s\n    total = arr.size\n    if total == 0:\n        return 0.0\n    count = int(np.count_nonzero(arr > thresh))\n    result = count / float(total)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with gradient magnitude in top quartile'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy).ravel()\n    if mag.size == 0:\n        return 0.0\n    thresh = float(np.percentile(mag, 75))\n    high = mag > thresh\n    result = high.sum() / float(mag.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Maximum horizontal run of foreground (threshold=median) normalized by width'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if w == 0:\n        return 0.0\n    med = np.median(arr) if arr.size else 0.0\n    mask = arr > med\n    max_run = 0\n    for r in range(h):\n        row = mask[r]\n        if not np.any(row):\n            continue\n        idx = np.nonzero(row)[0]\n        if idx.size == 0:\n            continue\n        # split indices into contiguous runs\n        diffs = np.diff(idx)\n        if diffs.size == 0:\n            lengths = [1]\n        else:\n            splits = np.where(diffs > 1)[0] + 1\n            runs = np.split(idx, splits)\n            lengths = [len(run) for run in runs]\n        row_max = max(lengths) if lengths else 0\n        if row_max > max_run:\n            max_run = row_max\n    result = max_run / float(w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry correlation of intensities (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = arr[:, :mid]\n    right = arr[:, -mid:] if mid > 0 else np.empty((h, 0))\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    right_flipped = np.fliplr(right)\n    # match shapes if widths differ by 1\n    min_w = min(left.shape[1], right_flipped.shape[1])\n    left_crop = left[:, :min_w].ravel()\n    right_crop = right_flipped[:, :min_w].ravel()\n    if left_crop.size == 0:\n        return 0.0\n    Lc = left_crop - left_crop.mean()\n    Rc = right_crop - right_crop.mean()\n    denom = (np.linalg.norm(Lc) * np.linalg.norm(Rc) + eps)\n    corr = float(np.dot(Lc, Rc) / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio via 3x3 mean high-pass filter (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    # compute 3x3 mean using shifts\n    s = np.zeros_like(arr, dtype=float)\n    s += arr\n    s += np.roll(arr, 1, axis=0)\n    s += np.roll(arr, -1, axis=0)\n    s += np.roll(arr, 1, axis=1)\n    s += np.roll(arr, -1, axis=1)\n    s += np.roll(np.roll(arr, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(arr, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(arr, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(arr, -1, axis=0), -1, axis=1)\n    low = s / 9.0\n    high = arr - low\n    high_energy = float(np.sum(np.abs(high)))\n    total_energy = float(np.sum(np.abs(arr))) + eps\n    result = high_energy / total_energy\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Corner uniformity: 1 - (mean corner std / overall std), clipped (1=center-uniform)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    ch = max(1, h // 8)\n    cw = max(1, w // 8)\n    c1 = arr[:ch, :cw]\n    c2 = arr[:ch, -cw:]\n    c3 = arr[-ch:, :cw]\n    c4 = arr[-ch:, -cw:]\n    corners = [c for c in (c1, c2, c3, c4) if c.size]\n    if not corners:\n        return 0.0\n    corner_stds = [float(np.std(c)) for c in corners]\n    mean_corner_std = float(np.mean(corner_stds))\n    overall_std = float(np.std(arr))\n    if overall_std <= eps:\n        return 1.0\n    result = 1.0 - (mean_corner_std / (overall_std + eps))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted spatial elongation (0=round/square, 1=highly elongated)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    weight = arr\n    total = float(weight.sum())\n    if total <= eps:\n        return 0.0\n    mean_y = float((weight * ys).sum() / (total + eps))\n    mean_x = float((weight * xs).sum() / (total + eps))\n    dy = ys - mean_y\n    dx = xs - mean_x\n    cov_yy = float((weight * (dy ** 2)).sum() / (total + eps))\n    cov_xx = float((weight * (dx ** 2)).sum() / (total + eps))\n    cov_xy = float((weight * (dx * dy)).sum() / (total + eps))\n    cov = np.array([[cov_xx, cov_xy], [cov_xy, cov_yy]])\n    try:\n        eig = np.linalg.eigvals(cov)\n    except Exception:\n        return 0.0\n    eig = np.real(eig)\n    if eig.size < 2:\n        return 0.0\n    max_e = float(np.max(eig))\n    min_e = float(np.min(eig))\n    if max_e <= eps:\n        return 0.0\n    ratio = min_e / (max_e + eps)\n    elongation = 1.0 - ratio\n    return float(np.clip(elongation, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center-minus-surround contrast: (center_mean - surround_mean) / image std'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    ch0 = h // 4\n    ch1 = 3 * h // 4\n    cw0 = w // 4\n    cw1 = 3 * w // 4\n    if ch1 <= ch0 or cw1 <= cw0:\n        return 0.0\n    center = arr[ch0:ch1, cw0:cw1]\n    if center.size == 0:\n        return 0.0\n    surround_mask = np.ones_like(arr, dtype=bool)\n    surround_mask[ch0:ch1, cw0:cw1] = False\n    if not np.any(surround_mask):\n        return 0.0\n    center_mean = float(np.mean(center))\n    surround_mean = float(np.mean(arr[surround_mask]))\n    overall_std = float(np.std(arr)) + eps\n    result = (center_mean - surround_mean) / overall_std\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels significantly brighter than image mean (sparse bright regions)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    mean = float(np.mean(arr))\n    std = float(np.std(arr)) + eps\n    thr = mean + 0.75 * std\n    cnt = float(np.count_nonzero(arr > thr))\n    total = float(arr.size) + eps\n    result = cnt / total\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio (width/height) of bounding box of thresholded bright region (clip 0..10)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    mean = arr.mean()\n    std = arr.std()\n    thr = mean + 0.5 * std\n    mask = arr > thr\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.where(mask)\n    miny, maxy = ys.min(), ys.max()\n    minx, maxx = xs.min(), xs.max()\n    bw = float(maxx - minx + 1)\n    bh = float(maxy - miny + 1) + eps\n    ratio = bw / bh\n    ratio = float(np.clip(ratio, 0.0, 10.0))\n    return float(ratio)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean normalized radial distance of pixels above median (0=centered, 1=at periphery)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    med = float(np.median(arr))\n    mask = arr > med\n    if not np.any(mask):\n        return 0.0\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    dist = np.hypot(ys - cy, xs - cx)\n    maxd = float(dist.max()) + eps\n    mean_dist = float(np.mean(dist[mask]))\n    result = mean_dist / maxd\n    result = float(np.clip(result, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized imbalance between brightest and darkest quadrant means (0..inf, clipped)'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    hy, wx = h // 2, w // 2\n    if hy < 1 or wx < 1:\n        return 0.0\n    q1 = arr[:hy, :wx].mean() if arr[:hy, :wx].size else 0.0\n    q2 = arr[:hy, wx:].mean() if arr[:hy, wx:].size else 0.0\n    q3 = arr[hy:, :wx].mean() if arr[hy:, :wx].size else 0.0\n    q4 = arr[hy:, wx:].mean() if arr[hy:, wx:].size else 0.0\n    means = np.array([q1, q2, q3, q4], dtype=float)\n    overall_std = float(np.std(arr)) + eps\n    imbalance = (means.max() - means.min()) / overall_std\n    result = float(max(0.0, imbalance))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average block standard deviation (coarseness) normalized by overall std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    # choose block size proportional to image size\n    bs_y = max(1, h // 8)\n    bs_x = max(1, w // 8)\n    bs_y = min(bs_y, h)\n    bs_x = min(bs_x, w)\n    by = (h + bs_y - 1) // bs_y\n    bx = (w + bs_x - 1) // bs_x\n    block_stds = []\n    for i in range(by):\n        for j in range(bx):\n            y0 = i * bs_y\n            x0 = j * bs_x\n            y1 = min(h, y0 + bs_y)\n            x1 = min(w, x0 + bs_x)\n            block = arr[y0:y1, x0:x1]\n            if block.size:\n                block_stds.append(float(block.std()))\n    if len(block_stds) == 0:\n        return 0.0\n    mean_block_std = float(np.mean(block_stds))\n    overall_std = float(np.std(arr)) + eps\n    result = mean_block_std / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio using 3x3 mean blur (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    # small images: if too small, just return 0\n    h, w = arr.shape\n    if h < 1 or w < 1:\n        return 0.0\n    pad = np.pad(arr, 1, mode='reflect')\n    # compute 3x3 mean efficiently\n    s = (pad[0:-2, 0:-2] + pad[0:-2, 1:-1] + pad[0:-2, 2:] +\n         pad[1:-1, 0:-2] + pad[1:-1, 1:-1] + pad[1:-1, 2:] +\n         pad[2:, 0:-2] + pad[2:, 1:-1] + pad[2:, 2:])\n    blur = s / 9.0\n    high = arr - blur\n    high_energy = float(np.sum(np.abs(high)))\n    total_energy = float(np.sum(np.abs(arr))) + eps\n    result = high_energy / (total_energy + eps)\n    result = float(np.clip(result, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dark-spot prominence: (global mean - mean around darkest pixel) / overall std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    min_idx = np.argmin(arr.ravel())\n    iy, ix = divmod(int(min_idx), w)\n    # neighborhood 7x7\n    r = max(1, min(h, w) // 10)\n    y0 = max(0, iy - r)\n    y1 = min(h, iy + r + 1)\n    x0 = max(0, ix - r)\n    x1 = min(w, ix + r + 1)\n    local = arr[y0:y1, x0:x1]\n    if local.size == 0:\n        return 0.0\n    global_mean = float(np.mean(arr))\n    local_mean = float(np.mean(local))\n    overall_std = float(np.std(arr)) + eps\n    result = (global_mean - local_mean) / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity histogram entropy (8 bins), normalized to [0,1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(image.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(image.astype(float))\n    flat = arr.ravel()\n    if flat.size == 0:\n        return 0.0\n    # normalize to data range\n    mn, mx = float(flat.min()), float(flat.max())\n    if mx <= mn + eps:\n        return 0.0\n    bins = 8\n    hist, _ = np.histogram(flat, bins=bins, range=(mn, mx))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p_nonzero = p[p > 0]\n    if p_nonzero.size == 0:\n        return 0.0\n    entropy = -np.sum(p_nonzero * np.log2(p_nonzero))\n    max_entropy = np.log2(bins)\n    result = float(entropy / (max_entropy + eps))\n    result = float(np.clip(result, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted centroid offset from image center normalized by diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    weight = arr\n    total = float(weight.sum())\n    if total <= eps:\n        return 0.0\n    mean_y = float((weight * ys).sum() / (total + eps))\n    mean_x = float((weight * xs).sum() / (total + eps))\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    dist = np.hypot(mean_y - cy, mean_x - cx)\n    diag = np.hypot(h, w) / 2.0 + eps\n    result = dist / diag\n    result = float(np.clip(result, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of strict local peaks (pixel > all 8 neighbors), robust to borders'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.astype(float).mean(axis=2))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 1 or w < 1:\n        return 0.0\n    pad = np.pad(arr, 1, mode='edge')\n    center = pad[1:-1, 1:-1]\n    neighbors = [\n        pad[0:-2, 0:-2], pad[0:-2, 1:-1], pad[0:-2, 2:],\n        pad[1:-1, 0:-2],               pad[1:-1, 2:],\n        pad[2:, 0:-2], pad[2:, 1:-1], pad[2:, 2:]\n    ]\n    comp = np.ones_like(center, dtype=bool)\n    for nb in neighbors:\n        comp &= (center > nb)\n    total = float(center.size) + eps\n    peaks = float(np.count_nonzero(comp))\n    result = peaks / total\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio: fraction of FFT magnitude in outer half of radial frequencies'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    # compute centered FFT magnitude\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n    except Exception:\n        return 0.0\n    mag = np.abs(F)\n    ys = np.arange(h)[:, None] - (h // 2)\n    xs = np.arange(w)[None, :] - (w // 2)\n    dist = np.hypot(ys, xs)\n    maxd = float(dist.max()) if dist.size else 1.0\n    if maxd <= 0:\n        return 0.0\n    # outer half frequencies\n    mask_outer = dist >= (0.5 * maxd)\n    total = float(mag.sum()) + 1e-12\n    outer = float(mag[mask_outer].sum())\n    result = outer / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-spot proportion: fraction of pixels brighter than mean+std (captures sparse highlights)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    a = np.nan_to_num(arr.astype(float))\n    n = a.size\n    if n == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thresh = m + s\n    count = float((a > thresh).sum())\n    result = count / float(n)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Local peak density: count of strict local maxima (8-neighbour) normalized by image area'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # pad with very small value so edges can be maxima\n    pad = -1e12\n    A = np.pad(a, 1, mode='constant', constant_values=pad)\n    center = A[1:-1, 1:-1]\n    neighs = [\n        A[0:-2, 0:-2], A[0:-2, 1:-1], A[0:-2, 2:],\n        A[1:-1, 0:-2],               A[1:-1, 2:],\n        A[2:, 0:-2],   A[2:, 1:-1],   A[2:, 2:]\n    ]\n    mask = np.ones_like(center, dtype=bool)\n    for nbh in neighs:\n        mask &= (center > nbh)\n    peaks = int(mask.sum())\n    area = float(h * w)\n    result = peaks / (area + 1e-12)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Strong-edge density: fraction of pixels with gradient magnitude > mean+std'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    m = float(mag.mean())\n    s = float(mag.std())\n    thresh = m + s\n    strong = float((mag > thresh).sum())\n    total = float(mag.size)\n    result = strong / (total + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center-vs-border contrast normalized by overall std (positive => center brighter)'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    ch0, ch1 = h // 4, 3 * h // 4\n    cw0, cw1 = w // 4, 3 * w // 4\n    if ch1 <= ch0 or cw1 <= cw0:\n        return 0.0\n    center = a[ch0:ch1, cw0:cw1]\n    if center.size == 0:\n        return 0.0\n    border_mask = np.ones_like(a, dtype=bool)\n    border_mask[ch0:ch1, cw0:cw1] = False\n    border = a[border_mask]\n    if border.size == 0:\n        return 0.0\n    center_mean = float(center.mean())\n    border_mean = float(border.mean())\n    overall_std = float(a.std()) + eps\n    result = (center_mean - border_mean) / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Coarse-to-fine gradient ratio (mean coarse gradient / mean fine gradient) -> >1 coarser texture'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy_f, gx_f = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag_f = np.hypot(gx_f, gy_f)\n    # coarse downsample by factor 2 (floor)\n    a_c = a[::2, ::2]\n    if a_c.size < 4:\n        # fallback: blur by simple local average (via slicing) then compute gradient\n        a_c = (a + np.roll(a, 1, axis=0) + np.roll(a, 1, axis=1) + np.roll(a, (1,1), axis=(0,1))) / 4.0\n    try:\n        gy_c, gx_c = np.gradient(a_c)\n    except Exception:\n        return 0.0\n    mag_c = np.hypot(gx_c, gy_c)\n    mean_f = float(mag_f.mean()) + eps\n    mean_c = float(mag_c.mean()) + eps\n    result = mean_c / mean_f\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Harris-like corner strength ratio: sum of positive corner response normalized by total gradient energy'\n    import numpy as np\n    eps = 1e-12\n    k = 0.04\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    A = gx * gx\n    B = gy * gy\n    C = gx * gy\n    # local sums using simple 3x3 box via convolution-like sum using slicing\n    def local_sum(X):\n        s = np.zeros_like(X)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s / 9.0\n    S_A = local_sum(A)\n    S_B = local_sum(B)\n    S_C = local_sum(C)\n    det = S_A * S_B - (S_C ** 2)\n    trace = S_A + S_B\n    R = det - k * (trace ** 2)\n    pos = R[R > 0].sum()\n    grad_energy = np.hypot(gx, gy).sum() + eps\n    result = pos / grad_energy\n    return float(max(0.0, result))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant orientation (aggregated) encoded in [-1..1] using sin(2*theta) from intensity PCA'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    weight = a\n    total = float(weight.sum())\n    if total <= eps:\n        return 0.0\n    mean_y = float((weight * ys).sum() / (total + eps))\n    mean_x = float((weight * xs).sum() / (total + eps))\n    dy = (ys - mean_y)\n    dx = (xs - mean_x)\n    cov_xx = float((weight * (dx ** 2)).sum() / (total + eps))\n    cov_yy = float((weight * (dy ** 2)).sum() / (total + eps))\n    cov_xy = float((weight * (dx * dy)).sum() / (total + eps))\n    cov = np.array([[cov_xx, cov_xy], [cov_xy, cov_yy]])\n    try:\n        vals, vecs = np.linalg.eigh(cov)\n    except Exception:\n        return 0.0\n    # largest eigenvector\n    idx = int(np.argmax(vals))\n    vx, vy = vecs[:, idx]\n    angle = float(np.arctan2(vy, vx))\n    # use sin(2*angle) to make orientation periodic by pi and map to [-1,1]\n    result = float(np.sin(2.0 * angle))\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness (Hasler-Susstrunk) metric normalized by 100 (0..~1+ for vivid images); 0 for pure grayscale'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim < 3 or arr.shape[2] < 3:\n        return 0.0\n    img = np.nan_to_num(arr.astype(float))\n    # use first three channels as R,G,B\n    R = img[:, :, 0]\n    G = img[:, :, 1]\n    B = img[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n    # normalize to a reasonable scale\n    result = colorfulness / (100.0 + 1e-12)\n    return float(max(0.0, result))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Rotational symmetry (180-degree): absolute normalized correlation with image rotated 180 degrees'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    a = np.nan_to_num(arr.astype(float))\n    b = np.rot90(a, 2)\n    if a.shape != b.shape:\n        # ensure same shape (should be same)\n        h = min(a.shape[0], b.shape[0])\n        w = min(a.shape[1], b.shape[1])\n        a = a[:h, :w]\n        b = b[:h, :w]\n    a_flat = a.ravel()\n    b_flat = b.ravel()\n    if a_flat.size == 0:\n        return 0.0\n    a_m = a_flat - a_flat.mean()\n    b_m = b_flat - b_flat.mean()\n    denom = np.sqrt((a_m**2).sum() * (b_m**2).sum()) + 1e-12\n    corr = float((a_m * b_m).sum() / denom)\n    return float(np.clip(abs(corr), 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry: normalized correlation between left and right halves (1 = symmetric)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = np.nan_to_num(img.mean(axis=2))\n    else:\n        img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = img[:, :mid]\n    right = img[:, w - mid:]\n    # flip right horizontally to align with left\n    right_flipped = np.fliplr(right)\n    a = left.ravel().astype(float)\n    b = right_flipped.ravel().astype(float)\n    if a.size == 0 or b.size == 0:\n        return 0.0\n    a = a - a.mean()\n    b = b - b.mean()\n    denom = np.sqrt((a * a).sum() * (b * b).sum()) + eps\n    corr = float((a * b).sum() / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio in 2D FFT (0..1), higher = more fine detail'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # compute centered magnitude spectrum\n    F = np.fft.fftshift(np.fft.fft2(arr))\n    mag2 = np.abs(F) ** 2\n    ys = np.arange(h) - (h // 2)\n    xs = np.arange(w) - (w // 2)\n    Y, X = np.meshgrid(ys, xs, indexing='ij')\n    dist = np.hypot(Y, X)\n    maxd = dist.max() if dist.size else 1.0\n    cutoff = 0.5 * maxd\n    high_energy = mag2[dist >= cutoff].sum()\n    total = mag2.sum() + eps\n    ratio = float(high_energy / total)\n    return float(np.clip(ratio, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean intensity of top 5% brightest pixels normalized by intensity range'\n    import numpy as np\n    eps = 1e-12\n    q = 95.0\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).ravel().astype(float))\n    else:\n        arr = np.nan_to_num(img.ravel().astype(float))\n    if arr.size == 0:\n        return 0.0\n    p = np.percentile(arr, q)\n    top = arr[arr >= p]\n    if top.size == 0:\n        return 0.0\n    mean_top = float(top.mean())\n    rng = float(arr.max() - arr.min()) + eps\n    result = (mean_top - float(arr.min())) / rng\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Foreground fraction using simple mean+std threshold (fraction of pixels above threshold)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    total = arr.size\n    if total == 0:\n        return 0.0\n    mu = float(arr.mean())\n    sd = float(arr.std())\n    thr = mu + sd\n    count = int(np.count_nonzero(arr > thr))\n    frac = float(count) / float(total)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0..1), 0 for grayscale inputs'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim < 3 or img.shape[2] < 3:\n        return 0.0\n    # use first three channels as R,G,B\n    arr = np.nan_to_num(img.astype(float))\n    R = arr[..., 0]\n    G = arr[..., 1]\n    B = arr[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    colorfulness = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    # normalize by typical 8-bit range if possible\n    vmax = float(max(arr.max(), 1.0))\n    result = colorfulness / (vmax + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local contrast (mean of 3x3 local std) normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    # local sums using 3x3 neighborhood by rolling (fast, avoids convolution lib)\n    def local_sum(X):\n        s = np.zeros_like(X, dtype=float)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s\n    s1 = local_sum(a)\n    s2 = local_sum(a * a)\n    local_mean = s1 / 9.0\n    local_var = (s2 / 9.0) - (local_mean ** 2)\n    local_var = np.maximum(local_var, 0.0)\n    local_std = np.sqrt(local_var)\n    mean_local_std = float(local_std.mean())\n    global_std = float(a.std()) + eps\n    result = mean_local_std / global_std\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Prominent peak count per 1000 pixels: local maxima above mean+0.5*std'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    mu = float(a.mean())\n    sd = float(a.std())\n    thr = mu + 0.5 * sd\n    # pad with very small values so edges are not treated as neighbors due to wrap\n    pad = np.full((h + 2, w + 2), -np.inf, dtype=float)\n    pad[1:-1, 1:-1] = a\n    center = pad[1:-1, 1:-1]\n    neighbors = [\n        pad[0:-2, 0:-2], pad[0:-2, 1:-1], pad[0:-2, 2:],\n        pad[1:-1, 0:-2],                 pad[1:-1, 2:],\n        pad[2:, 0:-2],   pad[2:, 1:-1],  pad[2:, 2:]\n    ]\n    greater = np.ones_like(center, dtype=bool)\n    for n in neighbors:\n        greater &= (center > n)\n    mask = greater & (center > thr)\n    count = int(np.count_nonzero(mask))\n    norm = (h * w) / 1000.0\n    result = float(count / (norm + 1e-12))\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Horizontal vs vertical edge dominance: (H - V) / (H + V) in [-1..1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(image.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(image.astype(float))\n    if arr.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    hor = float(np.sum(np.abs(gx)))\n    ver = float(np.sum(np.abs(gy)))\n    denom = hor + ver + eps\n    score = (hor - ver) / denom\n    return float(np.clip(score, -1.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border smoothness: std of outer border normalized by global std (low = smooth border)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    bw = max(1, min(h, w) // 8)\n    top = a[:bw, :]\n    bottom = a[-bw:, :]\n    left = a[:, :bw]\n    right = a[:, -bw:]\n    # combine border regions without duplicating corners excessively by concatenation\n    border = np.concatenate([top.ravel(), bottom.ravel(), left.ravel(), right.ravel()])\n    if border.size == 0:\n        return 0.0\n    border_std = float(np.std(border))\n    global_std = float(a.std()) + eps\n    result = border_std / global_std\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity histogram entropy (32 bins) normalized to [0..1]'\n    import numpy as np\n    eps = 1e-12\n    bins = 32\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).ravel().astype(float))\n    else:\n        arr = np.nan_to_num(img.ravel().astype(float))\n    if arr.size == 0:\n        return 0.0\n    vmin = float(arr.min())\n    vmax = float(arr.max())\n    if vmax <= vmin:\n        return 0.0\n    hist, _ = np.histogram(arr, bins=bins, range=(vmin, vmax))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p_nonzero = p[p > 0.0]\n    if p_nonzero.size == 0:\n        return 0.0\n    entropy = -np.sum(p_nonzero * np.log2(p_nonzero))\n    result = entropy / (np.log2(bins) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Pearson correlation between pixel intensity and radial distance from image center (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys = np.arange(h) - (h - 1) / 2.0\n    xs = np.arange(w) - (w - 1) / 2.0\n    yv = ys[:, None]\n    xv = xs[None, :]\n    dist = np.hypot(yv, xv)\n    dist = dist.ravel()\n    vals = a.ravel()\n    if vals.size < 2:\n        return 0.0\n    dist = (dist - dist.mean()) / (dist.std() + eps)\n    vals = (vals - vals.mean()) / (vals.std() + eps)\n    corr = float(np.sum(dist * vals) / (vals.size - 1 + eps))\n    # clip to [-1,1]\n    return float(max(-1.0, min(1.0, corr)))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels with high gradient magnitude (>90th percentile)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    if arr.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy).ravel()\n    if mag.size == 0:\n        return 0.0\n    thresh = np.percentile(mag, 90)\n    high_count = float(np.count_nonzero(mag > thresh))\n    frac = high_count / (mag.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score in [0..1] (1 = perfect symmetry)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if w < 2 or h == 0:\n        return 0.0\n    half = w // 2\n    left = a[:, :half]\n    right = a[:, -half:]\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    right_flipped = np.fliplr(right)\n    # if widths differ (odd w), crop the larger to match\n    if left.shape[1] != right_flipped.shape[1]:\n        m = min(left.shape[1], right_flipped.shape[1])\n        left = left[:, :m]\n        right_flipped = right_flipped[:, :m]\n    diff = np.abs(left - right_flipped)\n    norm = float(np.mean(np.abs(a))) + eps\n    score = 1.0 - (float(np.mean(diff)) / norm)\n    return float(np.clip(score, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast between top 10% brightest and bottom 10% darkest pixels normalized by overall std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.ravel(np.nan_to_num(img.astype(float)))\n    if a.size == 0:\n        return 0.0\n    p90 = np.percentile(a, 90)\n    p10 = np.percentile(a, 10)\n    top_mean = float(a[a >= p90].mean()) if np.any(a >= p90) else float(a.mean())\n    bot_mean = float(a[a <= p10].mean()) if np.any(a <= p10) else float(a.mean())\n    overall_std = float(a.std()) + eps\n    result = (top_mean - bot_mean) / overall_std\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio (>=1) of bounding box of very bright pixels (90th percentile); 0 if none'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    flat = arr.ravel()\n    if flat.size == 0:\n        return 0.0\n    thresh = float(np.percentile(flat, 90))\n    mask = arr > thresh\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.where(mask)\n    h, w = arr.shape\n    y0, y1 = ys.min(), ys.max()\n    x0, x1 = xs.min(), xs.max()\n    bh = float(y1 - y0 + 1)\n    bw = float(x1 - x0 + 1)\n    if bh <= 0 or bw <= 0:\n        return 0.0\n    ar = max(bw / bh, bh / bw)\n    # clip to avoid extreme values\n    return float(min(ar, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local 3x3 standard deviation normalized by global std (low = smooth texture)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute local mean and mean of squares via neighbor summation\n    s = np.zeros_like(a, dtype=float)\n    s_sq = np.zeros_like(a, dtype=float)\n    for X, S in ((a, s), (a*a, s_sq)):\n        S += X\n        S += np.roll(X, 1, axis=0)\n        S += np.roll(X, -1, axis=0)\n        S += np.roll(X, 1, axis=1)\n        S += np.roll(X, -1, axis=1)\n        S += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        S += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        S += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        S += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n    local_mean = s / 9.0\n    local_mean_sq = s_sq / 9.0\n    local_var = np.maximum(local_mean_sq - (local_mean**2), 0.0)\n    local_std = np.sqrt(local_var)\n    mean_local_std = float(np.mean(local_std))\n    global_std = float(a.std()) + eps\n    result = mean_local_std / global_std\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant gradient orientation concentration (0..1); 1 = all gradients aligned'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.size == 0 or mag.sum() <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    # map to [0, pi) since orientation 180deg == same\n    theta = np.mod(theta, np.pi)\n    bins = 16\n    hist, _ = np.histogram(theta.ravel(), bins=bins, range=(0.0, np.pi), weights=mag.ravel())\n    total = hist.sum() + eps\n    concentration = float(hist.max() / total)\n    return float(np.clip(concentration, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized offset of intensity centroid from geometric center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    total = float(a.sum()) + eps\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    cy = float((a * ys).sum()) / total\n    cx = float((a * xs).sum()) / total\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    diag = np.hypot(h, w) / 2.0 + eps\n    return float(np.clip(dist / diag, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of very dark pixels (<5th percentile)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    thresh = np.percentile(arr, 5)\n    frac = float(np.count_nonzero(arr < thresh)) / float(arr.size)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized count of strict local maxima (8-neighbor) that exceed mean+std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(image.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(image.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    center = a\n    neighs = []\n    for dy in (-1, 0, 1):\n        for dx in (-1, 0, 1):\n            if dy == 0 and dx == 0:\n                continue\n            neighs.append(np.roll(np.roll(a, dy, axis=0), dx, axis=1))\n    # strict greater than all neighbors\n    greater = np.ones_like(a, dtype=bool)\n    for n in neighs:\n        greater &= (center > n)\n    thr = center.mean() + center.std()\n    peaks = np.count_nonzero(greater & (center > thr))\n    total = float(a.size) + eps\n    return float(np.clip(peaks / total, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Centroid displacement: intensity-weighted center distance from image center normalized by half-diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if arr.size == 0:\n        return 0.0\n    total = arr.sum()\n    if total == 0:\n        return 0.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    cy = float((ys * arr).sum() / (total + eps))\n    cx = float((xs * arr).sum() / (total + eps))\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    half_diag = np.hypot(h, w) / 2.0 + eps\n    result = dist / half_diag\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry: 1 - normalized L1 difference between left half and mirrored right half (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if w < 2 or arr.size == 0:\n        return 0.0\n    mid = w // 2\n    if w % 2 == 0:\n        left = arr[:, :mid]\n        right = arr[:, mid:]\n    else:\n        left = arr[:, :mid]\n        right = arr[:, mid+1:]\n    # mirror right\n    right_m = np.fliplr(right)\n    minw = min(left.shape[1], right_m.shape[1])\n    if minw == 0:\n        return 0.0\n    left_c = left[:, :minw]\n    right_c = right_m[:, :minw]\n    diff = np.abs(left_c - right_c).mean()\n    norm = (np.abs(arr).mean() + eps)\n    score = 1.0 - (diff / norm)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio: mean absolute Laplacian divided by mean absolute intensity (higher -> more detail)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    # discrete Laplacian via roll (4-neighbor)\n    lap = (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) + np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a)\n    hf = float(np.mean(np.abs(lap)))\n    base = float(np.mean(np.abs(a))) + eps\n    result = hf / base\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) computed from histogram of intensities'\n    import numpy as np\n    eps = 1e-12\n    nbins = 64\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.ravel(np.nan_to_num(img.mean(axis=2).astype(float)))\n    else:\n        arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    amin, amax = arr.min(), arr.max()\n    if amax <= amin:\n        return 0.0\n    hist, _ = np.histogram(arr, bins=nbins, range=(amin, amax))\n    probs = hist.astype(float) / (hist.sum() + eps)\n    probs = probs[probs > 0]\n    ent = -np.sum(probs * np.log2(probs + eps))\n    max_ent = np.log2(nbins)\n    result = ent / (max_ent + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels near the minimum intensity (bottom 1% of range) measuring background sparsity (0..1)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.ravel(np.nan_to_num(img.mean(axis=2).astype(float)))\n    else:\n        arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    amin, amax = arr.min(), arr.max()\n    if amax <= amin:\n        # constant image: if constant is zero then all are near-min -> 1.0, else none\n        return 1.0 if amin == 0.0 else 0.0\n    thr = amin + 0.01 * (amax - amin)\n    count = float((arr <= thr).sum())\n    frac = count / float(arr.size)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Prominence of brightest pixels: mean of top 1% divided by mean of remaining pixels (>=1.0, higher -> more prominent)'\n    import numpy as np\n    eps = 1e-12\n    pct = 0.01\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        vals = np.ravel(np.nan_to_num(img.mean(axis=2).astype(float)))\n    else:\n        vals = np.ravel(np.nan_to_num(img.astype(float)))\n    n = vals.size\n    if n == 0:\n        return 0.0\n    k = max(1, int(np.ceil(pct * n)))\n    # use partition for efficiency\n    idx = np.argpartition(vals, -k)[-k:]\n    top = vals[idx]\n    top_mean = float(top.mean())\n    if n > k:\n        rest_mean = float((vals.sum() - top.sum()) / (n - k))\n    else:\n        rest_mean = top_mean\n    result = (top_mean + eps) / (rest_mean + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean normalized distance of top 0.5% brightest pixels to image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    pct = 0.005\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        vals = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        vals = np.nan_to_num(img.astype(float))\n    h, w = vals.shape\n    if vals.size == 0:\n        return 0.0\n    flat = vals.ravel()\n    n = flat.size\n    k = max(1, int(np.ceil(pct * n)))\n    idx = np.argpartition(flat, -k)[-k:]\n    ys, xs = np.unravel_index(idx, (h, w))\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dists = np.hypot(ys - center_y, xs - center_x)\n    mean_dist = float(dists.mean()) if dists.size else 0.0\n    half_diag = np.hypot(h, w) / 2.0 + eps\n    result = mean_dist / half_diag\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation consistency: magnitude of mean gradient unit vector weighted by magnitude (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total = float(mag.sum())\n    if total <= 0:\n        return 0.0\n    mean_x = float((gx * mag).sum() / (total + eps))\n    mean_y = float((gy * mag).sum() / (total + eps))\n    # normalize by magnitude scale: since gx*mag sum / total can exceed 1 in value scale,\n    # we convert to unit-vector measure by dividing by (mean magnitude) -> use total_mag_norm\n    mean_vec_mag = np.hypot(mean_x, mean_y)\n    # max possible is limited; clip to [0,1]\n    return float(np.clip(mean_vec_mag / (np.max(mag) + eps), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized dynamic range contrast: (max - min) / (std + eps)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.ravel(np.nan_to_num(img.mean(axis=2).astype(float)))\n    else:\n        arr = np.ravel(np.nan_to_num(img.astype(float)))\n    if arr.size == 0:\n        return 0.0\n    rng = float(arr.max() - arr.min())\n    s = float(arr.std())\n    result = rng / (s + eps)\n    # keep value bounded to avoid extreme outliers\n    return float(np.clip(result, 0.0, 100.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of rows whose mean exceeds overall mean+std (captures horizontal strokes) (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h = a.shape[0] if a.size else 0\n    if h == 0:\n        return 0.0\n    row_means = np.mean(a, axis=1)\n    overall_mean = float(a.mean())\n    overall_std = float(a.std())\n    thr = overall_mean + overall_std\n    count = float((row_means > thr).sum())\n    result = count / float(h + eps)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1), measures complexity of brightness distribution'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        # convert to luminance-like by averaging channels\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    a_flat = a.ravel()\n    # choose a modest number of bins for efficiency\n    bins = 32\n    if a_flat.size == 0:\n        return 0.0\n    lo = float(a_flat.min())\n    hi = float(a_flat.max())\n    if hi <= lo:\n        return 0.0\n    hist, _ = np.histogram(a_flat, bins=bins, range=(lo, hi), density=False)\n    probs = hist.astype(float) / (hist.sum() + 1e-12)\n    probs = probs[probs > 0]\n    if probs.size == 0:\n        return 0.0\n    entropy = -np.sum(probs * np.log(probs + 1e-12))\n    # normalize by log(bins) to keep in [0,1]\n    result = entropy / (np.log(bins) + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of image classified as foreground by a simple Otsu threshold'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    flat = img.ravel()\n    if flat.size == 0:\n        return 0.0\n    mn = float(flat.min())\n    mx = float(flat.max())\n    if mx <= mn:\n        return 0.0\n    # scale to 0..255 for histogram stability\n    scaled = np.floor((flat - mn) / (mx - mn) * 255.0).astype(int)\n    hist = np.bincount(scaled, minlength=256).astype(float)\n    total = hist.sum()\n    if total <= 0:\n        return 0.0\n    cum = np.cumsum(hist)\n    cum_mean = np.cumsum(hist * np.arange(256))\n    global_mean = cum_mean[-1] / (total + 1e-12)\n    # between-class variance for each threshold\n    var_between = ((global_mean * cum - cum_mean) ** 2) / (cum * (total - cum) + 1e-12)\n    thresh_idx = int(np.nanargmax(var_between))\n    thresh_val = mn + (thresh_idx / 255.0) * (mx - mn)\n    mask = img > thresh_val\n    frac = float(np.count_nonzero(mask)) / float(img.size + 1e-12)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Rotation-180 similarity: normalized correlation with the image rotated 180 degrees (-1..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    try:\n        rot = np.rot90(a, 2)\n    except Exception:\n        return 0.0\n    if a.shape != rot.shape:\n        rot = np.resize(rot, a.shape)\n    a_mean = a - a.mean()\n    r_mean = rot - rot.mean()\n    num = float((a_mean * r_mean).sum())\n    denom = float(np.sqrt((a_mean ** 2).sum() * (r_mean ** 2).sum()) + 1e-12)\n    result = num / denom\n    # clamp to [-1,1]\n    return float(max(-1.0, min(1.0, result)))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density in a central annulus (fraction of annulus pixels that are edge)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    # threshold edges adaptively\n    thr = mag.mean() + 0.5 * mag.std()\n    # radial annulus: between 0.2*min_dim and 0.5*min_dim\n    ys = np.arange(h)[:, None] - (h / 2.0)\n    xs = np.arange(w)[None, :] - (w / 2.0)\n    dist = np.hypot(ys, xs)\n    min_dim = float(min(h, w))\n    r0 = 0.2 * min_dim\n    r1 = 0.5 * min_dim\n    mask = (dist >= r0) & (dist <= r1)\n    if not np.any(mask):\n        return 0.0\n    edge_frac = float(np.count_nonzero((mag > thr) & mask)) / (float(np.count_nonzero(mask)) + 1e-12)\n    return float(np.clip(edge_frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local variance (3x3) normalized by global variance (higher = coarser texture)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # compute local mean and mean of squares using 3x3 neighborhood via roll trick\n    S = np.zeros_like(a)\n    S2 = np.zeros_like(a)\n    shifts = [(0,0),(1,0),(-1,0),(0,1),(0,-1),(1,1),(1,-1),(-1,1),(-1,-1)]\n    for dy, dx in shifts:\n        S += np.roll(np.roll(a, dy, axis=0), dx, axis=1)\n        S2 += np.roll(np.roll(a*a, dy, axis=0), dx, axis=1)\n    S /= 9.0\n    S2 /= 9.0\n    local_var = S2 - S*S\n    # avoid negative numerical artifacts\n    local_var = np.maximum(local_var, 0.0)\n    mean_local_var = float(local_var.mean())\n    global_var = float(a.var()) + 1e-12\n    result = mean_local_var / global_var\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio of nonzero/binary bounding box (width/height), returns 0.0 if undefined'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    # threshold at mean to find content\n    thr = a.mean()\n    mask = a > thr\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.where(mask)\n    minr, maxr = int(ys.min()), int(ys.max())\n    minc, maxc = int(xs.min()), int(xs.max())\n    height = maxr - minr + 1\n    width = maxc - minc + 1\n    if height <= 0:\n        return 0.0\n    result = float(width) / float(height)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0 for grayscale), higher = more colorful'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    # use first three channels as R,G,B (handle other channel orders by assumption)\n    rgb = np.nan_to_num(arr[..., :3].astype(float))\n    R = rgb[..., 0].ravel()\n    G = rgb[..., 1].ravel()\n    B = rgb[..., 2].ravel()\n    # Hasler & Suesstrunk metric\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = rg.std() if rg.size else 0.0\n    std_yb = yb.std() if yb.size else 0.0\n    mean_rg = rg.mean() if rg.size else 0.0\n    mean_yb = yb.mean() if yb.size else 0.0\n    result = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n    # normalize mildly by image size to avoid huge numbers, but keep meaningful scale\n    norm = float(max(1.0, np.sqrt(rgb.shape[0] * rgb.shape[1])))\n    return float(result / norm)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientation histogram (normalized 0..1), low => dominant orientation'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= 0:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    # bin orientations into 16 bins (fold sign since orientation modulo pi is often relevant)\n    orient = np.abs(theta)  # 0..pi\n    bins = 16\n    hist, _ = np.histogram(orient.ravel(), bins=bins, range=(0.0, np.pi), weights=mag.ravel())\n    probs = hist.astype(float) / (hist.sum() + 1e-12)\n    probs = probs[probs > 0]\n    if probs.size == 0:\n        return 0.0\n    entropy = -np.sum(probs * np.log(probs + 1e-12))\n    result = entropy / (np.log(bins) + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Interdecile contrast: (90th - 10th percentile) normalized by median (robust contrast)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    p90 = float(np.percentile(flat, 90))\n    p10 = float(np.percentile(flat, 10))\n    med = float(np.median(flat))\n    denom = abs(med) + 1e-8\n    result = (p90 - p10) / denom\n    return float(max(0.0, result))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance of bright-pixel centroid to image center (0=center, 1=corner or undefined)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 1.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    flat = a.ravel()\n    if flat.size == 0:\n        return 1.0\n    # choose bright pixels as top 5% by value\n    thr = float(np.percentile(flat, 95))\n    mask = a >= thr\n    if not np.any(mask):\n        return 1.0\n    ys, xs = np.nonzero(mask)\n    cy = ys.mean()\n    cx = xs.mean()\n    # center coordinates\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dy = cy - center_y\n    dx = cx - center_x\n    dist = np.hypot(dy, dx)\n    # normalize by image diagonal / 2 (max distance from center to corner)\n    maxd = np.hypot(center_y, center_x) + 1e-12\n    result = float(np.clip(dist / maxd, 0.0, 1.0))\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    '180-degree rotational symmetry (1 = perfect symmetry, 0 = very different)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    try:\n        rotated = np.rot90(arr, 2)\n    except Exception:\n        return 0.0\n    denom = np.mean(np.abs(arr)) + eps\n    diff = np.mean(np.abs(arr - rotated)) / denom\n    result = 1.0 - float(np.clip(diff, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-centroid centeredness (1 = centroid at image center, 0 = far away)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    total = float(arr.sum()) + eps\n    # Use intensity-weighted centroid\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    cy = float((arr * ys).sum()) / total\n    cx = float((arr * xs).sum()) / total\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    max_dist = np.hypot(center_y, center_x) + eps\n    result = 1.0 - (dist / max_dist)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1), higher = more diverse intensities'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    # use fixed number of bins for stability\n    bins = 32\n    try:\n        hist, _ = np.histogram(arr.ravel(), bins=bins, density=True)\n    except Exception:\n        return 0.0\n    prob = hist.clip(min=0.0)\n    prob_sum = prob.sum()\n    if prob_sum <= 0:\n        return 0.0\n    prob = prob / (prob_sum + eps)\n    entropy = -np.sum(np.where(prob > 0, prob * np.log(prob), 0.0))\n    max_entropy = np.log(bins) + eps\n    result = float(entropy / max_entropy)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientations (0..1), higher = many directions'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    theta = np.arctan2(gy, gx).ravel()  # -pi..pi\n    bins = 36\n    hist, _ = np.histogram(theta, bins=bins, range=(-np.pi, np.pi), density=True)\n    prob = hist.clip(min=0.0)\n    prob_sum = prob.sum()\n    if prob_sum <= 0:\n        return 0.0\n    prob = prob / (prob_sum + eps)\n    ent = -np.sum(np.where(prob > 0, prob * np.log(prob), 0.0))\n    result = float(ent / (np.log(bins) + eps))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of dark pixels (intensity < mean - std), 0..1'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    mean = float(arr.mean())\n    std = float(arr.std())\n    thr = mean - std\n    if arr.size == 0:\n        return 0.0\n    count = float((arr.ravel() < thr).sum())\n    result = count / (arr.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Corner vs center contrast: mean(corners) - mean(center) normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    bw = max(1, min(h, w) // 6)\n    # four corners\n    tl = a[:bw, :bw]\n    tr = a[:bw, -bw:]\n    bl = a[-bw:, :bw]\n    br = a[-bw:, -bw:]\n    corners = np.concatenate([tl.ravel(), tr.ravel(), bl.ravel(), br.ravel()]) if a.size else np.array([])\n    ch = h // 4\n    cw = w // 4\n    center = a[ch:ch+max(1,h//2), cw:cw+max(1,w//2)]\n    if corners.size == 0 or center.size == 0:\n        return 0.0\n    corner_mean = float(corners.mean())\n    center_mean = float(center.mean())\n    global_std = float(a.std()) + eps\n    result = (corner_mean - center_mean) / global_std\n    # normalize into roughly -3..3, then scale to -1..1 via tanh-like clamp\n    result = float(np.tanh(result))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-frequency energy ratio from 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n        mag = np.abs(F)\n    except Exception:\n        return 0.0\n    total = float(mag.sum()) + eps\n    # central low-frequency square\n    half = max(1, min(h, w) // 8)\n    cy, cx = h // 2, w // 2\n    y0 = max(0, cy - half); y1 = min(h, cy + half + 1)\n    x0 = max(0, cx - half); x1 = min(w, cx + half + 1)\n    low = float(mag[y0:y1, x0:x1].sum())\n    result = low / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right mirror symmetry (1 = perfectly symmetric left-right)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:]\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    # mirror right to compare to left\n    right_mirror = np.fliplr(right)\n    # crop to same shape if odd width\n    min_w = min(left.shape[1], right_mirror.shape[1])\n    left_c = left[:, :min_w]\n    right_c = right_mirror[:, :min_w]\n    denom = np.mean(np.abs(a)) + eps\n    diff = np.mean(np.abs(left_c - right_c)) / denom\n    result = 1.0 - float(np.clip(diff, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of medium-large bright blobs normalized (0..1), capped at 1.0'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = img.mean() + 0.5 * img.std()\n    mask = img > thr\n    if not mask.any():\n        return 0.0\n    visited = np.zeros(mask.shape, dtype=bool)\n    min_area = max(1, int(h * w * 0.005))  # ~0.5% of image\n    max_area = max(1, int(h * w * 0.5))    # avoid counting almost-full\n    count = 0\n    # simple flood fill\n    for y in range(h):\n        for x in range(w):\n            if mask[y, x] and not visited[y, x]:\n                stack = [(y, x)]\n                visited[y, x] = True\n                area = 0\n                while stack:\n                    cy, cx = stack.pop()\n                    area += 1\n                    if cy > 0 and mask[cy-1, cx] and not visited[cy-1, cx]:\n                        visited[cy-1, cx] = True; stack.append((cy-1, cx))\n                    if cy+1 < h and mask[cy+1, cx] and not visited[cy+1, cx]:\n                        visited[cy+1, cx] = True; stack.append((cy+1, cx))\n                    if cx > 0 and mask[cy, cx-1] and not visited[cy, cx-1]:\n                        visited[cy, cx-1] = True; stack.append((cy, cx-1))\n                    if cx+1 < w and mask[cy, cx+1] and not visited[cy, cx+1]:\n                        visited[cy, cx+1] = True; stack.append((cy, cx+1))\n                if area >= min_area and area <= max_area:\n                    count += 1\n                # early stop if many\n                if count >= 20:\n                    break\n        if count >= 20:\n            break\n    result = float(min(count, 20) / 20.0)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variability of local patch means (std of patch means) normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    patch = max(2, min(h, w) // 8)\n    # non-overlapping patches\n    means = []\n    for y in range(0, h, patch):\n        for x in range(0, w, patch):\n            block = a[y:y+patch, x:x+patch]\n            if block.size:\n                means.append(float(block.mean()))\n    if len(means) == 0:\n        return 0.0\n    local_std = float(np.std(means))\n    global_std = float(a.std()) + eps\n    result = local_std / global_std\n    # normalize into 0..1 via a smooth clamp\n    result = float(np.tanh(result))\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial symmetry: variability of mean intensities across concentric annuli around image center (higher = more radial structure)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    ys = np.arange(h)[:, None] - cy\n    xs = np.arange(w)[None, :] - cx\n    dist = np.hypot(ys, xs)\n    maxr = float(np.max(dist)) if dist.size else 0.0\n    if maxr <= 0:\n        return 0.0\n    nbins = max(4, int(min(h, w) / 4))\n    edges = np.linspace(0.0, maxr + eps, nbins + 1)\n    ring_means = []\n    for i in range(nbins):\n        mask = (dist >= edges[i]) & (dist < edges[i + 1])\n        if mask.any():\n            ring_means.append(float(arr[mask].mean()))\n    if len(ring_means) < 2:\n        return 0.0\n    ring_means = np.array(ring_means)\n    result = float(ring_means.std() / (arr.std() + eps))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness index (0 for grayscale images)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    # assume last dim is channels in RGB order if available\n    R = a[..., 0]\n    G = a[..., 1]\n    B = a[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    rg_mean, rg_std = float(rg.mean()), float(rg.std())\n    yb_mean, yb_std = float(yb.mean()), float(yb.std())\n    # Hasler & Suesstrunk metric\n    std_root = np.sqrt(rg_std ** 2 + yb_std ** 2)\n    mean_root = np.sqrt(rg_mean ** 2 + yb_mean ** 2)\n    result = std_root + 0.3 * mean_root\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity centroid offset normalized by image diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    total = float(arr.sum())\n    if total <= eps:\n        return 0.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    cy = float((h - 1) / 2.0)\n    cx = float((w - 1) / 2.0)\n    y_cent = float((arr * ys).sum() / total)\n    x_cent = float((arr * xs).sum() / total)\n    dist = float(np.hypot(y_cent - cy, x_cent - cx))\n    diag = float(np.hypot(cy, cx)) + eps\n    result = dist / diag\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average mirror similarity (left-right and top-bottom) scaled to [0..1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # left-right\n    midc = w // 2\n    if w % 2 == 0:\n        left = a[:, :midc]\n        right = a[:, midc:]\n    else:\n        left = a[:, :midc]\n        right = a[:, midc + 1:]\n    if left.size and right.size and left.shape == right.shape:\n        left_flip = np.fliplr(left)\n        mad_lr = float(np.mean(np.abs(left_flip - right)))\n    else:\n        mad_lr = float(np.mean(np.abs(a - np.fliplr(a))))  # fallback\n    # top-bottom\n    midr = h // 2\n    if h % 2 == 0:\n        top = a[:midr, :]\n        bottom = a[midr:, :]\n    else:\n        top = a[:midr, :]\n        bottom = a[midr + 1:, :]\n    if top.size and bottom.size and top.shape == bottom.shape:\n        top_flip = np.flipud(top)\n        mad_tb = float(np.mean(np.abs(top_flip - bottom)))\n    else:\n        mad_tb = float(np.mean(np.abs(a - np.flipud(a))))  # fallback\n    mean_abs = float(np.mean(np.abs(a))) + eps\n    score_lr = max(0.0, 1.0 - (mad_lr / mean_abs))\n    score_tb = max(0.0, 1.0 - (mad_tb / mean_abs))\n    result = (score_lr + score_tb) / 2.0\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local contrast: average absolute difference to 3x3 neighborhood normalized by global mean intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    # compute 3x3 local sum via rolling shifts\n    s = np.zeros_like(a)\n    s += a\n    s += np.roll(a, 1, axis=0)\n    s += np.roll(a, -1, axis=0)\n    s += np.roll(a, 1, axis=1)\n    s += np.roll(a, -1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), -1, axis=1)\n    local_mean = s / 9.0\n    local_diff = np.abs(a - local_mean)\n    mean_local_diff = float(np.mean(local_diff))\n    global_mean_abs = float(np.mean(np.abs(a))) + eps\n    result = mean_local_diff / global_mean_abs\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast of brightest 5% pixels vs rest normalized by image std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    arr = a.ravel()\n    if arr.size == 0:\n        return 0.0\n    p95 = float(np.percentile(arr, 95))\n    top = arr[arr >= p95]\n    rest = arr[arr < p95]\n    if top.size == 0:\n        return 0.0\n    top_mean = float(top.mean())\n    rest_mean = float(rest.mean()) if rest.size else float(top_mean)\n    overall_std = float(arr.std()) + eps\n    result = (top_mean - rest_mean) / overall_std\n    if result < 0.0:\n        result = 0.0\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gini coefficient of absolute intensities (0 = equal, 1 = highly unequal)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        vals = np.ravel(np.nan_to_num(img.mean(axis=2).astype(float)))\n    else:\n        vals = np.ravel(np.nan_to_num(img.astype(float)))\n    vals = np.abs(vals)\n    if vals.size == 0:\n        return 0.0\n    total = float(vals.sum())\n    if total == 0.0:\n        return 0.0\n    sorted_v = np.sort(vals)\n    n = sorted_v.size\n    idx = np.arange(1, n + 1, dtype=float)\n    gini = (np.sum((2.0 * idx - n - 1.0) * sorted_v)) / (n * total)\n    result = float(np.clip(gini, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Relative concentration of strong edges in the center region (ratio >1 means center more edge-dense)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + mag.std())\n    edges = mag > thr\n    total_edges = float(edges.sum())\n    total_area = float(arr.size)\n    # center region (middle half)\n    ch0, ch1 = h // 4, 3 * h // 4\n    cw0, cw1 = w // 4, 3 * w // 4\n    center_mask = np.zeros_like(edges, dtype=bool)\n    center_mask[ch0:ch1, cw0:cw1] = True\n    center_edges = float(np.logical_and(edges, center_mask).sum())\n    center_area = float(center_mask.sum()) + eps\n    overall_edge_density = (total_edges / (total_area + eps)) + eps\n    center_density = center_edges / center_area\n    result = center_density / overall_edge_density\n    # clip to a reasonable bound\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientations weighted by magnitude (0..1)'\n    import numpy as np\n    eps = 1e-12\n    nbins = 16\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total_mag = float(mag.sum())\n    if total_mag <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    # map to [0, 2*pi)\n    theta = (theta + 2 * np.pi) % (2 * np.pi)\n    bins = np.linspace(0.0, 2 * np.pi, nbins + 1)\n    hist = np.zeros(nbins, dtype=float)\n    # bin weighted by magnitude\n    inds = np.minimum(np.searchsorted(bins, theta, side='right') - 1, nbins - 1)\n    # accumulate\n    for i in range(nbins):\n        hist[i] = float(mag[inds == i].sum())\n    probs = hist / (hist.sum() + eps)\n    probs = probs[probs > 0]\n    ent = -np.sum(probs * np.log2(probs + eps))\n    max_ent = np.log2(nbins)\n    result = float(ent / (max_ent + eps))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of strong local maxima (pixels greater than all 8 neighbors and above mean+0.5*std)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    thr = float(a.mean() + 0.5 * a.std())\n    # compare to 8 neighbors\n    center = a\n    n1 = np.roll(center, 1, axis=0)\n    n2 = np.roll(center, -1, axis=0)\n    n3 = np.roll(center, 1, axis=1)\n    n4 = np.roll(center, -1, axis=1)\n    n5 = np.roll(n1, 1, axis=1)\n    n6 = np.roll(n1, -1, axis=1)\n    n7 = np.roll(n2, 1, axis=1)\n    n8 = np.roll(n2, -1, axis=1)\n    mask = (center > n1) & (center > n2) & (center > n3) & (center > n4) & (center > n5) & (center > n6) & (center > n7) & (center > n8)\n    mask = mask & (center > thr)\n    count = float(mask.sum())\n    area = float(a.size) + eps\n    result = count / area\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Foreground-background contrast: (mean of top 10% - mean of bottom 10%) normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    flat = arr.ravel()\n    if flat.size == 0:\n        return 0.0\n    p90 = float(np.percentile(flat, 90))\n    p10 = float(np.percentile(flat, 10))\n    top_mean = float(flat[flat >= p90].mean()) if np.any(flat >= p90) else float(flat.mean())\n    bot_mean = float(flat[flat <= p10].mean()) if np.any(flat <= p10) else float(flat.mean())\n    global_std = float(arr.std()) + eps\n    result = (top_mean - bot_mean) / global_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with gradient magnitude above the 75th percentile'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy).ravel()\n    if mag.size == 0:\n        return 0.0\n    thresh = float(np.percentile(mag, 75))\n    frac = float(np.count_nonzero(mag > thresh)) / float(mag.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry score: 1 = perfectly symmetric left-right, 0 = very asymmetric'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    if w % 2 == 0:\n        left = a[:, :mid]\n        right = a[:, mid:]\n    else:\n        left = a[:, :mid]\n        right = a[:, mid+1:]\n    # mirror right\n    right_m = np.fliplr(right)\n    # align shapes\n    min_w = min(left.shape[1], right_m.shape[1])\n    if min_w == 0:\n        return 0.0\n    left_c = left[:, :min_w]\n    right_c = right_m[:, :min_w]\n    diff = np.abs(left_c - right_c)\n    denom = (np.mean(np.abs(a)) + eps)\n    score = 1.0 - float(np.mean(diff)) / denom\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local contrast: average of patch std-devs normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    patch = max(2, min(h, w) // 8)\n    local_stds = []\n    for y in range(0, h, patch):\n        for x in range(0, w, patch):\n            block = a[y:y+patch, x:x+patch]\n            if block.size:\n                local_stds.append(float(np.std(block)))\n    if len(local_stds) == 0:\n        return 0.0\n    mean_local = float(np.mean(local_stds))\n    global_std = float(a.std()) + eps\n    result = mean_local / global_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian energy ratio: sum(|Laplacian|) / (sum(|image|)+eps)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    lap = np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) + np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a\n    lap_energy = float(np.sum(np.abs(lap)))\n    total = float(np.sum(np.abs(a))) + eps\n    result = lap_energy / total\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant frequency radius: normalized distance of strongest FFT peak from DC (0..1)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(image.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(image.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        F = np.fft.fft2(a)\n    except Exception:\n        return 0.0\n    S = np.abs(np.fft.fftshift(F))\n    cy, cx = h // 2, w // 2\n    S[cy, cx] = 0.0  # remove DC\n    idx = np.argmax(S)\n    if S.size == 0 or S.flat[idx] == 0:\n        return 0.0\n    iy, ix = np.unravel_index(int(idx), S.shape)\n    dist = float(np.hypot(iy - cy, ix - cx))\n    maxd = float(np.hypot(cy, cx))\n    if maxd == 0:\n        return 0.0\n    result = dist / maxd\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border uniform fraction: fraction of border pixels within 0.5*std of median'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    bw = max(1, min(h, w) // 10)\n    top = a[:bw, :].ravel()\n    bottom = a[-bw:, :].ravel()\n    left = a[:, :bw].ravel()\n    right = a[:, -bw:].ravel()\n    border = np.concatenate([top, bottom, left, right]) if (top.size + bottom.size + left.size + right.size) > 0 else np.array([], dtype=float)\n    if border.size == 0:\n        return 0.0\n    med = float(np.median(border))\n    std = float(np.std(border)) + eps\n    thr = 0.5 * std\n    frac = float(np.count_nonzero(np.abs(border - med) <= thr)) / float(border.size)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright blobiness: normalized count of local maxima above 90th percentile (0..1)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    p90 = float(np.percentile(flat, 90))\n    mask = a > p90\n    if not np.any(mask):\n        return 0.0\n    # local maxima in 3x3: pixel > all 8 neighbors\n    center = a\n    neighbors = []\n    for dy in (-1, 0, 1):\n        for dx in (-1, 0, 1):\n            if dy == 0 and dx == 0:\n                continue\n            neighbors.append(np.roll(np.roll(a, dy, axis=0), dx, axis=1))\n    is_max = np.ones_like(a, dtype=bool)\n    for n in neighbors:\n        is_max &= (a > n)\n    peaks = is_max & mask\n    count = float(np.count_nonzero(peaks))\n    area = float(h * w)\n    result = count / (area + 1.0)  # avoid tiny division issues\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity distribution entropy (normalized 0..1) using 16 bins'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    flat = arr.ravel()\n    if flat.size == 0:\n        return 0.0\n    bins = 16\n    hist, _ = np.histogram(flat, bins=bins, range=(flat.min(), flat.max()), density=False)\n    total = float(hist.sum())\n    if total <= 0:\n        return 0.0\n    p = hist.astype(float) / total\n    p = p[p > 0]\n    import math\n    entropy = -float((p * np.log(p)).sum())\n    norm = math.log(bins) if bins > 1 else 1.0\n    result = entropy / (norm + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity moment elongation: ratio of major/minor eigenvalues of weighted covariance (>=1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        wimg = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        wimg = np.nan_to_num(img.astype(float))\n    h, w = wimg.shape\n    flat = wimg.ravel()\n    if flat.size == 0 or np.all(flat == 0):\n        return 0.0\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    weight = wimg\n    total = float(weight.sum())\n    if total <= eps:\n        return 0.0\n    mean_y = float((weight * ys).sum() / (total + eps))\n    mean_x = float((weight * xs).sum() / (total + eps))\n    dy = (ys - mean_y)\n    dx = (xs - mean_x)\n    cov_xx = float((weight * (dx ** 2)).sum() / (total + eps))\n    cov_yy = float((weight * (dy ** 2)).sum() / (total + eps))\n    cov_xy = float((weight * (dx * dy)).sum() / (total + eps))\n    cov = np.array([[cov_xx, cov_xy], [cov_xy, cov_yy]])\n    try:\n        vals = np.linalg.eigvalsh(cov)\n    except Exception:\n        return 0.0\n    vals = np.sort(vals)\n    small = max(vals[0], eps)\n    ratio = max(vals[-1] / small, 1.0)\n    # clamp to avoid extreme values\n    return float(min(ratio, 100.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels much brighter than the image average (bright-spot coverage)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    m = float(np.mean(arr))\n    s = float(np.std(arr)) + eps\n    thr = m + s\n    count = float(np.count_nonzero(arr > thr))\n    result = count / float(arr.size)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized center-of-mass offset from image center (0=centered, 1=corner)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    yy, xx = np.indices((h, w))\n    mass = arr.sum()\n    if mass <= eps:\n        return 0.0\n    cy = (yy * arr).sum() / mass\n    cx = (xx * arr).sum() / mass\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dy = cy - center_y\n    dx = cx - center_x\n    dist = np.hypot(dy, dx)\n    maxdist = np.hypot(center_y, center_x) + eps\n    result = float(np.clip(dist / maxdist, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge pixel density: proportion of pixels with strong gradient magnitude'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    if arr.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(np.mean(mag) + np.std(mag))\n    count = float(np.count_nonzero(mag > thr))\n    result = count / float(arr.size + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average radial alignment of gradients (1=pointing outward from center)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 1 or w < 1:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    yy, xx = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    vx = xx - cx\n    vy = yy - cy\n    vr = np.hypot(vx, vy)\n    gm = np.hypot(gx, gy)\n    valid = (gm > eps) & (vr > 0)\n    if not np.any(valid):\n        return 0.0\n    dot = gx[valid] * vx[valid] + gy[valid] * vy[valid]\n    denom = (gm[valid] * vr[valid]) + eps\n    cosval = dot / denom\n    # map from [-1,1] to [0,1] where 1 = outward, 0 = inward\n    mapped = (cosval + 1.0) / 2.0\n    result = float(np.mean(mapped))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio using discrete Laplacian'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    # 4-neighbor Laplacian\n    lap = (-4.0 * a +\n           np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) +\n           np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1))\n    hf = float(np.sum(np.abs(lap)))\n    base = float(np.sum(np.abs(a))) + eps\n    result = hf / base\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized aspect ratio w/(h+ w) in (0..1) (closer to 1 => wide)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        h, w = img.shape[0], img.shape[1]\n    else:\n        h, w = img.shape[0], img.shape[1]\n    if h <= 0 and w <= 0:\n        return 0.0\n    denom = float(h + w)\n    if denom <= 0:\n        return 0.0\n    result = float(w / denom)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average run-lengths per row (binary runs of foreground) normalized by width'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if w == 0:\n        return 0.0\n    thr = float(np.mean(a))\n    mask = a > thr\n    # count runs per row: True where current is True and previous is False\n    prev = np.zeros_like(mask, dtype=bool)\n    prev[:, 1:] = mask[:, :-1]\n    runs_start = mask & (~prev)\n    runs_per_row = runs_start.sum(axis=1).astype(float)\n    avg_runs = float(np.mean(runs_per_row)) if runs_per_row.size else 0.0\n    result = avg_runs / float(max(1, w))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels in the largest intensity histogram bin (background dominance)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        arr = np.nan_to_num(img.astype(float)).ravel()\n    if arr.size == 0:\n        return 0.0\n    mn = float(arr.min())\n    mx = float(arr.max())\n    if mx <= mn:\n        return 1.0\n    bins = 32\n    hist, _ = np.histogram(arr, bins=bins, range=(mn, mx))\n    maxcount = float(hist.max()) if hist.size else 0.0\n    result = maxcount / float(arr.size)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness score (Hasler & S\u00fcsstrunk) for RGB images, 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    # use first three channels as R,G,B\n    R = np.nan_to_num(img[..., 0].astype(float))\n    G = np.nan_to_num(img[..., 1].astype(float))\n    B = np.nan_to_num(img[..., 2].astype(float))\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.abs(np.mean(rg)))\n    mean_yb = float(np.abs(np.mean(yb)))\n    colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n    return float(colorfulness + eps)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Peakiness of gradient orientation histogram (max bin fraction, 0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    theta = np.arctan2(gy, gx).ravel()\n    bins = 36\n    hist, _ = np.histogram(theta, bins=bins, range=(-np.pi, np.pi))\n    total = float(hist.sum()) + eps\n    peak = float(hist.max()) if hist.size else 0.0\n    result = peak / total\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score (0..1) based on normalized correlation with mirror'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mirrored = a[:, ::-1]\n    a_mean = a.mean()\n    m_mean = mirrored.mean()\n    a_zero = a - a_mean\n    m_zero = mirrored - m_mean\n    num = float((a_zero * m_zero).sum())\n    den = float(np.sqrt((a_zero**2).sum() * (m_zero**2).sum())) + eps\n    corr = num / den\n    score = float(np.clip(abs(corr), 0.0, 1.0))\n    return float(score)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels with high gradient magnitude (above 90th percentile)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy).ravel()\n    if mag.size == 0:\n        return 0.0\n    thresh = float(np.percentile(mag, 90))\n    high = float((mag > thresh).sum())\n    frac = high / float(mag.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness (Hasler-Susstrunk) for RGB images normalized by mean intensity (0..inf)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    # use first three channels as R,G,B\n    rgb = np.nan_to_num(arr[:, :, :3].astype(float))\n    R = rgb[:, :, 0]\n    G = rgb[:, :, 1]\n    B = rgb[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(rg))\n    mean_yb = float(np.mean(yb))\n    colorfulness = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    mean_int = float(np.mean(rgb)) + eps\n    result = colorfulness / mean_int\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Local entropy proxy: mean log(1 + local 3x3 std) (larger => more local complexity)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # local mean and mean of squares via 3x3 rolling sum (edge handled by roll)\n    def local_mean(X):\n        s = np.zeros_like(X, dtype=float)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s / 9.0\n    mean1 = local_mean(a)\n    mean2 = local_mean(a * a)\n    var = mean2 - (mean1 ** 2)\n    var = np.maximum(var, 0.0)\n    local_std = np.sqrt(var)\n    proxy = np.log1p(local_std)\n    result = float(np.mean(proxy))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Row-wise intensity variation: mean(std across rows) normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    row_stds = np.std(a, axis=1)\n    mean_row_std = float(np.mean(row_stds)) if row_stds.size else 0.0\n    global_std = float(np.std(a)) + eps\n    result = mean_row_std / global_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of total intensity located in outer border band (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    bw = max(1, min(h, w) // 8)\n    top = a[:bw, :]\n    bottom = a[-bw:, :]\n    left = a[:, :bw]\n    right = a[:, -bw:]\n    border = np.concatenate([top.ravel(), bottom.ravel(), left.ravel(), right.ravel()])\n    total = float(a.sum()) + eps\n    border_sum = float(border.sum())\n    result = border_sum / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Peak density: proportion of pixels that are strict local maxima among 8-neighborhood'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    center = a\n    neighbors = []\n    neighbors.append(np.roll(center, 1, axis=0))\n    neighbors.append(np.roll(center, -1, axis=0))\n    neighbors.append(np.roll(center, 1, axis=1))\n    neighbors.append(np.roll(center, -1, axis=1))\n    neighbors.append(np.roll(np.roll(center, 1, axis=0), 1, axis=1))\n    neighbors.append(np.roll(np.roll(center, 1, axis=0), -1, axis=1))\n    neighbors.append(np.roll(np.roll(center, -1, axis=0), 1, axis=1))\n    neighbors.append(np.roll(np.roll(center, -1, axis=0), -1, axis=1))\n    comp = np.ones_like(center, dtype=bool)\n    for nb in neighbors:\n        comp &= (center > nb)\n    count = float(np.count_nonzero(comp))\n    area = float(center.size)\n    return float(np.clip(count / (area + 1e-12), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted bounding-box aspect ratio (min(width/height, height/width) in (0..1])'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    total = float(a.sum())\n    if total <= eps:\n        return 0.0\n    row_sums = a.sum(axis=1)\n    col_sums = a.sum(axis=0)\n    cumr = np.cumsum(row_sums)\n    cumc = np.cumsum(col_sums)\n    low_frac = 0.01\n    high_frac = 1.0 - low_frac\n    r0 = int(np.searchsorted(cumr, total * low_frac))\n    r1 = int(np.searchsorted(cumr, total * high_frac))\n    c0 = int(np.searchsorted(cumc, total * low_frac))\n    c1 = int(np.searchsorted(cumc, total * high_frac))\n    # clamp\n    r0 = max(0, min(r0, h - 1))\n    r1 = max(0, min(r1, h - 1))\n    c0 = max(0, min(c0, w - 1))\n    c1 = max(0, min(c1, w - 1))\n    height = max(1, r1 - r0 + 1)\n    width = max(1, c1 - c0 + 1)\n    ratio = min(width / float(height + eps), height / float(width + eps))\n    return float(np.clip(ratio, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of rows that are mostly zero (sparse horizontal stripes)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    zero_mask = (a == 0)\n    row_zero_frac = zero_mask.sum(axis=1) / float(w)\n    stripe_rows = float((row_zero_frac >= 0.9).sum())\n    result = stripe_rows / float(h)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian normalized by mean intensity (higher => more rapid intensity change)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    up = np.roll(a, 1, axis=0)\n    down = np.roll(a, -1, axis=0)\n    left = np.roll(a, 1, axis=1)\n    right = np.roll(a, -1, axis=1)\n    lap = (up + down + left + right) - 4.0 * a\n    mean_abs_lap = float(np.mean(np.abs(lap)))\n    mean_int = float(np.mean(np.abs(a))) + eps\n    result = mean_abs_lap / mean_int\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity center-of-mass offset magnitude (0..1) relative to image center'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    total = a.sum()\n    if total == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = (ys * a).sum() / (total + eps)\n    cx = (xs * a).sum() / (total + eps)\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = float(np.hypot(cy - center_y, cx - center_x))\n    maxd = float(np.hypot(center_y, center_x)) + eps\n    result = dist / maxd\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian-to-signal energy ratio (higher => more high-frequency second-derivative content)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n        dxx = np.gradient(gx, axis=1)\n        dyy = np.gradient(gy, axis=0)\n    except Exception:\n        return 0.0\n    lap = dxx + dyy\n    energy_lap = float(np.sum(np.abs(lap)))\n    energy_img = float(np.sum(np.abs(a))) + eps\n    ratio = energy_lap / energy_img\n    # compress to (0..1)\n    result = ratio / (1.0 + ratio)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels in the top 10% intensity percentile (bright-spot sparsity)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        flat = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        flat = np.nan_to_num(img.astype(float)).ravel()\n    if flat.size == 0:\n        return 0.0\n    thr = float(np.percentile(flat, 90))\n    count = float(np.count_nonzero(flat > thr))\n    result = count / float(flat.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity skewness normalized to [-1..1] using tanh compression'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        flat = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        flat = np.nan_to_num(img.astype(float)).ravel()\n    if flat.size == 0:\n        return 0.0\n    m = float(flat.mean())\n    s = float(flat.std()) + eps\n    skew = float(np.mean(((flat - m) / s) ** 3))\n    # compress to [-1,1]\n    result = float(np.tanh(skew / 5.0))\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Row-wise line prevalence: fraction of rows with a contiguous bright run (detects horizontal strokes)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = arr.mean() + arr.std()\n    k = max(1, w // 10)\n    rows_with_run = 0\n    for r in range(h):\n        row = arr[r, :] > thr\n        if not np.any(row):\n            continue\n        # compute max run length\n        dif = np.diff(np.concatenate(([0], row.view(np.int8), [0])))\n        starts = np.where(dif == 1)[0]\n        ends = np.where(dif == -1)[0]\n        if starts.size and ends.size:\n            maxrun = int((ends - starts).max())\n        else:\n            maxrun = 0\n        if maxrun >= k:\n            rows_with_run += 1\n    result = rows_with_run / float(h + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Column-wise vertical stroke density: fraction of columns with a contiguous strong vertical gradient'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 2 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.abs(gy)  # vertical gradient magnitude\n    thr = mag.mean() + mag.std()\n    k = max(1, h // 10)\n    cols_with_run = 0\n    for c in range(w):\n        col = mag[:, c] > thr\n        if not np.any(col):\n            continue\n        dif = np.diff(np.concatenate(([0], col.view(np.int8), [0])))\n        starts = np.where(dif == 1)[0]\n        ends = np.where(dif == -1)[0]\n        if starts.size and ends.size:\n            maxrun = int((ends - starts).max())\n        else:\n            maxrun = 0\n        if maxrun >= k:\n            cols_with_run += 1\n    result = cols_with_run / float(w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Global contrast via (P95 - P5) normalized by (|P95|+|P5|) to be robust to sign'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        flat = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        flat = np.nan_to_num(img.astype(float)).ravel()\n    if flat.size == 0:\n        return 0.0\n    p95 = float(np.percentile(flat, 95))\n    p5 = float(np.percentile(flat, 5))\n    num = p95 - p5\n    denom = abs(p95) + abs(p5) + eps\n    result = num / denom\n    # map to [0,1] by taking absolute fraction of spread relative to magnitude sum\n    result = abs(result)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Diagonal symmetry correlation (main diagonal) for a centered square crop (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    s = min(h, w)\n    y0 = (h - s) // 2\n    x0 = (w - s) // 2\n    sq = a[y0:y0 + s, x0:x0 + s]\n    if sq.size == 0:\n        return 0.0\n    A = sq.ravel()\n    B = sq.T.ravel()\n    A_mean = A.mean()\n    B_mean = B.mean()\n    num = np.sum((A - A_mean) * (B - B_mean))\n    den = np.sqrt(np.sum((A - A_mean) ** 2) * np.sum((B - B_mean) ** 2)) + eps\n    corr = num / den\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Texture coarseness: mean local standard deviation over blocks normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    bs = max(1, min(h, w) // 8)\n    local_stds = []\n    for y in range(0, h, bs):\n        for x in range(0, w, bs):\n            block = arr[y:y + bs, x:x + bs]\n            if block.size:\n                local_stds.append(float(block.std()))\n    if not local_stds:\n        return 0.0\n    mean_local = float(np.mean(local_stds))\n    global_std = float(arr.std()) + eps\n    result = mean_local / global_std\n    # coarseness >1 possible; compress to (0..1) via ratio/(1+ratio)\n    result = result / (1.0 + result)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted eccentricity (elongation) of the pixel distribution in [0..1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    weights = a.ravel()\n    total = weights.sum()\n    if total <= eps:\n        return 0.0\n    x = xs.ravel().astype(float)\n    y = ys.ravel().astype(float)\n    x_mean = (weights * x).sum() / (total + eps)\n    y_mean = (weights * y).sum() / (total + eps)\n    dx = x - x_mean\n    dy = y - y_mean\n    cov_xx = (weights * (dx * dx)).sum() / (total + eps)\n    cov_yy = (weights * (dy * dy)).sum() / (total + eps)\n    cov_xy = (weights * (dx * dy)).sum() / (total + eps)\n    trace = cov_xx + cov_yy\n    det = cov_xx * cov_yy - cov_xy * cov_xy\n    # eigenvalues of 2x2 covariance\n    tmp = np.sqrt(max(0.0, (trace * trace) / 4.0 - det))\n    l1 = trace / 2.0 + tmp\n    l2 = trace / 2.0 - tmp\n    denom = (l1 + l2) + eps\n    if denom <= eps:\n        return 0.0\n    ecc = (l1 - l2) / denom\n    return float(np.clip(ecc, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of image energy in high-frequency bands of the 2D FFT (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n        P = np.abs(F) ** 2\n    except Exception:\n        return 0.0\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    ys = np.arange(h) - cy\n    xs = np.arange(w) - cx\n    Y, X = np.meshgrid(ys, xs, indexing='ij')\n    R = np.hypot(Y, X)\n    maxr = R.max() if R.size else 1.0\n    if maxr <= 0:\n        return 0.0\n    # define high freq as outer 40% of radius\n    hf_mask = R >= (0.6 * maxr)\n    total = float(P.sum()) + 1e-12\n    hf = float(P[hf_mask].sum())\n    return float(np.clip(hf / total, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Linear slope of mean intensity vs normalized radius from center (negative => center bright)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    ys = np.arange(h) - cy\n    xs = np.arange(w) - cx\n    Y, X = np.meshgrid(ys, xs, indexing='ij')\n    R = np.hypot(Y, X)\n    Rn = R / (R.max() + 1e-12)\n    # bin radii into 12 rings\n    bins = 12\n    inds = (Rn * (bins - 1)).astype(int).ravel()\n    vals = a.ravel()\n    ring_means = []\n    ring_centers = []\n    for i in range(bins):\n        mask = inds == i\n        if not np.any(mask):\n            # skip empty rings\n            continue\n        ring_means.append(float(vals[mask].mean()))\n        ring_centers.append((i / float(bins - 1)))\n    if len(ring_means) < 2:\n        return 0.0\n    xs = np.array(ring_centers)\n    ys = np.array(ring_means)\n    # simple robust slope via covariance\n    xm = xs.mean(); ym = ys.mean()\n    denom = np.sum((xs - xm) ** 2) + 1e-12\n    slope = np.sum((xs - xm) * (ys - ym)) / denom\n    # normalize slope by image std so comparable across ranges\n    norm = np.std(a) + 1e-12\n    result = -slope / norm  # negative slope => intensity decreases with radius => center bright -> positive result\n    # compress to reasonable range\n    return float(np.tanh(result))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Absolute log aspect ratio compressed to [0,1] (0 => square)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    try:\n        h, w = arr.shape[:2]\n    except Exception:\n        return 0.0\n    if h <= 0 or w <= 0:\n        return 0.0\n    val = abs(np.log(float(h) / float(w)))\n    # compress via tanh and scale to [0,1]\n    return float(np.tanh(val))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Skewness of intensity distribution (tanh-compressed, -1..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    n = a.size\n    if n < 3:\n        return 0.0\n    mu = float(a.mean())\n    sd = float(a.std())\n    if sd <= 0:\n        return 0.0\n    skew = float(((a - mu) ** 3).mean()) / (sd ** 3)\n    return float(np.tanh(skew / 3.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Shannon entropy of intensity histogram normalized to [0,1]'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    bins = 32\n    try:\n        hist, _ = np.histogram(flat, bins=bins, range=(flat.min(), flat.max()))\n    except Exception:\n        hist, _ = np.histogram(flat, bins=bins)\n    p = hist.astype(float)\n    s = p.sum()\n    if s <= 0:\n        return 0.0\n    p = p / s\n    p = p[p > 0]\n    ent = -np.sum(p * np.log2(p))\n    max_ent = np.log2(bins) if bins > 1 else 1.0\n    return float(np.clip(ent / (max_ent + 1e-12), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation coherence (0..1): how aligned are edge directions'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total = mag.sum() + eps\n    # unit vectors for angles weighted by magnitude\n    ux = (gx * (mag > 0)) / (mag + eps)\n    uy = (gy * (mag > 0)) / (mag + eps)\n    vx = (ux * mag).sum() / total\n    vy = (uy * mag).sum() / total\n    coherence = np.hypot(vx, vy)  # between 0 and 1\n    return float(np.clip(coherence, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0..1), 0 for grayscale'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    img = np.nan_to_num(arr.astype(float))\n    R = img[..., 0]\n    G = img[..., 1]\n    B = img[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(rg))\n    mean_yb = float(np.mean(yb))\n    # Hasenfratz colorfulness measure\n    cf = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    # normalize by typical dynamic range\n    denom = (np.iinfo(img.dtype).max if np.issubdtype(img.dtype, np.integer) else 255.0) if hasattr(img, 'dtype') else 255.0\n    norm = cf / (denom + 1e-12)\n    return float(np.clip(np.tanh(norm * 2.0), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels in the center region that are nonzero relative to whole image nonzero fraction (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    total_nz = int(np.count_nonzero(a))\n    if total_nz == 0:\n        return 0.0\n    ch0 = h // 4\n    ch1 = 3 * h // 4\n    cw0 = w // 4\n    cw1 = 3 * w // 4\n    center = a[ch0:ch1, cw0:cw1]\n    if center.size == 0:\n        return 0.0\n    center_nz = int(np.count_nonzero(center))\n    return float(np.clip(center_nz / float(total_nz), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant color/pixel value fraction (largest unique-value cluster) (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    # For color, quantize slightly to reduce unique counts\n    if arr.ndim == 3 and arr.shape[2] >= 3:\n        img = np.nan_to_num(arr.astype(float))\n        # reduce precision to 6 bits per channel to cluster similar colors\n        q = np.right_shift(img.astype(np.int64), 2)\n        flat = q.reshape(-1, q.shape[-1])\n        try:\n            # view rows as structured for unique\n            dtype = np.dtype(('i8', flat.shape[1]))\n            view = flat.view(dtype)\n            vals, counts = np.unique(view, return_counts=True)\n            maxc = counts.max() if counts.size else 0\n            total = flat.shape[0]\n            return float(np.clip(maxc / float(total + 1e-12), 0.0, 1.0))\n        except Exception:\n            flat1 = flat.reshape(-1, flat.shape[-1])\n            total = flat1.shape[0]\n            return float(1.0 if total > 0 and np.all(flat1[0] == flat1).all() else 0.0)\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n        if a.size == 0:\n            return 0.0\n        # quantize to 64 levels\n        mn, mx = a.min(), a.max()\n        if mx == mn:\n            return 1.0\n        bins = np.linspace(mn, mx, num=65)\n        inds = np.digitize(a, bins)\n        vals, counts = np.unique(inds, return_counts=True)\n        maxc = counts.max() if counts.size else 0\n        total = a.size\n        return float(np.clip(maxc / float(total + 1e-12), 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with strong gradient magnitude'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(np.mean(mag) + np.std(mag))\n    count = float(np.count_nonzero(mag > thr))\n    denom = float(mag.size) if mag.size else 1.0\n    result = count / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial symmetry similarity: how similar the image is to its flips/180-rotation (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    if arr.size == 0:\n        return 0.0\n    rng = float(arr.max() - arr.min()) + 1e-12\n    def sim(a, b):\n        return 1.0 - (np.mean(np.abs(a - b)) / rng)\n    try:\n        sim_h = sim(arr, np.flipud(arr))\n        sim_v = sim(arr, np.fliplr(arr))\n        sim_rot = sim(arr, np.rot90(arr, 2))\n    except Exception:\n        return 0.0\n    result = max(sim_h, sim_v, sim_rot)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-spot density: fraction of pixels that are strict local maxima above the 95th percentile'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = np.percentile(arr, 95)\n    # pad with -inf so edges can be maxima\n    pad = np.full((h + 2, w + 2), -np.inf, dtype=float)\n    pad[1:-1, 1:-1] = arr\n    center = pad[1:-1, 1:-1]\n    neighs = []\n    neighs.append(pad[:-2, :-2]); neighs.append(pad[:-2, 1:-1]); neighs.append(pad[:-2, 2:])\n    neighs.append(pad[1:-1, :-2]); neighs.append(pad[1:-1, 2:])\n    neighs.append(pad[2:, :-2]); neighs.append(pad[2:, 1:-1]); neighs.append(pad[2:, 2:])\n    greater = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        greater &= (center > n)\n    mask = (center > thr) & greater\n    count = float(np.count_nonzero(mask))\n    denom = float(center.size) if center.size else 1.0\n    result = count / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Central mass offset: normalized distance between intensity centroid and image center (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # shift to non-negative mass\n    arr_pos = arr - float(np.min(arr))\n    total = arr_pos.sum()\n    if total <= 0:\n        return 0.0\n    ys = np.arange(h, dtype=float)[:, None]\n    xs = np.arange(w, dtype=float)[None, :]\n    y_mean = float((arr_pos * ys).sum() / total)\n    x_mean = float((arr_pos * xs).sum() / total)\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    dist = np.hypot(y_mean - cy, x_mean - cx)\n    diag = np.hypot(h, w) / 2.0 + 1e-12\n    result = dist / diag\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Texture coarseness ratio: mean local variance(7x7) / mean local variance(3x3)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    if h < 1 or w < 1:\n        return 0.0\n    def mean_local_var(a, k):\n        if k <= 0 or k > a.shape[0] or k > a.shape[1]:\n            return 0.0\n        # integral images\n        I = np.zeros((h + 1, w + 1), dtype=float)\n        I2 = np.zeros((h + 1, w + 1), dtype=float)\n        I[1:, 1:] = np.cumsum(np.cumsum(a, axis=0), axis=1)\n        I2[1:, 1:] = np.cumsum(np.cumsum(a * a, axis=0), axis=1)\n        s = I[k:, k:] - I[:-k, k:] - I[k:, :-k] + I[:-k, :-k]\n        s2 = I2[k:, k:] - I2[:-k, k:] - I2[k:, :-k] + I2[:-k, :-k]\n        n = float(k * k)\n        var = (s2 - (s * s) / n) / n\n        # numerical issues\n        var = np.maximum(var, 0.0)\n        return float(var.mean()) if var.size else 0.0\n    v3 = mean_local_var(arr, 3)\n    v7 = mean_local_var(arr, 7)\n    eps = 1e-12\n    result = v7 / (v3 + eps)\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Max horizontal run fraction: longest consecutive above-mean run in any row normalized by width'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    if w == 0:\n        return 0.0\n    thr = float(np.mean(arr))\n    mask = arr > thr\n    max_run = 0\n    for i in range(h):\n        row = mask[i]\n        # compute run lengths in row\n        if row.any():\n            # differences trick\n            dif = np.diff(np.concatenate(([0], row.view(np.int8), [0])))\n            starts = np.where(dif == 1)[0]\n            ends = np.where(dif == -1)[0]\n            if starts.size and ends.size:\n                runs = ends - starts\n                max_run = max(int(runs.max()), max_run)\n    result = float(max_run) / float(w) if w else 0.0\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant gradient orientation fraction (peak bin energy / total) using 16 bins'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total = float(mag.sum()) + 1e-12\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    # orientation modulo pi (undirected)\n    theta_mod = np.mod(theta, np.pi)\n    bins = 16\n    bin_edges = np.linspace(0.0, np.pi, bins + 1)\n    hist = np.zeros(bins, dtype=float)\n    # compute weighted histogram\n    inds = np.minimum(np.searchsorted(bin_edges, theta_mod, side='right') - 1, bins - 1)\n    inds = np.clip(inds, 0, bins - 1)\n    flat_inds = inds.ravel()\n    flat_weights = mag.ravel()\n    for idx, wgt in zip(flat_inds, flat_weights):\n        hist[int(idx)] += float(wgt)\n    peak = float(hist.max())\n    result = peak / total\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized histogram entropy of intensities (0..1), uses 32 bins'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    flat = arr.ravel()\n    if flat.size == 0:\n        return 0.0\n    bins = 32\n    mn = float(flat.min()); mx = float(flat.max())\n    if mn == mx:\n        return 0.0\n    hist, _ = np.histogram(flat, bins=bins, range=(mn, mx))\n    probs = hist.astype(float) / (hist.sum() + 1e-12)\n    probs = probs[probs > 0.0]\n    ent = -np.sum(probs * np.log(probs))\n    # normalize by log(bins)\n    result = float(ent / (np.log(bins) + 1e-12))\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute curvature of strong edges: average angular change of gradient direction (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    theta = np.arctan2(gy, gx)\n    # compute wrapped differences\n    def wrap(diff):\n        return (diff + np.pi) % (2 * np.pi) - np.pi\n    dtx = wrap(np.diff(theta, axis=1))\n    dty = wrap(np.diff(theta, axis=0))\n    magx = mag[:, :-1]\n    magy = mag[:-1, :]\n    # only consider locations with sufficient magnitude\n    thr = float(np.mean(mag))\n    maskx = magx > thr\n    masky = magy > thr\n    vals = []\n    if maskx.any():\n        vals.append(np.abs(dtx[maskx]).mean())\n    if masky.any():\n        vals.append(np.abs(dty[masky]).mean())\n    if not vals:\n        return 0.0\n    mean_abs = float(np.mean(vals))\n    result = mean_abs / np.pi  # normalize to ~[0,1]\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean color saturation for RGB images (0..1), returns 0 for grayscale'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(arr.astype(float))\n    # take first three channels if more present\n    a3 = a[:, :, :3]\n    mx = a3.max(axis=2)\n    mn = a3.min(axis=2)\n    sat = (mx - mn) / (mx + 1e-12)  # avoid division by zero\n    # where mx is zero, saturation defined as 0\n    sat = np.where(mx > 0, sat, 0.0)\n    result = float(np.mean(sat))\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized displacement of intensity centroid from image center (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    total = float(a.sum())\n    if total == 0.0 or h == 0 or w == 0:\n        return 0.0\n    yy, xx = np.indices((h, w))\n    cx = (a * xx).sum() / total\n    cy = (a * yy).sum() / total\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    norm = (np.hypot(w, h) / 2.0) + 1e-12\n    result = np.clip(dist / norm, 0.0, 1.0)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Relative high-frequency (Laplacian) energy compared to overall intensity'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    # Discrete Laplacian via 4-neighbor stencil\n    lap = 4.0 * a - np.roll(a, 1, axis=0) - np.roll(a, -1, axis=0) - np.roll(a, 1, axis=1) - np.roll(a, -1, axis=1)\n    lap_energy = float(np.sum(np.abs(lap)))\n    total_energy = float(np.sum(np.abs(a))) + eps\n    result = lap_energy / (total_energy + lap_energy + eps)  # fraction of high-frequency energy\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of image pixels above an adaptive mean+std threshold (foreground density)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thresh = m + 0.5 * s\n    mask = a > thresh\n    result = float(np.count_nonzero(mask)) / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center-vs-border contrast normalized by global std (positive => center brighter)'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ch = max(1, h // 4)\n    cw = max(1, w // 4)\n    center = a[h//2 - ch//2:h//2 + (ch - ch//2), w//2 - cw//2:w//2 + (cw - cw//2)]\n    # border: outer ring excluding center area\n    border_mask = np.ones_like(a, dtype=bool)\n    br_top = max(0, h//2 - ch//2)\n    br_bot = min(h, h//2 + (ch - ch//2))\n    br_left = max(0, w//2 - cw//2)\n    br_right = min(w, w//2 + (cw - cw//2))\n    border_mask[br_top:br_bot, br_left:br_right] = False\n    border = a[border_mask]\n    if center.size == 0 or border.size == 0:\n        return 0.0\n    center_mean = float(center.mean())\n    border_mean = float(border.mean())\n    gstd = float(a.std()) + eps\n    result = (center_mean - border_mean) / gstd\n    # clip to reasonable range\n    return float(np.clip(result, -5.0, 5.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical mirror symmetry: absolute normalized correlation with left-right flipped image'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    b = np.fliplr(a)\n    if a.shape != b.shape or a.size == 0:\n        return 0.0\n    af = a.ravel()\n    bf = b.ravel()\n    afm = af - af.mean()\n    bfm = bf - bf.mean()\n    denom = np.sqrt((afm**2).sum() * (bfm**2).sum()) + 1e-12\n    corr = float((afm * bfm).sum() / denom)\n    return float(np.clip(abs(corr), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized inter-percentile range (90th-10th) divided by dynamic range (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    p90 = float(np.percentile(a, 90))\n    p10 = float(np.percentile(a, 10))\n    mn = float(a.min())\n    mx = float(a.max())\n    denom = (mx - mn) + 1e-12\n    result = (p90 - p10) / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of strict local intensity peaks (local max over 8 neighbors), normalized by area'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 1 or w < 1:\n        return 0.0\n    # compare to 8 neighbors\n    center = a\n    neighs = [\n        np.roll(np.roll(a,  0, axis=0),  1, axis=1),\n        np.roll(np.roll(a,  0, axis=0), -1, axis=1),\n        np.roll(np.roll(a,  1, axis=0),  0, axis=1),\n        np.roll(np.roll(a, -1, axis=0),  0, axis=1),\n        np.roll(np.roll(a,  1, axis=0),  1, axis=1),\n        np.roll(np.roll(a,  1, axis=0), -1, axis=1),\n        np.roll(np.roll(a, -1, axis=0),  1, axis=1),\n        np.roll(np.roll(a, -1, axis=0), -1, axis=1),\n    ]\n    is_peak = np.ones_like(a, dtype=bool)\n    for n in neighs:\n        is_peak &= (center > n)\n    # also require peaks to be above a modest threshold to avoid noise\n    thr = float(a.mean()) + 0.5 * float(a.std())\n    is_peak &= (a > thr)\n    count = float(np.count_nonzero(is_peak))\n    result = count / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted eccentricity (1=elongated, 0=circular) from second moments'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    yy, xx = np.indices((h, w))\n    total = float(a.sum())\n    if total <= 0:\n        return 0.0\n    cx = (a * xx).sum() / total\n    cy = (a * yy).sum() / total\n    x = xx - cx\n    y = yy - cy\n    # covariance entries\n    mu_xx = float((a * (x * x)).sum() / total)\n    mu_yy = float((a * (y * y)).sum() / total)\n    mu_xy = float((a * (x * y)).sum() / total)\n    # eigenvalues of covariance\n    trace = mu_xx + mu_yy\n    det = mu_xx * mu_yy - mu_xy * mu_xy\n    discr = max(trace * trace / 4.0 - det, 0.0)\n    eig1 = trace / 2.0 + np.sqrt(discr)\n    eig2 = trace / 2.0 - np.sqrt(discr)\n    if eig1 <= eps:\n        return 0.0\n    ecc = 1.0 - (eig2 / (eig1 + eps))\n    result = float(np.clip(ecc, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge pixel density: fraction of pixels with gradient magnitude above adaptive threshold'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    if mag.size == 0:\n        return 0.0\n    thr = float(mag.mean() + 0.5 * mag.std())\n    mask = mag > thr\n    result = float(np.count_nonzero(mask)) / float(mag.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of near-maximum (saturated) pixels to total pixels, robust to dynamic range'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    mn = float(a.min())\n    mx = float(a.max())\n    if mx - mn <= 1e-12:\n        return 0.0\n    high_thresh = mn + 0.98 * (mx - mn)\n    count = float(np.count_nonzero(a >= high_thresh))\n    result = count / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    '180-degree rotational symmetry (1 = perfect symmetry, 0 = very different)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    try:\n        rot = np.rot90(arr, 2)\n    except Exception:\n        return 0.0\n    denom = float(np.mean(np.abs(arr))) + eps\n    diff = np.abs(arr - rot)\n    mean_diff = float(np.mean(diff))\n    result = 1.0 - (mean_diff / denom)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Eccentricity of bright region from intensity-weighted covariance (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    thr = float(a.mean() + 0.5 * a.std())\n    mask = a > thr\n    if not np.any(mask):\n        # fallback to all pixels weighted by intensity above min\n        weights = np.maximum(a - a.min(), 0.0).ravel()\n        coords_y = np.repeat(np.arange(h), w).astype(float)\n        coords_x = np.tile(np.arange(w), h).astype(float)\n    else:\n        weights = a[mask].astype(float).ravel()\n        ys, xs = np.nonzero(mask)\n        coords_y = ys.astype(float)\n        coords_x = xs.astype(float)\n    total_w = weights.sum() + eps\n    if total_w <= eps:\n        return 0.0\n    cy = (weights * coords_y).sum() / total_w\n    cx = (weights * coords_x).sum() / total_w\n    dy = coords_y - cy\n    dx = coords_x - cx\n    cov_yy = (weights * (dy * dy)).sum() / total_w\n    cov_xx = (weights * (dx * dx)).sum() / total_w\n    cov_xy = (weights * (dx * dy)).sum() / total_w\n    # 2x2 covariance eigenvalues\n    trace = cov_xx + cov_yy\n    det = cov_xx * cov_yy - cov_xy * cov_xy\n    disc = max(0.0, trace * trace - 4.0 * det)\n    sqrt_disc = np.sqrt(disc)\n    lam1 = 0.5 * (trace + sqrt_disc)\n    lam2 = 0.5 * (trace - sqrt_disc)\n    lam1 = max(lam1, 0.0)\n    lam2 = max(lam2, 0.0)\n    result = 1.0 - (lam2 + eps) / (lam1 + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Coarseness measure: mean gradient after 3x3 blur divided by mean gradient (higher => coarse)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    # small 3x3 box blur implemented with shifts (no padding artifacts due to roll but acceptable)\n    up = np.roll(a, 1, axis=0)\n    down = np.roll(a, -1, axis=0)\n    left = np.roll(a, 1, axis=1)\n    right = np.roll(a, -1, axis=1)\n    up_left = np.roll(up, 1, axis=1)\n    up_right = np.roll(up, -1, axis=1)\n    down_left = np.roll(down, 1, axis=1)\n    down_right = np.roll(down, -1, axis=1)\n    blur = (a + up + down + left + right + up_left + up_right + down_left + down_right) / 9.0\n    try:\n        gy_o, gx_o = np.gradient(a)\n        gy_b, gx_b = np.gradient(blur)\n    except Exception:\n        return 0.0\n    mag_o = np.hypot(gx_o, gy_o)\n    mag_b = np.hypot(gx_b, gy_b)\n    mean_o = float(np.mean(mag_o)) + eps\n    mean_b = float(np.mean(mag_b))\n    result = mean_b / mean_o\n    # coarser textures -> higher ratio; clip to reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of strong edges located in central region (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + 0.5 * mag.std())\n    strong = mag > thr\n    if not np.any(strong):\n        return 0.0\n    ch1, ch2 = h // 4, 3 * h // 4\n    cw1, cw2 = w // 4, 3 * w // 4\n    center_mask = np.zeros_like(a, dtype=bool)\n    center_mask[ch1:ch2, cw1:cw2] = True\n    center_edges = float(np.sum(strong & center_mask))\n    total_edges = float(np.sum(strong)) + eps\n    result = center_edges / total_edges\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Principal axis angle of bright region normalized to [0..1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    thr = float(a.mean() + 0.5 * a.std())\n    mask = a > thr\n    if not np.any(mask):\n        return 0.5  # undefined orientation -> neutral\n    ys, xs = np.nonzero(mask)\n    weights = a[mask].astype(float)\n    total_w = weights.sum() + eps\n    cy = (weights * ys).sum() / total_w\n    cx = (weights * xs).sum() / total_w\n    dy = ys.astype(float) - cy\n    dx = xs.astype(float) - cx\n    cov_xx = (weights * (dx * dx)).sum() / total_w\n    cov_yy = (weights * (dy * dy)).sum() / total_w\n    cov_xy = (weights * (dx * dy)).sum() / total_w\n    # orientation of principal eigenvector\n    angle = 0.5 * np.arctan2(2.0 * cov_xy, (cov_xx - cov_yy))  # -pi/2..pi/2\n    # map to 0..1\n    result = (angle + (np.pi / 2.0)) / np.pi\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border uniformity: 1 = very uniform border, 0 = highly variable'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    bw = max(1, min(h, w) // 10)\n    top = a[0:bw, :].ravel()\n    bottom = a[-bw:, :].ravel() if bw <= h else np.array([], dtype=float)\n    left = a[:, 0:bw].ravel()\n    right = a[:, -bw:].ravel() if bw <= w else np.array([], dtype=float)\n    # combine but avoid double-counting corners by using masks\n    border_vals = np.concatenate([top, bottom, left, right])\n    if border_vals.size == 0:\n        return 0.0\n    std_border = float(np.std(border_vals))\n    mean_border = float(np.mean(border_vals))\n    rel = std_border / (abs(mean_border) + eps)\n    # map to [0,1] with diminishing returns\n    result = 1.0 / (1.0 + rel)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized count of local intensity maxima (peaks) above adaptive threshold'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean() + 0.5 * a.std())\n    # neighbors via roll but mask out wrap-around by setting wrapped rows/cols to -inf\n    neighbors = []\n    shifts = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n    for dy, dx in shifts:\n        nb = np.roll(a, shift=dy, axis=0)\n        nb = np.roll(nb, shift=dx, axis=1)\n        # mask wrapped rows/cols\n        if dy == -1:\n            nb[-1, :] = -np.inf\n        if dy == 1:\n            nb[0, :] = -np.inf\n        if dx == -1:\n            nb[:, -1] = -np.inf\n        if dx == 1:\n            nb[:, 0] = -np.inf\n        neighbors.append(nb)\n    gt_all = np.ones_like(a, dtype=bool)\n    for nb in neighbors:\n        gt_all &= (a > nb)\n    peaks = gt_all & (a > thr)\n    count = float(np.sum(peaks))\n    result = count / (float(h * w) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation concentration (0=uniform, 1=all edges same orientation)'\n    import numpy as np\n    eps = 1e-12\n    bins = 16\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(image.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(image.astype(float))\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    hist, _ = np.histogram(theta, bins=bins, range=(-np.pi, np.pi), weights=mag)\n    total = hist.sum() + eps\n    max_bin = float(hist.max())\n    result = max_bin / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-frequency energy ratio via FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    A = a - a.mean()\n    try:\n        F = np.fft.fft2(A)\n    except Exception:\n        return 0.0\n    P = np.abs(F) ** 2\n    # frequency coordinates\n    fy = np.fft.fftfreq(h)\n    fx = np.fft.fftfreq(w)\n    fy = fy[:, None]\n    fx = fx[None, :]\n    R = np.hypot(fy, fx)\n    cutoff = 0.125 * R.max() if R.max() > 0 else 0.0\n    low_mask = R <= cutoff\n    total_energy = P.sum() + eps\n    low_energy = P[low_mask].sum()\n    result = float(low_energy) / float(total_energy)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0 = single value, 1 = maximal entropy for bins)'\n    import numpy as np\n    eps = 1e-12\n    bins = 32\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(image.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(image.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    hist, _ = np.histogram(flat, bins=bins, range=(flat.min(), flat.max()))\n    total = hist.sum()\n    if total == 0:\n        return 0.0\n    p = hist.astype(float) / float(total)\n    p_nonzero = p[p > 0.0]\n    entropy = -np.sum(p_nonzero * np.log(p_nonzero + eps))\n    # normalize by log(bins)\n    max_ent = np.log(float(bins) + eps)\n    result = float(entropy / (max_ent + eps))\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels that are locally brighter than their 3x3 neighborhood mean'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # local mean via 3x3 box using rolls\n    s = np.zeros_like(arr)\n    s += arr\n    s += np.roll(arr, 1, axis=0)\n    s += np.roll(arr, -1, axis=0)\n    s += np.roll(arr, 1, axis=1)\n    s += np.roll(arr, -1, axis=1)\n    s += np.roll(np.roll(arr, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(arr, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(arr, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(arr, -1, axis=0), -1, axis=1)\n    local_mean = s / 9.0\n    mask = arr > (local_mean + eps)\n    result = float(mask.sum()) / float(arr.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of radial inner energy to combined inner+outer energy (1=all energy inner)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    yy, xx = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(yy - cy, xx - cx)\n    rmax = r.max() if r.size else 0.0\n    if rmax <= 0:\n        return 0.0\n    inner_mask = r <= (0.33 * rmax)\n    outer_mask = r >= (0.66 * rmax)\n    energy = arr.astype(float) ** 2\n    inner = float(energy[inner_mask].sum())\n    outer = float(energy[outer_mask].sum())\n    denom = inner + outer + eps\n    result = inner / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fill ratio: bright-region pixel count divided by its bounding-box area (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mu = float(arr.mean())\n    sigma = float(arr.std())\n    thresh = mu + 0.5 * sigma\n    mask = arr > thresh\n    if not mask.any():\n        return 0.0\n    ys, xs = np.where(mask)\n    y0, y1 = ys.min(), ys.max()\n    x0, x1 = xs.min(), xs.max()\n    bbox_area = float((y1 - y0 + 1) * (x1 - x0 + 1))\n    if bbox_area <= 0:\n        return 0.0\n    fill = float(mask.sum()) / (bbox_area + eps)\n    return float(np.clip(fill, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of total spectral energy concentrated in low-frequency band (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute centered FFT magnitude squared\n    F = np.fft.fft2(a)\n    F = np.fft.fftshift(F)\n    mag2 = np.abs(F) ** 2\n    yy, xx = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(yy - cy, xx - cx)\n    cutoff = max(1.0, min(h, w) / 8.0)\n    low_mask = r <= cutoff\n    total = float(mag2.sum()) + eps\n    low = float(mag2[low_mask].sum())\n    result = low / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Balance of bright vs dark pixels: (bright_count - dark_count) / total in [-1..1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    vmin = float(flat.min())\n    vmax = float(flat.max())\n    if vmax - vmin <= 0:\n        return 0.0\n    bright_thresh = vmax - 0.1 * (vmax - vmin)\n    dark_thresh = vmin + 0.1 * (vmax - vmin)\n    bright = float((flat >= bright_thresh).sum())\n    dark = float((flat <= dark_thresh).sum())\n    result = (bright - dark) / (flat.size + eps)\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient directionality strength: |mean gradient vector| / mean gradient magnitude (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    if arr.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    gx = gx.astype(float)\n    gy = gy.astype(float)\n    mag = np.hypot(gx, gy)\n    mean_vec_x = float((gx * mag).sum()) / (mag.sum() + eps)\n    mean_vec_y = float((gy * mag).sum()) / (mag.sum() + eps)\n    mean_mag = float(mag.mean() + eps)\n    strength = np.hypot(mean_vec_x, mean_vec_y) / mean_mag\n    return float(np.clip(strength, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Coarseness: mean absolute residual between image and local 5x5 blur, normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if arr.size == 0:\n        return 0.0\n    # 5x5 uniform blur via summed rolls (approx, efficient)\n    s = np.zeros_like(arr)\n    for dy in (-2, -1, 0, 1, 2):\n        for dx in (-2, -1, 0, 1, 2):\n            s += np.roll(np.roll(arr, dy, axis=0), dx, axis=1)\n    blurred = s / 25.0\n    residual = np.abs(arr - blurred).mean()\n    denom = float(arr.std() + eps)\n    result = residual / denom\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized count of local intensity peaks (local maxima) per pixel in [0..1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    # local maxima strict: greater than all 8 neighbors\n    center = a\n    neigh_max = np.maximum.reduce([\n        np.roll(np.roll(a, -1, axis=0), -1, axis=1),\n        np.roll(np.roll(a, -1, axis=0), 0, axis=1),\n        np.roll(np.roll(a, -1, axis=0), 1, axis=1),\n        np.roll(np.roll(a, 0, axis=0), -1, axis=1),\n        np.roll(np.roll(a, 0, axis=0), 1, axis=1),\n        np.roll(np.roll(a, 1, axis=0), -1, axis=1),\n        np.roll(np.roll(a, 1, axis=0), 0, axis=1),\n        np.roll(np.roll(a, 1, axis=0), 1, axis=1),\n    ])\n    peaks = (center > neigh_max) & (center > (a.mean() + 0.5 * a.std()))\n    count = float(peaks.sum())\n    result = count / (float(a.size) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average of left-right and top-bottom symmetry measured by Pearson correlation (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # left-right\n    midx = w // 2\n    left = arr[:, :midx]\n    right = arr[:, -midx:] if midx > 0 else np.empty_like(left)\n    if left.size == 0 or right.size == 0:\n        lr_corr = 0.0\n    else:\n        right_flipped = np.fliplr(right)\n        L = left.ravel()\n        R = right_flipped.ravel()\n        Lm = L.mean()\n        Rm = R.mean()\n        num = ((L - Lm) * (R - Rm)).mean()\n        den = (L.std() * R.std()) + eps\n        lr_corr = float(np.clip(num / den, -1.0, 1.0))\n    # top-bottom\n    midy = h // 2\n    top = arr[:midy, :]\n    bot = arr[-midy:, :] if midy > 0 else np.empty_like(top)\n    if top.size == 0 or bot.size == 0:\n        tb_corr = 0.0\n    else:\n        bot_flipped = np.flipud(bot)\n        T = top.ravel()\n        B = bot_flipped.ravel()\n        Tm = T.mean()\n        Bm = B.mean()\n        num = ((T - Tm) * (B - Bm)).mean()\n        den = (T.std() * B.std()) + eps\n        tb_corr = float(np.clip(num / den, -1.0, 1.0))\n    result = (lr_corr + tb_corr) / 2.0\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for color images (0 for grayscale), higher = more colorful'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(image.astype(float))\n    # ensure channels last\n    if a.shape[2] < 3:\n        return 0.0\n    R = a[..., 0]\n    G = a[..., 1]\n    B = a[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    colorfulness = np.sqrt(std_rg * std_rg + std_yb * std_yb) + 0.3 * np.sqrt(mean_rg * mean_rg + mean_yb * mean_yb)\n    return float(max(0.0, colorfulness))\n",
    "def feature(image: np.ndarray) -> float:\n    'Variance of mean intensities across the four image quadrants normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mid_h = h // 2\n    mid_w = w // 2\n    q1 = a[:mid_h, :mid_w]\n    q2 = a[:mid_h, mid_w:]\n    q3 = a[mid_h:, :mid_w]\n    q4 = a[mid_h:, mid_w:]\n    means = []\n    for q in (q1, q2, q3, q4):\n        means.append(float(np.mean(q)) if q.size else 0.0)\n    var_means = float(np.var(np.array(means)))\n    gstd = float(a.std()) + eps\n    result = var_means / gstd\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Sparsity of very bright pixels: fraction above the 95th percentile'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    total = a.size\n    if total == 0:\n        return 0.0\n    try:\n        thr = float(np.percentile(a, 95))\n    except Exception:\n        thr = float(a.mean())\n    cnt = int(np.count_nonzero(a > thr))\n    return float(cnt) / float(total)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant gradient angle as a fraction of full circle (0..1), weighted by gradient magnitude'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    bins = 36\n    hist, edges = np.histogram(theta, bins=bins, range=(-np.pi, np.pi), weights=mag)\n    if hist.sum() <= 0:\n        return 0.0\n    dominant_idx = int(np.argmax(hist))\n    # map bin center to fraction of circle\n    bin_center = 0.5 * (edges[dominant_idx] + edges[dominant_idx + 1])\n    frac = (bin_center + np.pi) / (2 * np.pi)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    '180-degree rotational symmetry: 1 - normalized mean absolute difference to rotated image (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 1.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 1.0\n    rot = np.flipud(np.fliplr(a)) if a.ndim == 2 else np.flip(a)\n    diff = np.abs(a - rot)\n    mean_diff = float(np.mean(diff))\n    mean_val = float(np.mean(np.abs(a))) + eps\n    score = 1.0 - (mean_diff / mean_val)\n    return float(np.clip(score, -1.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local variance (3x3) normalized by global standard deviation'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute local mean and local mean of squares using rolling sums\n    def local_mean(X):\n        s = np.zeros_like(X)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s / 9.0\n    mean_loc = local_mean(a)\n    mean_sq_loc = local_mean(a * a)\n    var_loc = mean_sq_loc - (mean_loc * mean_loc)\n    mean_local_variance = float(np.mean(var_loc))\n    gstd = float(a.std()) + eps\n    result = mean_local_variance / gstd\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of local gradient peaks (simple corner-like peaks) normalized by image area'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + mag.std())\n    # local maxima mask: greater than 8 neighbors\n    center = mag\n    neigh_max = np.maximum.reduce([\n        np.roll(np.roll(center, 1, 0), 1, 1),\n        np.roll(np.roll(center, 1, 0), -1, 1),\n        np.roll(np.roll(center, -1, 0), 1, 1),\n        np.roll(np.roll(center, -1, 0), -1, 1),\n        np.roll(center, 1, 0),\n        np.roll(center, -1, 0),\n        np.roll(center, 1, 1),\n        np.roll(center, -1, 1)\n    ])\n    peaks = (center > neigh_max) & (center > thr)\n    count = int(np.count_nonzero(peaks))\n    area = float(h * w) + eps\n    return float(count / area)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean inward border gradient: average gradient magnitude from outer 1-pixel ring toward center normalized by total edge energy'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    # outer ring mask (1-pixel)\n    mask = np.zeros_like(a, dtype=bool)\n    mask[0, :] = True\n    mask[-1, :] = True\n    mask[:, 0] = True\n    mask[:, -1] = True\n    outer_mag = mag[mask]\n    total = float(mag.sum()) + eps\n    if outer_mag.size == 0:\n        return 0.0\n    result = float(outer_mag.mean()) / total\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Lacunarity-like measure: average variance-to-mean of nonzero block counts across scales'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # binary map using mean threshold\n    thr = float(a.mean())\n    b = (a > thr).astype(np.int32)\n    scales = []\n    max_scale = min(h, w)\n    if max_scale < 2:\n        return 0.0\n    # choose a few block sizes\n    sizes = []\n    s = 2\n    while s <= max_scale and len(sizes) < 5:\n        sizes.append(s)\n        s *= 2\n    if not sizes:\n        sizes = [max(1, max_scale)]\n    ratios = []\n    for size in sizes:\n        bh = int(np.ceil(h / size))\n        bw = int(np.ceil(w / size))\n        counts = []\n        for i in range(bh):\n            for j in range(bw):\n                ys = slice(i * size, min((i + 1) * size, h))\n                xs = slice(j * size, min((j + 1) * size, w))\n                block = b[ys, xs]\n                counts.append(int(block.sum()))\n        counts = np.array(counts, dtype=float)\n        if counts.size == 0:\n            continue\n        mean_c = counts.mean()\n        var_c = counts.var()\n        # avoid division by zero\n        ratio = float(var_c / (mean_c + eps))\n        ratios.append(ratio)\n    if not ratios:\n        return 0.0\n    return float(np.mean(ratios))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Global intensity skewness (third standardized moment), clipped to [-10,10]'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    mu = float(a.mean())\n    sd = float(a.std()) + eps\n    skew = float(np.mean(((a - mu) / sd) ** 3))\n    skew = float(np.clip(skew, -10.0, 10.0))\n    return float(skew)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized maximum radius of bright region: farthest bright pixel from center / image diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean() + 0.5 * a.std())\n    mask = (a > thr)\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    dists = np.hypot(ys - cy, xs - cx)\n    maxd = float(dists.max()) if dists.size else 0.0\n    diag = np.hypot(h, w) / 2.0 + eps\n    result = maxd / diag\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial symmetry score: similarity between image and its 180-degree rotation (1 => perfectly symmetric)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    # compare with 180-degree rotated version\n    rev = a[::-1, ::-1]\n    if a.shape != rev.shape or a.size == 0:\n        return 0.0\n    diff = np.abs(a - rev)\n    mean_diff = float(diff.mean())\n    mean_mag = float(np.abs(a).mean()) + eps\n    score = 1.0 - (mean_diff / mean_mag)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-sparsity: fraction of pixels that are strong local bright outliers (sparse bright spots)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    m = float(flat.mean())\n    s = float(flat.std()) + eps\n    thr = m + 1.5 * s\n    count = float(np.count_nonzero(flat > thr))\n    frac = count / (flat.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity elongation: anisotropy of intensity distribution (0..1, higher => more elongated)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h * w == 0:\n        return 0.0\n    flat = a.ravel()\n    if flat.sum() == 0:\n        return 0.0\n    # coordinates\n    ys, xs = np.indices(a.shape)\n    weight = flat.reshape(a.shape)\n    total = float(weight.sum()) + eps\n    cx = float((weight * xs).sum()) / total\n    cy = float((weight * ys).sum()) / total\n    dx = (xs - cx).ravel()\n    dy = (ys - cy).ravel()\n    wv = weight.ravel()\n    cov_xx = float((wv * (dx * dx)).sum()) / total\n    cov_yy = float((wv * (dy * dy)).sum()) / total\n    cov_xy = float((wv * (dx * dy)).sum()) / total\n    # covariance matrix eigenvalues\n    tr = cov_xx + cov_yy\n    det = cov_xx * cov_yy - cov_xy * cov_xy\n    tmp = np.sqrt(max(0.0, (tr * tr) / 4.0 - det))\n    l1 = max(0.0, tr / 2.0 + tmp)\n    l2 = max(0.0, tr / 2.0 - tmp)\n    # anisotropy: 1 - (l2 / l1) (clamped), if l1 ~ 0 then 0\n    if l1 < eps:\n        return 0.0\n    anis = 1.0 - (l2 / (l1 + eps))\n    return float(np.clip(anis, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian energy normalized by mean intensity (higher => more fine texture or noise)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # compute discrete Laplacian via padded slices\n    pad = np.pad(a, pad_width=1, mode='edge').astype(float)\n    center = pad[1:-1, 1:-1]\n    up = pad[0:-2, 1:-1]\n    down = pad[2:, 1:-1]\n    left = pad[1:-1, 0:-2]\n    right = pad[1:-1, 2:]\n    lap = (up + down + left + right) - 4.0 * center\n    energy = float(np.mean(np.abs(lap)))\n    mean_int = float(np.mean(np.abs(a))) + eps\n    result = energy / mean_int\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border uniformity: fraction of border pixels within 5% of median border intensity (1 => uniform)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # collect border pixels (outer ring)\n    top = a[0, :]\n    bottom = a[-1, :]\n    left = a[1:-1, 0] if h > 2 else np.array([])\n    right = a[1:-1, -1] if h > 2 else np.array([])\n    border = np.concatenate([top.ravel(), bottom.ravel(), left.ravel(), right.ravel()]) if (top.size + bottom.size + left.size + right.size) > 0 else np.array([])\n    if border.size == 0:\n        return 0.0\n    med = float(np.median(border))\n    tol = max(1e-6, 0.05 * (np.abs(med) + eps))\n    close = np.abs(border - med) <= tol\n    frac = float(np.count_nonzero(close)) / float(border.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of strong local intensity peaks normalized by image area (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # threshold to avoid tiny fluctuations: mean + 0.5*std\n    mean = float(a.mean())\n    std = float(a.std())\n    thr = mean + 0.5 * std\n    # pad with -inf so border cannot be maxima by wrap\n    pad = np.pad(a, pad_width=1, mode='constant', constant_values=-np.inf)\n    center = pad[1:-1, 1:-1]\n    # compare to 8 neighbors\n    neighs = [\n        pad[0:-2, 0:-2], pad[0:-2, 1:-1], pad[0:-2, 2:],\n        pad[1:-1, 0:-2],                 pad[1:-1, 2:],\n        pad[2:, 0:-2],  pad[2:, 1:-1],  pad[2:, 2:]\n    ]\n    greater = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        greater &= (center > n)\n    strong = greater & (center > thr)\n    count = float(np.count_nonzero(strong))\n    frac = count / float(h * w + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Largest thresholded component fraction using mean intensity threshold (0..1)'\n    import numpy as np\n    from collections import deque\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        gray = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        gray = np.nan_to_num(arr.astype(float))\n    h, w = gray.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(gray.mean())\n    mask = gray > thr\n    if not mask.any():\n        return 0.0\n    visited = np.zeros_like(mask, dtype=bool)\n    max_area = 0\n    # 4-connected flood fill\n    for y in range(h):\n        for x in range(w):\n            if mask[y, x] and not visited[y, x]:\n                area = 0\n                dq = deque()\n                dq.append((y, x))\n                visited[y, x] = True\n                while dq:\n                    cy, cx = dq.popleft()\n                    area += 1\n                    # neighbors\n                    if cy > 0 and mask[cy-1, cx] and not visited[cy-1, cx]:\n                        visited[cy-1, cx] = True\n                        dq.append((cy-1, cx))\n                    if cy < h-1 and mask[cy+1, cx] and not visited[cy+1, cx]:\n                        visited[cy+1, cx] = True\n                        dq.append((cy+1, cx))\n                    if cx > 0 and mask[cy, cx-1] and not visited[cy, cx-1]:\n                        visited[cy, cx-1] = True\n                        dq.append((cy, cx-1))\n                    if cx < w-1 and mask[cy, cx+1] and not visited[cy, cx+1]:\n                        visited[cy, cx+1] = True\n                        dq.append((cy, cx+1))\n                if area > max_area:\n                    max_area = area\n    frac = float(max_area) / float(h * w + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean color saturation for RGB images (0..1), 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    rgb = np.nan_to_num(arr[:, :, :3].astype(float))\n    mx = np.max(rgb, axis=2)\n    mn = np.min(rgb, axis=2)\n    # saturation per pixel = (max - min) / (max + eps)\n    sat = (mx - mn) / (mx + eps)\n    mean_sat = float(np.mean(sat))\n    return float(np.clip(mean_sat, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge orientation concentration (circular concentration of gradient orientations, 0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.size == 0:\n        return 0.0\n    # orientation symmetry: use double-angle to make opposite directions equivalent\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    cos2 = np.cos(2.0 * theta)\n    sin2 = np.sin(2.0 * theta)\n    # weight by magnitude\n    W = mag.ravel()\n    S_cos = float((W * cos2.ravel()).sum())\n    S_sin = float((W * sin2.ravel()).sum())\n    Wsum = float(W.sum()) + eps\n    R = np.hypot(S_cos, S_sin) / Wsum\n    return float(np.clip(R, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Centroid offset: normalized distance between intensity centroid and image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices(a.shape)\n    total = float(a.sum())\n    if total == 0:\n        return 0.0\n    cx = float((a * xs).sum()) / total\n    cy = float((a * ys).sum()) / total\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    # normalize by image diagonal\n    diag = np.hypot(w, h) + eps\n    return float(np.clip(dist / diag, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance between intensity centroid and image center (0=centered, up to 1=corner)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    eps = 1e-12\n    total = float(a.sum())\n    if total <= eps:\n        return 0.0\n    ys = np.arange(h).reshape(h, 1)\n    xs = np.arange(w).reshape(1, w)\n    cy = float((a * ys).sum() / (total + eps))\n    cx = float((a * xs).sum() / (total + eps))\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = float(np.hypot(cy - center_y, cx - center_x))\n    max_dist = float(np.hypot(center_y, center_x)) + eps\n    result = float(np.clip(dist / max_dist, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Relative high-frequency energy via absolute Laplacian normalized by total absolute intensity'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    # discrete Laplacian (4-neighbor)\n    up = np.roll(a, -1, axis=0)\n    down = np.roll(a, 1, axis=0)\n    left = np.roll(a, -1, axis=1)\n    right = np.roll(a, 1, axis=1)\n    lap = (up + down + left + right) - 4.0 * a\n    energy = float(np.sum(np.abs(lap)))\n    denom = float(np.sum(np.abs(a))) + 1e-12\n    result = energy / denom\n    # clip to a reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right mirror symmetry: 1 - normalized mean absolute difference between image and its horizontal flip (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 1.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 1.0\n    flip = np.fliplr(a)\n    diff = np.abs(a - flip)\n    mean_diff = float(np.mean(diff))\n    mean_val = float(np.mean(np.abs(a))) + eps\n    score = 1.0 - (mean_diff / mean_val)\n    return float(np.clip(score, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of strict local maxima (pixels greater than all 8 neighbors), normalized by image area'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # pad with -inf so edges are handled without wrap\n    pad = np.pad(a, pad_width=((1, 1), (1, 1)), mode='constant', constant_values=-np.inf)\n    center = pad[1:-1, 1:-1]\n    neighs = [\n        pad[0:-2, 0:-2], pad[0:-2, 1:-1], pad[0:-2, 2:],\n        pad[1:-1, 0:-2],                 pad[1:-1, 2:],\n        pad[2:, 0:-2],   pad[2:, 1:-1],   pad[2:, 2:]\n    ]\n    mask = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        mask &= (center > n)\n    count = float(np.count_nonzero(mask))\n    denom = float(a.size)\n    return float(np.clip(count / (denom + 1e-12), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Simple intensity skewness: (mean - median) / std (robust, 0 if undefined)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).ravel().astype(float))\n    else:\n        vals = np.nan_to_num(arr.ravel().astype(float))\n    if vals.size == 0:\n        return 0.0\n    mean = float(vals.mean())\n    median = float(np.median(vals))\n    std = float(vals.std())\n    if std <= 1e-12:\n        return 0.0\n    result = (mean - median) / std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (Hasler-Suesstrunk style); returns 0 for grayscale'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(arr.astype(float))\n    R = a[:, :, 0]\n    G = a[:, :, 1]\n    B = a[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(rg))\n    mean_yb = float(np.mean(yb))\n    colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n    return float(max(0.0, colorfulness))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientation histogram (magnitude-weighted), normalized to [0,1]'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if np.sum(mag) <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    # map to orientation in [0, pi) (ignore direction)\n    orient = np.mod(theta, np.pi)\n    nbins = 16\n    hist, _ = np.histogram(orient.ravel(), bins=nbins, range=(0.0, np.pi), weights=mag.ravel())\n    total = float(hist.sum()) + eps\n    p = hist / total\n    # entropy normalized by log(nbins)\n    ent = -float(np.sum(np.where(p > 0, p * np.log(p), 0.0)))\n    norm = np.log(float(nbins))\n    result = float(ent / (norm + eps))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border uniformity: fraction of 1-pixel border pixels within 10% intensity range of border median (1 = uniform)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # collect border pixels\n    top = a[0, :]\n    bottom = a[-1, :]\n    left = a[:, 0]\n    right = a[:, -1]\n    border = np.concatenate((top.ravel(), bottom.ravel(), left.ravel(), right.ravel()))\n    if border.size == 0:\n        return 0.0\n    med = float(np.median(border))\n    span = float(border.max() - border.min())\n    tol = 0.1 * (span if span > 0 else (np.std(border) + 1e-8))\n    within = float(np.count_nonzero(np.abs(border - med) <= tol))\n    result = within / float(border.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Prominence of the dominant intensity histogram peak: (max - second) / total_counts (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).ravel().astype(float))\n    else:\n        vals = np.nan_to_num(arr.ravel().astype(float))\n    if vals.size == 0:\n        return 0.0\n    nbins = 32\n    hist, _ = np.histogram(vals, bins=nbins)\n    if hist.sum() <= 0:\n        return 0.0\n    sorted_counts = np.sort(hist)[::-1]\n    maxc = float(sorted_counts[0])\n    second = float(sorted_counts[1]) if sorted_counts.size > 1 else 0.0\n    total = float(hist.sum())\n    result = (maxc - second) / (total + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of non-overlapping 8x8 patches with low contrast (patch std < 10% of global std)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    gstd = float(a.std())\n    if gstd <= 1e-12:\n        return 1.0\n    ph = 8\n    pw = 8\n    # if image smaller than patch, treat whole image as one patch\n    if h < ph or w < pw:\n        pstdev = float(a.std())\n        return float(1.0 if pstdev < 0.1 * gstd else 0.0)\n    # non-overlapping grid\n    nh = h // ph\n    nw = w // pw\n    total_patches = nh * nw\n    if total_patches == 0:\n        return 0.0\n    low = 0\n    for i in range(nh):\n        for j in range(nw):\n            patch = a[i*ph:(i+1)*ph, j*pw:(j+1)*pw]\n            if patch.size == 0:\n                continue\n            if float(patch.std()) < 0.1 * gstd:\n                low += 1\n    result = float(low) / float(total_patches)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy fraction via discrete Laplacian normalized by total energy'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # discrete Laplacian using roll trick\n    lap = -4.0 * a\n    lap += np.roll(a, 1, axis=0)\n    lap += np.roll(a, -1, axis=0)\n    lap += np.roll(a, 1, axis=1)\n    lap += np.roll(a, -1, axis=1)\n    high_energy = float(np.sum(np.abs(lap)))\n    total_energy = float(np.sum(np.abs(a))) + eps\n    result = high_energy / total_energy\n    return float(np.clip(result, 0.0, 1e6))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) using 64 histogram bins'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    # choose bins based on data range\n    mn, mx = float(flat.min()), float(flat.max())\n    if mx <= mn:\n        return 0.0\n    bins = 64\n    hist, _ = np.histogram(flat, bins=bins, range=(mn, mx))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p_nonzero = p[p > 0.0]\n    entropy = -np.sum(p_nonzero * np.log(p_nonzero + eps))\n    # normalize by log(bins) to get [0,1]\n    result = entropy / (np.log(bins) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    '180-degree rotational symmetry correlation (-1..1)'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    rot = np.rot90(img, 2)\n    A = img.ravel()\n    B = rot.ravel()\n    A_mean = A.mean()\n    B_mean = B.mean()\n    num = np.sum((A - A_mean) * (B - B_mean))\n    den = np.sqrt(np.sum((A - A_mean)**2) * np.sum((B - B_mean)**2)) + eps\n    result = num / den\n    return float(np.clip(result, -1.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge pixel density: fraction of pixels with gradient magnitude above mean'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    try:\n        gy, gx = np.gradient(img)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean())\n    if mag.size == 0:\n        return 0.0\n    count = float((mag > thr).sum())\n    total = float(mag.size) + eps\n    result = count / total\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of small bright components (count of components with area <1% image area)'\n    import numpy as np\n    from collections import deque\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        gray = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        gray = np.nan_to_num(arr.astype(float))\n    h, w = gray.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(gray.mean())\n    mask = gray > thr\n    if not mask.any():\n        return 0.0\n    visited = np.zeros_like(mask, dtype=bool)\n    small_count = 0\n    max_small = max(1, (h * w) // 100)  # 1% area threshold\n    for y in range(h):\n        for x in range(w):\n            if mask[y, x] and not visited[y, x]:\n                area = 0\n                dq = deque()\n                dq.append((y, x))\n                visited[y, x] = True\n                while dq:\n                    cy, cx = dq.popleft()\n                    area += 1\n                    if cy > 0 and mask[cy-1, cx] and not visited[cy-1, cx]:\n                        visited[cy-1, cx] = True\n                        dq.append((cy-1, cx))\n                    if cy < h-1 and mask[cy+1, cx] and not visited[cy+1, cx]:\n                        visited[cy+1, cx] = True\n                        dq.append((cy+1, cx))\n                    if cx > 0 and mask[cy, cx-1] and not visited[cy, cx-1]:\n                        visited[cy, cx-1] = True\n                        dq.append((cy, cx-1))\n                    if cx < w-1 and mask[cy, cx+1] and not visited[cy, cx+1]:\n                        visited[cy, cx+1] = True\n                        dq.append((cy, cx+1))\n                if area < max_small:\n                    small_count += 1\n    # normalize by image area to keep value small and comparable\n    result = float(small_count) / (float(h * w) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Interquartile contrast: (90th-10th percentile) normalized by mean intensity'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    p90 = float(np.percentile(flat, 90))\n    p10 = float(np.percentile(flat, 10))\n    denom = float(np.mean(np.abs(flat))) + eps\n    result = (p90 - p10) / denom\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Centroid offset of top 5% brightest pixels normalized by diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    flat = img.ravel()\n    if flat.size == 0:\n        return 0.0\n    k = max(1, int(0.05 * flat.size))\n    thr = np.partition(flat, -k)[-k]\n    mask = img >= thr\n    if not mask.any():\n        return 0.0\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    total = float(mask.sum()) + eps\n    mean_y = float((mask * ys).sum() / total)\n    mean_x = float((mask * xs).sum() / total)\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    dist = np.hypot(mean_y - cy, mean_x - cx)\n    diag = np.hypot(h, w) / 2.0 + eps\n    result = dist / diag\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity skewness (third standardized moment) clipped to [-5,5]'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    mean = float(a.mean())\n    std = float(a.std()) + eps\n    skew = float(np.mean(((a - mean) / std) ** 3))\n    return float(np.clip(skew, -5.0, 5.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Local checkerboard score: fraction of 2x2 blocks that alternate above/below global mean'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gm = float(img.mean())\n    binm = img > gm\n    # examine all 2x2 blocks\n    blocks_h = h - 1\n    blocks_w = w - 1\n    # extract corners\n    tl = binm[0:blocks_h, 0:blocks_w]\n    tr = binm[0:blocks_h, 1:blocks_w+1]\n    bl = binm[1:blocks_h+1, 0:blocks_w]\n    br = binm[1:blocks_h+1, 1:blocks_w+1]\n    # checkerboard patterns: diagonal pairs equal and opposite to the other diagonal\n    diag_equal = (tl == br) & (tr == bl)\n    diagonals_diff = (tl != tr)\n    checker = diag_equal & diagonals_diff\n    total_blocks = float(blocks_h * blocks_w)\n    score = float(checker.sum()) / (total_blocks + 1e-12)\n    return float(np.clip(score, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0=constant, 1=max for chosen bins)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    vals = a.ravel()\n    # use a moderate number of bins\n    bins = 64\n    try:\n        hist, _ = np.histogram(vals, bins=bins, density=False)\n    except Exception:\n        return 0.0\n    total = float(hist.sum())\n    if total <= 0.0:\n        return 0.0\n    p = hist / total\n    p = p[p > 0.0]\n    entropy = -np.sum(p * np.log2(p))\n    max_entropy = np.log2(bins) if bins > 1 else 1.0\n    result = float(np.clip(entropy / max_entropy, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Centrosymmetry: normalized correlation with 180-degree rotated image (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    flipped = np.rot90(a, 2)\n    num = float(np.sum(a * flipped))\n    denom = np.sqrt(float(np.sum(a * a)) * float(np.sum(flipped * flipped))) + 1e-12\n    corr = num / denom\n    # map from [-1,1] to [0,1]\n    result = (corr + 1.0) / 2.0\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of low-frequency Fourier energy in central band (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        F = np.fft.fft2(a)\n        F = np.fft.fftshift(F)\n        mag = np.abs(F)\n    except Exception:\n        return 0.0\n    cy, cx = h // 2, w // 2\n    radius = max(1, min(h, w) // 8)\n    yy, xx = np.ogrid[:h, :w]\n    mask = (yy - cy) ** 2 + (xx - cx) ** 2 <= radius * radius\n    total = float(mag.sum()) + 1e-12\n    low = float(mag[mask].sum())\n    result = low / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bounding-box aspect-deviation of bright foreground (0=centered square..1=very elongated)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mean = float(a.mean())\n    std = float(a.std())\n    thr = mean + 0.5 * std\n    fg = a > thr\n    if not np.any(fg):\n        return 0.0\n    ys, xs = np.where(fg)\n    y0, y1 = ys.min(), ys.max()\n    x0, x1 = xs.min(), xs.max()\n    bh = max(1, (y1 - y0 + 1))\n    bw = max(1, (x1 - x0 + 1))\n    aspect = float(bw) / float(bh)\n    # deviation metric: 0 when aspect==1, approaching 1 when very elongated\n    result = abs(aspect - 1.0) / (aspect + 1.0)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local patch contrast normalized by global std (higher => more local variation)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    global_std = float(a.std()) + 1e-8\n    # choose block size ~ max(2, min(h,w)//8)\n    bs = max(2, min(h, w) // 8)\n    H = (h // bs) * bs\n    W = (w // bs) * bs\n    if H == 0 or W == 0:\n        # fallback: local contrast = global std / global std = 1.0\n        return 1.0 if global_std > 0 else 0.0\n    crops = a[:H, :W].reshape(H // bs, bs, W // bs, bs)\n    # compute std per block efficiently\n    block_std = np.nan_to_num(crops.std(axis=(1, 3)))\n    mean_block_std = float(block_std.mean())\n    result = mean_block_std / global_std\n    # clamp to reasonable range\n    return float(np.clip(result, 0.0, 5.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of edge orientations weighted by gradient magnitude (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total_mag = float(mag.sum())\n    if total_mag <= 0.0:\n        return 0.0\n    theta = np.arctan2(gy, gx).ravel()\n    weights = mag.ravel()\n    bins = 36\n    hist, _ = np.histogram(theta, bins=bins, range=(-np.pi, np.pi), weights=weights)\n    p = hist / (hist.sum() + 1e-12)\n    p = p[p > 0.0]\n    entropy = -np.sum(p * np.log2(p))\n    max_entropy = np.log2(bins) if bins > 1 else 1.0\n    result = float(np.clip(entropy / max_entropy, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of Laplacian blob centers per 1000 pixels (detects blob-like structures)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n        gyy, gyx = np.gradient(gy)\n        gxy, gxx = np.gradient(gx)\n        lap = gxx + gyy\n    except Exception:\n        return 0.0\n    mu = float(lap.mean())\n    sd = float(lap.std())\n    if sd <= 0:\n        return 0.0\n    # blobs often correspond to strong negative Laplacian (dark blob on bright bg)\n    thresh = mu - sd\n    count = int(np.count_nonzero(lap < thresh))\n    density = count / float(max(1, h * w)) * 1000.0\n    return float(density)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric (0 for grayscale), normalized by overall intensity std'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim < 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    R = a[:, :, 0]\n    G = a[:, :, 1]\n    B = a[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    colorfulness = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    norm = float(a.mean() if float(a.std()) <= 0 else a.std()) + 1e-8\n    result = colorfulness / norm\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of image that is locally smooth (low gradient magnitude)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    mean_mag = float(mag.mean())\n    if mean_mag <= 0:\n        return 1.0\n    # smooth pixels where magnitude is small relative to mean\n    mask = mag <= (0.5 * mean_mag)\n    frac = float(np.count_nonzero(mask)) / float(max(1, mag.size))\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Approximate edge fractal complexity: scale ratio of edge counts (0..1 scaled)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr1 = float(mag.mean())\n    N1 = float(np.count_nonzero(mag > thr1))\n    # downsample by 2x2 averaging where possible\n    hh = (h // 2) * 2\n    ww = (w // 2) * 2\n    if hh == 0 or ww == 0 or N1 <= 0:\n        return 0.0\n    small = a[:hh, :ww].reshape(hh // 2, 2, ww // 2, 2).mean(axis=(1, 3))\n    try:\n        gy2, gx2 = np.gradient(small)\n    except Exception:\n        return 0.0\n    mag2 = np.hypot(gx2, gy2)\n    thr2 = float(mag2.mean()) + 1e-12\n    N2 = float(np.count_nonzero(mag2 > thr2))\n    if N2 <= 0 or N1 <= 0:\n        return 0.0\n    # fractal-like exponent estimate\n    fd = np.log(N1 / N2) / np.log(2.0)\n    # normalize to a bounded range [0,1] assuming reasonable range up to 4\n    result = float(np.clip(fd / 4.0, 0.0, 1.0))\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Shannon entropy of intensity histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    vals = arr.ravel()\n    if vals.size == 0:\n        return 0.0\n    # use fixed number of bins\n    bins = 32\n    hist, _ = np.histogram(vals, bins=bins, range=(vals.min(), vals.max()), density=False)\n    p = hist.astype(float) / (hist.sum() + eps)\n    p = p[p > 0]\n    ent = -np.sum(p * np.log(p + eps))\n    # normalize by log(bins)\n    norm = ent / (np.log(bins) + eps)\n    return float(np.clip(norm, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels brighter than mean+0.5*std (foreground fraction)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    m = float(a.mean())\n    s = float(a.std())\n    thr = m + 0.5 * s\n    if a.size == 0:\n        return 0.0\n    frac = float(np.count_nonzero(a > thr)) / (a.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variance of mean intensity versus radius from center (normalized)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    y = np.arange(h)[:, None]\n    x = np.arange(w)[None, :]\n    r = np.hypot(y - cy, x - cx)\n    maxr = max(1.0, r.max())\n    # radial bins\n    nbins = int(min(64, max(2, int(maxr))))\n    bins = np.linspace(0.0, maxr, nbins + 1)\n    radial_means = []\n    for i in range(nbins):\n        mask = (r >= bins[i]) & (r < bins[i+1])\n        if np.any(mask):\n            radial_means.append(float(a[mask].mean()))\n    if len(radial_means) < 2:\n        return 0.0\n    rad_means = np.array(radial_means)\n    result = float(rad_means.var() / (a.std() + eps))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized offset of bright-pixel centroid from image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # threshold at median to get bright region\n    flat = a.ravel()\n    thr = float(np.median(flat)) if flat.size else 0.0\n    mask = a > thr\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    cy = ys.mean()\n    cx = xs.mean()\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    maxdist = np.hypot(center_y, center_x) + eps\n    return float(np.clip(dist / maxdist, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Shape elongation measured by eigenvalue ratio of intensity-weighted coords (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    # use intensity as weight but shift to be non-negative\n    vals = a - a.min()\n    total = vals.sum()\n    if total <= 0:\n        # fall back to binary\n        mask = a > np.median(flat)\n        ys, xs = np.nonzero(mask)\n        if ys.size == 0:\n            return 0.0\n        vals_coords = np.ones_like(ys, dtype=float)\n    else:\n        ys, xs = np.indices(a.shape)\n        ys = ys.ravel()\n        xs = xs.ravel()\n        vals_coords = vals.ravel()\n    wsum = vals_coords.sum() + eps\n    mean_y = (ys * vals_coords).sum() / wsum\n    mean_x = (xs * vals_coords).sum() / wsum\n    dy = ys - mean_y\n    dx = xs - mean_x\n    Cxx = (vals_coords * (dx * dx)).sum() / wsum + eps\n    Cyy = (vals_coords * (dy * dy)).sum() / wsum + eps\n    Cxy = (vals_coords * (dx * dy)).sum() / wsum\n    # covariance matrix eigenvalues\n    tr = Cxx + Cyy\n    det = Cxx * Cyy - Cxy * Cxy\n    # numerical safe eigenvalues\n    disc = max(0.0, tr * tr / 4.0 - det)\n    l1 = tr / 2.0 + np.sqrt(disc)\n    l2 = tr / 2.0 - np.sqrt(disc)\n    if l1 <= 0:\n        return 0.0\n    ratio = l2 / (l1 + eps)\n    # elongation: 1 - (min/max) so 0 (round) to ~1 (very elongated)\n    elong = 1.0 - float(np.clip(ratio, 0.0, 1.0))\n    return float(np.clip(elong, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientations (normalized 0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    if arr.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if np.all(mag == 0):\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    # map to [0, pi) because orientation ambiguous by sign of gradient\n    theta = np.mod(theta, np.pi)\n    # weight histogram by magnitude\n    nbins = 18\n    hist, _ = np.histogram(theta.ravel(), bins=nbins, range=(0.0, np.pi), weights=mag.ravel())\n    p = hist.astype(float) / (hist.sum() + eps)\n    p = p[p > 0]\n    ent = -np.sum(p * np.log(p + eps))\n    norm = ent / (np.log(nbins) + eps)\n    return float(np.clip(norm, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with gradient magnitude above median'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    if arr.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n        mag = np.hypot(gx, gy)\n    except Exception:\n        return 0.0\n    thr = float(np.median(mag.ravel()))\n    count = float(np.count_nonzero(mag > thr))\n    frac = count / (mag.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Difference between number of non-empty rows and columns (normalized)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = np.median(a.ravel())\n    row_nonzero = np.count_nonzero(np.any(a > thr, axis=1))\n    col_nonzero = np.count_nonzero(np.any(a > thr, axis=0))\n    denom = max(1, max(row_nonzero, col_nonzero))\n    result = (row_nonzero - col_nonzero) / denom\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Excess kurtosis of intensity distribution (Fisher, can be negative)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    vals = arr.ravel()\n    if vals.size < 4:\n        return 0.0\n    mean = vals.mean()\n    sd = vals.std()\n    if sd <= 0:\n        return 0.0\n    m4 = np.mean((vals - mean) ** 4)\n    kurt = m4 / (sd ** 4) - 3.0\n    # clip to a reasonable range\n    return float(np.clip(kurt, -10.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0 for grayscale), higher => more colorful'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    R = a[..., 0].ravel()\n    G = a[..., 1].ravel()\n    B = a[..., 2].ravel()\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = np.std(rg)\n    std_yb = np.std(yb)\n    mean_rg = np.abs(np.mean(rg))\n    mean_yb = np.abs(np.mean(yb))\n    std_root = np.sqrt(std_rg * std_rg + std_yb * std_yb)\n    mean_root = np.sqrt(mean_rg * mean_rg + mean_yb * mean_yb)\n    colorfulness = std_root + 0.3 * mean_root\n    return float(max(0.0, colorfulness + eps))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels that are near-zero intensity (sparsity), scale-invariant'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 1.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 1.0\n    mn = float(flat.min())\n    mx = float(flat.max())\n    dr = mx - mn\n    if dr <= eps:\n        # constant image: consider it 'near-zero' if value near 0\n        return 1.0 if abs(mn) <= 1e-6 else 0.0\n    scaled = (flat - mn) / dr\n    thresh = 0.05\n    near_zero = float(np.count_nonzero(scaled <= thresh))\n    result = near_zero / float(flat.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Relative brightness concentration in a central circle (positive => center brighter)'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    y = np.arange(h)[:, None]\n    x = np.arange(w)[None, :]\n    r = max(1.0, min(h, w) / 4.0)\n    mask = ((y - cy) ** 2 + (x - cx) ** 2) <= (r ** 2)\n    center = a[mask]\n    overall_mean = float(a.mean())\n    if center.size == 0:\n        return 0.0\n    center_mean = float(center.mean())\n    gstd = float(a.std()) + eps\n    result = (center_mean - overall_mean) / gstd\n    return float(np.clip(result, -10.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge pixel density: fraction of pixels with gradient magnitude above local threshold'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    mean = float(mag.mean())\n    std = float(mag.std())\n    thresh = mean + std  # adapt to image\n    count = float(np.count_nonzero(mag > thresh))\n    result = count / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) using 64 bins'\n    import numpy as np\n    eps = 1e-12\n    bins = 64\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    flat = img.ravel()\n    if flat.size == 0:\n        return 0.0\n    mn = float(flat.min())\n    mx = float(flat.max())\n    if mx <= mn:\n        return 0.0\n    scaled = np.floor((flat - mn) / (mx - mn) * (bins - 1)).astype(int)\n    hist = np.bincount(scaled, minlength=bins).astype(float)\n    p = hist / (hist.sum() + eps)\n    entropy = -float(np.sum(np.where(p > 0, p * np.log(p + eps), 0.0)))\n    max_ent = float(np.log(bins))\n    result = entropy / (max_ent + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized count of bright local maxima (3x3 neighborhood), higher => more spots'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    center = a\n    # neighbors via rolls (wrap-around, fast)\n    n1 = np.roll(center, 1, axis=0)\n    n2 = np.roll(center, -1, axis=0)\n    n3 = np.roll(center, 1, axis=1)\n    n4 = np.roll(center, -1, axis=1)\n    n5 = np.roll(n1, 1, axis=1)\n    n6 = np.roll(n1, -1, axis=1)\n    n7 = np.roll(n2, 1, axis=1)\n    n8 = np.roll(n2, -1, axis=1)\n    greater = (center > n1) & (center > n2) & (center > n3) & (center > n4) & (center > n5) & (center > n6) & (center > n7) & (center > n8)\n    thresh = float(center.mean() + center.std())\n    peaks = greater & (center > thresh)\n    count = float(np.count_nonzero(peaks))\n    result = count / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical center-of-mass of intensity normalized to [-1..1] (negative => top)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0:\n        return 0.0\n    # make non-negative weights\n    weights = a - float(a.min())\n    total = float(weights.sum())\n    if total <= eps:\n        return 0.0\n    ys = np.arange(h)[:, None]\n    cog_y = float((weights * ys).sum() / total)\n    result = 2.0 * (cog_y / max(1.0, (h - 1))) - 1.0\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted anisotropy from spatial covariance (0..1), 0=isotropic'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    weights = img - float(img.min())\n    S = float(weights.sum())\n    if S <= eps:\n        return 0.0\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    x_mean = float((weights * xs).sum() / S)\n    y_mean = float((weights * ys).sum() / S)\n    dx = xs - x_mean\n    dy = ys - y_mean\n    # compute weighted covariance elements\n    cov_xx = float(((weights * (dx ** 2)).sum()) / S)\n    cov_yy = float(((weights * (dy ** 2)).sum()) / S)\n    # cross term\n    cov_xy = float(((weights * dx * dy).sum()) / S)\n    # eigenvalues of 2x2 covariance\n    tr = cov_xx + cov_yy\n    det = cov_xx * cov_yy - cov_xy * cov_xy\n    disc = max(0.0, (tr * tr) / 4.0 - det)\n    l1 = tr / 2.0 + np.sqrt(disc)\n    l2 = tr / 2.0 - np.sqrt(disc)\n    result = (float(l1) - float(l2)) / (float(l1) + float(l2) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Laplacian energy ratio: sum(|L|) / sum(|I|) (higher => more high-frequency content)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n        gyy, gyx = np.gradient(gy)\n        gxy, gxx = np.gradient(gx)\n    except Exception:\n        return 0.0\n    lap = gxx + gyy\n    num = float(np.sum(np.abs(lap)))\n    den = float(np.sum(np.abs(a))) + eps\n    result = num / den\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Diagonal symmetry correlation: normalized correlation between image and its transpose (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 1.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    try:\n        t = a.T\n    except Exception:\n        return 0.0\n    if a.shape != t.shape:\n        # resize transpose to match original shape (fast, may repeat data)\n        t = np.resize(t, a.shape)\n    a_zero = a - a.mean()\n    t_zero = t - t.mean()\n    num = float((a_zero * t_zero).sum())\n    den = float(np.sqrt((a_zero ** 2).sum() * (t_zero ** 2).sum()) + eps)\n    corr = num / den\n    return float(np.clip(abs(corr), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Laplacian zero-crossing rate (fraction of pixels adjacent to a sign change)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n        gyy, gyx = np.gradient(gy)\n        gxy, gxx = np.gradient(gx)\n    except Exception:\n        return 0.0\n    lap = gxx + gyy\n    # detect sign changes between neighbors along rows and cols\n    prod_row = lap * np.roll(lap, 1, axis=0)\n    prod_col = lap * np.roll(lap, 1, axis=1)\n    zc = (prod_row < 0) | (prod_col < 0)\n    count = float(np.count_nonzero(zc))\n    h, w = lap.shape\n    result = count / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of zero pixels (sparsity) in the image [0..1]'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    a = np.nan_to_num(arr)\n    if a.ndim == 3:\n        h, w = a.shape[0], a.shape[1]\n    else:\n        h, w = a.shape\n    total = float(h) * float(w) + eps\n    zeros = float((a[..., :1] if a.ndim == 3 else a) .ravel().__array__().size)  # placeholder to ensure shape handling\n    # simpler count_nonzero on per-pixel intensity: collapse channels first\n    if a.ndim == 3:\n        inten = a.mean(axis=2)\n    else:\n        inten = a\n    zeros = float((inten == 0).sum())\n    result = zeros / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized median absolute deviation (MAD) relative to intensity range'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    med = float(np.median(flat))\n    mad = float(np.median(np.abs(flat - med)))\n    rng = float(flat.max() - flat.min()) + eps\n    result = mad / rng\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bounding-box aspect ratio (width/height) of bright region (>median), 0 if none'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    med = float(np.median(a))\n    mask = a > med\n    if not np.any(mask):\n        return 0.0\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n    r_indices = np.where(rows)[0]\n    c_indices = np.where(cols)[0]\n    if r_indices.size == 0 or c_indices.size == 0:\n        return 0.0\n    r0, r1 = int(r_indices[0]), int(r_indices[-1])\n    c0, c1 = int(c_indices[0]), int(c_indices[-1])\n    height = float(r1 - r0 + 1)\n    width = float(c1 - c0 + 1)\n    result = width / (height + eps)\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Skewness of row mean intensities (positive => heavier tail at bottom rows)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    row_means = a.mean(axis=1)\n    m = float(row_means.mean())\n    std = float(row_means.std()) + eps\n    skew = float(np.mean((row_means - m) ** 3)) / (std ** 3 + eps)\n    return float(np.clip(skew, -10.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local 3x3 standard deviation normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute local mean and mean of squares using rolling sums (3x3)\n    def local_mean(X):\n        s = np.zeros_like(X)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s / 9.0\n    local_m = local_mean(a)\n    local_m2 = local_mean(a * a)\n    local_var = np.maximum(0.0, local_m2 - (local_m ** 2))\n    local_std = np.sqrt(local_var)\n    mean_local_std = float(np.mean(local_std))\n    gstd = float(a.std()) + eps\n    result = mean_local_std / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) using 32 bins'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).ravel().astype(float))\n    else:\n        vals = np.nan_to_num(arr.ravel().astype(float))\n    if vals.size == 0:\n        return 0.0\n    nbins = 32\n    hist, _ = np.histogram(vals, bins=nbins)\n    total = float(hist.sum())\n    if total <= 0:\n        return 0.0\n    p = hist.astype(float) / total\n    p = p[p > 0]\n    ent = -float(np.sum(p * np.log(p)))\n    max_ent = float(np.log(nbins))\n    result = ent / (max_ent + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of strong local maxima (bright peaks) normalized by image area'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    # compare to 8 neighbors using rolls\n    neighs = []\n    neighs.append(np.roll(a, 1, axis=0))\n    neighs.append(np.roll(a, -1, axis=0))\n    neighs.append(np.roll(a, 1, axis=1))\n    neighs.append(np.roll(a, -1, axis=1))\n    neighs.append(np.roll(np.roll(a, 1, axis=0), 1, axis=1))\n    neighs.append(np.roll(np.roll(a, 1, axis=0), -1, axis=1))\n    neighs.append(np.roll(np.roll(a, -1, axis=0), 1, axis=1))\n    neighs.append(np.roll(np.roll(a, -1, axis=0), -1, axis=1))\n    greater = np.ones_like(a, dtype=bool)\n    for n in neighs:\n        greater &= (a > n)\n    # suppress borders because roll wraps\n    greater[0, :] = False\n    greater[-1, :] = False\n    greater[:, 0] = False\n    greater[:, -1] = False\n    thresh = np.percentile(a, 90.0)\n    peaks = greater & (a >= thresh)\n    count = float(peaks.sum())\n    area = float(h) * float(w) + eps\n    result = count / area\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean radial gradient alignment: 1 outward, -1 inward, 0 random'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    rx = xs - cx\n    ry = ys - cy\n    rmag = np.hypot(rx, ry)\n    gmag = np.hypot(gx, gy)\n    dot = gx * rx + gy * ry\n    denom = (gmag * rmag) + eps\n    align = dot / denom\n    # consider only pixels with meaningful gradient\n    mask = gmag > np.median(gmag)\n    if not np.any(mask):\n        return 0.0\n    mean_align = float(np.mean(align[mask]))\n    return float(np.clip(mean_align, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric (Hasler & S\u00fcsstrunk) normalized to [0..1], 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(arr.astype(float))\n    R = a[:, :, 0]\n    G = a[:, :, 1]\n    B = a[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    metric = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    rng = float(a.max() - a.min()) + eps\n    result = metric / rng\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of edges located in the center region (center-edge density ratio)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.size == 0:\n        return 0.0\n    thresh = float(np.median(mag))\n    edges = mag > thresh\n    ch0 = h // 4\n    ch1 = 3 * h // 4\n    cw0 = w // 4\n    cw1 = 3 * w // 4\n    center_mask = np.zeros_like(edges, dtype=bool)\n    center_mask[ch0:ch1, cw0:cw1] = True\n    total_edges = float(edges.sum()) + eps\n    center_edges = float((edges & center_mask).sum())\n    result = center_edges / total_edges\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry score: normalized correlation between left and mirrored right halves'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, w - mid:w]\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    right_flipped = np.fliplr(right)\n    L = left.ravel() - left.mean()\n    R = right_flipped.ravel() - right_flipped.mean()\n    denom = (np.linalg.norm(L) * np.linalg.norm(R) + eps)\n    corr = float(np.dot(L, R) / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of gradient energy concentrated in the central region (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total = float(mag.sum()) + eps\n    ch, cw = max(1, h // 4), max(1, w // 4)\n    cy0, cy1 = h//2 - ch//2, h//2 + (ch - ch//2)\n    cx0, cx1 = w//2 - cw//2, w//2 + (cw - cw//2)\n    center_sum = float(mag[cy0:cy1, cx0:cx1].sum())\n    return float(np.clip(center_sum / total, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Laplacian energy (high-frequency strength relative to overall intensity)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    lap = (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) +\n           np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a)\n    energy = float(np.sum(np.abs(lap)))\n    baseline = float(np.sum(np.abs(a))) + eps\n    return float(energy / baseline)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels within 2% of the dynamic range around the median (flatness)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    amin, amax = flat.min(), flat.max()\n    dr = max(eps, amax - amin)\n    thr = 0.02 * dr\n    med = float(np.median(flat))\n    frac = float(np.sum(np.abs(flat - med) <= thr)) / float(flat.size)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Global structure-tensor anisotropy (0..1): orientation dominance measure from gradients'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    A = float(np.sum(gx * gx))\n    B = float(np.sum(gy * gy))\n    C = float(np.sum(gx * gy))\n    lam_diff = np.sqrt((A - B) ** 2 + 4.0 * (C ** 2))\n    anis = lam_diff / (A + B + eps)\n    return float(np.clip(anis, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-frequency FFT energy fraction (0..1) using central radius = min(h,w)/8'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # subtract mean to focus on variations\n    f = np.fft.fftshift(np.fft.fft2(a - a.mean()))\n    mag = np.abs(f)\n    cy, cx = h // 2, w // 2\n    ry = np.arange(h) - cy\n    rx = np.arange(w) - cx\n    Y, X = np.ogrid[:h, :w]\n    dist = np.hypot(Y - cy, X - cx)\n    radius = max(1.0, min(h, w) / 8.0)\n    mask = dist <= radius\n    low = float(mag[mask].sum())\n    total = float(mag.sum()) + eps\n    return float(np.clip(low / total, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of prominent histogram peaks (modes) using 64 bins, returned as a small float'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        flat = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        flat = np.nan_to_num(img.astype(float)).ravel()\n    if flat.size == 0:\n        return 0.0\n    amin, amax = flat.min(), flat.max()\n    if amax <= amin:\n        return 0.0\n    nb = 64\n    hist, edges = np.histogram(flat, bins=nb, range=(amin, amax))\n    # small smoothing\n    kern = np.array([1, 2, 1], dtype=float)\n    hist_s = np.convolve(hist.astype(float), kern, mode='same')\n    thr = max(hist_s.mean() * 0.1, 1.0)\n    peaks = 0\n    for i in range(1, hist_s.size - 1):\n        if hist_s[i] > hist_s[i - 1] and hist_s[i] > hist_s[i + 1] and hist_s[i] >= thr:\n            peaks += 1\n    return float(peaks)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized displacement of foreground centroid (threshold=mean) from image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = a.mean()\n    mask = a > thr\n    if not mask.any():\n        return 0.0\n    ys, xs = np.where(mask)\n    cy = ys.mean()\n    cx = xs.mean()\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    half_diag = np.hypot(h, w) / 2.0 + eps\n    return float(np.clip(dist / half_diag, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Perimeter-to-area ratio of largest mean-thresholded component (higher => more complex)'\n    import numpy as np\n    from collections import deque\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        gray = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        gray = np.nan_to_num(img.astype(float))\n    h, w = gray.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = gray.mean()\n    mask = gray > thr\n    if not mask.any():\n        return 0.0\n    visited = np.zeros_like(mask, dtype=bool)\n    max_area = 0\n    max_perim = 0\n    for y in range(h):\n        for x in range(w):\n            if mask[y, x] and not visited[y, x]:\n                area = 0\n                perim = 0\n                dq = deque()\n                dq.append((y, x))\n                visited[y, x] = True\n                while dq:\n                    cy, cx = dq.popleft()\n                    area += 1\n                    # check 4-neighbors\n                    for ny, nx in ((cy-1, cx), (cy+1, cx), (cy, cx-1), (cy, cx+1)):\n                        if ny < 0 or ny >= h or nx < 0 or nx >= w or not mask[ny, nx]:\n                            perim += 1\n                        elif not visited[ny, nx]:\n                            visited[ny, nx] = True\n                            dq.append((ny, nx))\n                if area > max_area:\n                    max_area = area\n                    max_perim = perim\n    if max_area == 0:\n        return 0.0\n    ratio = float(max_perim) / (float(max_area) + eps)\n    return float(ratio)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variance of quadrant mean intensities normalized by global std (higher => asymmetric quadrant lighting)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    hy, wy = h // 2, w // 2\n    q1 = a[:hy, :wy]\n    q2 = a[:hy, wy:]\n    q3 = a[hy:, :wy]\n    q4 = a[hy:, wy:]\n    means = []\n    for q in (q1, q2, q3, q4):\n        if q.size:\n            means.append(float(q.mean()))\n        else:\n            means.append(0.0)\n    qvar = float(np.var(np.array(means)))\n    gstd = float(np.std(a)) + eps\n    return float(qvar / gstd)\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) computed from 16-bin histogram'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    # convert to single-channel intensity\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    arr = a.ravel()\n    if arr.size == 0:\n        return 0.0\n    # histogram\n    bins = 16\n    mn, mx = float(arr.min()), float(arr.max())\n    if mx <= mn:\n        return 0.0\n    hist, _ = np.histogram(arr, bins=bins, range=(mn, mx))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p = p[p > 0.0]\n    if p.size == 0:\n        return 0.0\n    entropy = -float((p * np.log(p + eps)).sum())\n    max_entropy = float(np.log(bins))\n    result = entropy / (max_entropy + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    '180-degree rotational symmetry score (1.0 = perfectly symmetric)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    flipped = np.flipud(np.fliplr(a))\n    diff = np.abs(a - flipped)\n    rng = float(a.max() - a.min()) + eps\n    mean_diff = float(diff.mean())\n    score = 1.0 - (mean_diff / rng)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Compactness of bright region: area to perimeter proxy for mask > mean'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean())\n    mask = a > thr\n    area = float(mask.sum())\n    if area <= 0.0:\n        return 0.0\n    # approximate perimeter via XOR with shifted masks (vertical + horizontal)\n    perim_v = np.sum(mask != np.roll(mask, 1, axis=0))\n    perim_h = np.sum(mask != np.roll(mask, 1, axis=1))\n    perimeter = float(perim_v + perim_h) + eps\n    # normalized compactness: area/(perimeter * sqrt(area)) -> roughly scale-invariant\n    result = (area / perimeter) / (np.sqrt(max(1.0, area)))\n    # clip to reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Circular variance of gradient orientations (0=aligned, 1=uniformly random)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    ang = np.arctan2(gy, gx)  # angles in [-pi, pi]\n    # resultant length R of unit vectors exp(i*theta)\n    v = np.exp(1j * ang.ravel())\n    R = float(np.abs(v.mean()))\n    circ_var = float(1.0 - R)\n    return float(np.clip(circ_var, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of FFT energy in low frequencies (central quarter of radial freq)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n    except Exception:\n        return 0.0\n    mag = np.abs(F)\n    ys = np.arange(h)[:, None] - (h // 2)\n    xs = np.arange(w)[None, :] - (w // 2)\n    dist = np.hypot(ys, xs)\n    maxd = float(dist.max()) if dist.size else 1.0\n    mask_center = dist <= (0.25 * maxd)\n    total = float(mag.sum()) + eps\n    center = float(mag[mask_center].sum())\n    result = center / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0 for grayscale)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    R = a[:, :, 0]\n    G = a[:, :, 1]\n    B = a[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    metric = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    return float(max(0.0, metric))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Diagonal edge density: fraction of pixels with strong diagonal gradients'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    # diagonal responses for the two diagonal directions\n    d1 = np.abs(gx + gy)\n    d2 = np.abs(gx - gy)\n    diag = np.maximum(d1, d2)\n    thr = float(np.median(diag) + diag.std())\n    strong = diag > thr\n    frac = float(np.count_nonzero(strong)) / float(diag.size)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of local peaks (8-neighbor maxima) above mean+std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compare to 8 neighbors using rolls\n    center = a\n    neighbors = [\n        np.roll(np.roll(a, dy, axis=0), dx, axis=1)\n        for dy in (-1, 0, 1) for dx in (-1, 0, 1) if not (dy == 0 and dx == 0)\n    ]\n    is_peak = np.ones_like(a, dtype=bool)\n    for n in neighbors:\n        is_peak &= (center > n)\n    thr = float(a.mean() + a.std())\n    mask = is_peak & (center > thr)\n    count = float(np.count_nonzero(mask))\n    result = count / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border homogeneity: border std divided by global std (smaller => more homogeneous)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thickness = max(1, min(h, w) // 10)\n    mask = np.zeros_like(a, dtype=bool)\n    mask[:thickness, :] = True\n    mask[-thickness:, :] = True\n    mask[:, :thickness] = True\n    mask[:, -thickness:] = True\n    border = a[mask]\n    if border.size == 0:\n        return 0.0\n    border_std = float(border.std())\n    global_std = float(a.std()) + eps\n    result = border_std / global_std\n    return float(np.clip(result, 0.0, 50.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Global intensity entropy (normalized to [0,1])'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    # use 256 bins across data range\n    amin, amax = float(flat.min()), float(flat.max())\n    if amax <= amin:\n        return 0.0\n    hist, _ = np.histogram(flat, bins=256, range=(amin, amax))\n    probs = hist.astype(float) / (hist.sum() + eps)\n    probs = probs[probs > 0]\n    ent = -float(np.sum(probs * np.log2(probs + eps)))\n    # normalize by max entropy log2(256)=8\n    result = ent / 8.0\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local contrast: mean |pixel - local3x3_mean| normalized by global std'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # 3x3 local mean via rolling sum\n    s = np.zeros_like(a)\n    s += a\n    s += np.roll(a, 1, axis=0)\n    s += np.roll(a, -1, axis=0)\n    s += np.roll(a, 1, axis=1)\n    s += np.roll(a, -1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), -1, axis=1)\n    local_mean = s / 9.0\n    diff = np.abs(a - local_mean)\n    gstd = float(a.std()) + eps\n    result = float(np.mean(diff)) / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity slope from center: linear slope of annular means normalized by std'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    yy = np.arange(h)[:, None]\n    xx = np.arange(w)[None, :]\n    r = np.hypot(yy - cy, xx - cx)\n    rmax = float(r.max()) + eps\n    # bin into up to 10 annuli\n    nbins = min(10, max(2, int(rmax)))\n    bins = np.linspace(0.0, rmax, nbins + 1)\n    inds = np.digitize(r.ravel(), bins) - 1\n    means = []\n    centers = []\n    flat = a.ravel()\n    for i in range(nbins):\n        mask = inds == i\n        if not np.any(mask):\n            continue\n        means.append(float(flat[mask].mean()))\n        centers.append(float((bins[i] + bins[i+1]) / 2.0))\n    if len(means) < 2:\n        return 0.0\n    x = np.array(centers)\n    y = np.array(means)\n    xm = x.mean()\n    ym = y.mean()\n    denom = np.sum((x - xm) ** 2) + eps\n    slope = np.sum((x - xm) * (y - ym)) / denom\n    result = slope / (float(a.std()) + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized count of 3x3 local maxima (strict peaks) as fraction of image area'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    center = a\n    neighbors = []\n    neighbors.append(np.roll(center, 1, axis=0))\n    neighbors.append(np.roll(center, -1, axis=0))\n    neighbors.append(np.roll(center, 1, axis=1))\n    neighbors.append(np.roll(center, -1, axis=1))\n    neighbors.append(np.roll(np.roll(center, 1, axis=0), 1, axis=1))\n    neighbors.append(np.roll(np.roll(center, 1, axis=0), -1, axis=1))\n    neighbors.append(np.roll(np.roll(center, -1, axis=0), 1, axis=1))\n    neighbors.append(np.roll(np.roll(center, -1, axis=0), -1, axis=1))\n    mask = np.ones_like(center, dtype=bool)\n    for n in neighbors:\n        mask &= (center > n)\n    # zero out border to avoid wrap-around artifacts\n    mask[0, :] = False\n    mask[-1, :] = False\n    mask[:, 0] = False\n    mask[:, -1] = False\n    count = float(np.count_nonzero(mask))\n    result = count / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Laplacian energy: sum(|Laplacian|)/sum(|pixel|)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    # discrete 4-neighbor Laplacian\n    lap = (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) + np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a)\n    num = float(np.sum(np.abs(lap)))\n    den = float(np.sum(np.abs(a))) + eps\n    result = num / den\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average structure-tensor coherence (0..1) indicating directional dominance'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    Jxx = gx * gx\n    Jyy = gy * gy\n    Jxy = gx * gy\n    # local averaging 3x3\n    def local_mean(X):\n        s = np.zeros_like(X)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s / 9.0\n    Sxx = local_mean(Jxx)\n    Syy = local_mean(Jyy)\n    Sxy = local_mean(Jxy)\n    trace = Sxx + Syy\n    diff = np.sqrt(np.maximum(trace * trace / 4.0 - (Sxx * Syy - Sxy * Sxy), 0.0))\n    # eigenvalues lambda1 >= lambda2\n    lam1 = trace / 2.0 + diff\n    lam2 = trace / 2.0 - diff\n    coherence = (lam1 - lam2) / (lam1 + lam2 + eps)\n    # ignore low-signal pixels by weighting with trace\n    weight = trace / (trace.mean() + eps)\n    if np.any(weight > 0):\n        mean_coh = float(np.sum(coherence * (weight > 0)) / (np.count_nonzero(weight > 0) + eps))\n    else:\n        mean_coh = 0.0\n    result = np.clip(mean_coh, 0.0, 1.0)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity percentile spread: (P90 - P10) / (P50 + eps)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        flat = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        flat = np.nan_to_num(arr.astype(float)).ravel()\n    if flat.size == 0:\n        return 0.0\n    p10 = float(np.percentile(flat, 10))\n    p50 = float(np.percentile(flat, 50))\n    p90 = float(np.percentile(flat, 90))\n    result = (p90 - p10) / (abs(p50) + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Foreground fill ratio: fraction of pixels > mean + 0.5*std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    mean = float(a.mean())\n    std = float(a.std())\n    thresh = mean + 0.5 * std\n    result = float(np.count_nonzero(a > thresh)) / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of diagonal-edge energy vs total edge energy (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    diag1 = np.abs(gx + gy)\n    diag2 = np.abs(gx - gy)\n    diag_energy = float(np.sum(diag1 + diag2))\n    axis_energy = float(np.sum(np.abs(gx) + np.abs(gy)))\n    denom = diag_energy + axis_energy + eps\n    result = diag_energy / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Color channel imbalance: mean absolute pairwise channel difference normalized by mean intensity (0 for grayscale)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 2:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    # use first three channels if more exist\n    ch = a[..., :3]\n    # channel means\n    m = ch.mean(axis=(0, 1))\n    # mean pairwise absolute differences\n    pd = (abs(m[0] - m[1]) + abs(m[0] - m[2]) + abs(m[1] - m[2])) / 3.0\n    overall = float(ch.mean()) + eps\n    result = float(pd) / overall\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    '90-10 percentile contrast normalized by mean intensity'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    p90 = float(np.percentile(a, 90))\n    p10 = float(np.percentile(a, 10))\n    mean = float(a.mean())\n    eps = 1e-8\n    result = (p90 - p10) / (abs(mean) + eps)\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Orientation entropy of gradients (0..1, higher = more diverse edge directions)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    ang = np.arctan2(gy, gx)  # -pi..pi\n    ang = np.mod(ang, np.pi)  # map to [0, pi)\n    nbins = 16\n    hist, _ = np.histogram(ang.ravel(), bins=nbins, range=(0.0, np.pi))\n    total = float(hist.sum()) + eps\n    p = hist / total\n    # entropy normalized to [0,1]\n    ent = -float(np.sum(np.where(p > 0, p * np.log(p + eps), 0.0)))\n    max_ent = float(np.log(nbins) + eps)\n    result = ent / max_ent\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio of foreground bounding box (width/height, >=0), 0 if no foreground'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    h, w = a.shape\n    # simple foreground: pixels > mean\n    thr = float(a.mean())\n    mask = a > thr\n    if not mask.any():\n        return 0.0\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n    r_idx = np.where(rows)[0]\n    c_idx = np.where(cols)[0]\n    if r_idx.size == 0 or c_idx.size == 0:\n        return 0.0\n    top, bot = int(r_idx[0]), int(r_idx[-1])\n    left, right = int(c_idx[0]), int(c_idx[-1])\n    bw = float(right - left + 1)\n    bh = float(bot - top + 1)\n    eps = 1e-8\n    result = bw / (bh + eps)\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels brighter than the median (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    med = float(np.median(a))\n    frac = float(np.count_nonzero(a > med)) / float(a.size)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of local peaks (strict local maxima among 8-neighbors), normalized by image area'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    h, w = a.shape\n    pad = np.pad(a, 1, mode='constant', constant_values=np.min(a) - 1.0)\n    center = pad[1:-1, 1:-1]\n    # neighbors\n    n1 = pad[0:-2, 0:-2]; n2 = pad[0:-2, 1:-1]; n3 = pad[0:-2, 2:]\n    n4 = pad[1:-1, 0:-2]; n5 = pad[1:-1, 2:]\n    n6 = pad[2:, 0:-2]; n7 = pad[2:, 1:-1]; n8 = pad[2:, 2:]\n    peaks = (center > n1) & (center > n2) & (center > n3) & (center > n4) & (center > n5) & (center > n6) & (center > n7) & (center > n8)\n    count = int(np.count_nonzero(peaks))\n    area = float(max(1, h * w))\n    result = count / area\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean normalized distance of bright pixels to image center (0..1, smaller=closer)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    h, w = a.shape\n    mean = float(a.mean())\n    std = float(a.std())\n    thr = mean + 0.5 * std\n    mask = a > thr\n    if not mask.any():\n        # fallback to top 10% brightest pixels\n        flat = a.ravel()\n        k = max(1, int(0.1 * flat.size))\n        thr = float(np.partition(flat, -k)[-k])\n        mask = a >= thr\n    if not mask.any():\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    d = np.hypot(ys - cy, xs - cx)\n    maxd = np.hypot(cy, cx) + 1e-12\n    result = float(d.mean() / maxd)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Correlation slope between row index and row-wise edge energy (signed, normalized)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    h, w = a.shape\n    if h < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    row_energy = np.sum(np.abs(gx) + np.abs(gy), axis=1)\n    y = np.arange(h).astype(float)\n    # slope via covariance\n    y_mean = y.mean()\n    r_mean = row_energy.mean()\n    cov = ((y - y_mean) * (row_energy - r_mean)).mean()\n    var_y = ((y - y_mean) ** 2).mean() + eps\n    slope = cov / var_y\n    # normalize by mean energy to keep scale reasonable\n    norm = float(max(eps, row_energy.mean()))\n    result = slope / norm\n    # clip to avoid extreme values\n    return float(np.clip(result, -5.0, 5.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean color saturation (0..1), 0 for grayscale images'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 2:\n        return 0.0\n    a = np.nan_to_num(arr.astype(float))\n    # use first three channels if more\n    ch = a[..., :3]\n    mx = np.max(ch, axis=2)\n    mn = np.min(ch, axis=2)\n    # saturation per pixel: (max-min)/max or 0 if max==0\n    denom = mx.copy()\n    denom[denom == 0] = np.nan\n    sat = (mx - mn) / denom\n    sat = np.nan_to_num(sat)  # pixels with max==0 become 0\n    result = float(np.clip(sat.mean(), 0.0, 1.0))\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local contrast: average absolute difference to 4-neighbors normalized by mean intensity'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    # differences with four neighbors using shifts\n    up = np.abs(a - np.roll(a, 1, axis=0))\n    down = np.abs(a - np.roll(a, -1, axis=0))\n    left = np.abs(a - np.roll(a, 1, axis=1))\n    right = np.abs(a - np.roll(a, -1, axis=1))\n    # average of these (each pixel counted 4 times)\n    local = (up + down + left + right) / 4.0\n    mean_local = float(local.mean())\n    overall = float(np.mean(np.abs(a))) + eps\n    result = mean_local / overall\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variance of Laplacian divided by image variance (texture roughness), non-negative'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    lap = (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) +\n           np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a)\n    var_lap = float(np.var(lap))\n    var_img = float(np.var(a)) + eps\n    result = var_lap / var_img\n    return float(max(0.0, result))\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry score: normalized correlation between top half and mirrored bottom half (-1..1)'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 2:\n        return 0.0\n    mid = h // 2\n    top = img[:mid, :]\n    bot = img[-mid:, :]\n    if top.size == 0 or bot.size == 0:\n        return 0.0\n    bot_flip = np.flipud(bot)\n    A = top.ravel()\n    B = bot_flip.ravel()\n    if A.size == 0 or B.size == 0:\n        return 0.0\n    A_mean = A.mean()\n    B_mean = B.mean()\n    num = np.sum((A - A_mean) * (B - B_mean))\n    den = np.sqrt(np.sum((A - A_mean) ** 2) * np.sum((B - B_mean) ** 2)) + eps\n    result = num / den\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Foreground pixel fraction above (mean + 0.5*std), measures sparsity/density (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thresh = m + 0.5 * s\n    fg = np.count_nonzero(a > thresh)\n    result = fg / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted centroid offset normalized by image diagonal (0..1), 0=centered'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    total = float(a.sum())\n    if total <= eps:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = float((ys * a).sum()) / total\n    cx = float((xs * a).sum()) / total\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = float(np.hypot(cy - center_y, cx - center_x))\n    maxd = float(np.hypot(center_y, center_x)) + eps\n    result = dist / maxd\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity profile variance normalized by global variance (higher => rings or radial structure)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    ys, xs = np.indices(a.shape)\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    maxr = int(np.floor(r.max()))\n    if maxr <= 0:\n        return 0.0\n    radial_means = []\n    for rad in range(maxr + 1):\n        mask = (np.floor(r) == rad)\n        if np.any(mask):\n            radial_means.append(float(a[mask].mean()))\n    radial_means = np.array(radial_means)\n    if radial_means.size < 2:\n        return 0.0\n    var_radial = float(radial_means.var())\n    global_var = float(a.var()) + eps\n    result = var_radial / global_var\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation entropy (0..1), 0=single dominant orientation, 1=uniform'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total = mag.sum()\n    if total <= eps:\n        return 0.0\n    ang = np.arctan2(gy, gx)  # -pi..pi\n    bins = 16\n    hist, _ = np.histogram(ang, bins=bins, range=(-np.pi, np.pi), weights=mag)\n    probs = hist.astype(float) / (hist.sum() + eps)\n    probs = probs[probs > 0.0]\n    if probs.size == 0:\n        return 0.0\n    ent = -np.sum(probs * np.log(probs))\n    result = float(ent / (np.log(bins) + eps))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Local intensity peak density: fraction of pixels that are strict 3x3 local maxima above mean+0.5*std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thresh = m + 0.5 * s\n    pad_val = float(a.min()) - 1.0\n    b = np.pad(a, 1, mode='constant', constant_values=pad_val)\n    center = b[1:-1, 1:-1]\n    neighs = [\n        b[0:-2, 0:-2], b[0:-2, 1:-1], b[0:-2, 2:],\n        b[1:-1, 0:-2],               b[1:-1, 2:],\n        b[2:,   0:-2], b[2:,   1:-1], b[2:,   2:]\n    ]\n    greater = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        greater &= (center > n)\n    peaks = (center > thresh) & greater\n    count = float(np.count_nonzero(peaks))\n    result = count / float(h * w)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels within one standard deviation of the mean (0..1), low => high contrast'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    if s == 0.0:\n        return 1.0\n    low = m - s\n    high = m + s\n    mask = (a >= low) & (a <= high)\n    result = float(np.count_nonzero(mask)) / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric (Hasler-Suesstrunk) for RGB images; returns 0 for non-RGB'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(arr.astype(float))\n    R = a[..., 0]\n    G = a[..., 1]\n    B = a[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    colorfulness = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    return float(max(0.0, colorfulness))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-frequency energy fraction: average-pooled image energy divided by total energy (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 1 or w < 1:\n        return 0.0\n    # 3x3 box blur via roll-sum\n    s = np.zeros_like(a)\n    s += a\n    s += np.roll(a, 1, axis=0)\n    s += np.roll(a, -1, axis=0)\n    s += np.roll(a, 1, axis=1)\n    s += np.roll(a, -1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), -1, axis=1)\n    blur = s / 9.0\n    low_energy = float(np.sum(np.abs(blur)))\n    total_energy = float(np.sum(np.abs(a))) + eps\n    result = low_energy / total_energy\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical-edge centeredness (0..1), 1 => vertical edges concentrated at image center'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    gx_mag = np.abs(gx)\n    col_energy = gx_mag.sum(axis=0)\n    total = col_energy.sum()\n    if total <= eps:\n        return 0.0\n    xs = np.arange(w, dtype=float)\n    centroid = float((col_energy * xs).sum() / (total + eps))\n    center_x = (w - 1) / 2.0\n    dist = abs(centroid - center_x)\n    maxd = center_x if center_x > 0 else eps\n    dist_norm = dist / maxd\n    result = 1.0 - np.clip(dist_norm, 0.0, 1.0)\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Shannon entropy of intensity histogram (0..1 normalized)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    # use fixed number of bins\n    bins = 64\n    hist, _ = np.histogram(flat, bins=bins, density=True)\n    p = hist.astype(float)\n    p = p[p > 0]\n    if p.size == 0:\n        return 0.0\n    ent = -float(np.sum(p * np.log(p + eps)))\n    # normalize by log(bins)\n    result = ent / (np.log(bins) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center vs border contrast normalized by global std'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape if a.ndim == 2 else (0, 0)\n    if h == 0 or w == 0:\n        return 0.0\n    ch0, cw0 = h // 4, w // 4\n    center = a[ch0:3*ch0 or h, cw0:3*cw0 or w]\n    # border as rim of width at least 1\n    rim = max(1, min(h, w) // 10)\n    top = a[:rim, :]\n    bottom = a[-rim:, :]\n    left = a[rim:-rim if rim < h else None, :rim]\n    right = a[rim:-rim if rim < h else None, -rim:]\n    border_stack = [m for m in (top, bottom, left, right) if m.size]\n    if not border_stack or center.size == 0:\n        return 0.0\n    border = np.concatenate([m.ravel() for m in border_stack])\n    mean_center = float(np.mean(center))\n    mean_border = float(np.mean(border))\n    overall_std = float(np.std(a)) + eps\n    result = (mean_center - mean_border) / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Pearson correlation between intensity and distance from center in [-1..1]'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    d = np.hypot(ys - cy, xs - cx).ravel()\n    if d.size == 0:\n        return 0.0\n    d = d / (d.max() + eps)\n    vals = a.ravel().astype(float)\n    if vals.std() < eps or d.std() < eps:\n        return 0.0\n    cov = np.mean((vals - vals.mean()) * (d - d.mean()))\n    corr = cov / (vals.std() * d.std() + eps)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    '180-degree rotational symmetry score (1.0 = perfect symmetry)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    rotated = np.rot90(a, 2)\n    if a.shape != rotated.shape:\n        # fall back to center-cropped comparison\n        h, w = a.shape\n        rh, rw = rotated.shape\n        h0 = min(h, rh)\n        w0 = min(w, rw)\n        a = a[:h0, :w0]\n        rotated = rotated[:h0, :w0]\n    diff = np.abs(a - rotated)\n    denom = float(np.mean(np.abs(a)) + eps)\n    score = 1.0 - float(np.mean(diff)) / denom\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels significantly brighter than local statistics (sparse highlights)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    mu = float(flat.mean())\n    sigma = float(flat.std()) + eps\n    thr = mu + sigma\n    count = int(np.count_nonzero(flat > thr))\n    frac = float(count) / float(flat.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientations (0..1 normalized) measuring directional disorder'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    ang = np.arctan2(gy, gx).ravel()  # -pi..pi\n    # map to [0, 2pi)\n    ang = (ang + np.pi) % (2 * np.pi)\n    bins = 36\n    hist, _ = np.histogram(ang, bins=bins, range=(0.0, 2 * np.pi), density=True)\n    p = hist.astype(float)\n    p = p[p > 0]\n    if p.size == 0:\n        return 0.0\n    ent = -float(np.sum(p * np.log(p + eps)))\n    result = ent / (np.log(bins) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Peak prominence: (max - mean) / std clipped to [0..100]'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    mx = float(flat.max())\n    mu = float(flat.mean())\n    sigma = float(flat.std()) + eps\n    val = (mx - mu) / sigma\n    result = float(np.clip(val, 0.0, 100.0))\n    return result\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Rim intensity bias: mean rim intensity minus interior mean normalized by std'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    rim = max(1, min(h, w) // 12)\n    rim_mask = np.zeros_like(a, dtype=bool)\n    rim_mask[:rim, :] = True\n    rim_mask[-rim:, :] = True\n    rim_mask[:, :rim] = True\n    rim_mask[:, -rim:] = True\n    rim_vals = a[rim_mask]\n    inner_vals = a[~rim_mask]\n    if rim_vals.size == 0 or inner_vals.size == 0:\n        return 0.0\n    diff = float(rim_vals.mean() - inner_vals.mean())\n    overall_std = float(a.std()) + eps\n    result = diff / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian (second-derivative) magnitude as texture measure'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n        gyy, gyx = np.gradient(gy)\n        gxy, gxx = np.gradient(gx)\n    except Exception:\n        return 0.0\n    lap = gyy + gxx\n    result = float(np.mean(np.abs(lap)))\n    # normalize by mean absolute intensity to keep scale stable\n    denom = float(np.mean(np.abs(a)) + eps)\n    return float(result / denom)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Color-channel mean imbalance (std of channel means normalized), 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim < 3 or arr.shape[2] < 2:\n        return 0.0\n    img = np.nan_to_num(arr.astype(float))\n    ch = img.reshape(-1, img.shape[2])\n    if ch.size == 0:\n        return 0.0\n    means = ch.mean(axis=0)\n    mmean = float(means.mean())\n    std_means = float(means.std())\n    result = std_means / (abs(mmean) + eps)\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Pearson correlation between pixel intensity and distance from image center (positive => edges brighter)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys = np.arange(h) - (h - 1) / 2.0\n    xs = np.arange(w) - (w - 1) / 2.0\n    dy = ys[:, None] ** 2\n    dx = xs[None, :] ** 2\n    dist = np.sqrt(dy + dx)\n    x = dist.ravel()\n    y = arr.ravel()\n    if x.size == 0 or y.size == 0:\n        return 0.0\n    xm = x.mean()\n    ym = y.mean()\n    xstd = x.std() + eps\n    ystd = y.std() + eps\n    cov = ((x - xm) * (y - ym)).mean()\n    corr = cov / (xstd * ystd)\n    # clamp to reasonable numeric range\n    return float(np.clip(corr, -5.0, 5.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry of intensity (1 => perfectly symmetric, -1 => perfectly anti-symmetric)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:] if mid > 0 else np.empty_like(left)\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    # flip right horizontally to compare with left\n    right_flipped = np.fliplr(right)\n    # if shapes differ (odd width), crop to min width\n    minw = min(left.shape[1], right_flipped.shape[1])\n    left_c = left[:, :minw].ravel()\n    right_c = right_flipped[:, :minw].ravel()\n    if left_c.size == 0:\n        return 0.0\n    # zero-mean normalized cross-correlation\n    L = left_c - left_c.mean()\n    R = right_c - right_c.mean()\n    denom = (np.linalg.norm(L) * np.linalg.norm(R) + eps)\n    score = float(np.dot(L, R) / denom)\n    return float(np.clip(score, -1.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian normalized by global intensity std (higher => stronger fine details)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n        gyy, gyx = np.gradient(gy)\n        gxy, gxx = np.gradient(gx)\n    except Exception:\n        return 0.0\n    lap = gxx + gyy\n    mean_abs = float(np.mean(np.abs(lap)))\n    gstd = float(a.std()) + eps\n    result = mean_abs / gstd\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of non-zero pixel density in center region to non-zero density in border (>=0)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ch = max(1, h // 4)\n    cw = max(1, w // 4)\n    r0 = max(0, h//2 - ch//2)\n    r1 = min(h, r0 + ch)\n    c0 = max(0, w//2 - cw//2)\n    c1 = min(w, c0 + cw)\n    center = a[r0:r1, c0:c1]\n    border_mask = np.ones_like(a, dtype=bool)\n    border_mask[r0:r1, c0:c1] = False\n    border = a[border_mask]\n    if center.size == 0:\n        return 0.0\n    center_nonzero = float(np.count_nonzero(center)) / float(center.size + eps)\n    border_nonzero = float(np.count_nonzero(border)) / (float(border.size) + eps)\n    # if border is almost empty (very dark), return center density\n    if border_nonzero < eps:\n        return float(np.clip(center_nonzero, 0.0, 1.0))\n    result = center_nonzero / (border_nonzero + eps)\n    return float(np.clip(result, 0.0, 100.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Shannon entropy of intensity histogram normalized to [0,1]'\n    import numpy as np\n    eps = 1e-12\n    bins = 32\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        arr = np.nan_to_num(img.astype(float)).ravel()\n    if arr.size == 0:\n        return 0.0\n    # shift to positive range for histogram stability\n    amin = arr.min()\n    arr_shift = arr - amin\n    if arr_shift.max() <= 0:\n        return 0.0\n    hist, _ = np.histogram(arr_shift, bins=bins, range=(0.0, float(arr_shift.max())), density=False)\n    total = hist.sum()\n    if total <= 0:\n        return 0.0\n    p = hist.astype(float) / float(total)\n    ent = -np.sum(np.where(p > 0, p * np.log(p), 0.0))\n    norm = np.log(float(bins)) + eps\n    return float(np.clip(ent / norm, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant gradient direction as a normalized angle in [-1..1] (signed)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    s = float(mag.sum())\n    if s <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    mean_sin = float((mag * np.sin(theta)).sum()) / s\n    mean_cos = float((mag * np.cos(theta)).sum()) / s\n    angle = np.arctan2(mean_sin, mean_cos)  # -pi..pi\n    # normalize to [-1,1]\n    return float(angle / np.pi)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of mean absolute Laplacian in center region to mean absolute Laplacian in outer region'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n        gyy, gyx = np.gradient(gy)\n        gxy, gxx = np.gradient(gx)\n    except Exception:\n        return 0.0\n    lap = gxx + gyy\n    ch = max(1, h // 4)\n    cw = max(1, w // 4)\n    r0 = max(0, h//2 - ch//2)\n    r1 = min(h, r0 + ch)\n    c0 = max(0, w//2 - cw//2)\n    c1 = min(w, c0 + cw)\n    center = np.abs(lap[r0:r1, c0:c1])\n    border_mask = np.ones_like(lap, dtype=bool)\n    border_mask[r0:r1, c0:c1] = False\n    border = np.abs(lap[border_mask])\n    if center.size == 0 or border.size == 0:\n        return 0.0\n    mean_c = float(center.mean())\n    mean_b = float(border.mean())\n    result = mean_c / (mean_b + eps)\n    return float(np.clip(result, 0.0, 100.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Sharpness around the brightest pixel: local std in small window normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    idx = np.argmax(a)\n    r = int(idx // w)\n    c = int(idx % w)\n    win = 3\n    r0 = max(0, r - win)\n    r1 = min(h, r + win + 1)\n    c0 = max(0, c - win)\n    c1 = min(w, c + win + 1)\n    local = a[r0:r1, c0:c1]\n    if local.size == 0:\n        return 0.0\n    local_std = float(local.std())\n    global_std = float(a.std()) + eps\n    result = local_std / global_std\n    return float(np.clip(result, 0.0, 100.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio (>=1) of bounding box of bright region (pixels > mean+std); 0 if no bright region'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean() + a.std())\n    mask = a > thr\n    if not np.any(mask):\n        return 0.0\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n    ymin = int(np.argmax(rows))\n    ymax = int(mask.shape[0] - 1 - np.argmax(rows[::-1]))\n    xmin = int(np.argmax(cols))\n    xmax = int(mask.shape[1] - 1 - np.argmax(cols[::-1]))\n    height = max(1, ymax - ymin + 1)\n    width = max(1, xmax - xmin + 1)\n    ratio = float(width) / float(height + eps)\n    # report ratio >=1 for scale-invariant measure\n    if ratio < 1.0:\n        ratio = 1.0 / (ratio + eps)\n    return float(np.clip(ratio, 0.0, 100.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gini coefficient of intensity distribution (0 equal, 1 extremely unequal)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        vals = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(img.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    # shift to non-negative\n    v = vals - vals.min()\n    if np.allclose(v, 0.0):\n        return 0.0\n    v_sorted = np.sort(v)\n    n = v_sorted.size\n    index = np.arange(1, n + 1)\n    total = v_sorted.sum()\n    if total <= eps:\n        return 0.0\n    gini = (2.0 * np.sum(index * v_sorted) - (n + 1) * total) / (n * total + eps)\n    return float(np.clip(gini, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Pearson correlation between pixel radius from center and intensity (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    r = np.sqrt((ys - cy) ** 2 + (xs - cx) ** 2).ravel()\n    vals = a.ravel()\n    if vals.size == 0:\n        return 0.0\n    r_mean = float(r.mean())\n    v_mean = float(vals.mean())\n    r_std = float(r.std()) + eps\n    v_std = float(vals.std()) + eps\n    cov = float(((r - r_mean) * (vals - v_mean)).mean())\n    corr = cov / (r_std * v_std + eps)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels with strong gradient magnitude (edge density)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(arr)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thresh = float(mag.mean() + 0.5 * mag.std())\n    count = float(np.count_nonzero(mag > thresh))\n    total = float(h * w) + eps\n    result = count / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio of bounding box of bright region (>=1.0), 0.0 if no bright region'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mu = float(arr.mean())\n    sigma = float(arr.std())\n    thresh = mu + 0.5 * sigma\n    mask = arr > thresh\n    if not mask.any():\n        return 0.0\n    ys, xs = np.where(mask)\n    y0, y1 = ys.min(), ys.max()\n    x0, x1 = xs.min(), xs.max()\n    bh = float(y1 - y0 + 1)\n    bw = float(x1 - x0 + 1)\n    if bh <= 0 or bw <= 0:\n        return 0.0\n    aspect = bw / (bh + eps)\n    if aspect < 1.0:\n        aspect = 1.0 / (aspect + eps)\n    # cap to a reasonable range\n    result = float(np.clip(aspect, 1.0, 50.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) using 16 histogram bins'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    vals = a.ravel()\n    if vals.size == 0:\n        return 0.0\n    bins = 16\n    hist, _ = np.histogram(vals, bins=bins, density=False)\n    prob = hist.astype(float) / (hist.sum() + 1e-12)\n    prob = prob[prob > 0]\n    if prob.size == 0:\n        return 0.0\n    entropy = -float(np.sum(prob * np.log(prob + 1e-12)))\n    # normalize by max entropy = log(bins)\n    max_ent = np.log(float(bins))\n    result = entropy / (max_ent + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted centroid offset from image center normalized to [0..1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # make intensities non-negative to compute meaningful centroid\n    a_shift = a - float(a.min())\n    total = float(a_shift.sum())\n    if total <= eps:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = float((xs * a_shift).sum()) / total\n    cy = float((ys * a_shift).sum()) / total\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = np.sqrt((cx - center_x) ** 2 + (cy - center_y) ** 2)\n    max_dist = np.sqrt(center_x ** 2 + center_y ** 2) + eps\n    result = dist / max_dist\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of gradient energy to total intensity variance (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    grad_energy = float((gx * gx + gy * gy).sum())\n    var_energy = float(((a - a.mean()) ** 2).sum())\n    result = grad_energy / (var_energy + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right (vertical axis) symmetry: 1 - normalized mean abs difference (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 1.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 1.0\n    try:\n        mirror = np.fliplr(a)\n    except Exception:\n        return 0.0\n    diff = np.abs(a - mirror)\n    mean_diff = float(diff.mean())\n    mean_val = float(np.mean(np.abs(a))) + eps\n    score = 1.0 - (mean_diff / mean_val)\n    return float(np.clip(score, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean intensity of top 1% brightest pixels normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    vals = a.ravel()\n    n = vals.size\n    if n == 0:\n        return 0.0\n    k = max(1, int(0.01 * n))\n    # partial sort for efficiency\n    thresh = np.partition(vals, -k)[-k]\n    topk = vals[vals >= thresh]\n    if topk.size == 0:\n        return 0.0\n    mean_top = float(topk.mean())\n    gstd = float(vals.std()) + eps\n    result = (mean_top - float(vals.mean())) / gstd\n    return float(np.clip(result, -50.0, 50.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of rows that are nearly uniform (low row-wise std)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0:\n        return 0.0\n    row_std = a.std(axis=1)\n    thresh = float(row_std.mean()) * 0.5\n    count = float(np.count_nonzero(row_std < thresh))\n    result = count / (float(h) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Orientation-preferring edge coherence: +1=>horizontal, -1=>vertical, magnitude accounts for coherence'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # [-pi, pi]\n    # use doubled angle to treat theta and theta+pi as same orientation\n    cos2 = np.cos(2.0 * theta) * mag\n    sin2 = np.sin(2.0 * theta) * mag\n    cos_sum = float(cos2.sum())\n    sin_sum = float(sin2.sum())\n    mag_sum = float(mag.sum()) + eps\n    # orientation preference in [-1,1]; positive => horizontal-dominant, negative => vertical-dominant\n    result = cos_sum / mag_sum\n    return float(np.clip(result, -1.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of strong vertical edges (gx) relative to image size'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag_v = np.abs(gx)\n    # threshold relative to typical vertical gradient strength\n    thr = float(np.mean(mag_v) + 0.5 * np.std(mag_v))\n    count = float(np.count_nonzero(mag_v > thr))\n    denom = float(a.size)\n    result = count / (denom + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average pixel-wise left-right and top-bottom symmetry score (1=perfect symmetry)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 1 or w < 1:\n        return 0.0\n    gstd = float(img.std()) + eps\n    # left-right\n    midw = w // 2\n    if midw == 0:\n        lr_sym = 1.0\n    else:\n        left = img[:, :midw]\n        right = img[:, -midw:]\n        right_flipped = np.fliplr(right)\n        lr_diff = np.mean(np.abs(left - right_flipped))\n        lr_sym = max(0.0, 1.0 - (lr_diff / (gstd + eps)))\n    # top-bottom\n    midh = h // 2\n    if midh == 0:\n        tb_sym = 1.0\n    else:\n        top = img[:midh, :]\n        bot = img[-midh:, :]\n        bot_flipped = np.flipud(bot)\n        tb_diff = np.mean(np.abs(top - bot_flipped))\n        tb_sym = max(0.0, 1.0 - (tb_diff / (gstd + eps)))\n    result = (lr_sym + tb_sym) / 2.0\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Coarseness estimate: fraction of energy in low-frequency (box-smoothed) component'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # 3x3 box blur via rolling sum (efficient, boundary-wrap okay)\n    s = np.zeros_like(a)\n    s += a\n    s += np.roll(a, 1, axis=0)\n    s += np.roll(a, -1, axis=0)\n    s += np.roll(a, 1, axis=1)\n    s += np.roll(a, -1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), -1, axis=1)\n    low = s / 9.0\n    high = a - low\n    e_low = float((low * low).mean())\n    e_high = float((high * high).mean())\n    result = e_low / (e_low + e_high + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of isolated bright pixels above mean+2*std (isolated means no neighboring bright pixels)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    thr = float(a.mean() + 2.0 * a.std())\n    mask = (a > thr).astype(np.int32)\n    # neighbor count in 3x3 (including self)\n    s = np.zeros_like(mask)\n    s += mask\n    s += np.roll(mask, 1, axis=0)\n    s += np.roll(mask, -1, axis=0)\n    s += np.roll(mask, 1, axis=1)\n    s += np.roll(mask, -1, axis=1)\n    s += np.roll(np.roll(mask, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(mask, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(mask, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(mask, -1, axis=0), -1, axis=1)\n    isolated = (mask == 1) & (s == 1)\n    count = float(np.count_nonzero(isolated))\n    denom = float(a.size)\n    result = count / (denom + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized image aspect difference (0=square, 1=extreme aspect)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    # shape may be (h,w,...) or (h,w)\n    if arr.ndim >= 2:\n        h, w = int(arr.shape[0]), int(arr.shape[1])\n    else:\n        return 0.0\n    if h <= 0 or w <= 0:\n        return 0.0\n    result = abs(h - w) / float(max(h, w))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized entropy of intensity histogram (32 bins, 0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 32\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    a_min = float(a.min())\n    a_max = float(a.max())\n    if a_max <= a_min + eps:\n        return 0.0\n    hist, _ = np.histogram(a, bins=bins, range=(a_min, a_max))\n    prob = hist.astype(float) / (hist.sum() + eps)\n    prob = prob[prob > 0.0]\n    ent = -float((prob * np.log(prob)).sum())\n    norm = np.log(float(bins) + eps)\n    result = ent / (norm + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bias of extreme saturation: high_frac - low_frac in normalized intensity range (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    mn = float(a.min()); mx = float(a.max())\n    if mx <= mn + eps:\n        return 0.0\n    rng = mx - mn\n    low_thr = mn + 0.05 * rng\n    high_thr = mx - 0.05 * rng\n    low_frac = float(np.count_nonzero(a <= low_thr)) / (a.size + eps)\n    high_frac = float(np.count_nonzero(a >= high_thr)) / (a.size + eps)\n    result = high_frac - low_frac\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast of the brightest quadrant: (max_quadrant_mean - global_mean) / global_std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    midy, midx = h // 2, w // 2\n    q1 = a[:midy, :midx]    # top-left\n    q2 = a[:midy, midx:]    # top-right\n    q3 = a[midy:, :midx]    # bottom-left\n    q4 = a[midy:, midx:]    # bottom-right\n    means = []\n    for q in (q1, q2, q3, q4):\n        means.append(float(q.mean()) if q.size else 0.0)\n    global_mean = float(a.mean())\n    global_std = float(a.std()) + eps\n    max_q = max(means)\n    result = (max_q - global_mean) / global_std\n    # crop to reasonable range\n    return float(np.clip(result, -5.0, 5.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Correlation between radial distance from center and mean intensity of annuli (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    yy, xx = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(yy - cy, xx - cx)\n    # bin radii into up to 10 annuli\n    nbins = min(10, max(2, int(min(h, w) // 2)))\n    # use percentiles of r to ensure bins have samples\n    edges = np.percentile(r.ravel(), np.linspace(0, 100, nbins + 1))\n    means = []\n    centers = []\n    for i in range(nbins):\n        mask = (r >= edges[i]) & (r <= edges[i + 1])\n        if np.count_nonzero(mask) == 0:\n            means.append(0.0)\n        else:\n            means.append(float(img[mask].mean()))\n        centers.append(float((edges[i] + edges[i + 1]) / 2.0))\n    means = np.array(means)\n    centers = np.array(centers)\n    if np.allclose(means, means[0]) or np.allclose(centers, centers[0]):\n        return 0.0\n    # Pearson correlation\n    cm = centers - centers.mean()\n    mm = means - means.mean()\n    denom = (np.sqrt((cm * cm).sum() * (mm * mm).sum()) + eps)\n    corr = float((cm * mm).sum() / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientations weighted by magnitude (0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 16\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # [-pi, pi]\n    # map to [0, 2*pi)\n    theta = (theta + 2.0 * np.pi) % (2.0 * np.pi)\n    # weighted histogram\n    hist, _ = np.histogram(theta.ravel(), bins=bins, range=(0.0, 2.0 * np.pi), weights=mag.ravel())\n    p = hist.astype(float) / (hist.sum() + eps)\n    p = p[p > 0.0]\n    ent = -float((p * np.log(p)).sum())\n    norm = np.log(float(bins) + eps)\n    result = ent / (norm + eps)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels brighter than mean+0.1*std (simple foreground ratio)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    m = float(a.mean())\n    s = float(a.std())\n    thr = m + 0.1 * s\n    fg = float(np.count_nonzero(a > thr))\n    result = fg / (float(a.size) + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity slope from center (positive => brighter toward edge)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    y, x = np.indices((h, w))\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    r = np.hypot(y - cy, x - cx)\n    maxr = float(r.max()) + eps\n    nbins = min(12, max(3, int(maxr)))\n    bins = np.linspace(0.0, maxr, nbins + 1)\n    inds = np.digitize(r.ravel(), bins) - 1\n    bin_centers = (bins[:-1] + bins[1:]) / 2.0\n    means = []\n    centers = []\n    for i in range(nbins):\n        mask = inds == i\n        if not np.any(mask):\n            continue\n        means.append(float(a.ravel()[mask].mean()))\n        centers.append(bin_centers[i])\n    if len(means) < 2:\n        return 0.0\n    try:\n        slope = float(np.polyfit(centers, means, 1)[0])\n    except Exception:\n        return 0.0\n    intensity_range = float(a.max() - a.min()) + eps\n    # normalize by range and max radius to make comparable across sizes\n    result = slope / (intensity_range * maxr + eps)\n    return float(np.clip(result, -10.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge pixel density (fraction of pixels with strong gradient)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(np.percentile(mag, 75))\n    count = float(np.count_nonzero(mag > thr))\n    result = count / (a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientations (0..1), weighted by gradient magnitude'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy).ravel()\n    if mag.sum() <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx).ravel()  # [-pi, pi]\n    nbins = 16\n    hist, _ = np.histogram(theta, bins=nbins, range=(-np.pi, np.pi), weights=mag)\n    total = float(hist.sum()) + eps\n    p = hist.astype(float) / total\n    p = p[p > 0]\n    ent = -float(np.sum(p * np.log(p + eps)))\n    max_ent = float(np.log(nbins))\n    result = ent / (max_ent + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average positive Harris-like corner response normalized by image energy'\n    import numpy as np\n    eps = 1e-12\n    k = 0.04\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    Ixx = gx * gx\n    Iyy = gy * gy\n    Ixy = gx * gy\n    def local_mean(X):\n        s = np.zeros_like(X)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s / 9.0\n    Sxx = local_mean(Ixx)\n    Syy = local_mean(Iyy)\n    Sxy = local_mean(Ixy)\n    det = Sxx * Syy - Sxy * Sxy\n    trace = Sxx + Syy\n    R = det - k * (trace ** 2)\n    Rpos = R[R > 0]\n    if Rpos.size == 0:\n        return 0.0\n    norm = float(np.sum(np.abs(a))) + eps\n    result = float(np.mean(Rpos)) / (norm + eps)\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Horizontal symmetry correlation between left and right halves (-1..1, + means symmetric)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    half = w // 2\n    left = a[:, :half]\n    right = a[:, -half:]\n    # flip right to align with left\n    right_f = np.fliplr(right)\n    # ensure same shape\n    if left.shape != right_f.shape:\n        minc = min(left.shape[1], right_f.shape[1])\n        left = left[:, :minc]\n        right_f = right_f[:, :minc]\n    L = left.ravel()\n    Rv = right_f.ravel()\n    if L.size == 0:\n        return 0.0\n    Lm = float(L.mean())\n    Rm = float(Rv.mean())\n    Lc = L - Lm\n    Rc = Rv - Rm\n    denom = (np.sqrt((Lc ** 2).sum() * (Rc ** 2).sum()) + eps)\n    corr = float((Lc * Rc).sum() / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant intensity-mode offset from mid-range (-1..1, negative=>darker mode)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).ravel().astype(float))\n    else:\n        vals = np.nan_to_num(arr.ravel().astype(float))\n    if vals.size == 0:\n        return 0.0\n    nbins = 32\n    vmin, vmax = float(vals.min()), float(vals.max())\n    if vmax <= vmin + eps:\n        return 0.0\n    hist, edges = np.histogram(vals, bins=nbins, range=(vmin, vmax))\n    idx = int(np.argmax(hist))\n    bin_center = (edges[idx] + edges[idx + 1]) / 2.0\n    mid = (vmin + vmax) / 2.0\n    result = (bin_center - mid) / ((vmax - vmin) / 2.0 + eps)\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of bright local maxima above mean+2*std normalized by image area'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thr = m + 2.0 * s\n    # compare to 8 neighbors using rolls; then zero out border to avoid wrap-around artifacts\n    center = a\n    neighs = []\n    neighs.append(np.roll(center, 1, axis=0))\n    neighs.append(np.roll(center, -1, axis=0))\n    neighs.append(np.roll(center, 1, axis=1))\n    neighs.append(np.roll(center, -1, axis=1))\n    neighs.append(np.roll(np.roll(center, 1, axis=0), 1, axis=1))\n    neighs.append(np.roll(np.roll(center, 1, axis=0), -1, axis=1))\n    neighs.append(np.roll(np.roll(center, -1, axis=0), 1, axis=1))\n    neighs.append(np.roll(np.roll(center, -1, axis=0), -1, axis=1))\n    mask = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        mask &= (center > n)\n    mask &= (center > thr)\n    # remove border rows/cols because rolls wrap\n    mask[0, :] = False\n    mask[-1, :] = False\n    mask[:, 0] = False\n    mask[:, -1] = False\n    count = float(np.count_nonzero(mask))\n    result = count / (float(a.size) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Autocorrelation decay along horizontal shifts (positive => faster decay)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 5:\n        return 0.0\n    shifts = [1, 2, 3, 4]\n    corrs = []\n    for dx in shifts:\n        left = a[:, :-dx].ravel()\n        right = a[:, dx:].ravel()\n        if left.size == 0:\n            corrs.append(0.0)\n            continue\n        Lm = float(left.mean())\n        Rm = float(right.mean())\n        Lc = left - Lm\n        Rc = right - Rm\n        denom = (np.sqrt((Lc ** 2).sum() * (Rc ** 2).sum()) + eps)\n        corrs.append(float((Lc * Rc).sum() / denom))\n    corrs = np.array(corrs, dtype=float)\n    if not np.isfinite(corrs).any():\n        return 0.0\n    # fit slope of correlation vs shift\n    try:\n        slope = float(np.polyfit(shifts, corrs, 1)[0])\n    except Exception:\n        return 0.0\n    # normalize by corr at shift=1 magnitude\n    denom = (abs(corrs[0]) + eps)\n    result = -slope / denom\n    return float(np.clip(result, -10.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean color saturation for RGB images (0..1), 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(arr.astype(float))\n    mx = np.max(a[..., :3], axis=2)\n    mn = np.min(a[..., :3], axis=2)\n    sat = (mx - mn) / (mx + eps)\n    result = float(np.mean(sat))\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Global contrast: (max-min) normalized by global std'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    vmin = float(a.min())\n    vmax = float(a.max())\n    gstd = float(a.std()) + eps\n    result = (vmax - vmin) / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with strong gradient magnitude'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.size == 0:\n        return 0.0\n    thr = float(mag.mean()) + 0.5 * float(mag.std())\n    mask = mag > thr\n    result = float(np.count_nonzero(mask)) / float(mag.size + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity correlation: Pearson corr between radius and intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    r_flat = r.ravel()\n    v_flat = a.ravel()\n    if r_flat.size == 0 or v_flat.size == 0:\n        return 0.0\n    r_mean = float(r_flat.mean())\n    v_mean = float(v_flat.mean())\n    r_dev = r_flat - r_mean\n    v_dev = v_flat - v_mean\n    num = float((r_dev * v_dev).sum())\n    den = float(np.sqrt((r_dev ** 2).sum() * (v_dev ** 2).sum())) + eps\n    corr = num / den\n    corr = max(-1.0, min(1.0, corr))\n    return float(corr)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Quadrant contrast spread: (max quadrant mean - min quadrant mean) normalized by global std'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mh = h // 2\n    mw = w // 2\n    q1 = a[:mh, :mw]\n    q2 = a[:mh, mw:]\n    q3 = a[mh:, :mw]\n    q4 = a[mh:, mw:]\n    means = []\n    for q in (q1, q2, q3, q4):\n        means.append(float(q.mean()) if q.size else 0.0)\n    mx = max(means)\n    mn = min(means)\n    gstd = float(a.std()) + eps\n    result = (mx - mn) / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio from 2D FFT (outer frequencies / total)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute power spectrum\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n    except Exception:\n        return 0.0\n    ps = (np.abs(F) ** 2)\n    ys, xs = np.indices(ps.shape)\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    dist = np.hypot(xs - cx, ys - cy)\n    maxd = float(np.hypot(cx, cy)) + eps\n    # define high frequency as >50% of max radius\n    hf_mask = dist > (0.5 * maxd)\n    total = float(ps.sum()) + eps\n    hf = float(ps[hf_mask].sum())\n    result = hf / total\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Local peak density: fraction of strict local maxima above local threshold'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    center = a\n    # compare to 8 neighbors using roll (wraps, so we'll zero edges afterwards)\n    n1 = np.roll(center, 1, axis=0)\n    n2 = np.roll(center, -1, axis=0)\n    n3 = np.roll(center, 1, axis=1)\n    n4 = np.roll(center, -1, axis=1)\n    n5 = np.roll(n1, 1, axis=1)\n    n6 = np.roll(n1, -1, axis=1)\n    n7 = np.roll(n2, 1, axis=1)\n    n8 = np.roll(n2, -1, axis=1)\n    peak_mask = (center > n1) & (center > n2) & (center > n3) & (center > n4) & (center > n5) & (center > n6) & (center > n7) & (center > n8)\n    # remove wrapped borders (first/last row/col)\n    peak_mask[0, :] = False\n    peak_mask[-1, :] = False\n    peak_mask[:, 0] = False\n    peak_mask[:, -1] = False\n    thr = float(center.mean()) + 0.5 * float(center.std())\n    peak_mask = peak_mask & (center > thr)\n    count = float(np.count_nonzero(peak_mask))\n    area = float(h * w) + eps\n    result = count / area\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry: mean absolute difference between halves normalized by intensity range'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:]\n    if left.shape[1] != right.shape[1]:\n        # ensure same width by cropping the larger one\n        minw = min(left.shape[1], right.shape[1])\n        left = left[:, :minw]\n        right = right[:, :minw]\n    right_flipped = np.fliplr(right)\n    if left.size == 0 or right_flipped.size == 0:\n        return 0.0\n    mad = float(np.mean(np.abs(left - right_flipped)))\n    denom = float(a.max() - a.min()) + eps\n    result = mad / denom\n    result = float(np.clip(result, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variance of Laplacian normalized by squared global std (focus/texture measure)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    lap = (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) + np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a)\n    var_lap = float(np.var(lap))\n    denom = float(a.std() ** 2) + eps\n    result = var_lap / denom\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0 for non-RGB)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    # use first three channels as R,G,B\n    R = a[..., 0]\n    G = a[..., 1]\n    B = a[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    result = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of edge orientations (0..1), weighted by edge magnitude'\n    import numpy as np\n    eps = 1e-12\n    bins = 16\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # [-pi, pi]\n    # orientation modulo pi (so direction and opposite are same)\n    theta_mod = np.mod(theta, np.pi)  # [0, pi)\n    # bin indices\n    bin_idx = np.floor((theta_mod / np.pi) * bins).astype(int)\n    bin_idx = np.clip(bin_idx, 0, bins - 1)\n    hist = np.zeros(bins, dtype=float)\n    # accumulate weighted by magnitude\n    for b in range(bins):\n        hist[b] = float(mag[bin_idx == b].sum())\n    total = hist.sum() + eps\n    p = hist / total\n    p_nonzero = p[p > 0]\n    ent = -float(np.sum(p_nonzero * np.log(p_nonzero + eps)))\n    norm = ent / (np.log(bins) + eps)\n    result = float(np.clip(norm, 0.0, 1.0))\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of non-zero pixels (sparsity of content)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    # collapse channels if present\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    total = a.size\n    if total == 0:\n        return 0.0\n    nonzero = float(np.count_nonzero(a))\n    result = nonzero / float(total)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized RMS contrast: std / (mean + eps)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    mean = float(a.mean())\n    std = float(a.std())\n    result = std / (abs(mean) + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Shannon entropy of intensity histogram (bits)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    mn = float(a.min())\n    mx = float(a.max())\n    if mx <= mn:\n        return 0.0\n    # use 256 bins or fewer if tiny array\n    bins = 256 if a.size >= 256 else max(2, a.size)\n    hist, _ = np.histogram(a, bins=bins, range=(mn, mx))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p_nonzero = p[p > 0]\n    if p_nonzero.size == 0:\n        return 0.0\n    entropy = -float((p_nonzero * np.log2(p_nonzero)).sum())\n    return float(entropy)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average row-wise edge complexity (normalized sign-change rate)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2 or h == 0:\n        return 0.0\n    diffs = np.abs(np.diff(a, axis=1))\n    # baseline threshold: median of diffs\n    med = float(np.median(diffs)) + eps\n    edge_bool = diffs > (med * 0.5)  # detect significant changes\n    # count transitions per row normalized by possible transitions\n    per_row = edge_bool.sum(axis=1) / float(max(1, edge_bool.shape[1]))\n    result = float(per_row.mean())\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center-to-border brightness fraction (center mean over center+outer)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    ys = np.arange(h)[:, None] - cy\n    xs = np.arange(w)[None, :] - cx\n    R = np.hypot(ys, xs)\n    maxr = float(R.max()) if R.size else 1.0\n    if maxr <= 0:\n        return 0.0\n    center_rad = max(1.0, maxr * 0.2)\n    outer_mask = (R >= (maxr * 0.6))\n    center_mask = (R <= center_rad)\n    center_mean = float(a[center_mask].mean()) if np.any(center_mask) else 0.0\n    outer_mean = float(a[outer_mask].mean()) if np.any(outer_mask) else 0.0\n    result = center_mean / (center_mean + outer_mean + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted elongation (1 - minor/major eigenvalue), 0..1'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if a.sum() == 0 or h < 2 or w < 2:\n        return 0.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    total = float(a.sum()) + eps\n    mean_y = float((ys * a).sum() / total)\n    mean_x = float((xs * a).sum() / total)\n    yy = (ys - mean_y)\n    xx = (xs - mean_x)\n    # compute covariance elements\n    Iyy = float((a * (yy ** 2)).sum() / total)\n    Ixx = float((a * (xx ** 2)).sum() / total)\n    Ixy = float((a * (xx * yy)).sum() / total)\n    # covariance matrix eigenvalues\n    trace = Ixx + Iyy\n    det = Ixx * Iyy - Ixy * Ixy\n    tmp = max(trace * trace / 4.0 - det, 0.0)\n    diff = float(np.sqrt(tmp))\n    lam1 = trace / 2.0 + diff\n    lam2 = trace / 2.0 - diff\n    if lam1 <= 0:\n        return 0.0\n    elong = 1.0 - (lam2 + eps) / (lam1 + eps)\n    return float(np.clip(elong, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Perceptual colorfulness metric (normalized), 0..~1 for typical images'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    # use first three channels as R,G,B\n    try:\n        R = np.nan_to_num(img[..., 0].astype(float))\n        G = np.nan_to_num(img[..., 1].astype(float))\n        B = np.nan_to_num(img[..., 2].astype(float))\n    except Exception:\n        return 0.0\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    raw = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    # normalize by dynamic range estimate\n    maxval = float(np.max(img)) if img.size else 1.0\n    denom = max(maxval, 1.0)\n    result = raw / (denom + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean gradient magnitude normalized by mean intensity'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    mean_mag = float(mag.mean())\n    mean_int = float(np.mean(a))\n    result = mean_mag / (abs(mean_int) + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Rotation-180 self-correlation (circular symmetry), -1..1'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    b = np.rot90(a, 2)\n    if b.shape != a.shape:\n        # align shapes if odd/even by cropping to min dims\n        min_h = min(a.shape[0], b.shape[0])\n        min_w = min(a.shape[1], b.shape[1])\n        a = a[:min_h, :min_w]\n        b = b[:min_h, :min_w]\n    A = a.ravel()\n    B = b.ravel()\n    if A.size == 0:\n        return 0.0\n    Am = float(A.mean())\n    Bm = float(B.mean())\n    Ac = A - Am\n    Bc = B - Bm\n    denom = (np.sqrt((Ac ** 2).sum() * (Bc ** 2).sum()) + eps)\n    corr = float((Ac * Bc).sum() / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of pixels above the 95th intensity percentile (highlight sparsity)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    thr = float(np.percentile(a, 95))\n    count = float((a > thr).sum())\n    result = count / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized global intensity entropy (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    vals = a.ravel()\n    # choose moderate number of bins\n    bins = 32\n    try:\n        hist, _ = np.histogram(vals, bins=bins)\n    except Exception:\n        return 0.0\n    hist = hist.astype(float) + eps\n    p = hist / hist.sum()\n    entropy = -np.sum(p * np.log(p + eps))\n    # normalize by log(bins)\n    norm = entropy / (np.log(bins) + eps)\n    return float(np.clip(norm, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio from 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n    except Exception:\n        return 0.0\n    P = np.abs(F) ** 2\n    cy, cx = h // 2, w // 2\n    ys = np.arange(h)[:, None] - cy\n    xs = np.arange(w)[None, :] - cx\n    r = np.hypot(ys, xs)\n    # define low-frequency radius as 0.25 * max dim\n    r0 = max(1.0, 0.25 * max(h, w))\n    high_energy = P[r > r0].sum()\n    total_energy = P.sum() + eps\n    ratio = high_energy / total_energy\n    return float(np.clip(ratio, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized displacement of bright-pixel centroid from image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    N = a.size\n    if N == 0:\n        return 0.0\n    # select top 10% brightest pixels\n    k = max(1, int(0.10 * N))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    thr = np.partition(flat, -k)[-k]\n    mask = a >= thr\n    if not np.any(mask):\n        return 0.0\n    # intensity-weighted centroid among masked pixels\n    weights = a * mask\n    total_w = weights.sum() + eps\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    cy = float((weights * ys).sum() / total_w)\n    cx = float((weights * xs).sum() / total_w)\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    max_dist = np.hypot(center_y, center_x) + eps\n    norm = dist / max_dist\n    return float(np.clip(norm, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variance of radial mean intensities normalized by global variance'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    r = np.hypot(ys - cy, xs - cx)\n    # number of radial bins\n    nbins = max(4, int(min(h, w) / 2))\n    if nbins <= 1:\n        return 0.0\n    rmax = r.max()\n    if rmax <= 0:\n        return 0.0\n    bins = np.linspace(0.0, rmax, nbins + 1)\n    inds = np.digitize(r.ravel(), bins) - 1\n    means = []\n    flat = a.ravel()\n    for i in range(nbins):\n        sel = flat[inds == i]\n        if sel.size:\n            means.append(float(sel.mean()))\n    if len(means) <= 1:\n        return 0.0\n    rad_var = float(np.var(np.array(means)))\n    global_var = float(np.var(a)) + eps\n    result = rad_var / global_var\n    # keep in reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry correlation mapped to [0,1] (1 => perfect symmetry)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w < 2:\n        return 0.0\n    mid = w // 2\n    if w % 2 == 0:\n        left = a[:, :mid]\n        right = a[:, mid:]\n    else:\n        left = a[:, :mid]\n        right = a[:, mid+1:]\n    # match widths\n    minw = min(left.shape[1], right.shape[1])\n    if minw == 0:\n        return 0.0\n    L = left[:, :minw].ravel()\n    R = np.fliplr(right)[:, :minw].ravel()\n    Lm = L - L.mean()\n    Rm = R - R.mean()\n    denom = (np.sqrt((Lm**2).sum() * (Rm**2).sum()) + eps)\n    corr = float((Lm * Rm).sum() / denom)\n    # map [-1,1] to [0,1]\n    return float(np.clip((corr + 1.0) / 2.0, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density via discrete Laplacian proportion (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # Laplacian via neighbor sums: lap = (4*center - up - down - left - right)\n    center = a\n    up = np.roll(a, 1, axis=0)\n    down = np.roll(a, -1, axis=0)\n    left = np.roll(a, 1, axis=1)\n    right = np.roll(a, -1, axis=1)\n    lap = (4.0 * center - up - down - left - right)\n    mag = np.abs(lap)\n    maxv = mag.max()\n    if maxv <= eps:\n        return 0.0\n    thresh = 0.25 * maxv\n    dense = float(np.count_nonzero(mag > thresh)) / float(mag.size)\n    return float(np.clip(dense, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Top/bottom 1% contrast normalized by dynamic range (signed)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    N = flat.size\n    if N == 0:\n        return 0.0\n    k = max(1, int(0.01 * N))\n    # get means of top and bottom k\n    idx_top = np.argpartition(flat, -k)[-k:]\n    idx_bot = np.argpartition(flat, k)[:k]\n    top_mean = float(flat[idx_top].mean())\n    bot_mean = float(flat[idx_bot].mean())\n    rng = float(flat.max() - flat.min()) + eps\n    contrast = (top_mean - bot_mean) / rng\n    return float(np.clip(contrast, -10.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average row-wise periodicity score (0..1) based on strongest non-DC FFT peak'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w < 2:\n        return 0.0\n    scores = []\n    for row in a:\n        r = row - row.mean()\n        P = np.abs(np.fft.rfft(r)) ** 2\n        if P.size <= 1:\n            scores.append(0.0)\n            continue\n        # exclude DC (index 0)\n        total = P.sum() + eps\n        peak = P[1:].max()\n        scores.append(float(peak / total))\n    if len(scores) == 0:\n        return 0.0\n    return float(np.clip(np.mean(scores), 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0..1), 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    # use first three channels as R,G,B\n    img = np.nan_to_num(arr.astype(float))\n    R = img[:, :, 0].ravel()\n    G = img[:, :, 1].ravel()\n    B = img[:, :, 2].ravel()\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = np.std(rg)\n    std_yb = np.std(yb)\n    mean_rg = np.mean(rg)\n    mean_yb = np.mean(yb)\n    colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n    # normalize by typical intensity range\n    max_val = max(1.0, np.max(img) - np.min(img), eps)\n    norm = colorfulness / max_val\n    return float(np.clip(norm, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean log-gradient magnitude normalized by max log-gradient (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    g = np.hypot(gx, gy)\n    if g.size == 0:\n        return 0.0\n    logg = np.log1p(g)\n    maxlog = logg.max()\n    if maxlog <= eps:\n        return 0.0\n    meanlog = float(logg.mean())\n    result = meanlog / (maxlog + eps)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian variance (measure of local high-frequency content / focus)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    # discrete Laplacian via 4-neighbor stencil\n    lap = (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) +\n           np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a)\n    lap_var = float(np.var(lap))\n    img_var = float(np.var(a)) + eps\n    result = lap_var / img_var\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with gradient magnitude above mean+std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + mag.std())\n    count = int(np.count_nonzero(mag > thr))\n    result = float(count) / float(mag.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) using 64-bin histogram'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        arr = np.nan_to_num(img.astype(float)).ravel()\n    if arr.size == 0:\n        return 0.0\n    bins = 64\n    mn, mx = float(arr.min()), float(arr.max())\n    if mx <= mn:\n        return 0.0\n    hist, _ = np.histogram(arr, bins=bins, range=(mn, mx))\n    total = float(hist.sum()) + eps\n    p = hist.astype(float) / total\n    p_nonzero = p[p > 0]\n    if p_nonzero.size == 0:\n        return 0.0\n    entropy = -float((p_nonzero * np.log(p_nonzero + eps)).sum())\n    max_ent = float(np.log(len(hist) + eps))\n    result = entropy / (max_ent + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial brightness slope from center (negative => darker toward corners), normalized by std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    r = np.sqrt((ys - cy) ** 2 + (xs - cx) ** 2)\n    rmax = float(r.max()) + eps\n    rnorm = (r / rmax).ravel()\n    vals = a.ravel()\n    # bin radial values into 10 rings and compute mean intensity per ring\n    bins = 10\n    inds = np.floor(rnorm * bins).astype(int)\n    inds = np.clip(inds, 0, bins - 1)\n    means = []\n    radii = []\n    for b in range(bins):\n        mask = inds == b\n        if mask.sum() == 0:\n            continue\n        means.append(float(vals[mask].mean()))\n        radii.append(float((b + 0.5) / bins)\n                     )\n    if len(means) < 2:\n        return 0.0\n    radii = np.array(radii)\n    means = np.array(means)\n    # simple linear slope\n    denom = (np.sum((radii - radii.mean()) ** 2) + eps)\n    slope = float(np.sum((radii - radii.mean()) * (means - means.mean())) / denom)\n    result = slope / (float(a.std()) + eps)\n    return float(np.clip(result, -10.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Eccentricity of bright region (0..1, 0=circular, 1=elongated) using weighted second moments'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    thr = float(a.mean() + a.std())\n    mask = a > thr\n    if np.count_nonzero(mask) < 3:\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    weights = a[mask] - thr\n    weights = np.maximum(weights, 0.0)\n    wsum = float(weights.sum()) + eps\n    cy = float((weights * ys).sum()) / wsum\n    cx = float((weights * xs).sum()) / wsum\n    dy = ys - cy\n    dx = xs - cx\n    # weighted covariance\n    c00 = float((weights * dx * dx).sum()) / wsum\n    c11 = float((weights * dy * dy).sum()) / wsum\n    c01 = float((weights * dx * dy).sum()) / wsum\n    cov = np.array([[c00, c01], [c01, c11]])\n    # eigenvalues\n    try:\n        vals = np.linalg.eigvalsh(cov)\n    except Exception:\n        return 0.0\n    lam1, lam2 = float(max(vals)), float(min(vals))\n    if lam1 + lam2 <= eps:\n        return 0.0\n    ecc = (np.sqrt(lam1) - np.sqrt(lam2)) / (np.sqrt(lam1) + np.sqrt(lam2) + eps)\n    return float(np.clip(ecc, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of rows with strong vertical transitions (texture/striping indicator)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w < 2:\n        return 0.0\n    row_diff_means = np.mean(np.abs(np.diff(a, axis=1)), axis=1)\n    overall_mean = float(row_diff_means.mean()) + eps\n    # a row is \"high-transition\" if its mean diffs exceed 1.5x overall mean\n    high = row_diff_means > (1.5 * overall_mean)\n    result = float(np.count_nonzero(high)) / float(h)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Number of prominent histogram modes (smoothed 64-bin histogram), clipped to 10'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        arr = np.nan_to_num(img.astype(float)).ravel()\n    if arr.size == 0:\n        return 0.0\n    bins = 64\n    mn, mx = float(arr.min()), float(arr.max())\n    if mx <= mn:\n        return 0.0\n    hist, _ = np.histogram(arr, bins=bins, range=(mn, mx))\n    # smooth with 3-point moving average\n    k = np.array([1.0, 1.0, 1.0]) / 3.0\n    padded = np.pad(hist.astype(float), (1, 1), mode='edge')\n    smooth = np.convolve(padded, k, mode='valid')\n    avg = float(smooth.mean()) + eps\n    # count local maxima above average\n    peaks = 0\n    for i in range(1, smooth.size - 1):\n        if smooth[i] > smooth[i - 1] and smooth[i] > smooth[i + 1] and smooth[i] > avg:\n            peaks += 1\n    result = float(min(peaks, 10))\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Downsample stability: mean absolute difference to 2x down-up sample normalized by dynamic range'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    h2 = (h // 2) * 2\n    w2 = (w // 2) * 2\n    if h2 == 0 or w2 == 0:\n        return 0.0\n    ac = a[:h2, :w2]\n    # average 2x2 blocks\n    ac_rs = ac.reshape(h2 // 2, 2, w2 // 2, 2)\n    down = ac_rs.mean(axis=(1, 3))\n    # upsample by repeating\n    up = np.repeat(np.repeat(down, 2, axis=0), 2, axis=1)\n    diff = np.abs(ac - up)\n    denom = float(a.max() - a.min()) + eps\n    result = float(diff.mean()) / denom\n    return float(np.clip(result, 0.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-pixel compactness: ratio of bright pixel count to bounding-box area (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    thr = float(a.mean() + 0.5 * a.std())\n    mask = a > thr\n    count = int(np.count_nonzero(mask))\n    if count == 0:\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    y0, y1 = int(ys.min()), int(ys.max())\n    x0, x1 = int(xs.min()), int(xs.max())\n    bbox_area = float((y1 - y0 + 1) * (x1 - x0 + 1)) + eps\n    compactness = float(count) / bbox_area\n    # clip to [0,1]\n    return float(np.clip(compactness, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of image considered foreground by simple adaptive threshold (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    mean = float(a.mean())\n    std = float(a.std()) + eps\n    thresh = mean + 0.5 * std\n    mask = a > thresh\n    result = float(np.count_nonzero(mask)) / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Degree of 180-degree rotational symmetry (0..1, 1 = identical under rotation)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    # rotate 180 degrees\n    rot = np.rot90(a, 2)\n    if a.shape != rot.shape:\n        return 0.0\n    a_flat = a.ravel()\n    r_flat = rot.ravel()\n    a_mean = a_flat.mean()\n    r_mean = r_flat.mean()\n    a_c = a_flat - a_mean\n    r_c = r_flat - r_mean\n    denom = np.sqrt((a_c ** 2).sum() * (r_c ** 2).sum()) + eps\n    corr = float((a_c * r_c).sum() / denom)\n    result = float(np.clip(abs(corr), 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Relative corner density via a lightweight Harris-like response (0..1)'\n    import numpy as np\n    eps = 1e-12\n    k = 0.04\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    gy, gx = np.gradient(a)\n    A = gx * gx\n    B = gy * gy\n    C = gx * gy\n    # local sums via 3x3 neighborhood using rolls\n    def local_sum(x):\n        s = np.zeros_like(x)\n        for dy in (-1, 0, 1):\n            for dx in (-1, 0, 1):\n                s += np.roll(np.roll(x, dy, axis=0), dx, axis=1)\n        return s\n    Sxx = local_sum(A)\n    Syy = local_sum(B)\n    Sxy = local_sum(C)\n    det = Sxx * Syy - Sxy * Sxy\n    trace = Sxx + Syy\n    R = det - k * (trace ** 2)\n    # robust threshold: count proportion above small fraction of max\n    rmax = float(np.max(R))\n    if rmax <= 0:\n        return 0.0\n    thresh = rmax * 1e-3\n    count = float(np.count_nonzero(R > thresh))\n    result = count / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio relative to total image variance (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # local mean via 3x3 average using rolls\n    local = np.zeros_like(a)\n    for dy in (-1, 0, 1):\n        for dx in (-1, 0, 1):\n            local += np.roll(np.roll(a, dy, axis=0), dx, axis=1)\n    local = local / 9.0\n    high = a - local\n    energy_high = float((high ** 2).sum())\n    energy_total = float(((a - a.mean()) ** 2).sum()) + eps\n    result = energy_high / energy_total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0 for grayscale)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    arr = np.nan_to_num(img.astype(float))\n    R = arr[:, :, 0]\n    G = arr[:, :, 1]\n    B = arr[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    # Hasler and S\u00fcsstrunk colorfulness\n    result = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    return float(max(0.0, result + 0.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance of intensity-weighted centroid of bright region from image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # pick bright pixels relative to median\n    med = float(np.median(a))\n    std = float(a.std()) + eps\n    mask = a > (med + 0.25 * std)\n    if np.count_nonzero(mask) == 0:\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    weights = a[ys, xs].astype(float)\n    if weights.sum() <= 0:\n        return 0.0\n    cx = float((xs * weights).sum()) / float(weights.sum())\n    cy = float((ys * weights).sum()) / float(weights.sum())\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    maxd = np.hypot(center_x, center_y) + eps\n    result = dist / maxd\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Elongation of the main bright region estimated by PCA of pixel coordinates (0..1, 1 = very elongated)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    # threshold to select meaningful pixels\n    thresh = float(np.median(a)) + 0.5 * (float(a.std()) + eps)\n    ys, xs = np.nonzero(a > thresh)\n    if ys.size < 5:\n        return 0.0\n    vals = a[ys, xs].astype(float)\n    # weight coordinates by intensity\n    wsum = vals.sum() + eps\n    cx = (xs * vals).sum() / wsum\n    cy = (ys * vals).sum() / wsum\n    X = np.vstack(((xs - cx) * np.sqrt(vals), (ys - cy) * np.sqrt(vals)))\n    cov = np.cov(X)\n    # ensure positive semidef\n    try:\n        eig = np.linalg.eigvalsh(cov)\n    except Exception:\n        return 0.0\n    eig = np.sort(np.maximum(eig, 0.0))\n    if eig[-1] <= eps:\n        return 0.0\n    ratio = eig[-1] / (eig[0] + eps)\n    # elongation normalized to [0,1): map ratio -> 1 - (1/ratio) (0 for circle, closer to 1 for high elongation)\n    result = 1.0 - (1.0 / (ratio + 1.0))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Circular variance of gradient orientations weighted by magnitude (0..1, 0 = strongly aligned)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    total = float(mag.sum()) + eps\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    c = float((np.cos(theta) * mag).sum()) / total\n    s = float((np.sin(theta) * mag).sum()) / total\n    R = np.hypot(c, s)  # resultant length in [0,1]\n    circ_var = 1.0 - float(np.clip(R, 0.0, 1.0))\n    return float(circ_var)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variability of local contrast across a 4x4 grid (higher => more uneven texture)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    gs = 4\n    ys = np.linspace(0, h, gs + 1, dtype=int)\n    xs = np.linspace(0, w, gs + 1, dtype=int)\n    local_stds = []\n    for i in range(gs):\n        for j in range(gs):\n            block = a[ys[i]:ys[i+1], xs[j]:xs[j+1]]\n            if block.size:\n                local_stds.append(float(block.std()))\n    if len(local_stds) == 0:\n        return 0.0\n    local_stds = np.array(local_stds)\n    global_std = float(a.std()) + eps\n    # variability normalized\n    result = float(np.var(local_stds) / (global_std ** 2 + eps))\n    # clip to reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Peakiness: ratio of 99th percentile minus median to global std (higher => strong peaks)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    med = float(np.median(a))\n    p99 = float(np.percentile(a, 99))\n    std = float(a.std()) + eps\n    result = (p99 - med) / std\n    return float(np.clip(result, 0.0, 50.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry: normalized correlation with horizontally flipped image (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    flipped = np.fliplr(a)\n    num = float(np.sum(a * flipped))\n    denom = np.sqrt(float(np.sum(a * a)) * float(np.sum(flipped * flipped))) + 1e-12\n    corr = num / denom\n    # map from [-1,1] to [0,1]\n    result = (corr + 1.0) / 2.0\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity correlation: how intensity correlates with distance from center (-1..1, positive => intensity increases outward)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    yy = np.arange(h) - (h - 1) / 2.0\n    xx = np.arange(w) - (w - 1) / 2.0\n    ry = yy[:, None] ** 2\n    rx = xx[None, :] ** 2\n    dist = np.sqrt(ry + rx)\n    d = dist.ravel()\n    v = img.ravel()\n    if d.size == 0 or v.size == 0:\n        return 0.0\n    d = (d - d.mean()) / (d.std() + eps)\n    v = (v - v.mean()) / (v.std() + eps)\n    corr = float(np.sum(d * v) / (float(d.size) + eps))\n    # corr in roughly [-1,1], clamp\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy fraction: fraction of total absolute energy in (image - 3x3 box blur) (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    s = np.zeros_like(a)\n    s += a\n    s += np.roll(a, 1, axis=0)\n    s += np.roll(a, -1, axis=0)\n    s += np.roll(a, 1, axis=1)\n    s += np.roll(a, -1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), -1, axis=1)\n    blur = s / 9.0\n    high = a - blur\n    high_energy = float(np.sum(np.abs(high)))\n    total_energy = float(np.sum(np.abs(a))) + eps\n    result = high_energy / total_energy\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient-orientation entropy: normalized entropy of gradient orientations weighted by magnitude (0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 18\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # [-pi, pi]\n    # orientation modulo pi: map to [0, pi)\n    orient = (theta % np.pi)\n    # binning\n    bins_edges = np.linspace(0.0, np.pi, bins + 1)\n    inds = np.minimum(bins - 1, np.floor((orient / np.pi) * bins).astype(int))\n    hist = np.zeros(bins, dtype=float)\n    flat_inds = inds.ravel()\n    flat_mag = mag.ravel()\n    for i, m in zip(flat_inds, flat_mag):\n        hist[int(i)] += float(m)\n    p = hist / (hist.sum() + eps)\n    entropy = -float(np.sum(np.where(p > 0, p * np.log(p + eps), 0.0)))\n    max_ent = float(np.log(bins))\n    result = entropy / (max_ent + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local variance normalized by global variance (>=0)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    # local mean and mean of squares via 3x3 box\n    s = np.zeros_like(a)\n    sq = a * a\n    ssq = np.zeros_like(a)\n    s += a\n    s += np.roll(a, 1, axis=0)\n    s += np.roll(a, -1, axis=0)\n    s += np.roll(a, 1, axis=1)\n    s += np.roll(a, -1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), -1, axis=1)\n    ssq += sq\n    ssq += np.roll(sq, 1, axis=0)\n    ssq += np.roll(sq, -1, axis=0)\n    ssq += np.roll(sq, 1, axis=1)\n    ssq += np.roll(sq, -1, axis=1)\n    ssq += np.roll(np.roll(sq, 1, axis=0), 1, axis=1)\n    ssq += np.roll(np.roll(sq, 1, axis=0), -1, axis=1)\n    ssq += np.roll(np.roll(sq, -1, axis=0), 1, axis=1)\n    ssq += np.roll(np.roll(sq, -1, axis=0), -1, axis=1)\n    local_mean = s / 9.0\n    local_sq_mean = ssq / 9.0\n    local_var = np.maximum(0.0, local_sq_mean - local_mean * local_mean)\n    mean_local_var = float(np.mean(local_var))\n    global_var = float(np.var(a)) + eps\n    result = mean_local_var / global_var\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Foreground bounding-box aspect bias: (w/h - 1) / (w/h + 1) in [-1,1], positive => wider than tall'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    flat = img.ravel()\n    if flat.size == 0:\n        return 0.0\n    try:\n        thr = float(np.percentile(flat, 50.0))\n    except Exception:\n        thr = float(np.mean(flat))\n    mask = img > thr\n    if not mask.any():\n        return 0.0\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n    row_idx = np.where(rows)[0]\n    col_idx = np.where(cols)[0]\n    if row_idx.size == 0 or col_idx.size == 0:\n        return 0.0\n    h_box = int(row_idx[-1] - row_idx[0] + 1)\n    w_box = int(col_idx[-1] - col_idx[0] + 1)\n    if h_box <= 0:\n        return 0.0\n    ratio = float(w_box) / float(h_box)\n    result = (ratio - 1.0) / (ratio + 1.0 + 1e-12)\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center gradient-energy fraction: fraction of gradient magnitude energy inside central region (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    ch = max(1, h // 4)\n    cw = max(1, w // 4)\n    r0 = h//2 - ch//2\n    r1 = r0 + ch\n    c0 = w//2 - cw//2\n    c1 = c0 + cw\n    r0 = max(0, r0); r1 = min(h, r1); c0 = max(0, c0); c1 = min(w, c1)\n    center_energy = float(np.sum(mag[r0:r1, c0:c1]))\n    total_energy = float(np.sum(mag)) + eps\n    result = center_energy / total_energy\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of strong Laplacian responses: proportion of pixels with abs(laplacian) > mean+std (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    lap = (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) + np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a)\n    lap_abs = np.abs(lap)\n    thr = float(lap_abs.mean() + lap_abs.std())\n    count = int(np.count_nonzero(lap_abs > thr))\n    result = float(count) / float(lap_abs.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gini coefficient of pixel intensities (0..1, 0 equal, 1 very unequal)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    # shift to non-negative\n    mn = float(vals.min())\n    if not np.isfinite(mn):\n        return 0.0\n    vals = vals - mn\n    if vals.sum() <= eps:\n        return 0.0\n    y = np.sort(vals)\n    n = y.size\n    index = np.arange(1, n + 1, dtype=float)\n    G = (2.0 * np.sum(index * y) / (n * np.sum(y))) - (n + 1.0) / n\n    result = float(np.clip(G, 0.0, 1.0))\n    return result\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-region circularity: 4*pi*area / perimeter^2 for bright mask (0..1), higher => more circular'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    thr = float(np.mean(flat) + np.std(flat))\n    mask = a > thr\n    area = int(np.count_nonzero(mask))\n    if area == 0:\n        return 0.0\n    # perimeter: mask pixels with at least one neighbor False\n    neigh = np.zeros_like(mask, dtype=int)\n    neigh += np.roll(mask, 1, axis=0)\n    neigh += np.roll(mask, -1, axis=0)\n    neigh += np.roll(mask, 1, axis=1)\n    neigh += np.roll(mask, -1, axis=1)\n    neigh += np.roll(np.roll(mask, 1, axis=0), 1, axis=1)\n    neigh += np.roll(np.roll(mask, 1, axis=0), -1, axis=1)\n    neigh += np.roll(np.roll(mask, -1, axis=0), 1, axis=1)\n    neigh += np.roll(np.roll(mask, -1, axis=0), -1, axis=1)\n    perimeter = int(np.count_nonzero(mask & (neigh < 8)))\n    if perimeter <= 0:\n        return 0.0\n    circ = 4.0 * np.pi * float(area) / (float(perimeter) * float(perimeter) + eps)\n    return float(np.clip(circ, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge pixel density: fraction of pixels with gradient magnitude above 1.5 * median'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    med = float(np.median(mag))\n    if not np.isfinite(med):\n        med = 0.0\n    thresh = med * 1.5 + eps\n    edge_count = float((mag > thresh).sum())\n    area = float(h * w)\n    result = edge_count / (area + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial profile variance: normalized variance of mean intensities over concentric rings (0..inf)'\n    import numpy as np\n    eps = 1e-12\n    rings = 6\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    dist = np.hypot(ys - cy, xs - cx)\n    maxd = float(dist.max()) + eps\n    # assign to ring indices 0..rings-1\n    idx = np.floor((dist / maxd) * rings).astype(int)\n    idx = np.clip(idx, 0, rings - 1)\n    means = []\n    for k in range(rings):\n        sel = (idx == k)\n        if np.any(sel):\n            means.append(float(a[sel].mean()))\n        else:\n            means.append(0.0)\n    means = np.array(means, dtype=float)\n    if not np.isfinite(means).any():\n        return 0.0\n    var = float(means.var())\n    norm = float(np.mean(means) + eps)\n    result = var / norm\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Texture coarseness: ratio of fine-scale mean abs-gradient to coarse-scale mean abs-gradient'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    fine = np.mean(np.hypot(gx, gy))\n    # simple 2x2 average pooling to make coarse image\n    ph = 2\n    pw = 2\n    h2 = h - (h % ph)\n    w2 = w - (w % pw)\n    if h2 < ph or w2 < pw:\n        coarse = a.copy()\n    else:\n        a_crop = a[:h2, :w2]\n        a_pool = a_crop.reshape(h2 // ph, ph, w2 // pw, pw).mean(axis=(1, 3))\n        coarse = a_pool\n    if coarse.size < 2:\n        coarse_grad = eps\n    else:\n        try:\n            gyc, gxc = np.gradient(coarse)\n            coarse_grad = np.mean(np.hypot(gxc, gyc))\n        except Exception:\n            coarse_grad = eps\n    result = fine / (coarse_grad + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean per-pixel color saturation (0..1), 0 for grayscale; measures channel spread'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    mx = np.max(a, axis=2)\n    mn = np.min(a, axis=2)\n    sat = (mx - mn) / (mx + eps)\n    result = float(np.mean(sat))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Quadrant imbalance: normalized standard deviation of the four quadrant means'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    hm = h // 2\n    wm = w // 2\n    q1 = a[:hm, :wm]\n    q2 = a[:hm, wm:]\n    q3 = a[hm:, :wm]\n    q4 = a[hm:, wm:]\n    means = np.array([float(np.mean(x)) if x.size else 0.0 for x in (q1, q2, q3, q4)], dtype=float)\n    std_q = float(means.std())\n    gstd = float(a.std()) + eps\n    result = std_q / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-region bounding-box elongation (0..1): 0=square, 1=very elongated, based on pixels > mean+std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    thr = float(a.mean() + a.std())\n    mask = a > thr\n    if np.count_nonzero(mask) < 3:\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    y0, y1 = int(ys.min()), int(ys.max())\n    x0, x1 = int(xs.min()), int(xs.max())\n    bh = max(1, y1 - y0 + 1)\n    bw = max(1, x1 - x0 + 1)\n    aspect = float(min(bw, bh) / (max(bw, bh) + eps))\n    # elongation: 0 => square, 1 => line-like\n    result = float(1.0 - aspect)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram peakiness: fraction of histogram bins (16 bins) that are local maxima'\n    import numpy as np\n    bins = 16\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.ravel(np.nan_to_num(arr.mean(axis=2).astype(float)))\n    else:\n        vals = np.ravel(np.nan_to_num(arr.astype(float)))\n    if vals.size == 0:\n        return 0.0\n    try:\n        hist, _ = np.histogram(vals, bins=bins, range=(vals.min(), vals.max()))\n    except Exception:\n        return 0.0\n    if hist.size < 3:\n        return 0.0\n    peaks = 0\n    for i in range(hist.size):\n        left = hist[i - 1] if i - 1 >= 0 else -1\n        right = hist[i + 1] if i + 1 < hist.size else -1\n        if hist[i] > left and hist[i] > right:\n            peaks += 1\n    result = float(peaks) / float(hist.size)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-contrast fraction: fraction of pixels within 5% of dynamic range around median'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.ravel(np.nan_to_num(arr.mean(axis=2).astype(float)))\n    else:\n        vals = np.ravel(np.nan_to_num(arr.astype(float)))\n    if vals.size == 0:\n        return 0.0\n    vmin = float(vals.min())\n    vmax = float(vals.max())\n    dr = vmax - vmin\n    if dr <= 0:\n        return 1.0\n    med = float(np.median(vals))\n    tol = 0.05 * dr\n    frac = float(np.count_nonzero((vals >= med - tol) & (vals <= med + tol))) / float(vals.size)\n    return float(frac)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical stripe strength: std of column means normalized by image std (higher => vertical structure)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    col_means = np.mean(a, axis=0)\n    col_std = float(col_means.std())\n    gstd = float(a.std()) + eps\n    result = col_std / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Block-wise spatial entropy (0..1): entropy of 4x4 block mean distribution normalized by max entropy'\n    import numpy as np\n    eps = 1e-12\n    bins = 16\n    blocks_y = 4\n    blocks_x = 4\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute approximate block means by splitting ranges\n    ys = np.linspace(0, h, blocks_y + 1, dtype=int)\n    xs = np.linspace(0, w, blocks_x + 1, dtype=int)\n    block_means = []\n    for i in range(blocks_y):\n        for j in range(blocks_x):\n            r0, r1 = ys[i], ys[i + 1]\n            c0, c1 = xs[j], xs[j + 1]\n            blk = a[r0:r1, c0:c1]\n            if blk.size:\n                block_means.append(float(blk.mean()))\n            else:\n                block_means.append(0.0)\n    block_means = np.array(block_means, dtype=float)\n    if block_means.size == 0:\n        return 0.0\n    try:\n        hist, _ = np.histogram(block_means, bins=bins, range=(block_means.min(), block_means.max()))\n    except Exception:\n        return 0.0\n    p = hist.astype(float)\n    s = p.sum()\n    if s <= 0:\n        return 0.0\n    p = p / s\n    p_nonzero = p[p > 0]\n    ent = -float((p_nonzero * np.log2(p_nonzero)).sum())\n    max_ent = float(np.log2(bins))\n    result = ent / (max_ent + eps)\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry: normalized negative mean absolute difference between left and mirrored right (positive => symmetric)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, w - mid:]\n    # flip right horizontally to compare\n    right_flipped = np.fliplr(right)\n    # crop to same width if mismatch\n    minw = min(left.shape[1], right_flipped.shape[1])\n    left_c = left[:, :minw]\n    right_c = right_flipped[:, :minw]\n    if left_c.size == 0:\n        return 0.0\n    diff = np.abs(left_c - right_c).mean()\n    norm = a.std() + eps\n    result = 1.0 - (diff / norm)  # closer to 1 means more symmetric\n    return float(np.clip(result, -1.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial decay correlation: negative Pearson correlation between ring mean intensity and radius (positive => brighter center)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    ys, xs = np.indices(a.shape)\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    R = np.hypot(xs - cx, ys - cy)\n    maxr = R.max()\n    if maxr <= 0:\n        return 0.0\n    nbins = min(12, max(2, int(maxr)))\n    bins = np.linspace(0.0, maxr + eps, nbins + 1)\n    idx = np.digitize(R.ravel(), bins) - 1\n    means = []\n    radii = []\n    for i in range(nbins):\n        mask = (idx == i)\n        if not np.any(mask):\n            continue\n        means.append(float(a.ravel()[mask].mean()))\n        radii.append(float((bins[i] + bins[i+1]) / 2.0))\n    means = np.array(means)\n    radii = np.array(radii)\n    if means.size < 2 or np.allclose(means, means[0]):\n        return 0.0\n    # correlation: positive when intensity increases with radius; we invert so positive => center brighter\n    try:\n        corr = np.corrcoef(radii, means)[0, 1]\n    except Exception:\n        corr = 0.0\n    result = float(-corr)\n    return float(np.clip(result, -1.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'FFT anisotropy: normalized difference between horizontal and vertical spectral energy (positive => horizontal textures)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # compute magnitude spectrum\n    F = np.fft.fftshift(np.fft.fft2(a))\n    mag = np.abs(F)\n    cy = h // 2\n    cx = w // 2\n    # energy concentrated along central row vs central column\n    row_energy = float(mag[cy, :].sum())\n    col_energy = float(mag[:, cx].sum())\n    total = float(mag.sum()) + eps\n    result = (row_energy - col_energy) / total\n    return float(np.clip(result, -1.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Block coarseness: ratio of coarse-block mean variance to fine-block mean variance (higher => coarser texture)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    min_dim = min(h, w)\n    if min_dim < 2:\n        return 0.0\n    # coarse and fine scales\n    coarse_bs = max(2, min_dim // 8)\n    fine_bs = max(1, coarse_bs // 4)\n    def block_means(X, bs):\n        if bs <= 1:\n            return X.ravel()\n        H = (X.shape[0] // bs) * bs\n        W = (X.shape[1] // bs) * bs\n        if H == 0 or W == 0:\n            return X.ravel()\n        Xc = X[:H, :W].reshape((H//bs, bs, W//bs, bs))\n        bm = Xc.mean(axis=(1,3)).ravel()\n        return bm\n    bm_coarse = block_means(a, coarse_bs)\n    bm_fine = block_means(a, fine_bs)\n    var_coarse = float(bm_coarse.var())\n    var_fine = float(bm_fine.var()) + eps\n    result = var_coarse / var_fine\n    # clip to reasonable range\n    return float(np.clip(result, 0.0, 100.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Local peak count: proportion of strict local maxima (8-neighborhood) after padding'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # pad with very small value so edges are handled (no wrap)\n    pad = np.pad(a, 1, mode='constant', constant_values=-np.inf)\n    center = pad[1:-1, 1:-1]\n    neighs = [\n        pad[:-2, :-2], pad[:-2, 1:-1], pad[:-2, 2:],\n        pad[1:-1, :-2],               pad[1:-1, 2:],\n        pad[2:, :-2],  pad[2:, 1:-1], pad[2:, 2:]\n    ]\n    is_peak = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        is_peak &= (center > n)\n    count = float(np.count_nonzero(is_peak))\n    result = count / (float(center.size) + eps)\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'IQR contrast normalized by global std (interquartile range divided by std)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    vals = arr.ravel()\n    if vals.size == 0:\n        return 0.0\n    p75 = float(np.percentile(vals, 75))\n    p25 = float(np.percentile(vals, 25))\n    iqr = p75 - p25\n    std = float(vals.std()) + eps\n    result = iqr / std\n    return float(np.clip(result, 0.0, 100.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge orientation entropy (0..1): entropy of gradient orientations normalized by log(bins)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    ori = np.arctan2(gy, gx)  # -pi..pi\n    # map to 0..pi to ignore direction (180-degree symmetry)\n    ori = np.mod(ori, np.pi)\n    bins = 16\n    hist, _ = np.histogram(ori.ravel(), bins=bins, range=(0.0, np.pi))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p = p[p > 0]\n    ent = -np.sum(p * np.log(p + eps))\n    norm = ent / (np.log(bins) + eps)\n    return float(np.clip(norm, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Corner consistency: normalized std of the four small corner patch means (lower => uniform background)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ch = max(1, h // 8)\n    cw = max(1, w // 8)\n    c1 = a[:ch, :cw]\n    c2 = a[:ch, -cw:]\n    c3 = a[-ch:, :cw]\n    c4 = a[-ch:, -cw:]\n    means = np.array([float(x.mean()) if x.size else 0.0 for x in (c1, c2, c3, c4)])\n    overall_std = float(a.std()) + eps\n    result = float(means.std()) / overall_std\n    return float(np.clip(result, 0.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity moment eccentricity (0..1): anisotropy of intensity distribution (1 = highly elongated)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices(a.shape)\n    # make non-negative weights\n    weights = a - a.min()\n    total = float(weights.sum())\n    if total == 0.0:\n        return 0.0\n    cx = float((weights * xs).sum()) / total\n    cy = float((weights * ys).sum()) / total\n    xcen = xs - cx\n    ycen = ys - cy\n    mxx = float((weights * (xcen**2)).sum()) / total\n    myy = float((weights * (ycen**2)).sum()) / total\n    mxy = float((weights * (xcen * ycen)).sum()) / total\n    trace = mxx + myy\n    det = mxx * myy - mxy * mxy\n    diff = np.sqrt(max(0.0, (trace * trace) / 4.0 - det))\n    lam1 = trace / 2.0 + diff\n    lam2 = trace / 2.0 - diff\n    ecc = (lam1 - lam2) / (lam1 + lam2 + eps)\n    return float(np.clip(ecc, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian normalized by image std (texture roughness)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    pa = np.pad(a, 1, mode='reflect')\n    center = pa[1:-1, 1:-1]\n    up = pa[:-2, 1:-1]\n    down = pa[2:, 1:-1]\n    left = pa[1:-1, :-2]\n    right = pa[1:-1, 2:]\n    lap = (up + down + left + right) - 4.0 * center\n    mean_abs_lap = float(np.mean(np.abs(lap)))\n    norm = float(a.std()) + eps\n    result = mean_abs_lap / norm\n    return float(np.clip(result, 0.0, 100.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy fraction: ratio of local high-pass energy to total energy (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h * w == 0:\n        return 0.0\n    # local 3x3 mean via rolling sum (fast, no convolution)\n    s = np.zeros_like(a)\n    s += a\n    s += np.roll(a, 1, axis=0)\n    s += np.roll(a, -1, axis=0)\n    s += np.roll(a, 1, axis=1)\n    s += np.roll(a, -1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), -1, axis=1)\n    local_mean = s / 9.0\n    highpass = a - local_mean\n    hf_energy = float(np.sum(np.abs(highpass)))\n    total_energy = float(np.sum(np.abs(a - a.mean()))) + eps\n    result = hf_energy / total_energy\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial gradient alignment: how strongly gradients point toward/away from image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy) + eps\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    rx = xs - cx\n    ry = ys - cy\n    rnorm = np.hypot(rx, ry) + eps\n    # unit radial vector\n    ux = rx / rnorm\n    uy = ry / rnorm\n    # dot product between gradient direction and radial direction\n    dot = (gx * ux + gy * uy) / mag\n    # weight by magnitude and absolute alignment (inward/outward both count)\n    weighted = np.abs(dot) * mag\n    result = float(weighted.sum()) / float(mag.sum() + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Saturation ratio: fraction of pixels near the image maximum intensity (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    vmin = float(a.min())\n    vmax = float(a.max())\n    if vmax - vmin < eps:\n        return 0.0\n    thresh = vmin + 0.95 * (vmax - vmin)\n    sat_count = float(np.count_nonzero(a >= thresh))\n    total = float(a.size)\n    result = sat_count / max(eps, total)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation entropy (0..1) measuring directional complexity'\n    import numpy as np\n    eps = 1e-12\n    bins = 16\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    # histogram weighted by magnitude\n    hist, _ = np.histogram(theta, bins=bins, range=(-np.pi, np.pi), weights=mag)\n    p = hist / (hist.sum() + eps)\n    # entropy normalized by log(bins)\n    ent = -np.sum(np.where(p > 0, p * np.log(p), 0.0))\n    result = ent / (np.log(bins) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Central cross edge strength: mean gradient magnitude on middle row and column normalized by global mean'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    mid_r = h // 2\n    mid_c = w // 2\n    # take a 3-pixel wide cross if possible\n    r_inds = slice(max(0, mid_r - 1), min(h, mid_r + 2))\n    c_inds = slice(max(0, mid_c - 1), min(w, mid_c + 2))\n    cross_mask = np.zeros_like(mag, dtype=bool)\n    cross_mask[r_inds, :] = True\n    cross_mask[:, c_inds] = True\n    cross_mean = float(np.mean(mag[cross_mask])) if np.any(cross_mask) else 0.0\n    global_mean = float(np.mean(mag)) + eps\n    result = cross_mean / global_mean\n    # typical range might exceed 1; clip to reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border-to-interior variance ratio (border variance / interior variance)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    bw = max(1, min(h, w) // 8)\n    top = a[:bw, :]\n    bot = a[-bw:, :]\n    left = a[:, :bw]\n    right = a[:, -bw:]\n    border = np.concatenate([top.ravel(), bot.ravel(), left.ravel(), right.ravel()])\n    interior = a[bw:-bw, bw:-bw]\n    border_var = float(np.var(border)) if border.size else 0.0\n    interior_var = float(np.var(interior)) if interior.size else 0.0\n    if interior_var <= eps:\n        result = float(border_var / (interior_var + eps))\n    else:\n        result = float(border_var / interior_var)\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-region aspect ratio of thresholded area (1 => square, 0 => very elongated or absent)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h * w == 0:\n        return 0.0\n    mu = float(np.mean(a))\n    sigma = float(np.std(a))\n    thresh = mu + 0.5 * sigma\n    mask = a > thresh\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.where(mask)\n    minr, maxr = ys.min(), ys.max()\n    minc, maxc = xs.min(), xs.max()\n    bh = maxr - minr + 1\n    bw = maxc - minc + 1\n    if bh <= 0 or bw <= 0:\n        return 0.0\n    ar = float(min(bw, bh)) / float(max(bw, bh))\n    return float(np.clip(ar, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of high-contrast patches in a 4x4 grid (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    gs = 4\n    ys = np.linspace(0, h, gs + 1, dtype=int)\n    xs = np.linspace(0, w, gs + 1, dtype=int)\n    global_std = float(a.std()) + eps\n    high_count = 0\n    total = 0\n    for i in range(gs):\n        for j in range(gs):\n            block = a[ys[i]:ys[i+1], xs[j]:xs[j+1]]\n            if block.size == 0:\n                continue\n            total += 1\n            if float(block.std()) > 1.5 * global_std:\n                high_count += 1\n    if total == 0:\n        return 0.0\n    result = float(high_count) / float(total)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of bright local maxima per area: normalized number of local peaks (>neighbors and > mean+0.5*std)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mu = float(a.mean())\n    sigma = float(a.std())\n    thresh = mu + 0.5 * sigma\n    # local max: greater than 8 neighbors\n    center = a\n    neighs = []\n    neighs.append(np.roll(a, 1, axis=0))\n    neighs.append(np.roll(a, -1, axis=0))\n    neighs.append(np.roll(a, 1, axis=1))\n    neighs.append(np.roll(a, -1, axis=1))\n    neighs.append(np.roll(np.roll(a, 1, axis=0), 1, axis=1))\n    neighs.append(np.roll(np.roll(a, 1, axis=0), -1, axis=1))\n    neighs.append(np.roll(np.roll(a, -1, axis=0), 1, axis=1))\n    neighs.append(np.roll(np.roll(a, -1, axis=0), -1, axis=1))\n    greater = np.ones_like(a, dtype=bool)\n    for n in neighs:\n        greater &= (center > n)\n    peaks = greater & (center > thresh)\n    # suppress border wrap artifacts: zero out rows/cols that were rolled from opposite side\n    peaks[0, :] = peaks[-1, :] = peaks[:, 0] = peaks[:, -1] = False\n    count = float(np.count_nonzero(peaks))\n    area = float(h * w)\n    result = count / (area + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness index (Hasler & Suesstrunk style), normalized by mean intensity (0 for grayscale)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    # use first three channels\n    R = a[..., 0]\n    G = a[..., 1]\n    B = a[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(abs(rg.mean()))\n    mean_yb = float(abs(yb.mean()))\n    colorfulness = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    overall_mean = float(a.mean()) + eps\n    result = colorfulness / overall_mean\n    return float(np.clip(result, 0.0, 10.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1), higher => more diverse intensities'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    # use grayscale projection for color images\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    vals = a.ravel()\n    # choose number of bins but not more than unique values and at least 2\n    try:\n        uniq = np.unique(vals)\n        nb = min(256, max(2, uniq.size))\n    except Exception:\n        nb = 256\n    hist, _ = np.histogram(vals, bins=nb, range=(vals.min(), vals.max()))\n    probs = hist.astype(float) / (hist.sum() + 1e-12)\n    probs = probs[probs > 0]\n    if probs.size == 0:\n        return 0.0\n    ent = -np.sum(probs * np.log(probs))\n    max_ent = np.log(probs.size)\n    result = 0.0 if max_ent <= 0 else ent / max_ent\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation concentration (0..1), 1 => most gradients align to one direction'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    mag_sum = mag.sum()\n    if mag_sum == 0:\n        return 0.0\n    ang = np.arctan2(gy, gx)\n    # mean resultant length weighted by magnitude\n    cx = np.cos(ang)\n    sx = np.sin(ang)\n    rcx = (mag * cx).sum() / (mag_sum + 1e-12)\n    rsx = (mag * sx).sum() / (mag_sum + 1e-12)\n    R = np.hypot(rcx, rsx)\n    result = float(np.clip(R, 0.0, 1.0))\n    return result\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels much brighter than average (above mean + 1*std)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thresh = m + s\n    count = float((a > thresh).sum())\n    result = count / (float(a.size) + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio of non-zero-pixel bounding box mapped to [0,1] (0.5=>square or empty)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.5\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    nz = np.nonzero(a)\n    if len(nz[0]) == 0:\n        return 0.5\n    rmin, rmax = nz[0].min(), nz[0].max()\n    cmin, cmax = nz[1].min(), nz[1].max()\n    h = float(max(1, rmax - rmin + 1))\n    w = float(max(1, cmax - cmin + 1))\n    ratio = w / h\n    # map ratio to [0,1] by using r' = ratio if ratio<=1 else 1/ratio, then scale to (0,1]\n    rprime = ratio if ratio <= 1.0 else 1.0 / ratio\n    # rprime in (0,1], map to [0,1] linearly (1 => 1, small => near 0)\n    result = float(np.clip(rprime, 0.0, 1.0))\n    return result\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian normalized by mean absolute intensity (higher => more detail)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    # discrete Laplacian via 4-neighbor: lap = 4*center - sum(neighbors)\n    center = a\n    up = np.roll(a, -1, axis=0)\n    down = np.roll(a, 1, axis=0)\n    left = np.roll(a, -1, axis=1)\n    right = np.roll(a, 1, axis=1)\n    lap = (4.0 * center) - (up + down + left + right)\n    mad = float(np.mean(np.abs(lap)))\n    mean_abs = float(np.mean(np.abs(a))) + 1e-12\n    result = mad / mean_abs\n    # reasonable clipping\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Approximate corner density (0..1): fraction of strong Harris-like corners'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    A = gx * gx\n    B = gx * gy\n    C = gy * gy\n    # local sum over 3x3 via shifts\n    shifts = [(0,0),(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n    As = sum(np.roll(np.roll(A, r, axis=0), c, axis=1) for r,c in shifts)\n    Bs = sum(np.roll(np.roll(B, r, axis=0), c, axis=1) for r,c in shifts)\n    Cs = sum(np.roll(np.roll(C, r, axis=0), c, axis=1) for r,c in shifts)\n    k = 0.04\n    R = (As * Cs) - (Bs * Bs) - k * ((As + Cs) ** 2)\n    # threshold relative to robust statistics\n    thr = np.median(R) + 0.5 * (np.std(R) + 1e-12)\n    corners = (R > thr)\n    result = float(corners.sum()) / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness (higher => more colorful). Returns 0 for grayscale inputs'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    # assume channels in order R,G,B\n    R = a[..., 0]\n    G = a[..., 1]\n    B = a[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n    # normalize by typical intensity scale to keep values reasonable\n    scale = (np.mean(a) + 1e-12)\n    result = colorfulness / scale\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge pixel density using gradient magnitude threshold (0..1)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    m = mag.mean()\n    s = mag.std()\n    thr = m + 0.5 * s\n    edges = (mag > thr)\n    result = float(edges.sum()) / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized offset of intensity centroid from image center (0..1), 1 => far from center'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    ys, xs = np.indices((h, w))\n    weights = a.copy()\n    total = float(weights.sum())\n    if total == 0:\n        return 0.0\n    cy = float((ys * weights).sum()) / total\n    cx = float((xs * weights).sum()) / total\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    maxd = np.hypot(center_x, center_y) + 1e-12\n    result = dist / maxd\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram peakiness: how much the largest intensity bin dominates (0..1)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    vals = a.ravel()\n    if vals.size == 0:\n        return 0.0\n    # choose bins but not too many\n    nb = min(128, max(8, int(np.sqrt(vals.size))))\n    hist, _ = np.histogram(vals, bins=nb, range=(vals.min(), vals.max()))\n    mean_bin = float(hist.mean()) + 1e-12\n    peak = float(hist.max())\n    # peakiness ratio then map to 0..1 via peak/(peak+mean)\n    score = peak / (peak + mean_bin)\n    return float(np.clip(score, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance of intensity center-of-mass from image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    total = arr.sum() + eps\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    com_x = float((arr * xs).sum() / total)\n    com_y = float((arr * ys).sum() / total)\n    dist = np.hypot(com_x - cx, com_y - cy)\n    norm = np.hypot(w, h) / 2.0 + eps\n    result = dist / norm\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Robust contrast: interquartile range divided by image std (clipped)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = arr.mean(axis=2)\n    a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    p25 = float(np.percentile(a, 25))\n    p75 = float(np.percentile(a, 75))\n    iqr = p75 - p25\n    std = float(a.std()) + eps\n    result = iqr / std\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Solidity: fraction of foreground pixels within their bounding box (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(np.median(a))\n    mask = a > thr\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.where(mask)\n    ymin, ymax = ys.min(), ys.max()\n    xmin, xmax = xs.min(), xs.max()\n    bbox_area = float((ymax - ymin + 1) * (xmax - xmin + 1)) + eps\n    fill = float(mask.sum()) / bbox_area\n    return float(np.clip(fill, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation coherence: magnitude of vector-sum of gradients (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    sum_gx = float(gx.sum())\n    sum_gy = float(gy.sum())\n    num = np.hypot(sum_gx, sum_gy)\n    denom = float(np.hypot(gx, gy).sum()) + eps\n    result = num / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-frequency energy ratio from 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute power spectrum\n    F = np.fft.fftshift(np.fft.fft2(a))\n    power = (np.abs(F) ** 2)\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    ys, xs = np.indices((h, w))\n    r = np.hypot(xs - cx, ys - cy)\n    cutoff = max(1.0, min(h, w) / 8.0)\n    low_mask = r <= cutoff\n    total = float(power.sum()) + eps\n    low = float(power[low_mask].sum())\n    result = low / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy based on 32-bin histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    a = np.nan_to_num(img.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    bins = 32\n    hist, _ = np.histogram(a, bins=bins)\n    total = hist.sum()\n    if total == 0:\n        return 0.0\n    p = hist.astype(float) / float(total)\n    ent = -np.sum(np.where(p > 0, p * np.log2(p + eps), 0.0))\n    max_ent = np.log2(bins) + eps\n    result = ent / max_ent\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Corner-like density via high Laplacian magnitude fraction (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n        gyy, _ = np.gradient(gy)\n        _, gxx = np.gradient(gx)\n        lap = gxx + gyy\n    except Exception:\n        return 0.0\n    mag = np.abs(lap).ravel()\n    if mag.size == 0:\n        return 0.0\n    thr = float(np.percentile(mag, 90))\n    high = mag > thr\n    result = float(high.sum()) / float(mag.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry: correlation between top and mirrored bottom halves (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 2:\n        return 1.0\n    half = h // 2\n    top = arr[:half, :]\n    if h % 2 == 0:\n        bottom = arr[half:, :]\n    else:\n        bottom = arr[half+1:, :]\n    # make same shape\n    minh = min(top.shape[0], bottom.shape[0])\n    top = top[:minh, :].ravel()\n    bottom = bottom[:minh, :][::-1, :].ravel()\n    if top.size == 0:\n        return 1.0\n    ta = top - top.mean()\n    ba = bottom - bottom.mean()\n    num = np.sum(ta * ba)\n    den = np.sqrt((ta*ta).sum() * (ba*ba).sum()) + eps\n    corr = float(num / den)\n    return float(np.clip(abs(corr), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized centroid distance between bright and dark regions (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mean = float(a.mean())\n    std = float(a.std()) + eps\n    bright_thr = mean + 0.25 * std\n    dark_thr = mean - 0.25 * std\n    ys, xs = np.indices((h, w))\n    bright_mask = a > bright_thr\n    dark_mask = a < dark_thr\n    if not np.any(bright_mask) or not np.any(dark_mask):\n        return 0.0\n    bsum = float(a[bright_mask].sum()) + eps\n    dsum = float((mean - a[dark_mask]).sum()) + eps  # weight dark by depth below mean\n    bx = float((a * bright_mask * xs).sum()) / bsum\n    by = float((a * bright_mask * ys).sum()) / bsum\n    dx = float(((mean - a) * dark_mask * xs).sum()) / dsum\n    dy = float(((mean - a) * dark_mask * ys).sum()) / dsum\n    dist = np.hypot(bx - dx, by - dy)\n    norm = np.hypot(w, h) / 2.0 + eps\n    result = dist / norm\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity variation: variance of ring means normalized by overall variance (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    maxr = float(r.max()) + eps\n    bins = max(4, int(min(h, w) // 4))\n    edges = np.linspace(0.0, maxr, bins + 1)\n    means = []\n    for i in range(bins):\n        mask = (r >= edges[i]) & (r < edges[i+1])\n        if np.any(mask):\n            means.append(float(a[mask].mean()))\n    if len(means) < 2:\n        return 0.0\n    means = np.array(means)\n    var_means = float(means.var())\n    overall_var = float(a.var()) + eps\n    result = var_means / overall_var\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized aspect ratio (width/height) clipped to [0..5]'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    try:\n        h, w = img.shape[:2]\n    except Exception:\n        return 0.0\n    if h <= 0:\n        return 0.0\n    ar = float(w) / float(h)\n    result = float(np.clip(ar, 0.0, 5.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels near intensity extremes (within 5% of range)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = img.mean(axis=2).astype(float)\n    else:\n        arr = img.astype(float)\n    vmin = float(arr.min())\n    vmax = float(arr.max())\n    if vmax == vmin:\n        return 0.0\n    thr = 0.05 * (vmax - vmin)\n    high = np.count_nonzero(arr >= (vmax - thr))\n    low = np.count_nonzero(arr <= (vmin + thr))\n    count = int(high + low)\n    frac = float(count) / float(arr.size)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Shannon entropy of the intensity histogram (bits, 0 for constant images)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = img.mean(axis=2).astype(float).ravel()\n    else:\n        arr = img.astype(float).ravel()\n    if arr.size == 0:\n        return 0.0\n    vmin = arr.min()\n    vmax = arr.max()\n    if vmax == vmin:\n        return 0.0\n    bins = 256\n    try:\n        hist, _ = np.histogram(arr, bins=bins, range=(vmin, vmax))\n    except Exception:\n        hist, _ = np.histogram(arr, bins=bins)\n    p = hist.astype(float)\n    s = p.sum()\n    if s <= 0:\n        return 0.0\n    p = p / s\n    p = p[p > 0.0]\n    entropy = -float(np.sum(p * np.log2(p)))\n    return float(entropy)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right mirror symmetry correlation mapped to [0..1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = img.mean(axis=2).astype(float)\n    else:\n        a = img.astype(float)\n    h, w = a.shape\n    if w < 2 or h < 1:\n        return 0.0\n    left = a[:, : (w // 2)]\n    right = a[:, (w - (w // 2)):]\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    right_flipped = np.fliplr(right)\n    # ensure same shape\n    if left.shape != right_flipped.shape:\n        # crop larger to match smaller\n        mh = min(left.shape[0], right_flipped.shape[0])\n        mw = min(left.shape[1], right_flipped.shape[1])\n        left = left[:mh, :mw]\n        right_flipped = right_flipped[:mh, :mw]\n    L = left.ravel()\n    R = right_flipped.ravel()\n    Lm = L.mean()\n    Rm = R.mean()\n    Lc = L - Lm\n    Rc = R - Rm\n    num = float((Lc * Rc).sum())\n    den = float(np.sqrt((Lc * Lc).sum() * (Rc * Rc).sum()))\n    if den <= 0.0:\n        return 0.0\n    corr = num / (den + eps)\n    # map from [-1,1] to [0,1]\n    result = float(np.clip((corr + 1.0) / 2.0, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance of the brightest pixel to image center (0..1)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = img.mean(axis=2).astype(float)\n    else:\n        a = img.astype(float)\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    idx = int(np.argmax(a))\n    y, x = np.unravel_index(idx, a.shape)\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    dist = float(np.hypot(x - cx, y - cy))\n    maxdist = float(np.hypot(cx, cy))\n    if maxdist <= 0.0:\n        return 0.0\n    result = float(np.clip(dist / maxdist, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average local 3x3 variance normalized by global variance (0 if too small)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = img.mean(axis=2).astype(float)\n    else:\n        a = img.astype(float)\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    # compute sum and sumsq for each 3x3 block via shifted slices\n    s00 = a[:-2, :-2]; s01 = a[:-2, 1:-1]; s02 = a[:-2, 2:]\n    s10 = a[1:-1, :-2]; s11 = a[1:-1, 1:-1]; s12 = a[1:-1, 2:]\n    s20 = a[2:, :-2]; s21 = a[2:, 1:-1]; s22 = a[2:, 2:]\n    sum_block = s00 + s01 + s02 + s10 + s11 + s12 + s20 + s21 + s22\n    sumsq_block = (s00*s00 + s01*s01 + s02*s02 + s10*s10 + s11*s11 + s12*s12 + s20*s20 + s21*s21 + s22*s22)\n    mean_block = sum_block / 9.0\n    mean_sq_block = sumsq_block / 9.0\n    var_block = mean_sq_block - mean_block * mean_block\n    local_var_mean = float(np.mean(var_block))\n    global_var = float(a.var())\n    result = local_var_mean / (global_var + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Checkerboardness: fraction of 2x2 blocks with alternating sign around block mean'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = img.mean(axis=2).astype(float)\n    else:\n        a = img.astype(float)\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    a00 = a[:-1, :-1]; a01 = a[:-1, 1:]; a10 = a[1:, :-1]; a11 = a[1:, 1:]\n    bmean = (a00 + a01 + a10 + a11) / 4.0\n    v00 = a00 - bmean; v01 = a01 - bmean; v10 = a10 - bmean; v11 = a11 - bmean\n    pos_pattern = (v00 > 0) & (v11 > 0) & (v01 < 0) & (v10 < 0)\n    neg_pattern = (v00 < 0) & (v11 < 0) & (v01 > 0) & (v10 > 0)\n    mask = pos_pattern | neg_pattern\n    total = mask.size\n    if total == 0:\n        return 0.0\n    frac = float(np.count_nonzero(mask)) / float(total)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Maximum horizontal run of pixels above the row mean normalized by image width'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = img.mean(axis=2).astype(float)\n    else:\n        a = img.astype(float)\n    h, w = a.shape\n    if w == 0 or h == 0:\n        return 0.0\n    max_run = 0\n    for row in a:\n        thr = float(row.mean())\n        b = (row > thr).astype(np.int8)\n        if b.sum() == 0:\n            continue\n        # pad with zeros to detect edges\n        padded = np.concatenate(([0], b, [0]))\n        diffs = np.diff(padded)\n        starts = np.where(diffs == 1)[0]\n        ends = np.where(diffs == -1)[0]\n        if starts.size and ends.size:\n            lengths = ends - starts\n            if lengths.size:\n                mr = int(lengths.max())\n                if mr > max_run:\n                    max_run = mr\n    result = float(max_run) / float(max(1, w))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Color channel mean divergence: std of channel means normalized by overall mean'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 2:\n        return 0.0\n    channels = img.shape[2]\n    try:\n        means = img.mean(axis=(0, 1)).astype(float)\n    except Exception:\n        means = np.array([float(np.mean(img[..., c])) for c in range(channels)])\n    mean_of_means = float(means.mean())\n    std_of_means = float(means.std())\n    result = std_of_means / (abs(mean_of_means) + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Corner brightness contrast: (mean corners - mean center) normalized by global std'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = img.mean(axis=2).astype(float)\n    else:\n        a = img.astype(float)\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    cs = max(1, min(h, w) // 8)\n    corners = []\n    corners.append(a[:cs, :cs])\n    corners.append(a[:cs, -cs:])\n    corners.append(a[-cs:, :cs])\n    corners.append(a[-cs:, -cs:])\n    corner_mean = float(np.mean([c.mean() if c.size else 0.0 for c in corners]))\n    cs2 = max(1, min(h, w) // 4)\n    cy = max(0, (h // 2) - (cs2 // 2))\n    cx = max(0, (w // 2) - (cs2 // 2))\n    center = a[cy:cy + cs2, cx:cx + cs2]\n    if center.size == 0:\n        return 0.0\n    center_mean = float(center.mean())\n    gstd = float(a.std()) + eps\n    result = (corner_mean - center_mean) / gstd\n    return float(np.clip(result, -5.0, 5.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with gradient magnitude above a robust threshold'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    med = float(np.median(mag))\n    std = float(mag.std())\n    thresh = med + 0.5 * std\n    frac = float(np.count_nonzero(mag > thresh)) / (mag.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical transition rate: normalized count of binary transitions along columns (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w == 0:\n        return 0.0\n    thr = float(np.median(a))\n    b = (a > thr).astype(np.int8)\n    transitions = np.abs(np.diff(b, axis=0))\n    count = float(transitions.sum())\n    norm = float((h - 1) * w) + eps\n    return float(np.clip(count / norm, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity entropy (normalized to [0,1]) using a moderate histogram resolution'\n    import numpy as np\n    eps = 1e-12\n    NB = 32\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        flat = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        flat = np.nan_to_num(arr.astype(float)).ravel()\n    if flat.size == 0:\n        return 0.0\n    vmin = float(flat.min())\n    vmax = float(flat.max())\n    if vmax <= vmin:\n        return 0.0\n    hist, _ = np.histogram(flat, bins=NB, range=(vmin, vmax))\n    total = float(hist.sum()) + eps\n    p = hist.astype(float) / total\n    ent = -float(np.sum(p * np.log(p + eps)))\n    norm = float(np.log(NB))\n    return float(np.clip(ent / (norm + eps), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Central symmetry: similarity to 180-degree rotated image (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    a_rot = np.rot90(a, 2)\n    diff_mean = float(np.mean(np.abs(a - a_rot)))\n    norm = float(np.mean(np.abs(a))) + eps\n    sim = 1.0 - (diff_mean / norm)\n    return float(np.clip(sim, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Annular prominence: how much an outer ring differs from the center (signed, clipped)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    maxr = float(r.max()) if r.size else 1.0\n    center_r = max(1.0, min(h, w) / 8.0)\n    inner_r = max(center_r + 1.0, 0.25 * maxr)\n    outer_r = max(inner_r + 1.0, 0.6 * maxr)\n    center_mask = r <= center_r\n    ann_mask = (r >= inner_r) & (r <= outer_r)\n    center_mean = float(a[center_mask].mean()) if center_mask.any() else 0.0\n    ann_mean = float(a[ann_mask].mean()) if ann_mask.any() else 0.0\n    gstd = float(a.std()) + eps\n    result = (ann_mean - center_mean) / gstd\n    return float(np.clip(result, -10.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian energy normalized by mean absolute intensity (texture measure)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    lap = (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) +\n           np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a)\n    energy = float(np.mean(np.abs(lap)))\n    denom = float(np.mean(np.abs(a))) + eps\n    result = energy / denom\n    return float(np.clip(result, 0.0, 100.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram peakiness: how concentrated intensity mass is in a single histogram bin (0..1)'\n    import numpy as np\n    NB = 64\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        flat = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        flat = np.nan_to_num(arr.astype(float)).ravel()\n    if flat.size == 0:\n        return 0.0\n    vmin = float(flat.min())\n    vmax = float(flat.max())\n    if vmax <= vmin:\n        return 0.0\n    hist, _ = np.histogram(flat, bins=NB, range=(vmin, vmax))\n    total = float(hist.sum()) + eps\n    max_count = float(hist.max())\n    mean_count = total / float(NB)\n    peak_ratio = max_count / (mean_count + eps)\n    result = (peak_ratio - 1.0) / (float(NB) - 1.0)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation concentration (resultant vector length 0..1 indicates dominant orientation)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy) + eps\n    sum_mag = float(mag.sum()) + eps\n    vx = float(gx.sum()) / sum_mag\n    vy = float(gy.sum()) / sum_mag\n    R = float(np.hypot(vx, vy))\n    return float(np.clip(R, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-spot prominence: (max - local mean) normalized by image std (0..10 clipped)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    mx = float(a.max())\n    coords = np.argwhere(a == mx)\n    if coords.size == 0:\n        return 0.0\n    y, x = coords[0]\n    r = max(1, min(h, w) // 20)\n    y0 = max(0, y - r)\n    y1 = min(h, y + r + 1)\n    x0 = max(0, x - r)\n    x1 = min(w, x + r + 1)\n    local = a[y0:y1, x0:x1]\n    local_mean = float(local.mean()) if local.size else 0.0\n    gstd = float(a.std()) + eps\n    prominence = (mx - local_mean) / gstd\n    result = max(0.0, prominence)\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of extreme pixels near min/max (useful for clipping/overexposure detection)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        flat = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        flat = np.nan_to_num(arr.astype(float)).ravel()\n    if flat.size == 0:\n        return 0.0\n    p_low = float(np.percentile(flat, 1))\n    p_high = float(np.percentile(flat, 99))\n    count_low = float(np.count_nonzero(flat <= p_low))\n    count_high = float(np.count_nonzero(flat >= p_high))\n    frac = (count_low + count_high) / (flat.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast between brightest and darkest corner means normalized by image std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    patch = max(1, min(h, w) // 8)\n    corners = []\n    corners.append(arr[0:patch, 0:patch])\n    corners.append(arr[0:patch, w-patch:w])\n    corners.append(arr[h-patch:h, 0:patch])\n    corners.append(arr[h-patch:h, w-patch:w])\n    means = [float(np.mean(c)) if c.size else 0.0 for c in corners]\n    if not means:\n        return 0.0\n    diff = float(max(means) - min(means))\n    std = float(np.std(arr)) + eps\n    result = diff / std\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variance of discrete Laplacian (sharpness measure), normalized by mean magnitude'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n        gy_y, gy_x = np.gradient(gy)\n        gx_y, gx_x = np.gradient(gx)\n    except Exception:\n        return 0.0\n    lap = gx_x + gy_y\n    var_lap = float(np.var(lap))\n    norm = float(np.mean(np.abs(a))) + eps\n    result = var_lap / norm\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Max of horizontal and vertical mirror similarity (1=perfect symmetry)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    def sim(mat, flipped):\n        diff = np.abs(mat - flipped)\n        denom = np.mean(np.abs(mat - np.mean(mat))) + eps\n        score = 1.0 - (np.mean(diff) / denom)\n        return float(np.clip(score, 0.0, 1.0))\n    hor_sim = sim(a, a[:, ::-1])\n    ver_sim = sim(a, a[::-1, :])\n    result = max(hor_sim, ver_sim)\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of border pixels within one std of border median (1 = very uniform border)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    t = max(1, min(h, w) // 10)\n    masks = []\n    masks.append(arr[0:t, :])\n    masks.append(arr[h-t:h, :])\n    masks.append(arr[:, 0:t])\n    masks.append(arr[:, w-t:w])\n    border_vals = np.concatenate([m.ravel() for m in masks if m.size], axis=0) if any(m.size for m in masks) else np.array([])\n    if border_vals.size == 0:\n        return 0.0\n    med = float(np.median(border_vals))\n    std = float(np.std(border_vals)) + eps\n    within = np.count_nonzero(np.abs(border_vals - med) <= std)\n    result = float(within) / float(border_vals.size)\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance between intensity-weighted centroid and image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    ys, xs = np.indices((h, w))\n    weights = a - a.min()\n    W = float(weights.sum())\n    if W <= 0.0:\n        return 0.0\n    cx = float((weights * xs).sum()) / W\n    cy = float((weights * ys).sum()) / W\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    norm = np.hypot(center_x, center_y) + eps\n    result = float(np.clip(dist / norm, 0.0, 1.0))\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant orientation strength from weighted coordinate covariance (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    ys, xs = np.indices((h, w))\n    weights = a - a.min()\n    W = float(weights.sum())\n    if W <= 0.0:\n        return 0.0\n    mx = float((weights * xs).sum()) / W\n    my = float((weights * ys).sum()) / W\n    dx = xs - mx\n    dy = ys - my\n    cxx = float((weights * (dx * dx)).sum()) / W\n    cyy = float((weights * (dy * dy)).sum()) / W\n    cxy = float((weights * (dx * dy)).sum()) / W\n    cov = np.array([[cxx, cxy], [cxy, cyy]], dtype=float)\n    try:\n        vals = np.linalg.eigvalsh(cov)\n    except Exception:\n        return 0.0\n    vals = np.sort(vals)[::-1]  # descending\n    lam1 = float(vals[0])\n    lam2 = float(vals[1])\n    strength = (lam1 - lam2) / (lam1 + lam2 + eps)\n    result = float(np.clip(strength, 0.0, 1.0))\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast between mean of top10% and bottom10% intensities normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    p90 = np.percentile(flat, 90)\n    p10 = np.percentile(flat, 10)\n    top_mean = float(flat[flat >= p90].mean()) if np.any(flat >= p90) else float(np.mean(flat))\n    bot_mean = float(flat[flat <= p10].mean()) if np.any(flat <= p10) else float(np.mean(flat))\n    std = float(np.std(flat)) + eps\n    result = (top_mean - bot_mean) / std\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Median of non-overlapping block variances (coarseness), normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    blocks = max(1, min(h, w) // 8)\n    bh = max(1, h // blocks)\n    bw = max(1, w // blocks)\n    variances = []\n    for y in range(0, h, bh):\n        for x in range(0, w, bw):\n            block = arr[y:y+bh, x:x+bw]\n            if block.size:\n                variances.append(float(block.var()))\n    if not variances:\n        return 0.0\n    median_var = float(np.median(variances))\n    denom = float(np.std(arr)) + eps\n    result = median_var / denom\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of local intensity peaks above mean+std (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mu = float(a.mean())\n    sd = float(a.std())\n    thresh = mu + sd\n    padded = np.pad(a, pad_width=1, mode='constant', constant_values=-np.inf)\n    center = padded[1:-1, 1:-1]\n    local_max = np.ones_like(center, dtype=bool)\n    for dy in (-1, 0, 1):\n        for dx in (-1, 0, 1):\n            if dy == 0 and dx == 0:\n                continue\n            neigh = padded[1+dy:h+1+dy, 1+dx:w+1+dx]\n            local_max &= (center > neigh)\n    peaks = (local_max & (center > thresh))\n    count = int(np.count_nonzero(peaks))\n    result = float(count) / float(h * w + eps)\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean chroma (max-min per pixel) normalized by image dynamic range; 0 for non-color'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim < 3 or img.shape[2] < 3:\n        return 0.0\n    arr = np.nan_to_num(img.astype(float))\n    maxc = arr.max(axis=2)\n    minc = arr.min(axis=2)\n    chroma = maxc - minc\n    mean_chroma = float(np.mean(chroma))\n    dyn = float(arr.max() - arr.min()) + eps\n    result = mean_chroma / dyn\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio deviation from square, in [0..1) (0 => square)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    try:\n        h, w = arr.shape[:2]\n    except Exception:\n        return 0.0\n    if h <= 0 or w <= 0:\n        return 0.0\n    ratio = float(h) / float(w)\n    # normalized deviation: |r-1|/(r+1) yields 0..1\n    result = abs(ratio - 1.0) / (ratio + 1.0)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels considerably darker than global mean (captures dark regions)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    mu = float(a.mean())\n    sd = float(a.std()) + eps\n    thr = mu - 0.5 * sd\n    count = float(np.count_nonzero(a < thr))\n    result = count / float(a.size + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized variance of quadrant means (high => quadrants differ)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape if a.size else (0, 0)\n    if h == 0 or w == 0:\n        return 0.0\n    mh = h // 2\n    mw = w // 2\n    quads = []\n    quads.append(a[0:mh, 0:mw])\n    quads.append(a[0:mh, mw:w])\n    quads.append(a[mh:h, 0:mw])\n    quads.append(a[mh:h, mw:w])\n    means = np.array([float(q.mean()) if q.size else 0.0 for q in quads], dtype=float)\n    gstd = float(a.std()) + eps\n    var_means = float(means.var())\n    result = var_means / (gstd * gstd + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Corner gradient energy fraction (how much gradient energy is in corners)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    corner_size = max(1, int(min(h, w) * 0.15))\n    # four corners\n    c1 = mag[0:corner_size, 0:corner_size]\n    c2 = mag[0:corner_size, -corner_size:]\n    c3 = mag[-corner_size:, 0:corner_size]\n    c4 = mag[-corner_size:, -corner_size:]\n    corner_energy = float(c1.sum() + c2.sum() + c3.sum() + c4.sum())\n    total_energy = float(mag.sum()) + eps\n    result = corner_energy / total_energy\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio using local mean subtraction (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    # local 3x3 mean via roll (fast, periodic rolled edges but acceptable)\n    s = np.zeros_like(a)\n    s += a\n    s += np.roll(a, 1, axis=0)\n    s += np.roll(a, -1, axis=0)\n    s += np.roll(a, 1, axis=1)\n    s += np.roll(a, -1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), -1, axis=1)\n    local_mean = s / 9.0\n    high = a - local_mean\n    hv = float(np.sum(np.abs(high)))\n    gv = float(np.sum(np.abs(a))) + eps\n    result = hv / gv\n    # clip to [0,1]\n    result = max(0.0, min(1.0, result))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant edge orientation normalized to [-1..1] (-1=>-180deg, 1=>180deg)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    total = float(mag.sum())\n    if total <= eps:\n        return 0.0\n    angles = np.arctan2(gy, gx)  # radians in [-pi, pi]\n    ang_deg = np.degrees(angles).ravel()\n    weights = mag.ravel()\n    # histogram into 36 bins (~10 degrees)\n    nbins = 36\n    hist, edges = np.histogram(ang_deg, bins=nbins, range=(-180.0, 180.0), weights=weights)\n    idx = int(np.argmax(hist))\n    # bin center\n    center = 0.5 * (edges[idx] + edges[idx + 1])\n    # normalize to [-1..1]\n    result = float(center / 180.0)\n    # clip\n    result = max(-1.0, min(1.0, result))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Per-image color channel spread (std of channel means) normalized by mean brightness (colorfulness proxy)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    # mean per channel\n    means = a.mean(axis=(0, 1))\n    spread = float(np.std(means))\n    avg = float(means.mean()) + eps\n    result = spread / avg\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of border pixels brighter than global mean (object touching border or bright frame)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mu = float(a.mean())\n    # border mask\n    top = a[0, :]\n    bottom = a[-1, :]\n    left = a[:, 0]\n    right = a[:, -1]\n    border_vals = np.concatenate([top.ravel(), bottom.ravel(), left.ravel(), right.ravel()])\n    if border_vals.size == 0:\n        return 0.0\n    count = float(np.count_nonzero(border_vals > mu))\n    result = count / float(border_vals.size + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Horizontal mirror symmetry score (0..1), 1 means perfect horizontal mirror'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2 or h < 1:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:]\n    # flip right horizontally to compare\n    right = right[:, ::-1]\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    # ensure shapes match\n    if left.shape != right.shape:\n        # crop to smallest common shape\n        mh = min(left.shape[0], right.shape[0])\n        mw = min(left.shape[1], right.shape[1])\n        left = left[:mh, :mw]\n        right = right[:mh, :mw]\n    # normalized cross-correlation\n    Lm = left.mean()\n    Rm = right.mean()\n    Lz = left - Lm\n    Rz = right - Rm\n    num = float((Lz * Rz).sum())\n    den = float(np.sqrt((Lz * Lz).sum() * (Rz * Rz).sum()) + eps)\n    score = num / den\n    score = max(-1.0, min(1.0, score))\n    return float(abs(score))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of very bright pixels (top 0.5 percentile) indicating sparse highlights'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).ravel().astype(float))\n    else:\n        vals = np.nan_to_num(arr.ravel().astype(float))\n    if vals.size == 0:\n        return 0.0\n    try:\n        thr = float(np.percentile(vals, 99.5))\n    except Exception:\n        thr = float(vals.max())\n    count = float(np.count_nonzero(vals >= thr))\n    result = count / float(vals.size + eps)\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right brightness bias normalized by global std (positive => left brighter)'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left_mean = float(np.mean(arr[:, :mid])) if arr[:, :mid].size else 0.0\n    right_mean = float(np.mean(arr[:, -mid:])) if arr[:, -mid:].size else 0.0\n    gstd = float(arr.std()) + eps\n    result = (left_mean - right_mean) / gstd\n    return float(np.clip(result, -10.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Shannon entropy of intensity histogram (0..1 normalized)'\n    import numpy as np\n    eps = 1e-12\n    arr_in = np.asarray(image)\n    if arr_in.size == 0:\n        return 0.0\n    if arr_in.ndim == 3:\n        arr = np.nan_to_num(arr_in.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr_in.astype(float))\n    # use 16-bin histogram\n    try:\n        hist, _ = np.histogram(arr.ravel(), bins=16, range=(float(arr.min()), float(arr.max()+eps)))\n    except Exception:\n        return 0.0\n    total = hist.sum()\n    if total <= 0:\n        return 0.0\n    p = hist.astype(float) / float(total)\n    ent = -np.sum(np.where(p > 0, p * np.log(p + eps), 0.0))\n    # normalize by log(bins)\n    result = ent / (np.log(16.0) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of FFT energy in high frequencies (outer quarter of radial freq)'\n    import numpy as np\n    eps = 1e-12\n    arr_in = np.asarray(image)\n    if arr_in.size == 0:\n        return 0.0\n    if arr_in.ndim == 3:\n        a = np.nan_to_num(arr_in.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr_in.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n    except Exception:\n        return 0.0\n    mag = np.abs(F)\n    ys = np.arange(h)[:, None] - (h // 2)\n    xs = np.arange(w)[None, :] - (w // 2)\n    dist = np.hypot(ys, xs)\n    maxd = float(dist.max()) if dist.size else 1.0\n    mask_outer = dist >= (0.75 * maxd)\n    total = float(mag.sum()) + eps\n    outer = float(mag[mask_outer].sum())\n    result = outer / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels near extremes (saturation / clipping indicator)'\n    import numpy as np\n    eps = 1e-12\n    arr_in = np.asarray(image)\n    if arr_in.size == 0:\n        return 0.0\n    if arr_in.ndim == 3:\n        a = np.nan_to_num(arr_in.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr_in.astype(float))\n    mn = float(a.min())\n    mx = float(a.max())\n    if not np.isfinite(mn) or not np.isfinite(mx):\n        return 0.0\n    rng = mx - mn\n    if rng <= eps:\n        return 0.0\n    # consider pixels within 5% of range from either extreme\n    low_thr = mn + 0.05 * rng\n    high_thr = mx - 0.05 * rng\n    frac_low = float((a <= low_thr).sum()) / float(a.size)\n    frac_high = float((a >= high_thr).sum()) / float(a.size)\n    result = frac_low + frac_high\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bounding-box occupancy of bright region (area of bbox containing bright pixels / image area)'\n    import numpy as np\n    eps = 1e-12\n    arr_in = np.asarray(image)\n    if arr_in.size == 0:\n        return 0.0\n    if arr_in.ndim == 3:\n        a = np.nan_to_num(arr_in.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr_in.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean() + 0.5 * a.std())\n    mask = a > thr\n    if not mask.any():\n        return 0.0\n    ys, xs = np.where(mask)\n    y0, y1 = int(ys.min()), int(ys.max())\n    x0, x1 = int(xs.min()), int(xs.max())\n    bbox_area = float((y1 - y0 + 1) * (x1 - x0 + 1))\n    img_area = float(h * w) + eps\n    result = bbox_area / img_area\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry (absolute Pearson correlation between left half and mirrored right half)'\n    import numpy as np\n    arr_in = np.asarray(image)\n    if arr_in.size == 0:\n        return 0.0\n    if arr_in.ndim == 3:\n        a = np.nan_to_num(arr_in.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr_in.astype(float))\n    h, w = a.shape\n    if h == 0 or w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:]\n    # if shapes differ, crop larger to match\n    if left.shape[1] != right.shape[1]:\n        minw = min(left.shape[1], right.shape[1])\n        left = left[:, :minw]\n        right = right[:, -minw:]\n    right_flipped = np.fliplr(right)\n    L = left.ravel()\n    R = right_flipped.ravel()\n    if L.size == 0:\n        return 0.0\n    Lm = L.mean()\n    Rm = R.mean()\n    denom = (np.std(L) * np.std(R))\n    if denom == 0:\n        return 0.0\n    corr = float(((L - Lm) * (R - Rm)).mean() / (denom + 1e-12))\n    return float(np.clip(abs(corr), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized count of local intensity peaks (distinct bright spots) per pixel'\n    import numpy as np\n    eps = 1e-12\n    arr_in = np.asarray(image)\n    if arr_in.size == 0:\n        return 0.0\n    if arr_in.ndim == 3:\n        a = np.nan_to_num(arr_in.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr_in.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    # compare each pixel to its 8 neighbors using shifted arrays (wrap avoided by masking border)\n    shifts = [(0,1),(0,-1),(1,0),(-1,0),(1,1),(1,-1),(-1,1),(-1,-1)]\n    center = a\n    greater = np.ones_like(a, dtype=bool)\n    for dy, dx in shifts:\n        neigh = np.roll(np.roll(a, dy, axis=0), dx, axis=1)\n        greater &= (center > neigh)\n    # remove border false positives due to roll wrap\n    greater[0,:] = False\n    greater[-1,:] = False\n    greater[:,0] = False\n    greater[:,-1] = False\n    # only count peaks that stand above local statistics\n    thr = float(a.mean() + 0.5 * a.std())\n    peaks = greater & (a > thr)\n    count = float(peaks.sum())\n    result = count / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity slope: linear fit slope of mean intensity vs radius (normalized)'\n    import numpy as np\n    eps = 1e-12\n    arr_in = np.asarray(image)\n    if arr_in.size == 0:\n        return 0.0\n    if arr_in.ndim == 3:\n        a = np.nan_to_num(arr_in.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr_in.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    dist = np.hypot(xs - cx, ys - cy)\n    maxd = float(dist.max()) + eps\n    # bin into 8 concentric rings\n    bins = np.linspace(0.0, maxd, num=9)\n    means = []\n    centers = []\n    for i in range(len(bins)-1):\n        mask = (dist >= bins[i]) & (dist < bins[i+1])\n        if not mask.any():\n            continue\n        centers.append(0.5*(bins[i]+bins[i+1]) / maxd)\n        means.append(float(a[mask].mean()))\n    if len(means) < 2:\n        return 0.0\n    x = np.array(centers)\n    y = np.array(means)\n    # linear fit\n    A = np.vstack([x, np.ones_like(x)]).T\n    try:\n        m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n    except Exception:\n        return 0.0\n    # normalize slope by intensity range\n    intensity_range = float(y.max() - y.min()) + eps\n    result = float(m * (1.0 / intensity_range))\n    # clamp to reasonable range\n    return float(np.clip(result, -10.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric (0 for grayscale, higher => more colorful)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(arr.astype(float))\n    R = a[..., 0]\n    G = a[..., 1]\n    B = a[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n    return float(np.clip(colorfulness, 0.0, 1e6))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with strong gradient magnitude'\n    import numpy as np\n    eps = 1e-12\n    arr_in = np.asarray(image)\n    if arr_in.size == 0:\n        return 0.0\n    if arr_in.ndim == 3:\n        a = np.nan_to_num(arr_in.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr_in.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + mag.std())\n    strong = (mag >= thr).sum()\n    result = float(strong) / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels effectively zero (sparsity) relative to max intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    maxv = float(np.nanmax(arr)) if arr.size else 0.0\n    if maxv <= eps:\n        # image completely zero -> maximally sparse\n        return 1.0\n    tol = maxv * 0.01  # consider near-zero as <1% of max\n    count_zero_like = float(np.count_nonzero(np.abs(arr) <= tol))\n    result = count_zero_like / float(arr.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of local intensity peaks (peaks per pixel) using 8-neighbor test'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    # threshold to ignore tiny fluctuations\n    mean = float(a.mean())\n    std = float(a.std()) + eps\n    thresh = mean + 0.5 * std\n    # compare to 8 neighbors via roll (fast; wrap-around acceptable as edge case)\n    neighs = []\n    shifts = [(0,1),(0,-1),(1,0),(-1,0),(1,1),(1,-1),(-1,1),(-1,-1)]\n    for dy, dx in shifts:\n        neighs.append(np.roll(np.roll(a, dy, axis=0), dx, axis=1))\n    neigh_max = np.maximum.reduce(neighs)\n    peaks = (a > neigh_max) & (a > thresh)\n    count = float(np.count_nonzero(peaks))\n    result = count / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect compactness score of non-zero content (1 = square-like, 0 = line-like or empty)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    mask = arr != 0\n    if not mask.any():\n        return 0.0\n    ys, xs = np.where(mask)\n    h_bbox = ys.max() - ys.min() + 1\n    w_bbox = xs.max() - xs.min() + 1\n    if h_bbox <= 0 or w_bbox <= 0:\n        return 0.0\n    ratio = float(min(h_bbox / (w_bbox + eps), w_bbox / (h_bbox + eps)))\n    # ratio in (0,1]; return as-is (1 => square, smaller => elongated)\n    result = float(np.clip(ratio, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Centrality of intensity: higher when intensity concentrated near image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    rx = xs - cx\n    ry = ys - cy\n    r = np.hypot(rx, ry)\n    maxr = float(r.max()) + eps\n    weights = a.clip(min=0.0)\n    s = float(weights.sum())\n    if s <= eps:\n        return 0.0\n    weighted_r = float((weights * r).sum())\n    # centrality: 1 when weighted_r very small, 0 when weighted_r close to max possible (maxr*s)\n    centrality = 1.0 - (weighted_r / (maxr * s + eps))\n    return float(np.clip(centrality, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of FFT energy in high frequencies (outer third of radial freq)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n    except Exception:\n        return 0.0\n    mag = np.abs(F)\n    ys = np.arange(h)[:, None] - (h // 2)\n    xs = np.arange(w)[None, :] - (w // 2)\n    dist = np.hypot(ys, xs)\n    maxd = float(dist.max()) if dist.size else 1.0\n    mask_high = dist >= (0.66 * maxd)\n    total = float(mag.sum()) + eps\n    high = float(mag[mask_high].sum())\n    result = high / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Color channel variation proxy: mean per-pixel inter-channel range (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3 and img.shape[2] >= 2:\n        # compute per-pixel range across channels\n        arr = np.nan_to_num(img.astype(float))\n        per_pixel_range = arr.max(axis=2) - arr.min(axis=2)\n        # normalize by dynamic range of image channels\n        max_possible = float(per_pixel_range.max()) + eps\n        # if all channels identical, this will be zero\n        mean_range = float(per_pixel_range.mean())\n        # if values likely in 0..255 or 0..1, scale by observed max to produce 0..1\n        denom = max_possible if max_possible > eps else (mean_range + eps)\n        result = mean_range / (denom + eps)\n        return float(np.clip(result, 0.0, 1.0))\n    else:\n        # grayscale has no channel variation\n        return 0.0\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0 = constant, 1 = maximum entropy over bins)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    bins = 64\n    try:\n        hist, _ = np.histogram(flat, bins=bins, range=(float(flat.min()), float(flat.max())) if flat.max() > flat.min() else (0.0, 1.0))\n    except Exception:\n        return 0.0\n    total = float(hist.sum())\n    if total <= 0:\n        return 0.0\n    p = hist.astype(float) / total\n    p_nonzero = p[p > 0]\n    if p_nonzero.size == 0:\n        return 0.0\n    entropy = -float((p_nonzero * np.log2(p_nonzero)).sum())\n    # normalize by log2(bins)\n    max_ent = np.log2(bins)\n    result = entropy / (max_ent + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Structure tensor coherence: degree to which gradients align to a single orientation (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    Ix2 = (gx * gx)\n    Iy2 = (gy * gy)\n    Ixy = (gx * gy)\n    A = float(Ix2.sum())\n    C = float(Iy2.sum())\n    B = float(Ixy.sum())\n    trace = A + C\n    det = A * C - B * B\n    if trace <= eps:\n        return 0.0\n    # eigenvalues of 2x2 matrix: (trace +/- sqrt(trace^2 - 4det))/2\n    discr = max(trace * trace - 4.0 * det, 0.0)\n    l1 = 0.5 * (trace + np.sqrt(discr))\n    l2 = 0.5 * (trace - np.sqrt(discr))\n    if (l1 + l2) <= eps:\n        return 0.0\n    coherence = (l1 - l2) / (l1 + l2 + eps)\n    return float(np.clip(coherence, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian normalized by image std (higher => sharper)'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    # Laplacian via 4-neighbor difference: neighbors_sum - 4*center\n    up = np.roll(a, -1, axis=0)\n    down = np.roll(a, 1, axis=0)\n    left = np.roll(a, -1, axis=1)\n    right = np.roll(a, 1, axis=1)\n    lap = (up + down + left + right) - 4.0 * a\n    mean_abs_lap = float(np.mean(np.abs(lap)))\n    overall_std = float(a.std()) + eps\n    result = mean_abs_lap / overall_std\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of pixels above the 90th intensity percentile (bright-pixel ratio)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    # collapse color to intensity\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(img.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    p90 = float(np.percentile(a, 90))\n    count = float(np.count_nonzero(a > p90))\n    result = count / (a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of spectral energy concentrated in low-frequency band (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape[:2]\n    if h == 0 or w == 0:\n        return 0.0\n    # FFT power spectrum\n    F = np.fft.fftshift(np.fft.fft2(a))\n    P = np.abs(F) ** 2\n    # low-frequency central square (relative size 1/8)\n    r_h = max(1, h // 8)\n    r_w = max(1, w // 8)\n    cy, cx = h // 2, w // 2\n    mask = np.zeros_like(P, dtype=bool)\n    mask[cy - r_h:cy + r_h + 1, cx - r_w:cx + r_w + 1] = True\n    low_energy = float(P[mask].sum())\n    total_energy = float(P.sum()) + eps\n    result = low_energy / total_energy\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian magnitude normalized by mean intensity (texture strength)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    # discrete Laplacian kernel convolution (4-neighbor)\n    lap = -4 * a\n    lap += np.roll(a, 1, axis=0)\n    lap += np.roll(a, -1, axis=0)\n    lap += np.roll(a, 1, axis=1)\n    lap += np.roll(a, -1, axis=1)\n    mag = np.mean(np.abs(lap))\n    mean_int = np.mean(np.abs(a)) + eps\n    result = mag / mean_int\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of connected components in a median-thresholded binary image (4-connectivity), capped at 100'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(np.median(a))\n    mask = a > thr\n    visited = np.zeros_like(mask, dtype=bool)\n    comps = 0\n    stack = []\n    for y in range(h):\n        for x in range(w):\n            if mask[y, x] and not visited[y, x]:\n                comps += 1\n                if comps >= 100:\n                    return float(100.0)\n                # flood fill (4-connectivity)\n                stack.append((y, x))\n                visited[y, x] = True\n                while stack:\n                    cy, cx = stack.pop()\n                    if cy > 0 and mask[cy - 1, cx] and not visited[cy - 1, cx]:\n                        visited[cy - 1, cx] = True\n                        stack.append((cy - 1, cx))\n                    if cy + 1 < h and mask[cy + 1, cx] and not visited[cy + 1, cx]:\n                        visited[cy + 1, cx] = True\n                        stack.append((cy + 1, cx))\n                    if cx > 0 and mask[cy, cx - 1] and not visited[cy, cx - 1]:\n                        visited[cy, cx - 1] = True\n                        stack.append((cy, cx - 1))\n                    if cx + 1 < w and mask[cy, cx + 1] and not visited[cy, cx + 1]:\n                        visited[cy, cx + 1] = True\n                        stack.append((cy, cx + 1))\n    return float(comps)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Eccentricity (major/minor axis ratio) of the largest foreground blob (clipped 0..10)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(np.median(a))\n    mask = a > thr\n    if not np.any(mask):\n        return 0.0\n    # label components to find largest (simple flood fill)\n    visited = np.zeros_like(mask, dtype=bool)\n    largest_coords = None\n    largest_size = 0\n    stack = []\n    for y in range(h):\n        for x in range(w):\n            if mask[y, x] and not visited[y, x]:\n                coords = []\n                stack.append((y, x))\n                visited[y, x] = True\n                while stack:\n                    cy, cx = stack.pop()\n                    coords.append((cy, cx))\n                    if cy > 0 and mask[cy - 1, cx] and not visited[cy - 1, cx]:\n                        visited[cy - 1, cx] = True\n                        stack.append((cy - 1, cx))\n                    if cy + 1 < h and mask[cy + 1, cx] and not visited[cy + 1, cx]:\n                        visited[cy + 1, cx] = True\n                        stack.append((cy + 1, cx))\n                    if cx > 0 and mask[cy, cx - 1] and not visited[cy, cx - 1]:\n                        visited[cy, cx - 1] = True\n                        stack.append((cy, cx - 1))\n                    if cx + 1 < w and mask[cy, cx + 1] and not visited[cy, cx + 1]:\n                        visited[cy, cx + 1] = True\n                        stack.append((cy, cx + 1))\n                size = len(coords)\n                if size > largest_size:\n                    largest_size = size\n                    largest_coords = np.array(coords)\n    if largest_coords is None or largest_coords.shape[0] < 2:\n        return 0.0\n    ys = largest_coords[:, 0].astype(float)\n    xs = largest_coords[:, 1].astype(float)\n    # covariance matrix of coordinates\n    cx_m = xs.mean()\n    cy_m = ys.mean()\n    cov_xx = np.mean((xs - cx_m) ** 2)\n    cov_yy = np.mean((ys - cy_m) ** 2)\n    cov_xy = np.mean((xs - cx_m) * (ys - cy_m))\n    # eigenvalues\n    trace = cov_xx + cov_yy\n    det = cov_xx * cov_yy - cov_xy * cov_xy\n    diff = np.sqrt(max(0.0, (trace * trace) / 4.0 - det))\n    lam1 = trace / 2.0 + diff\n    lam2 = trace / 2.0 - diff\n    lam2 = max(lam2, eps)\n    ratio = lam1 / lam2\n    result = float(np.clip(ratio, 0.0, 10.0))\n    return result\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean pairwise distance among top-brightest pixels (top 1%), normalized by image diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    flat = a.ravel()\n    n = flat.size\n    k = max(2, int(max(2, n * 0.01)))\n    # get indices of top-k brightest\n    idx = np.argpartition(-flat, k - 1)[:k]\n    ys = (idx // w).astype(float)\n    xs = (idx % w).astype(float)\n    if xs.size < 2:\n        return 0.0\n    # compute mean pairwise distance efficiently\n    vx = xs[:, None] - xs[None, :]\n    vy = ys[:, None] - ys[None, :]\n    dists = np.sqrt(vx * vx + vy * vy)\n    # take upper triangle excluding diagonal\n    iu = np.triu_indices(dists.shape[0], k=1)\n    if iu[0].size == 0:\n        return 0.0\n    mean_dist = float(np.mean(dists[iu]))\n    diag = np.hypot(h - 1, w - 1) + eps\n    result = mean_dist / diag\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric (Hasler-Suesstrunk) for RGB images, 0 for grayscale'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    # Use first three channels as R,G,B\n    R = a[:, :, 0].ravel()\n    G = a[:, :, 1].ravel()\n    B = a[:, :, 2].ravel()\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(rg))\n    mean_yb = float(np.mean(yb))\n    # Hasler & Suesstrunk colorfulness\n    color = np.sqrt(std_rg * std_rg + std_yb * std_yb) + 0.3 * np.sqrt(mean_rg * mean_rg + mean_yb * mean_yb)\n    return float(max(0.0, color))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border uniformity: average normalized absolute difference between border strips and global mean (0..10)'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    strip_h = max(1, h // 10)\n    strip_w = max(1, w // 10)\n    top = a[:strip_h, :].ravel()\n    bottom = a[-strip_h:, :].ravel()\n    left = a[:, :strip_w].ravel()\n    right = a[:, -strip_w:].ravel()\n    border = np.concatenate([top, bottom, left, right])\n    if border.size == 0:\n        return 0.0\n    global_mean = float(np.mean(a))\n    global_std = float(np.std(a)) + eps\n    diff = np.mean(np.abs(border - global_mean)) / global_std\n    return float(diff)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Principal-component variance ratio of bright pixels (fraction of variance explained by first PC, 0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(np.median(a))\n    ys, xs = np.where(a > thr)\n    if xs.size < 2:\n        return 0.0\n    xs_f = xs.astype(float)\n    ys_f = ys.astype(float)\n    mx = xs_f.mean()\n    my = ys_f.mean()\n    X = np.vstack([xs_f - mx, ys_f - my])\n    cov = np.cov(X)\n    # handle degenerate\n    if cov.shape != (2, 2):\n        return 0.0\n    vals = np.linalg.eigvalsh(cov)\n    vals = np.sort(vals)[::-1]  # descending\n    total = float(vals.sum()) + eps\n    ratio = float(vals[0]) / total\n    return float(np.clip(ratio, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average absolute symmetry correlation with horizontal and vertical flips (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    def norm_corr(u, v):\n        uf = u.ravel().astype(float)\n        vf = v.ravel().astype(float)\n        su = uf - uf.mean()\n        sv = vf - vf.mean()\n        nu = np.sqrt((su * su).sum())\n        nv = np.sqrt((sv * sv).sum())\n        denom = (nu * nv) + eps\n        return float(np.sum(su * sv) / denom)\n    corr_h = norm_corr(a, np.flipud(a))\n    corr_v = norm_corr(a, np.fliplr(a))\n    # take absolute correlation (symmetry magnitude) and average\n    result = (abs(corr_h) + abs(corr_v)) / 2.0\n    result = float(np.clip(result, 0.0, 1.0))\n    return result\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score (1.0 = perfect mirror symmetry)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2 or h == 0:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:] if mid > 0 else np.empty_like(left)\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    right_flipped = np.fliplr(right)\n    # ensure same shape\n    if left.shape != right_flipped.shape:\n        # trim larger to match smaller\n        min_cols = min(left.shape[1], right_flipped.shape[1])\n        left = left[:, :min_cols]\n        right_flipped = right_flipped[:, :min_cols]\n    diff = np.abs(left - right_flipped)\n    denom = np.mean(np.abs(a)) + eps\n    score = 1.0 - float(np.mean(diff) / denom)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of pixels that are strong edges (edge density)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(np.mean(mag) + np.std(mag))\n    count = float(np.count_nonzero(mag > thr))\n    return float(np.clip(count / (h * w + eps), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    bins = 32\n    hist, _ = np.histogram(vals, bins=bins, density=False)\n    total = float(hist.sum()) + eps\n    p = hist / total\n    p_nonzero = p[p > 0]\n    ent = -float((p_nonzero * np.log(p_nonzero)).sum())\n    max_ent = float(np.log(bins) + eps)\n    return float(np.clip(ent / max_ent, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bounding-box fill of bright region (fraction of image area)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mean = float(a.mean())\n    std = float(a.std())\n    thr = mean + 0.25 * std\n    mask = a > thr\n    if not np.any(mask):\n        return 0.0\n    coords = np.argwhere(mask)\n    ys = coords[:, 0]\n    xs = coords[:, 1]\n    y0, y1 = int(ys.min()), int(ys.max())\n    x0, x1 = int(xs.min()), int(xs.max())\n    bbox_area = float((y1 - y0 + 1) * (x1 - x0 + 1))\n    return float(np.clip(bbox_area / (h * w + eps), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of corner-like points (high both-horizontal-and-vertical gradient)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    thr = float(np.percentile(mag, 70))\n    corners = (np.abs(gx) > thr) & (np.abs(gy) > thr)\n    count = float(np.count_nonzero(corners))\n    return float(np.clip(count / (h * w + eps), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio from 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # subtract mean to emphasize texture\n    a0 = a - a.mean()\n    F = np.fft.fftshift(np.fft.fft2(a0))\n    P = np.abs(F) ** 2\n    center_h = max(1, h // 8)\n    center_w = max(1, w // 8)\n    ch0 = h // 2 - center_h\n    ch1 = h // 2 + center_h + (0 if center_h * 2 == center_h * 2 else 0)\n    cw0 = w // 2 - center_w\n    cw1 = w // 2 + center_w + (0 if center_w * 2 == center_w * 2 else 0)\n    # sum low-frequency in central rectangle\n    lf = P[ch0:ch1, cw0:cw1].sum()\n    total = P.sum() + eps\n    hf = float(max(0.0, total - lf))\n    return float(np.clip(hf / total, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Pearson correlation between radius and mean ring intensity (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    r_int = np.floor(r).astype(int)\n    max_r = int(r_int.max())\n    if max_r <= 0:\n        return 0.0\n    flat_r = r_int.ravel()\n    flat_i = img.ravel()\n    counts = np.bincount(flat_r)\n    sums = np.bincount(flat_r, weights=flat_i)\n    valid = counts > 0\n    if valid.sum() < 2:\n        return 0.0\n    means = sums[valid] / (counts[valid] + eps)\n    radii = np.arange(0, max_r + 1)[valid].astype(float)\n    # compute Pearson correlation\n    mr = means.mean()\n    rr = radii.mean()\n    num = float(((means - mr) * (radii - rr)).sum())\n    den = float(np.sqrt(((means - mr) ** 2).sum() * ((radii - rr) ** 2).sum()) + eps)\n    corr = num / den if den > 0 else 0.0\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean color saturation for color images (0..1), 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    img = np.nan_to_num(arr.astype(float))\n    # take first three channels if more present\n    ch = img[..., :3]\n    mn = ch.min(axis=2)\n    mx = ch.max(axis=2)\n    sat = (mx - mn) / (mx + eps)\n    mean_sat = float(np.mean(sat))\n    return float(np.clip(mean_sat, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local-variance to global-variance ratio (texture contrast, >=0)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    global_var = float(a.var()) + eps\n    # block size proportional to image\n    bs = max(1, min(h, w) // 8)\n    # crop to multiple of bs\n    h0 = (h // bs) * bs\n    w0 = (w // bs) * bs\n    if h0 == 0 or w0 == 0:\n        # fallback to pixelwise local variance via 3x3 windows\n        pad = np.pad(a, 1, mode='reflect')\n        local_vars = []\n        for dy in range(3):\n            for dx in range(3):\n                patch = pad[dy:dy + h, dx:dx + w]\n                local_vars.append(patch)\n        stacked = np.stack(local_vars, axis=0)\n        lv = stacked.var(axis=0)\n        mean_lv = float(lv.mean())\n    else:\n        cropped = a[:h0, :w0]\n        reshaped = cropped.reshape(h0 // bs, bs, w0 // bs, bs)\n        # compute variance over each block\n        blocks = reshaped.swapaxes(1, 2).reshape(-1, bs, bs)\n        if blocks.size == 0:\n            return 0.0\n        var_blocks = blocks.reshape(blocks.shape[0], -1).var(axis=1)\n        mean_lv = float(np.mean(var_blocks))\n    ratio = mean_lv / global_var\n    return float(np.clip(ratio, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientation histogram (orientation complexity 0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    angles = np.arctan2(gy, gx)  # [-pi,pi]\n    # map orientation modulo pi (so 0..pi)\n    orients = np.mod(angles, np.pi)\n    bins = 18\n    hist, _ = np.histogram(orients, bins=bins, range=(0.0, np.pi), density=False)\n    total = float(hist.sum()) + eps\n    p = hist / total\n    p_nonzero = p[p > 0]\n    ent = -float((p_nonzero * np.log(p_nonzero)).sum())\n    max_ent = float(np.log(bins) + eps)\n    return float(np.clip(ent / max_ent, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity centroid offset (0..1): distance between image center and intensity-weighted centroid'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    total = float(arr.sum())\n    if total <= eps or h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    x_cent = float((xs * arr).sum()) / total\n    y_cent = float((ys * arr).sum()) / total\n    dist = float(np.hypot(x_cent - cx, y_cent - cy))\n    diag = float(np.hypot(w, h)) + eps\n    result = dist / diag\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center vs outer mean contrast: (center_mean - outer_mean) / (overall_mean + eps)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < 3 or w < 3:\n        return 0.0\n    ch0, ch1 = h // 4, 3 * h // 4\n    cw0, cw1 = w // 4, 3 * w // 4\n    center = arr[ch0:ch1, cw0:cw1]\n    if center.size == 0:\n        return 0.0\n    center_mean = float(center.mean())\n    outer_mask = np.ones_like(arr, dtype=bool)\n    outer_mask[ch0:ch1, cw0:cw1] = False\n    outer = arr[outer_mask]\n    outer_mean = float(outer.mean()) if outer.size else 0.0\n    overall_mean = float(arr.mean()) + eps\n    result = (center_mean - outer_mean) / overall_mean\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry score (0..1): 1 means perfectly symmetric across vertical axis'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if w < 2:\n        return 1.0\n    half = w // 2\n    left = arr[:, :half]\n    right = arr[:, w - half:]\n    # Flip right horizontally for comparison; if sizes differ, crop to min width\n    if left.shape[1] != right.shape[1]:\n        minw = min(left.shape[1], right.shape[1])\n        left = left[:, :minw]\n        right = right[:, -minw:]\n    right_flipped = np.fliplr(right)\n    diff = np.abs(left - right_flipped)\n    denom = np.mean(np.abs(arr)) + eps\n    norm_diff = float(diff.mean()) / denom\n    score = 1.0 - norm_diff\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of high-gradient pixels: proportion of pixels with gradient magnitude > mean+std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + mag.std())\n    count = float((mag > thr).sum())\n    total = float(mag.size) + eps\n    result = count / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-intensity pixel proportion: fraction of pixels below the 10th percentile'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        vals = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(img.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    thr = float(np.percentile(vals, 10))\n    result = float((vals < thr).sum()) / float(vals.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center-to-border variance ratio: var(center) / (var(border) + eps), capped at 100'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    ch0, ch1 = h // 4, 3 * h // 4\n    cw0, cw1 = w // 4, 3 * w // 4\n    center = a[ch0:ch1, cw0:cw1]\n    border_mask = np.ones_like(a, dtype=bool)\n    border_mask[ch0:ch1, cw0:cw1] = False\n    border = a[border_mask]\n    if center.size == 0 or border.size == 0:\n        return 0.0\n    var_center = float(np.var(center))\n    var_border = float(np.var(border))\n    result = var_center / (var_border + eps)\n    return float(np.clip(result, 0.0, 100.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Local maxima density: count of pixels greater than all 8 neighbors divided by image area'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    center = a\n    # compare to 8 neighbors using shifts\n    neighbors = []\n    for dy in (-1, 0, 1):\n        for dx in (-1, 0, 1):\n            if dy == 0 and dx == 0:\n                continue\n            neighbors.append(np.roll(np.roll(a, dy, axis=0), dx, axis=1))\n    cmp_all = np.ones_like(a, dtype=bool)\n    for n in neighbors:\n        cmp_all &= (center > n)\n    # ignore wrap-around artifacts by zeroing borders that compared to wrapped values\n    cmp_all[0, :] = False\n    cmp_all[-1, :] = False\n    cmp_all[:, 0] = False\n    cmp_all[:, -1] = False\n    count = float(cmp_all.sum())\n    area = float(h * w)\n    result = count / (area + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation dispersion (circular variance 0..1), weighted by gradient magnitude'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    mag_sum = float(mag.sum()) + eps\n    if mag_sum <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)\n    cos_sum = float((np.cos(theta) * mag).sum())\n    sin_sum = float((np.sin(theta) * mag).sum())\n    R = np.hypot(cos_sum, sin_sum) / mag_sum\n    circ_var = 1.0 - R\n    return float(np.clip(circ_var, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean color saturation for RGB images (0..1), 0 for non-RGB'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0 or img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    rgb = a[:, :, :3]\n    mx = rgb.max(axis=2)\n    mn = rgb.min(axis=2)\n    v = mx\n    s = np.zeros_like(v, dtype=float)\n    valid = v > eps\n    s[valid] = (v[valid] - mn[valid]) / (v[valid] + eps)\n    result = float(s.mean())\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-region spread: mean distance of pixels above median to their centroid normalized by image diagonal'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        vals = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        vals = np.nan_to_num(img.astype(float))\n    h, w = vals.shape\n    if vals.size == 0:\n        return 0.0\n    thr = float(np.median(vals))\n    mask = vals > thr\n    if not mask.any():\n        return 0.0\n    ys, xs = np.indices((h, w))\n    weights = vals * mask\n    total = float(weights.sum())\n    if total <= eps:\n        # fallback to unweighted centroid of mask\n        total = float(mask.sum())\n        if total <= 0:\n            return 0.0\n        cx = float((xs * mask).sum()) / total\n        cy = float((ys * mask).sum()) / total\n        dists = np.hypot(xs[mask] - cx, ys[mask] - cy)\n        mean_dist = float(dists.mean())\n    else:\n        cx = float((xs * weights).sum()) / total\n        cy = float((ys * weights).sum()) / total\n        dists = np.hypot(xs[mask] - cx, ys[mask] - cy)\n        mean_dist = float(dists.mean())\n    diag = float(np.hypot(w, h)) + eps\n    result = mean_dist / diag\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels in the top 5% intensity (bright-spot density)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    try:\n        thresh = float(np.percentile(flat, 95))\n    except Exception:\n        return 0.0\n    count = float((flat > thresh).sum())\n    result = count / float(flat.size)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of strict local maxima (pixels greater than 8 neighbors)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 3 or w < 3:\n        return 0.0\n    C = img[1:-1, 1:-1]\n    up = img[:-2, 1:-1]\n    down = img[2:, 1:-1]\n    left = img[1:-1, :-2]\n    right = img[1:-1, 2:]\n    ul = img[:-2, :-2]\n    ur = img[:-2, 2:]\n    dl = img[2:, :-2]\n    dr = img[2:, 2:]\n    mask = (C > up) & (C > down) & (C > left) & (C > right) & (C > ul) & (C > ur) & (C > dl) & (C > dr)\n    count = float(np.count_nonzero(mask))\n    result = count / float(h * w)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio compactness of non-background bounding box (1=square, 0=very elongated)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mn = float(img.min())\n    rng = float(img.max() - mn)\n    eps = 1e-12\n    if rng <= eps:\n        return 0.0\n    fg = img > (mn + rng * 1e-6)  # treat strictly above min as foreground\n    if not np.any(fg):\n        return 0.0\n    ys, xs = np.where(fg)\n    y0, y1 = int(ys.min()), int(ys.max())\n    x0, x1 = int(xs.min()), int(xs.max())\n    bw = float(x1 - x0 + 1)\n    bh = float(y1 - y0 + 1)\n    if bw <= 0 or bh <= 0:\n        return 0.0\n    ratio = min(bw, bh) / max(bw, bh)\n    return float(np.clip(ratio, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean gradient magnitude normalized by mean absolute intensity (edge energy)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.size == 0:\n        return 0.0\n    denom = float(np.mean(np.abs(a))) + 1e-12\n    result = float(np.mean(mag)) / denom\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized spectral centroid (0=low-frequency, 1=high-frequency)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    img = img - img.mean()\n    try:\n        F = np.fft.fftshift(np.fft.fft2(img))\n    except Exception:\n        return 0.0\n    M = np.abs(F)\n    if M.sum() <= 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    maxr = float(r.max()) + 1e-12\n    centroid = float((M * r).sum()) / float(M.sum())\n    result = centroid / maxr\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Absolute correlation with image rotated by 180 degrees (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    b = np.rot90(a, 2)\n    if a.size != b.size:\n        a = a.ravel()\n        b = b.ravel()\n    A = a.ravel().astype(float)\n    B = b.ravel().astype(float)\n    if A.size == 0:\n        return 0.0\n    Am = A.mean()\n    Bm = B.mean()\n    Az = A - Am\n    Bz = B - Bm\n    num = float((Az * Bz).sum())\n    den = float(np.sqrt((Az * Az).sum() * (Bz * Bz).sum()) + eps)\n    if den <= eps:\n        return 0.0\n    corr = num / den\n    return float(np.clip(abs(corr), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Std of mean intensities across 4 concentric rings normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    maxr = r.max() + eps\n    bins = 4\n    ring_ids = np.floor((r / maxr) * bins).astype(int)\n    ring_ids = np.clip(ring_ids, 0, bins - 1)\n    means = []\n    for k in range(bins):\n        sel = (ring_ids == k)\n        if sel.any():\n            means.append(float(a[sel].mean()))\n    if len(means) <= 1:\n        return 0.0\n    global_std = float(a.std()) + eps\n    result = float(np.std(np.array(means))) / global_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of pixels within 1% of image min or max (saturation ratio)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    mn = float(flat.min())\n    mx = float(flat.max())\n    rng = mx - mn\n    if rng <= 0:\n        return 0.0\n    thr = rng * 0.01\n    sat = ((flat <= mn + thr) | (flat >= mx - thr)).sum()\n    result = float(sat) / float(flat.size)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean log(1+|Laplacian|) normalized by its max (texture sharpness)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 3 or w < 3:\n        return 0.0\n    C = img[1:-1, 1:-1]\n    up = img[:-2, 1:-1]\n    down = img[2:, 1:-1]\n    left = img[1:-1, :-2]\n    right = img[1:-1, 2:]\n    lap = np.abs(up + down + left + right - 4.0 * C)\n    if lap.size == 0:\n        return 0.0\n    loglap = np.log1p(lap)\n    meanlog = float(loglap.mean())\n    maxlog = float(loglap.max()) + eps\n    result = meanlog / maxlog\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Row fragmentation: fraction of row-to-row presence transitions (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h <= 1:\n        return 0.0\n    mn = float(a.min())\n    mx = float(a.max())\n    rng = mx - mn\n    if rng <= 1e-12:\n        return 0.0\n    # a row is considered 'present' if its max exceeds near-min threshold\n    thresh = mn + rng * 0.02\n    row_present = np.max(a, axis=1) > thresh\n    transitions = np.count_nonzero(row_present[:-1] != row_present[1:])\n    result = float(transitions) / float(h - 1)\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels above a simple foreground threshold (mean+std)'\n    import numpy as np\n    eps = 1e-12\n    arr_in = np.asarray(image)\n    if arr_in.size == 0:\n        return 0.0\n    if arr_in.ndim == 3:\n        a = np.nan_to_num(arr_in.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr_in.astype(float))\n    if a.size == 0:\n        return 0.0\n    thr = float(a.mean() + a.std())\n    mask = a > thr\n    result = float(np.count_nonzero(mask)) / float(a.size + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Shannon entropy of intensity distribution (normalized to [0..1])'\n    import numpy as np\n    eps = 1e-12\n    bins = 16\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    flat = img.ravel()\n    if flat.size == 0:\n        return 0.0\n    hist, _ = np.histogram(flat, bins=bins, range=(flat.min(), flat.max()))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p = p[p > 0]\n    if p.size == 0:\n        return 0.0\n    entropy = -float((p * np.log(p + eps)).sum())\n    # normalize by max possible entropy log(bins)\n    result = entropy / (np.log(bins) + eps)\n    result = float(np.clip(result, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant edge orientation as a fraction of 180deg (0..1) using gradient-weighted histogram'\n    import numpy as np\n    eps = 1e-12\n    bins = 8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= 0:\n        return 0.0\n    # orientation in [0,180)\n    ang = (np.degrees(np.arctan2(gy, gx)) + 180.0) % 180.0\n    # histogram\n    inds = np.floor(ang / (180.0 / bins)).astype(int)\n    inds = np.clip(inds, 0, bins - 1)\n    hist = np.zeros(bins, dtype=float)\n    for b in range(bins):\n        hist[b] = mag[inds == b].sum()\n    if hist.sum() <= 0:\n        return 0.0\n    dom = float(np.argmax(hist))\n    result = dom / float(bins - 1 + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized count of local bright peaks (3x3 maxima above mean+std)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h * w == 0:\n        return 0.0\n    thr = float(img.mean() + img.std())\n    center = img\n    # compare to 8 neighbors using rolled versions (wrap-around is acceptable here)\n    neigh_ge = np.ones_like(img, dtype=bool)\n    shifts = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n    for dy, dx in shifts:\n        neigh = np.roll(np.roll(img, dy, axis=0), dx, axis=1)\n        neigh_ge &= (center >= neigh)\n    peaks = neigh_ge & (center > thr)\n    count = float(np.count_nonzero(peaks))\n    result = count / float(h * w + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Approximate Harris corner density (fraction of pixels with strong corner response)'\n    import numpy as np\n    eps = 1e-12\n    k = 0.04\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h * w == 0:\n        return 0.0\n    gy, gx = np.gradient(img)\n    Ixx = gx * gx\n    Iyy = gy * gy\n    Ixy = gx * gy\n    # local sum over 3x3 window via rolled sum (wrap-around)\n    Sxx = np.zeros_like(Ixx)\n    Syy = np.zeros_like(Iyy)\n    Sxy = np.zeros_like(Ixy)\n    for dy in (-1, 0, 1):\n        for dx in (-1, 0, 1):\n            Sxx += np.roll(np.roll(Ixx, dy, axis=0), dx, axis=1)\n            Syy += np.roll(np.roll(Iyy, dy, axis=0), dx, axis=1)\n            Sxy += np.roll(np.roll(Ixy, dy, axis=0), dx, axis=1)\n    # Harris response\n    det = Sxx * Syy - Sxy * Sxy\n    trace = Sxx + Syy\n    R = det - k * (trace ** 2)\n    # threshold: use mean + std of positive responses\n    pos = R[R > 0]\n    if pos.size == 0:\n        return 0.0\n    thr = float(pos.mean() + pos.std())\n    corners = (R > thr)\n    density = float(np.count_nonzero(corners)) / float(h * w + eps)\n    return float(density)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Strength of vertical banding: std of column means normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    col_means = a.mean(axis=0)\n    gstd = float(a.std()) + eps\n    result = float(col_means.std()) / gstd\n    # clip to reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Compactness of bright region: perimeter^2 / area for pixels above mean+std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    thr = float(img.mean() + img.std())\n    mask = img > thr\n    area = float(np.count_nonzero(mask))\n    if area < 1:\n        return 0.0\n    # perimeter: count mask pixels that have at least one 4-neighbor outside mask (using rolls)\n    outside = np.zeros_like(mask, dtype=bool)\n    outside |= ~np.roll(mask, 1, axis=0)\n    outside |= ~np.roll(mask, -1, axis=0)\n    outside |= ~np.roll(mask, 1, axis=1)\n    outside |= ~np.roll(mask, -1, axis=1)\n    border = mask & outside\n    perim = float(np.count_nonzero(border))\n    result = (perim * perim) / (area + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast between top quartile mean and bottom quartile mean normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    flat = img.ravel()\n    if flat.size == 0:\n        return 0.0\n    q1 = np.percentile(flat, 25)\n    q3 = np.percentile(flat, 75)\n    low_mean = float(flat[flat <= q1].mean()) if np.any(flat <= q1) else float(flat.mean())\n    high_mean = float(flat[flat >= q3].mean()) if np.any(flat >= q3) else float(flat.mean())\n    gstd = float(flat.std()) + eps\n    result = (high_mean - low_mean) / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized offset of intensity center-of-mass from image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    vals = a - a.min()\n    total = float(vals.sum()) + eps\n    cy = float((vals * ys).sum()) / total\n    cx = float((vals * xs).sum()) / total\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    maxd = np.hypot(center_x, center_y) + eps\n    result = dist / maxd\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio from FFT (fraction of total energy in outer half of frequencies)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    img = img - img.mean()\n    try:\n        F = np.fft.fftshift(np.fft.fft2(img))\n    except Exception:\n        return 0.0\n    M = np.abs(F) ** 2\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    maxr = float(r.max()) + eps\n    # high frequency region: r > 0.5*maxr\n    mask = r > (0.5 * maxr)\n    total = float(M.sum()) + eps\n    high = float(M[mask].sum())\n    result = high / total\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Shannon entropy of intensity histogram (0..1, higher => more complex)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    bins = 32\n    hist, _ = np.histogram(a.ravel(), bins=bins, range=(float(a.min()), float(a.max()) + eps))\n    hist = hist.astype(float)\n    total = hist.sum() + eps\n    p = hist / total\n    p_nonzero = p[p > 0]\n    H = -np.sum(p_nonzero * np.log(p_nonzero + eps))\n    # normalize by log(bins)\n    result = H / (np.log(bins) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0 for grayscale), normalized by max intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    arr = np.nan_to_num(img.astype(float))\n    R = arr[:, :, 0]\n    G = arr[:, :, 1]\n    B = arr[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(rg))\n    mean_yb = float(np.mean(yb))\n    raw = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    norm = float(np.nanmax(arr)) + eps\n    result = raw / norm\n    return float(np.clip(result, 0.0, 5.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect bias of non-empty bounding box (positive => wider, negative => taller) in [-1..1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    tol = (np.nanmax(a) - np.nanmin(a)) * 0.05 + eps\n    mask = np.abs(a - np.nanmean(a)) > tol\n    if not mask.any():\n        return 0.0\n    rows = np.where(mask.any(axis=1))[0]\n    cols = np.where(mask.any(axis=0))[0]\n    if rows.size == 0 or cols.size == 0:\n        return 0.0\n    r0, r1 = rows[0], rows[-1]\n    c0, c1 = cols[0], cols[-1]\n    bbox_h = max(1, r1 - r0 + 1)\n    bbox_w = max(1, c1 - c0 + 1)\n    aspect = float(bbox_w) / float(bbox_h)\n    # map aspect to [-1,1]: (aspect-1)/(aspect+1) where >0 means wider\n    result = (aspect - 1.0) / (aspect + 1.0 + eps)\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Eccentricity of bright-region centroid (0=centered .. 1=corner) using top 10% pixels'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    thresh = np.percentile(a, 90)\n    mask = a >= thresh\n    if not mask.any():\n        return 0.0\n    weights = a * mask\n    s = float(weights.sum()) + eps\n    ys, xs = np.indices((h, w))\n    cx = float((weights * xs).sum()) / s\n    cy = float((weights * ys).sum()) / s\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dx = cx - center_x\n    dy = cy - center_y\n    dist = np.hypot(dx, dy)\n    maxdist = np.hypot(center_x, center_y) + eps\n    result = dist / maxdist\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of border pixels (10% width) that are nonzero (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    bw = max(1, min(h, w) // 10)\n    top = a[0:bw, :]\n    bot = a[-bw:, :]\n    left = a[:, 0:bw]\n    right = a[:, -bw:]\n    border = np.concatenate([top.ravel(), bot.ravel(), left.ravel(), right.ravel()])\n    if border.size == 0:\n        return 0.0\n    tol = (np.nanmax(a) - np.nanmin(a)) * 0.02 + eps\n    nonzero = float(np.count_nonzero(np.abs(border) > tol))\n    result = nonzero / float(border.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bimodality score of intensity histogram (0..1, higher => clearer two peaks)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    bins = 64\n    hist, edges = np.histogram(a.ravel(), bins=bins)\n    hist = hist.astype(float)\n    if hist.sum() == 0:\n        return 0.0\n    # find two highest peaks\n    inds = np.argsort(hist)[-2:]\n    p1, p2 = min(inds), max(inds)\n    p1v = hist[p1]\n    p2v = hist[p2]\n    if p1 == p2:\n        return 0.0\n    between = hist[p1:p2+1]\n    min_between = float(np.min(between)) if between.size > 0 else 0.0\n    denom = min(p1v, p2v) + eps\n    score = 1.0 - (min_between / denom)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant gradient orientation score (-1 vertical .. +1 horizontal), weighted by confidence'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.sum() < eps:\n        return 0.0\n    ang = np.arctan2(gy, gx)  # -pi..pi\n    # compute resultant vector of angle doubled so that 180deg apart orientations align\n    cx = np.sum(np.cos(2.0 * ang) * mag)\n    sy = np.sum(np.sin(2.0 * ang) * mag)\n    R = np.hypot(cx, sy)\n    Rnorm = R / (mag.sum() + eps)  # [0..1] confidence\n    theta = 0.5 * np.arctan2(sy, cx + eps)  # mean angle\n    # use cos(2*theta) to map to [-1..1] where +1 ~ horizontal, -1 ~ vertical\n    orient = np.cos(2.0 * theta)\n    result = float(np.clip(orient * Rnorm, -1.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of connected bright components (threshold mean) normalized by image area'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    thresh = np.nanmean(a)\n    mask = a > thresh\n    visited = np.zeros_like(mask, dtype=bool)\n    comps = 0\n    # simple stack-based 4-connectivity flood fill\n    coords = np.argwhere(mask)\n    if coords.size == 0:\n        return 0.0\n    for (rstart, cstart) in coords:\n        if visited[rstart, cstart]:\n            continue\n        if not mask[rstart, cstart]:\n            continue\n        comps += 1\n        stack = [(int(rstart), int(cstart))]\n        visited[rstart, cstart] = True\n        while stack:\n            r, c = stack.pop()\n            # neighbors\n            if r > 0 and mask[r-1, c] and not visited[r-1, c]:\n                visited[r-1, c] = True; stack.append((r-1, c))\n            if r+1 < h and mask[r+1, c] and not visited[r+1, c]:\n                visited[r+1, c] = True; stack.append((r+1, c))\n            if c > 0 and mask[r, c-1] and not visited[r, c-1]:\n                visited[r, c-1] = True; stack.append((r, c-1))\n            if c+1 < w and mask[r, c+1] and not visited[r, c+1]:\n                visited[r, c+1] = True; stack.append((r, c+1))\n    # normalize by area (components per 1000 pixels)\n    result = comps / (float(a.size) / 1000.0 + eps)\n    return float(np.clip(result, 0.0, float(max(0.0, result))))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Coarse-to-fine energy ratio using 2x2 block averaging (higher => smoother)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # trim to even dimensions\n    he = h - (h % 2)\n    we = w - (w % 2)\n    a_trim = a[:he, :we]\n    if a_trim.size == 0:\n        return 0.0\n    # block mean 2x2\n    a_blocks = a_trim.reshape(he//2, 2, we//2, 2).mean(axis=(1, 3))\n    # upsample by repeating\n    coarse = np.repeat(np.repeat(a_blocks, 2, axis=0), 2, axis=1)\n    # pad if original had odd row/col\n    if he != h or we != w:\n        coarse_full = np.zeros_like(a)\n        coarse_full[:he, :we] = coarse\n        if he < h:\n            coarse_full[he:, :we] = np.mean(a[he:, :we])\n        if we < w:\n            coarse_full[:he, we:] = np.mean(a[:he, we:])\n        if he < h and we < w:\n            coarse_full[he:, we:] = np.mean(a[he:, we:])\n        coarse = coarse_full\n    fine = a - coarse\n    coarse_energy = float(np.sum(coarse * coarse))\n    fine_energy = float(np.sum(fine * fine)) + eps\n    result = coarse_energy / fine_energy\n    # compress scale to reasonable range\n    result = np.log1p(result)\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Interquartile contrast: (75th-25th percentile) relative to mean intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    p75 = np.percentile(a, 75)\n    p25 = np.percentile(a, 25)\n    meanv = float(np.mean(a)) + eps\n    result = (float(p75) - float(p25)) / meanv\n    return float(np.clip(result, 0.0, 10.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of pixels brighter than mean+std (bright-sparsity)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h * w == 0:\n        return 0.0\n    thr = float(img.mean() + img.std())\n    count = float(np.count_nonzero(img > thr))\n    result = count / float(h * w + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of 2D FFT energy concentrated in low-frequency band (centered)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    F = np.fft.fftshift(np.fft.fft2(a))\n    power = np.abs(F) ** 2\n    cy, cx = h // 2, w // 2\n    maxr = max(1, min(h, w) // 8)\n    ys, xs = np.ogrid[:h, :w]\n    r = np.hypot(ys - cy, xs - cx)\n    low_mask = (r <= maxr)\n    total = float(power.sum()) + eps\n    low = float(power[low_mask].sum())\n    result = low / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variance of discrete Laplacian (focus/sharpness measure)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h * w == 0:\n        return 0.0\n    up = np.roll(img, -1, axis=0)\n    down = np.roll(img, 1, axis=0)\n    left = np.roll(img, -1, axis=1)\n    right = np.roll(img, 1, axis=1)\n    lap = 4.0 * img - (up + down + left + right)\n    var = float(np.var(lap))\n    # normalize by image dynamic range to be robust across scales\n    dyn = float(np.max(img) - np.min(img)) + eps\n    result = var / (dyn ** 2 + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric (0 for grayscale), normalized by image range'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    img = np.nan_to_num(arr.astype(float))\n    R = img[:, :, 0]\n    G = img[:, :, 1]\n    B = img[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(rg))\n    mean_yb = float(np.mean(yb))\n    raw = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    rng = float(np.max(img) - np.min(img)) + eps\n    result = raw / rng\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean log-variance of non-overlapping blocks (texture coarseness)'\n    import numpy as np\n    eps = 1e-12\n    BLOCK = 8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    bh = max(1, min(BLOCK, h))\n    bw = max(1, min(BLOCK, w))\n    # number of full blocks\n    nh = h // bh\n    nw = w // bw\n    if nh == 0 or nw == 0:\n        # fallback to global variance\n        v = float(np.var(a))\n        return float(np.log(v + eps))\n    vals = []\n    for i in range(nh):\n        for j in range(nw):\n            block = a[i*bh:(i+1)*bh, j*bw:(j+1)*bw]\n            vals.append(float(np.var(block)))\n    vals = np.array(vals, dtype=float)\n    mean_log_var = float(np.mean(np.log(vals + eps)))\n    # scale to a reasonable numeric range\n    return float(mean_log_var)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Concentration of edge orientations (0..1), 1 if all gradient vectors align'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    tot = mag.sum()\n    if tot <= eps:\n        return 0.0\n    ux = gx / (mag + eps)\n    uy = gy / (mag + eps)\n    sx = (ux * mag).sum()\n    sy = (uy * mag).sum()\n    resultant = np.hypot(sx, sy)\n    result = resultant / (tot + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Foreground solidity: foreground area divided by its bounding-box area (thresholded)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h * w == 0:\n        return 0.0\n    thr = float(img.mean() + 0.5 * img.std())\n    fg = (img > thr)\n    area = float(np.count_nonzero(fg))\n    if area == 0.0:\n        return 0.0\n    ys, xs = np.where(fg)\n    if ys.size == 0 or xs.size == 0:\n        return 0.0\n    ymin, ymax = ys.min(), ys.max()\n    xmin, xmax = xs.min(), xs.max()\n    bbox_area = float((ymax - ymin + 1) * (xmax - xmin + 1))\n    if bbox_area <= eps:\n        return 0.0\n    result = area / (bbox_area + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized variance of mean intensities across concentric radial bins (ringiness)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    maxr = int(np.ceil(r.max()))\n    if maxr <= 0:\n        return 0.0\n    means = []\n    for rr in range(maxr + 1):\n        mask = (np.floor(r) == rr)\n        if np.any(mask):\n            means.append(float(a[mask].mean()))\n    means = np.array(means, dtype=float)\n    if means.size <= 1:\n        return 0.0\n    var_r = float(np.var(means))\n    global_var = float(np.var(a)) + eps\n    result = var_r / global_var\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized gap between 90th and 50th percentiles of intensities'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        flat = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        flat = np.nan_to_num(arr.astype(float)).ravel()\n    if flat.size == 0:\n        return 0.0\n    p50 = float(np.percentile(flat, 50.0))\n    p90 = float(np.percentile(flat, 90.0))\n    rng = float(flat.max() - flat.min()) + eps\n    result = (p90 - p50) / rng\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute corner contrast: average absolute difference between corners and global mean, normalized'\n    import numpy as np\n    eps = 1e-12\n    COR = 0.1  # fraction of min dimension for corner patch size\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h * w == 0:\n        return 0.0\n    bw = max(1, int(min(h, w) * COR))\n    patches = []\n    patches.append(img[:bw, :bw])         # top-left\n    patches.append(img[:bw, -bw:])        # top-right\n    patches.append(img[-bw:, :bw])        # bottom-left\n    patches.append(img[-bw:, -bw:])       # bottom-right\n    global_mean = float(img.mean())\n    diffs = [abs(p.mean() - global_mean) for p in patches if p.size]\n    if len(diffs) == 0:\n        return 0.0\n    avg_diff = float(np.mean(diffs))\n    overall_std = float(np.std(img)) + eps\n    result = avg_diff / overall_std\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian variance as a simple focus/sharpness measure (higher => more texture)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # discrete Laplacian via 4-neighbor stencil using rolls\n    lap = 4.0 * a - (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) + np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1))\n    # ignore wrap artifacts by zeroing borders\n    lap[0, :] = 0.0\n    lap[-1, :] = 0.0\n    lap[:, 0] = 0.0\n    lap[:, -1] = 0.0\n    var = float(np.var(lap))\n    # normalize by overall intensity scale\n    norm = float(np.mean(np.abs(a))) + eps\n    result = var / norm\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels near intensity extremes (saturated/clipped proportion)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    vmin = float(arr.min())\n    vmax = float(arr.max())\n    if vmax == vmin:\n        return 0.0\n    rng = vmax - vmin\n    thr = max(1e-6, 0.01 * rng)  # within 1% of range\n    sat = np.logical_or(arr <= vmin + thr, arr >= vmax - thr)\n    result = float(np.count_nonzero(sat)) / float(arr.size)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Global edge-orientation coherence (0..1): how aligned gradient directions are'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy) + eps\n    # unit vectors weighted by magnitude -> resultant length normalized by total magnitude\n    sx = float((gx * mag).sum())\n    sy = float((gy * mag).sum())\n    # but above sums produce scale; better use unit orientation sum weighted by mag\n    ux = (gx / mag) * mag\n    uy = (gy / mag) * mag\n    Rx = float(ux.sum())\n    Ry = float(uy.sum())\n    resultant = np.hypot(Rx, Ry)\n    total = float(mag.sum()) + eps\n    result = resultant / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio in gradients (proportion of gradient energy in strong edges)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    if mag.size == 0:\n        return 0.0\n    med = float(np.median(mag)) + eps\n    # count energy above a multiple of median (captures strong high-frequency content)\n    mask = mag > (2.0 * med)\n    hf_energy = float((mag * mask).sum())\n    total = float(mag.sum()) + eps\n    result = hf_energy / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram tail asymmetry: normalized difference between upper and lower tail masses (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        vals = np.nan_to_num(img.mean(axis=2).ravel().astype(float))\n    else:\n        vals = np.nan_to_num(img.ravel().astype(float))\n    if vals.size == 0:\n        return 0.0\n    p25 = float(np.percentile(vals, 25.0))\n    p75 = float(np.percentile(vals, 75.0))\n    lower = np.count_nonzero(vals < p25)\n    upper = np.count_nonzero(vals > p75)\n    total = float(vals.size) + eps\n    result = (upper - lower) / total\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average number of binary edge crossings per row (texture frequency)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if w < 3 or h < 1:\n        return 0.0\n    # detect strong column-wise differences\n    col_diff = np.abs(np.diff(arr, axis=1))\n    thr = float(np.std(col_diff)) * 0.5 + eps\n    edges = col_diff > thr  # shape (h, w-1)\n    # crossings per row are transitions in this binary edge map\n    transitions = np.count_nonzero(np.diff(edges.astype(np.int8), axis=1), axis=1)  # length h\n    avg_trans = float(np.mean(transitions)) if transitions.size else 0.0\n    # normalize by possible transitions (w-2) to keep scale comparable\n    denom = max(1.0, float(max(1, w - 2)))\n    result = avg_trans / denom\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'RGB channel contrast: (max channel mean - min channel mean) normalized by gray std (0 if not RGB)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    # use first three channels\n    ch_means = a[..., :3].mean(axis=(0, 1))\n    diff = float(np.max(ch_means) - np.min(ch_means))\n    gray = a[..., :3].mean(axis=2)\n    gstd = float(np.std(gray)) + eps\n    result = diff / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Central disk prominence: center mean vs surrounding annulus normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    rx = xs - cx\n    ry = ys - cy\n    r = np.hypot(rx, ry)\n    R = max(1.0, min(h, w) / 6.0)\n    inner = r <= R\n    outer = np.logical_and(r > R, r <= (2.0 * R))\n    if not np.any(inner) or not np.any(outer):\n        return 0.0\n    center_mean = float(a[inner].mean())\n    ann_mean = float(a[outer].mean())\n    gstd = float(a.std()) + eps\n    result = (center_mean - ann_mean) / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of energy in high spatial frequencies via 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    F = np.fft.fft2(a)\n    P = np.abs(F) ** 2\n    P_shift = np.fft.fftshift(P)\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    cutoff = max(1.0, min(h, w) / 8.0)\n    high = P_shift[r > cutoff].sum()\n    total = P_shift.sum() + eps\n    result = float(high) / float(total)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized count of strong local maxima (strictly greater than 8 neighbors, normalized by image area)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    center = a[1:-1, 1:-1]\n    neighbors = [\n        a[:-2, :-2], a[:-2, 1:-1], a[:-2, 2:],\n        a[1:-1, :-2],              a[1:-1, 2:],\n        a[2:, :-2],  a[2:, 1:-1],  a[2:, 2:]\n    ]\n    cond = np.ones_like(center, dtype=bool)\n    for nb in neighbors:\n        cond &= (center > nb)\n    # only count maxima that are above a threshold (median + std) to ignore noise\n    thr = float(np.median(a)) + float(np.std(a))\n    cond &= (center > thr)\n    count = float(np.count_nonzero(cond))\n    area = float(h * w) + eps\n    result = count / area\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Shannon entropy of the image intensity histogram (higher => more complexity)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    vmin = float(a.min())\n    vmax = float(a.max())\n    if vmax <= vmin + eps:\n        return 0.0\n    hist, _ = np.histogram(a.ravel(), bins=256, range=(vmin, vmax))\n    prob = hist.astype(float) / (hist.sum() + eps)\n    prob = prob[prob > 0]\n    ent = -np.sum(prob * np.log2(prob + eps))\n    return float(ent)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels near global min or max intensity (saturation at both ends)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    vmin = float(a.min()); vmax = float(a.max())\n    if vmax <= vmin + eps:\n        return 0.0\n    tol = 0.05 * (vmax - vmin)\n    near_min = (a <= vmin + tol)\n    near_max = (a >= vmax - tol)\n    frac = float(np.count_nonzero(near_min | near_max)) / float(a.size)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance between intensity-weighted centroid and image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    ys, xs = np.indices((h, w))\n    total = float(a.sum())\n    if total <= eps:\n        return 0.0\n    cx = (xs * a).sum() / (total + eps)\n    cy = (ys * a).sum() / (total + eps)\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    max_dist = np.hypot(center_x, center_y) + eps\n    result = dist / max_dist\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of low-frequency energy to total energy in the 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image, dtype=float)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2))\n    else:\n        a = np.nan_to_num(img)\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # remove DC bias\n    a = a - a.mean()\n    F = np.fft.fftshift(np.fft.fft2(a))\n    power = np.abs(F) ** 2\n    kh = max(1, min(h, w) // 8)\n    kw = kh\n    ch = h // 2; cw = w // 2\n    low = power[ch - kh: ch + kh + 1, cw - kw: cw + kw + 1]\n    low_energy = float(low.sum())\n    total_energy = float(power.sum()) + eps\n    result = low_energy / total_energy\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dispersion of gradient directions (circular variance: 0 aligned, 1 dispersed)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    M = mag.sum()\n    if M <= eps:\n        return 0.0\n    ux = (gx / (mag + eps)).ravel()\n    uy = (gy / (mag + eps)).ravel()\n    # resultant vector length normalized by total weight\n    R = np.hypot(ux.sum(), uy.sum()) / (mag.size + eps) * (mag.size / (M + eps))\n    # convert to circular variance like measure; clamp to [0,1]\n    circ_var = float(np.clip(1.0 - R, 0.0, 1.0))\n    return float(circ_var)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of image area occupied by the bounding box of bright foreground (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    vmin = float(a.min()); vmax = float(a.max())\n    if vmax <= vmin + eps:\n        return 0.0\n    thresh = float(a.mean()) + 0.25 * (vmax - vmin)\n    mask = a > thresh\n    if not mask.any():\n        return 0.0\n    ys, xs = np.where(mask)\n    y0 = ys.min(); y1 = ys.max()\n    x0 = xs.min(); x1 = xs.max()\n    bbox_area = float((y1 - y0 + 1) * (x1 - x0 + 1))\n    frac = bbox_area / float(h * w)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized count of peaks in the intensity histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    vmin = float(a.min()); vmax = float(a.max())\n    if vmax <= vmin + eps:\n        return 0.0\n    bins = 32\n    hist, _ = np.histogram(a.ravel(), bins=bins, range=(vmin, vmax))\n    if hist.sum() == 0:\n        return 0.0\n    # simple smoothing\n    s = np.convolve(hist.astype(float), np.array([1.0, 1.0, 1.0]) / 3.0, mode='same')\n    peaks = 0\n    mean_s = s.mean()\n    for i in range(1, len(s) - 1):\n        if s[i] > s[i - 1] and s[i] > s[i + 1] and s[i] > mean_s:\n            peaks += 1\n    result = float(peaks) / float(bins)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average local entropy over non-overlapping blocks (higher => textured)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    block = max(4, min(h, w) // 8)\n    bh = block; bw = block\n    n_h = h // bh\n    n_w = w // bw\n    if n_h == 0 or n_w == 0:\n        # fallback single-block entropy\n        vals = a.ravel()\n        if vals.size == 0:\n            return 0.0\n        hist, _ = np.histogram(vals, bins=16)\n        p = hist.astype(float) / (hist.sum() + eps)\n        p = p[p > 0]\n        ent = -np.sum(p * np.log2(p + eps))\n        return float(ent)\n    ents = []\n    for i in range(n_h):\n        for j in range(n_w):\n            block_vals = a[i * bh:(i + 1) * bh, j * bw:(j + 1) * bw].ravel()\n            if block_vals.size == 0:\n                continue\n            hist, _ = np.histogram(block_vals, bins=16)\n            p = hist.astype(float) / (hist.sum() + eps)\n            p = p[p > 0]\n            ent = -np.sum(p * np.log2(p + eps))\n            ents.append(ent)\n    if not ents:\n        return 0.0\n    return float(np.mean(ents))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of border pixels within a small tolerance of the border median (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    rim = max(1, min(h, w) // 12)\n    parts = []\n    parts.append(a[:rim, :].ravel())\n    parts.append(a[-rim:, :].ravel())\n    if h > 2 * rim:\n        parts.append(a[rim:-rim, :rim].ravel())\n        parts.append(a[rim:-rim, -rim:].ravel())\n    border = np.concatenate([p for p in parts if p.size])\n    if border.size == 0:\n        return 0.0\n    med = float(np.median(border))\n    std = float(border.std())\n    vmin = float(a.min()); vmax = float(a.max())\n    range_tol = max(0.02 * (vmax - vmin), 0.25 * std, eps)\n    consistent = np.abs(border - med) <= range_tol\n    frac = float(np.count_nonzero(consistent)) / float(border.size)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Correlation between a square patch and its transpose as a main-diagonal symmetry score (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    m = min(h, w)\n    if m < 2:\n        return 0.0\n    # use top-left m x m square for a consistent region\n    sq = a[:m, :m]\n    sqT = sq.T\n    L = sq.ravel() - sq.mean()\n    R = sqT.ravel() - sqT.mean()\n    denom = (np.linalg.norm(L) * np.linalg.norm(R) + eps)\n    corr = float(np.dot(L, R) / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels with gradient magnitude above mean+std (edge density)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(img)\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + mag.std())\n    mask = mag > thr\n    result = float(np.count_nonzero(mask)) / float(h * w + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized cross-correlation between left half and mirrored right half (1 = perfect symmetry)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = img[:, :mid]\n    right = img[:, -mid:] if mid > 0 else np.empty_like(left)\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    right_flipped = np.fliplr(right)\n    L = left.ravel().astype(float)\n    R = right_flipped.ravel().astype(float)\n    mL = L.mean()\n    mR = R.mean()\n    num = ((L - mL) * (R - mR)).sum()\n    den = (np.sqrt(((L - mL) ** 2).sum() * ((R - mR) ** 2).sum()) + eps)\n    corr = float(num) / float(den)\n    # map from [-1,1] to [0,1] so 1 = perfect symmetry, 0 = perfect anti-symmetry\n    result = (corr + 1.0) / 2.0\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Kurtosis of column-sum intensities (higher -> sharper vertical concentration)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    cols = img.sum(axis=0)\n    if cols.size == 0:\n        return 0.0\n    m = float(cols.mean())\n    s = float(cols.std()) + eps\n    kurt = float(np.mean(((cols - m) ** 4))) / (s ** 4 + eps)\n    # convert to excess kurtosis (0 for normal) for interpretability\n    excess = kurt - 3.0\n    # clamp to a reasonable range\n    return float(np.clip(excess, -50.0, 100.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels brighter than mean + 0.5*std (simple foreground ratio)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        vals = np.nan_to_num(arr.astype(float))\n    thr = float(vals.mean() + 0.5 * vals.std())\n    mask = vals > thr\n    n_total = float(vals.size) + eps\n    result = float(np.count_nonzero(mask)) / n_total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0 for grayscale)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    # use first three channels as R,G,B\n    R = np.nan_to_num(arr[:, :, 0].astype(float))\n    G = np.nan_to_num(arr[:, :, 1].astype(float))\n    B = np.nan_to_num(arr[:, :, 2].astype(float))\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(np.abs(rg.mean()))\n    mean_yb = float(np.abs(yb.mean()))\n    # Hasler & Suesstrunk metric\n    score = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    return float(score)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of foreground pixels that lie within a narrow image border (object touches border)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    bw = max(1, int(round(0.05 * min(h, w))))\n    # define foreground as pixels greater than mean (robust enough)\n    mask = img > img.mean()\n    fg_count = float(np.count_nonzero(mask))\n    if fg_count <= 0:\n        return 0.0\n    top = mask[:bw, :].sum()\n    bottom = mask[-bw:, :].sum()\n    left = mask[:, :bw].sum()\n    right = mask[:, -bw:].sum()\n    border_cnt = float(top + bottom + left + right)\n    # pixels in corners counted twice; approximate is acceptable for efficiency\n    result = border_cnt / (fg_count + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial spectral centroid of 2D FFT (0 = low-frequency dominated, 1 = high-frequency dominated)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n    except Exception:\n        return 0.0\n    P = np.abs(F) ** 2\n    cy, cx = h // 2, w // 2\n    ys = np.arange(h)[:, None] - cy\n    xs = np.arange(w)[None, :] - cx\n    r = np.hypot(ys, xs)\n    total = P.sum() + eps\n    centroid = (r * P).sum() / total\n    rmax = float(np.hypot(max(cy, h - cy), max(cx, w - cx))) + eps\n    norm = centroid / rmax\n    return float(np.clip(norm, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Structure-tensor coherence (0 = isotropic, 1 = strong single orientation)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(img)\n    Jxx = (gx * gx).mean()\n    Jyy = (gy * gy).mean()\n    Jxy = (gx * gy).mean()\n    trace = Jxx + Jyy\n    det = Jxx * Jyy - Jxy * Jxy\n    # eigenvalues of 2x2: (trace \u00b1 sqrt(trace^2 - 4 det))/2\n    disc = max(trace * trace - 4.0 * det, 0.0)\n    l1 = 0.5 * (trace + np.sqrt(disc))\n    l2 = 0.5 * (trace - np.sqrt(disc))\n    # coherence measure\n    result = (l1 - l2) / (l1 + l2 + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Interquartile range (75th-25th percentile) normalized by median (robust contrast)'\n    import numpy as np\n    eps = 1e-12\n    vals = np.asarray(image)\n    if vals.size == 0:\n        return 0.0\n    if vals.ndim == 3:\n        vals = np.nan_to_num(vals.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(vals.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    p25 = float(np.percentile(vals, 25))\n    p75 = float(np.percentile(vals, 75))\n    med = float(np.median(vals))\n    denom = (abs(med) + eps)\n    result = (p75 - p25) / denom\n    return float(max(0.0, result))\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local variance (3x3) normalized by global variance (0..10)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute local mean (3x3) using rolls\n    s = np.zeros_like(a)\n    s += a\n    s += np.roll(a, 1, axis=0)\n    s += np.roll(a, -1, axis=0)\n    s += np.roll(a, 1, axis=1)\n    s += np.roll(a, -1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, 1, axis=0), -1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), 1, axis=1)\n    s += np.roll(np.roll(a, -1, axis=0), -1, axis=1)\n    local_mean = s / 9.0\n    local_var = np.mean((a - local_mean) ** 2)\n    global_var = float(a.var()) + eps\n    result = local_var / global_var\n    # clip to avoid extreme values from tiny denominators\n    return float(np.clip(result, 0.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Shannon entropy of intensity histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 32\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    # compute histogram\n    mn, mx = float(a.min()), float(a.max())\n    if mn == mx:\n        return 0.0\n    hist, _ = np.histogram(a, bins=bins, range=(mn, mx))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p = p[p > 0]\n    entropy = -np.sum(p * np.log2(p + eps))\n    max_entropy = np.log2(bins)\n    result = entropy / (max_entropy + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast between bright and dark tails: (mean90 - mean10)/std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    p90 = np.percentile(a, 90)\n    p10 = np.percentile(a, 10)\n    bright = a[a >= p90]\n    dark = a[a <= p10]\n    if bright.size == 0 or dark.size == 0:\n        return 0.0\n    mean90 = float(bright.mean())\n    mean10 = float(dark.mean())\n    std = float(a.std()) + eps\n    result = (mean90 - mean10) / std\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels with strong gradients (adaptive threshold) (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + 0.5 * mag.std())\n    count = float((mag > thr).sum())\n    result = count / (float(mag.size) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized imbalance of quadrant means (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mh = h // 2\n    mw = w // 2\n    q1 = a[:mh, :mw]\n    q2 = a[:mh, mw:]\n    q3 = a[mh:, :mw]\n    q4 = a[mh:, mw:]\n    quads = np.array([q.mean() if q.size else 0.0 for q in (q1, q2, q3, q4)], dtype=float)\n    global_std = float(a.std()) + eps\n    quad_std = float(quads.std())\n    result = quad_std / global_std\n    # reasonable cap\n    return float(np.clip(result, 0.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio using Laplacian (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # simple 4-neighbor Laplacian\n    neigh = (np.roll(a, 1, 0) + np.roll(a, -1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 1)) / 4.0\n    lap = a - neigh\n    hf_energy = float((lap ** 2).sum())\n    total_energy = float((a ** 2).sum()) + eps\n    result = hf_energy / (total_energy + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score based on correlation (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:] if mid > 0 else np.array([], dtype=float).reshape(h, 0)\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    # mirror right to align\n    right_mir = np.fliplr(right)\n    # crop to common shape\n    min_cols = min(left.shape[1], right_mir.shape[1])\n    left_c = left[:, :min_cols].ravel()\n    right_c = right_mir[:, :min_cols].ravel()\n    if left_c.size == 0 or right_c.size == 0:\n        return 0.0\n    # zero-mean correlation\n    left_c -= left_c.mean()\n    right_c -= right_c.mean()\n    denom = np.sqrt((left_c ** 2).sum() * (right_c ** 2).sum()) + eps\n    corr = float((left_c * right_c).sum()) / denom\n    result = max(0.0, corr)  # negative correlation considered no symmetry\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientation histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= eps:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    # bin orientations into [0..pi) so opposite directions count the same\n    theta = np.mod(theta, np.pi)\n    hist, _ = np.histogram(theta, bins=bins, range=(0.0, np.pi), weights=mag)\n    p = hist.astype(float) / (hist.sum() + eps)\n    p = p[p > 0]\n    entropy = -np.sum(p * np.log2(p + eps))\n    max_entropy = np.log2(bins)\n    result = entropy / (max_entropy + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Annular contrast: (mean annulus - mean center)/std overall'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    maxr = r.max() if r.size else 0.0\n    if maxr <= eps:\n        return 0.0\n    # center radius = 0.25 * max, annulus between 0.35 and 0.6 of max (adaptive)\n    r0 = 0.25 * maxr\n    r1 = 0.35 * maxr\n    r2 = 0.6 * maxr\n    center_mask = r <= r0\n    ann_mask = (r >= r1) & (r <= r2)\n    if not center_mask.any() or not ann_mask.any():\n        return 0.0\n    center_mean = float(a[center_mask].mean())\n    ann_mean = float(a[ann_mask].mean())\n    overall_std = float(a.std()) + eps\n    result = (ann_mean - center_mean) / overall_std\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gini coefficient of intensity distribution (0..1), measures concentration'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    # shift to non-negative\n    vals = vals - vals.min()\n    if vals.sum() <= eps:\n        return 0.0\n    sorted_vals = np.sort(vals)\n    n = sorted_vals.size\n    idx = np.arange(1, n + 1, dtype=float)\n    gini = (2.0 * np.sum(idx * sorted_vals) / (n * sorted_vals.sum())) - (n + 1.0) / n\n    result = float(np.clip(gini, 0.0, 1.0))\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram entropy of image intensities (bits), normalized by log2(bins)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    # use fixed number of bins for stability\n    bins = 32\n    vmin = float(a.min())\n    vmax = float(a.max())\n    if vmax <= vmin:\n        return 0.0\n    hist, _ = np.histogram(a.ravel(), bins=bins, range=(vmin, vmax), density=True)\n    # avoid log(0)\n    hist = hist + eps\n    ent = -np.sum(hist * np.log2(hist))\n    # normalize by max entropy log2(bins)\n    result = ent / (np.log2(bins) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance of intensity center-of-mass from geometric center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    total = float(img.sum()) + eps\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    mx = float((img * xs).sum()) / total\n    my = float((img * ys).sum()) / total\n    dx = mx - cx\n    dy = my - cy\n    # normalize by image diagonal\n    diag = np.hypot(w, h) + eps\n    result = np.hypot(dx, dy) / diag\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of strong local maxima (peaks) relative to image area'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    # threshold: median + 0.5*std\n    med = float(np.median(a))\n    std = float(a.std())\n    thresh = med + 0.5 * std\n    # pad with very low value so edges are not compared to wrapped neighbors\n    pad_val = float(a.min()) - 1.0\n    p = np.pad(a, pad_width=1, mode='constant', constant_values=pad_val)\n    center = p[1:-1, 1:-1]\n    neighs = [\n        p[0:-2, 0:-2], p[0:-2, 1:-1], p[0:-2, 2:],\n        p[1:-1, 0:-2],                p[1:-1, 2:],\n        p[2:, 0:-2],   p[2:, 1:-1],   p[2:, 2:]\n    ]\n    # strict greater than all neighbors and above threshold\n    mask = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        mask &= (center > n)\n    mask &= (center > thresh)\n    count = float(np.count_nonzero(mask))\n    area = float(center.size) + eps\n    result = count / area\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian normalized by image dynamic range (texture measure)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    # compute discrete Laplacian with padding to avoid wrap\n    pad_val = 0.0\n    p = np.pad(a, pad_width=1, mode='edge').astype(float)\n    center = p[1:-1, 1:-1]\n    up = p[0:-2, 1:-1]\n    down = p[2:, 1:-1]\n    left = p[1:-1, 0:-2]\n    right = p[1:-1, 2:]\n    lap = (up + down + left + right - 4.0 * center)\n    mean_abs = float(np.mean(np.abs(lap)))\n    dynamic = float(a.max() - a.min()) + eps\n    result = mean_abs / dynamic\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right brightness bias normalized by image std (positive => left brighter)'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left_mean = float(np.mean(img[:, :mid])) if img[:, :mid].size else 0.0\n    right_mean = float(np.mean(img[:, -mid:])) if img[:, -mid:].size else 0.0\n    overall_std = float(np.std(img)) + eps\n    result = (left_mean - right_mean) / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted eccentricity from second moments (0..1), 0=circular, 1=elongated'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # shift coordinates to center\n    ys, xs = np.indices((h, w))\n    xs = xs.astype(float)\n    ys = ys.astype(float)\n    total = float(a.sum())\n    if total == 0.0:\n        return 0.0\n    mx = (a * xs).sum() / total\n    my = (a * ys).sum() / total\n    x = xs - mx\n    y = ys - my\n    # covariance elements\n    Cxx = float((a * (x * x)).sum()) / total\n    Cyy = float((a * (y * y)).sum()) / total\n    Cxy = float((a * (x * y)).sum()) / total\n    # eigenvalues of covariance matrix\n    trace = Cxx + Cyy\n    det = Cxx * Cyy - Cxy * Cxy\n    # numerical stability\n    disc = max(0.0, trace * trace / 4.0 - det)\n    l1 = trace / 2.0 + np.sqrt(disc)\n    l2 = trace / 2.0 - np.sqrt(disc)\n    # avoid division by zero\n    mxeig = max(l1, l2, eps)\n    mineig = min(l1, l2)\n    # eccentricity-like measure: 1 - (minor / major)\n    result = 1.0 - max(0.0, float(mineig) / float(mxeig))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of gradient energy concentrated near the image border (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    # border width proportional to size\n    bw = max(1, min(h, w) // 8)\n    mask = np.zeros_like(a, dtype=bool)\n    mask[:bw, :] = True\n    mask[-bw:, :] = True\n    mask[:, :bw] = True\n    mask[:, -bw:] = True\n    total = float(mag.sum()) + eps\n    border = float(mag[mask].sum())\n    result = border / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Inter-quadrant mean absolute difference normalized by overall mean (0..1+)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    hy = h // 2\n    wx = w // 2\n    q1 = img[:hy, :wx]\n    q2 = img[:hy, wx:]\n    q3 = img[hy:, :wx]\n    q4 = img[hy:, wx:]\n    means = []\n    for q in (q1, q2, q3, q4):\n        means.append(float(q.mean()) if q.size else 0.0)\n    means = np.array(means)\n    pairwise = np.abs(means[:, None] - means[None, :])\n    # average off-diagonal\n    avg_diff = float(pairwise.sum() - np.trace(pairwise)) / (12.0 + eps)\n    overall_mean = float(np.abs(img).mean()) + eps\n    result = avg_diff / overall_mean\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Percentile contrast: (90th-10th) divided by median intensity'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    p90 = float(np.percentile(a, 90))\n    p10 = float(np.percentile(a, 10))\n    med = float(np.percentile(a, 50)) + eps\n    result = (p90 - p10) / med\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized centroid offset of bright region from image center (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # foreground mask using mean threshold (robust and cheap)\n    thr = float(np.mean(a))\n    mask = a > thr\n    if not mask.any():\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    cy = ys.mean()\n    cx = xs.mean()\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    maxdist = np.hypot(center_y, center_x) + 1e-12\n    result = dist / maxdist\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels significantly brighter than image mean (foreground sparsity)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thr = m + 0.5 * s  # modest threshold above mean\n    count = float((a > thr).sum())\n    result = count / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local variance over 3x3 neighborhoods normalized by global variance (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h * w == 0:\n        return 0.0\n    # use integral images to compute 3x3 sums efficiently\n    pad = np.pad(a, 1, mode='reflect').astype(float)\n    s = pad.cumsum(axis=0).cumsum(axis=1)\n    s2 = (pad * pad).cumsum(axis=0).cumsum(axis=1)\n    # window sum for each original pixel (3x3)\n    k = 3\n    total = (s[k:, k:] - s[:-k, k:] - s[k:, :-k] + s[:-k, :-k])\n    total2 = (s2[k:, k:] - s2[:-k, k:] - s2[k:, :-k] + s2[:-k, :-k])\n    area = float(k * k)\n    local_var = (total2 / area) - (total / area) ** 2\n    # ensure non-negative numerical\n    local_var = np.maximum(local_var, 0.0)\n    global_var = float(np.var(a)) + eps\n    mean_local_var = float(np.mean(local_var))\n    result = mean_local_var / global_var\n    # normalize to reasonable range and clip to [0,1]\n    return float(np.clip(result / (1.0 + result), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized mean absolute Laplacian (image sharpness) relative to intensity std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # discrete Laplacian: 4*center - neighbors\n    up = np.roll(a, 1, axis=0)\n    down = np.roll(a, -1, axis=0)\n    left = np.roll(a, 1, axis=1)\n    right = np.roll(a, -1, axis=1)\n    lap = (4.0 * a) - (up + down + left + right)\n    # zero out wrap-around artifacts by masking border\n    lap[0, :] = lap[0, :] * 1.0\n    lap[-1, :] = lap[-1, :] * 1.0\n    lap[:, 0] = lap[:, 0] * 1.0\n    lap[:, -1] = lap[:, -1] * 1.0\n    mad = float(np.mean(np.abs(lap)))\n    denom = float(np.std(a)) + eps\n    result = mad / denom\n    # clip to reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean color saturation for RGB images normalized by image dynamic range (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim < 3 or arr.shape[2] < 3:\n        return 0.0\n    # use first three channels as RGB-like\n    rgb = np.nan_to_num(arr[..., :3].astype(float))\n    # per-pixel mean\n    m = rgb.mean(axis=2, keepdims=True)\n    diff = rgb - m\n    chroma = np.sqrt(((diff ** 2).sum(axis=2)) / 3.0)\n    rng = float(rgb.max() - rgb.min()) + eps\n    result = float(np.mean(chroma) / rng)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score: Pearson correlation between left and mirrored right (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:] if mid > 0 else None\n    if right is None or left.size == 0 or right.size == 0:\n        return 0.0\n    # mirror right horizontally\n    right_mir = np.fliplr(right)\n    # crop to same shape if widths differ by one\n    min_w = min(left.shape[1], right_mir.shape[1])\n    left_crop = left[:, :min_w].ravel()\n    right_crop = right_mir[:, :min_w].ravel()\n    if left_crop.size == 0:\n        return 0.0\n    lx = left_crop - left_crop.mean()\n    rx = right_crop - right_crop.mean()\n    denom = (np.sqrt((lx ** 2).sum() * (rx ** 2).sum()) + eps)\n    corr = float((lx * rx).sum() / denom)\n    result = abs(corr)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Harris-like corner density (fraction of strong corner responses, 0..1)'\n    import numpy as np\n    eps = 1e-12\n    k = 0.04\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    A = gx * gx\n    B = gx * gy\n    C = gy * gy\n    # smooth with simple 3x3 average via convolution using cumsum for speed\n    padA = np.pad(A, 1, mode='reflect')\n    padB = np.pad(B, 1, mode='reflect')\n    padC = np.pad(C, 1, mode='reflect')\n    sA = padA.cumsum(axis=0).cumsum(axis=1)\n    sB = padB.cumsum(axis=0).cumsum(axis=1)\n    sC = padC.cumsum(axis=0).cumsum(axis=1)\n    ksize = 3\n    sumA = (sA[ksize:, ksize:] - sA[:-ksize, ksize:] - sA[ksize:, :-ksize] + sA[:-ksize, :-ksize]) / 9.0\n    sumB = (sB[ksize:, ksize:] - sB[:-ksize, ksize:] - sB[ksize:, :-ksize] + sB[:-ksize, :-ksize]) / 9.0\n    sumC = (sC[ksize:, ksize:] - sC[:-ksize, ksize:] - sC[ksize:, :-ksize] + sC[:-ksize, :-ksize]) / 9.0\n    # Harris response\n    R = (sumA * sumC - sumB * sumB) - k * (sumA + sumC) ** 2\n    Rmax = float(np.max(R))\n    if Rmax <= 0:\n        return 0.0\n    thr = max(1e-8, 0.01 * Rmax)\n    count = float((R > thr).sum())\n    result = count / float(h * w)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio from 2D FFT (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        f = np.fft.fft2(a)\n        fshift = np.fft.fftshift(f)\n        mag = np.abs(fshift)\n    except Exception:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    maxr = r.max() + 1e-12\n    # high freq if radius > 0.5 * maxradius\n    mask = r > (0.5 * maxr)\n    total = float(mag.sum()) + 1e-12\n    high = float(mag[mask].sum())\n    result = high / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute vertical neighbor difference normalized by intensity range'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.shape[0] < 2:\n        return 0.0\n    diff = np.abs(np.diff(a, axis=0))\n    mad = float(diff.mean())\n    rng = float(a.max() - a.min()) + eps\n    result = mad / rng\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominance of largest intensity histogram bin (16 bins) (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    bins = 16\n    lo = float(a.min())\n    hi = float(a.max())\n    if hi <= lo:\n        return 1.0\n    hist, _ = np.histogram(a, bins=bins, range=(lo, hi))\n    top = float(hist.max())\n    result = top / float(a.size)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local standard deviation over 3x3 neighborhoods (texture coarseness)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # local sums via rolling\n    def local_sum(X):\n        s = np.zeros_like(X, dtype=float)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s\n    s1 = local_sum(a) / 9.0\n    s2 = local_sum(a * a) / 9.0\n    var = s2 - (s1 * s1)\n    var = np.where(var >= 0, var, 0.0)\n    local_std = np.sqrt(var)\n    result = float(np.mean(local_std))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Shannon entropy of intensity histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).ravel().astype(float))\n    else:\n        vals = np.nan_to_num(arr.ravel().astype(float))\n    if vals.size == 0:\n        return 0.0\n    vmin = float(vals.min())\n    vmax = float(vals.max())\n    if vmax <= vmin:\n        return 0.0\n    bins = 256\n    hist, _ = np.histogram(vals, bins=bins, range=(vmin, vmax))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p_nonzero = p[p > 0]\n    if p_nonzero.size == 0:\n        return 0.0\n    entropy = -float(np.sum(p_nonzero * np.log2(p_nonzero)))\n    max_entropy = np.log2(float(bins))\n    result = entropy / (max_entropy + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of gradient energy attributable to horizontal edges (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    abs_gx = np.abs(gx)\n    abs_gy = np.abs(gy)\n    hor_energy = float(abs_gy.sum())\n    total = float(abs_gx.sum() + abs_gy.sum() + eps)\n    result = hor_energy / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of columns containing at least one bright pixel (activity across width)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w == 0:\n        return 0.0\n    mean = float(a.mean())\n    std = float(a.std())\n    thr = mean + 0.5 * std\n    cols_active = np.count_nonzero(np.any(a >= thr, axis=0))\n    result = float(cols_active) / float(w + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of center-region pixels that are high contrast relative to global stats'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ch0 = h // 4\n    cw0 = w // 4\n    center = a[ch0:3*ch0 if 3*ch0>ch0 else h, cw0:3*cw0 if 3*cw0>cw0 else w]\n    if center.size == 0:\n        return 0.0\n    gmean = float(a.mean())\n    gstd = float(a.std()) + eps\n    thr = gmean + 0.75 * gstd\n    count = float(np.count_nonzero(center >= thr))\n    result = count / float(center.size + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average normalized count of binary intensity transitions per row (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w < 2:\n        return 0.0\n    thr = float(a.mean())\n    binrows = (a > thr).astype(np.int8)\n    transitions = np.abs(np.diff(binrows, axis=1))\n    row_counts = transitions.sum(axis=1).astype(float)\n    norm = max(1.0, float(w - 1))\n    avg_trans = float(row_counts.mean()) / norm\n    return float(np.clip(avg_trans, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bounding-box fill ratio for pixels above a mild threshold (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean()) + 0.1 * float(a.std())\n    mask = a >= thr\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    rmin, rmax = int(ys.min()), int(ys.max())\n    cmin, cmax = int(xs.min()), int(xs.max())\n    bbox_area = float((rmax - rmin + 1) * (cmax - cmin + 1))\n    result = bbox_area / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Diagonal symmetry score comparing image to its transpose (1 = perfect)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    m = min(h, w)\n    if m == 0:\n        return 0.0\n    crop = a[:m, :m]\n    trans = crop.T\n    diff = np.abs(crop - trans)\n    norm = float(np.mean(np.abs(crop))) + eps\n    score = 1.0 - (float(np.mean(diff)) / norm)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast between mid annulus and inner disk normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    mm = float(min(h, w))\n    r1 = mm / 8.0\n    r2 = mm / 3.0\n    inner_mask = r <= r1\n    ring_mask = (r > r1) & (r <= r2)\n    if not np.any(inner_mask) or not np.any(ring_mask):\n        return 0.0\n    inner_mean = float(a[inner_mask].mean())\n    ring_mean = float(a[ring_mask].mean())\n    gstd = float(a.std()) + eps\n    result = (ring_mean - inner_mean) / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Peak-to-mean normalized by std (how extreme the brightest pixel is)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    mx = float(a.max())\n    mean = float(a.mean())\n    std = float(a.std()) + eps\n    result = (mx - mean) / std\n    # clamp to reasonable range\n    return float(np.clip(result, 0.0, 50.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Relative horizontal vs vertical gradient energy (-1..1, + => more horizontal)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    gx_energy = float(np.sum(np.abs(gx)))\n    gy_energy = float(np.sum(np.abs(gy)))\n    denom = gx_energy + gy_energy + eps\n    result = (gx_energy - gy_energy) / denom\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels whose gradient magnitude exceeds mean+0.5*std (edge density)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + 0.5 * mag.std())\n    count = float(np.count_nonzero(mag > thr))\n    denom = float(mag.size) + eps\n    result = count / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute center-symmetric intensity difference normalized by global std (asymmetry)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    flipped = np.flip(np.flip(a, 0), 1)\n    diff = np.abs(a - flipped)\n    mean_diff = float(np.mean(diff))\n    gstd = float(a.std()) + eps\n    result = mean_diff / gstd\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Perimeter mean minus global mean normalized by global std (positive => edges brighter)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # build perimeter mask\n    mask = np.zeros_like(a, dtype=bool)\n    mask[0, :] = True\n    mask[-1, :] = True\n    mask[:, 0] = True\n    mask[:, -1] = True\n    per_mean = float(np.mean(a[mask])) if mask.any() else 0.0\n    gmean = float(a.mean())\n    gstd = float(a.std()) + eps\n    result = (per_mean - gmean) / gstd\n    return float(np.clip(result, -5.0, 5.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized mean absolute Laplacian (high-frequency energy relative to mean intensity)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    # discrete Laplacian via neighbor rolls\n    lap = (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) +\n           np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a)\n    mean_abs_lap = float(np.mean(np.abs(lap)))\n    mean_abs = float(np.mean(np.abs(a))) + eps\n    result = mean_abs_lap / mean_abs\n    return float(np.clip(result, 0.0, 50.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Interquartile intensity range divided by global std (contrast estimate)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    p25 = float(np.percentile(a, 25))\n    p75 = float(np.percentile(a, 75))\n    iqr = p75 - p25\n    gstd = float(a.std()) + eps\n    result = iqr / gstd\n    return float(np.clip(result, 0.0, 20.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score (0..1), 1 => perfectly symmetric'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w < 2:\n        return 0.0\n    mid = w // 2\n    if mid == 0:\n        return 0.0\n    left = a[:, :mid]\n    right = a[:, -mid:]\n    right_flipped = np.fliplr(right)\n    if left.shape != right_flipped.shape:\n        # safety check, crop to smallest common shape\n        min_h = min(left.shape[0], right_flipped.shape[0])\n        min_w = min(left.shape[1], right_flipped.shape[1])\n        left = left[:min_h, :min_w]\n        right_flipped = right_flipped[:min_h, :min_w]\n    mean_abs_diff = float(np.mean(np.abs(left - right_flipped)))\n    mean_scale = float(np.mean((np.abs(left) + np.abs(right_flipped)) * 0.5)) + eps\n    score = 1.0 - (mean_abs_diff / mean_scale)\n    result = float(np.clip(score, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Sharpness of brightest spot: (max - local mean)/global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    idx = int(np.argmax(a.ravel()))\n    h, w = a.shape\n    r, c = divmod(idx, w)\n    r0 = max(0, r - 1)\n    r1 = min(h, r + 2)\n    c0 = max(0, c - 1)\n    c1 = min(w, c + 2)\n    local = a[r0:r1, c0:c1]\n    max_val = float(a[r, c])\n    local_mean = float(np.mean(local)) if local.size else max_val\n    gstd = float(a.std()) + eps\n    result = (max_val - local_mean) / gstd\n    return float(np.clip(result, -10.0, 50.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of gradient orientation histogram (0..1, 1 => maximal orientation disorder)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    total_mag = float(mag.sum())\n    if total_mag <= eps:\n        return 0.0\n    ang = np.arctan2(gy, gx)  # -pi..pi\n    # orientation ignoring sign -> map to [0, pi)\n    orient = np.mod(ang, np.pi)\n    bins = 12\n    hist, _ = np.histogram(orient, bins=bins, range=(0.0, np.pi), weights=mag)\n    p = hist.astype(float) / (hist.sum() + eps)\n    p_nonzero = p[p > 0]\n    if p_nonzero.size == 0:\n        return 0.0\n    entropy = -float((p_nonzero * np.log2(p_nonzero)).sum())\n    # normalize by max possible entropy log2(bins)\n    max_ent = float(np.log2(bins))\n    result = entropy / (max_ent + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels that are strong local maxima (strict local max and > mean+std)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    # pad with -inf so edges are handled without wrap\n    pad = np.pad(a, 1, mode='constant', constant_values=-np.inf)\n    center = pad[1:-1, 1:-1]\n    neighs = [\n        pad[0:-2, 0:-2], pad[0:-2, 1:-1], pad[0:-2, 2:],\n        pad[1:-1, 0:-2],                 pad[1:-1, 2:],\n        pad[2:, 0:-2], pad[2:, 1:-1],   pad[2:, 2:]\n    ]\n    local_max = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        local_max &= (center > n)\n    thr = float(a.mean() + a.std())\n    strong_mask = local_max & (center > thr)\n    count = float(np.count_nonzero(strong_mask))\n    result = count / (float(a.size) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels with strong gradient (edge density), normalized 0..1'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + 0.5 * mag.std())\n    strong = np.count_nonzero(mag > thr)\n    result = strong / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Elongation of bright region based on weighted covariance (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    thr = float(np.median(flat))\n    mask = a > thr\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    weights = a[mask] - thr\n    weights = np.maximum(weights, 0.0)\n    if weights.sum() <= 0:\n        weights = np.ones_like(weights)\n    wsum = float(weights.sum()) + eps\n    cx = float((xs * weights).sum()) / wsum\n    cy = float((ys * weights).sum()) / wsum\n    dx = xs - cx\n    dy = ys - cy\n    Sxx = float((weights * (dx * dx)).sum()) / wsum + eps\n    Syy = float((weights * (dy * dy)).sum()) / wsum + eps\n    Sxy = float((weights * (dx * dy)).sum()) / wsum\n    trace = Sxx + Syy\n    det = Sxx * Syy - Sxy * Sxy\n    # eigenvalues of 2x2 covariance\n    temp = np.sqrt(max(trace * trace / 4.0 - det, 0.0))\n    lam1 = trace / 2.0 + temp\n    lam2 = trace / 2.0 - temp\n    if lam1 <= 0:\n        result = 0.0\n    else:\n        result = float(np.clip(1.0 - (lam2 / (lam1 + eps)), 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) using 32 bins'\n    import numpy as np\n    eps = 1e-12\n    bins = 32\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    mn = float(vals.min())\n    mx = float(vals.max())\n    if mx <= mn:\n        return 0.0\n    hist, _ = np.histogram(vals, bins=bins, range=(mn, mx))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p = p[p > 0]\n    if p.size == 0:\n        return 0.0\n    entropy = -float((p * np.log2(p)).sum())\n    max_ent = np.log2(bins)\n    result = float(np.clip(entropy / (max_ent + eps), 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average absolute normalized autocorrelation with small shifts (0..1)'\n    import numpy as np\n    eps = 1e-12\n    maxshift = 5\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    shifts = []\n    rng = range(1, min(maxshift, max(h, w) - 1) + 1)\n    for dy in rng:\n        shifts.append((dy, 0))\n    for dx in rng:\n        shifts.append((0, dx))\n    for d in rng:\n        shifts.append((d, d))\n    corrs = []\n    mu = a.mean()\n    sdev = a.std() + eps\n    for dy, dx in shifts:\n        ys1 = slice(0, h - dy)\n        xs1 = slice(0, w - dx)\n        ys2 = slice(dy, h)\n        xs2 = slice(dx, w)\n        A = a[ys1, xs1]\n        B = a[ys2, xs2]\n        if A.size == 0:\n            continue\n        Am = A.mean()\n        Bm = B.mean()\n        As = A.std() + eps\n        Bs = B.std() + eps\n        num = ((A - Am) * (B - Bm)).mean()\n        corr = num / (As * Bs + eps)\n        corrs.append(abs(corr))\n    if len(corrs) == 0:\n        return 0.0\n    result = float(np.clip(float(np.mean(corrs)), 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of strong corner responses (Harris-like) normalized 0..1'\n    import numpy as np\n    eps = 1e-12\n    k = 0.04\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    gy, gx = np.gradient(a)\n    Jxx = gx * gx\n    Jyy = gy * gy\n    Jxy = gx * gy\n    # local 3x3 mean\n    def local_mean(X):\n        s = np.zeros_like(X)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s / 9.0\n    Sxx = local_mean(Jxx)\n    Syy = local_mean(Jyy)\n    Sxy = local_mean(Jxy)\n    det = Sxx * Syy - Sxy * Sxy\n    trace = Sxx + Syy\n    R = det - k * (trace ** 2)\n    thr = float(R.mean() + R.std())\n    strong = np.count_nonzero(R > thr)\n    result = strong / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score (1.0 = perfectly symmetric, 0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 1.0\n    left = a[:, :w//2]\n    right = a[:, w - (w//2):][:, ::-1]\n    # match sizes\n    minw = min(left.shape[1], right.shape[1])\n    left = left[:, :minw]\n    right = right[:, :minw]\n    denom = float(np.mean(np.abs(a)) + a.std() + eps)\n    diff = float(np.mean(np.abs(left - right)))\n    score = 1.0 - (diff / denom)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of very dark pixels (below mean - 0.5*std), 0..1'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thr = m - 0.5 * s\n    dark = np.count_nonzero(a < thr)\n    result = dark / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant global gradient orientation mapped to [-1..1] (-1 = -90deg, 1 = +90deg)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    Jxx = (gx * gx).sum()\n    Jyy = (gy * gy).sum()\n    Jxy = (gx * gy).sum()\n    denom = (Jxx - Jyy)\n    angle = 0.5 * np.arctan2(2.0 * Jxy, denom + eps)  # radians in [-pi/2, pi/2]\n    result = float(angle / (np.pi / 2.0))\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Local peak fraction: fraction of pixels greater than 8 neighbors and reasonably bright (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    m = a.mean()\n    s = a.std()\n    bright_thr = m + 0.1 * s\n    center = a\n    nlist = [\n        np.roll(a, 1, axis=0),\n        np.roll(a, -1, axis=0),\n        np.roll(a, 1, axis=1),\n        np.roll(a, -1, axis=1),\n        np.roll(np.roll(a, 1, axis=0), 1, axis=1),\n        np.roll(np.roll(a, 1, axis=0), -1, axis=1),\n        np.roll(np.roll(a, -1, axis=0), 1, axis=1),\n        np.roll(np.roll(a, -1, axis=0), -1, axis=1),\n    ]\n    greater = np.ones_like(a, dtype=bool)\n    for n in nlist:\n        greater &= (center > n)\n    peaks = greater & (center > bright_thr)\n    # exclude wrap-around artifacts by zeroing borders where rolls introduced wraps\n    peaks[0, :] = False\n    peaks[-1, :] = False\n    peaks[:, 0] = False\n    peaks[:, -1] = False\n    result = float(np.count_nonzero(peaks)) / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Annular contrast: brightness of a ring around center normalized by global std (-5..5)'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    maxr = r.max() + eps\n    radius = 0.5 * maxr\n    width = max(1.0, 0.12 * maxr)\n    ann_mask = (np.abs(r - radius) <= width)\n    center_mask = (r <= max(1.0, radius * 0.5))\n    outer_mask = (r >= min(maxr, radius * 1.5))\n    if not np.any(ann_mask):\n        return 0.0\n    ann_mean = float(a[ann_mask].mean())\n    cen_mean = float(a[center_mask].mean()) if np.any(center_mask) else ann_mean\n    out_mean = float(a[outer_mask].mean()) if np.any(outer_mask) else ann_mean\n    denom = float(a.std()) + eps\n    result = (ann_mean - 0.5 * (cen_mean + out_mean)) / denom\n    return float(np.clip(result, -5.0, 5.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Peakiness: normalized (max - mean) relative to dynamic range (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    mx = float(a.max())\n    mn = float(a.min())\n    mean = float(a.mean())\n    eps = 1e-12\n    denom = (mx - mn) + eps\n    if denom <= eps:\n        return 0.0\n    result = (mx - mean) / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Relative brightness of top 5% pixels: (mean_top - mean_all)/(|mean_all|+eps)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    p = 95.0\n    thr = float(np.percentile(a, p))\n    top = a[a >= thr]\n    if top.size == 0:\n        return 0.0\n    mean_all = float(a.mean())\n    mean_top = float(top.mean())\n    eps = 1e-12\n    result = (mean_top - mean_all) / (abs(mean_all) + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of center-region pixels above global mean (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape[:2]\n    if h == 0 or w == 0:\n        return 0.0\n    ch = h // 4\n    cw = w // 4\n    center = a[ch:3*ch if 3*ch > ch else h, cw:3*cw if 3*cw > cw else w]\n    if center.size == 0:\n        return 0.0\n    global_mean = float(a.mean())\n    count = float(np.count_nonzero(center > global_mean))\n    result = count / float(center.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average local contrast: mean of patch stds normalized by global std'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # choose patch size around 8x8 relative to image\n    ph = max(1, min(8, h))\n    pw = max(1, min(8, w))\n    stds = []\n    for i in range(0, h, ph):\n        for j in range(0, w, pw):\n            patch = a[i:min(i+ph, h), j:min(j+pw, w)]\n            if patch.size:\n                stds.append(patch.std())\n    if len(stds) == 0:\n        return 0.0\n    local_mean_std = float(np.mean(stds))\n    global_std = float(a.std()) + 1e-12\n    result = local_mean_std / global_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio from 2D FFT (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        F = np.fft.fftshift(np.fft.fft2(img))\n        power = np.abs(F) ** 2\n    except Exception:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    radius = max(1.0, min(h, w) * 0.1)\n    high_mask = r > radius\n    total = float(power.sum()) + 1e-12\n    high = float((power * high_mask).sum())\n    result = high / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry score: 1 - normalized L1 difference between left and mirrored right (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    if w % 2 == 0:\n        right = a[:, mid:]\n    else:\n        right = a[:, mid+1:]\n    # make shapes equal\n    minw = min(left.shape[1], right.shape[1])\n    if minw == 0:\n        return 0.0\n    left = left[:, :minw]\n    right = right[:, :minw]\n    right_flip = np.fliplr(right)\n    diff = np.abs(left - right_flip)\n    denom = np.mean(np.abs(a)) + 1e-12\n    score = 1.0 - (np.mean(diff) / denom)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized offset of bright-pixel centroid to image center (0=centered, ~1=corner)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    flat = a.ravel()\n    n = flat.size\n    k = max(1, int(0.01 * n))  # top 1% pixels\n    if k == 0:\n        k = 1\n    idx = np.argpartition(flat, -k)[-k:]\n    ys, xs = np.unravel_index(idx, (h, w))\n    cy = ys.mean()\n    cx = xs.mean()\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    max_dist = np.hypot(center_x, center_y) + 1e-12\n    result = dist / max_dist\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of bright connected components (4-neighbors) normalized by image area'\n    import numpy as np\n    from collections import deque\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mean = float(a.mean())\n    std = float(a.std())\n    thr = mean + std\n    mask = a >= thr\n    visited = np.zeros_like(mask, dtype=bool)\n    comps = 0\n    for i in range(h):\n        for j in range(w):\n            if mask[i, j] and not visited[i, j]:\n                comps += 1\n                # BFS\n                dq = deque()\n                dq.append((i, j))\n                visited[i, j] = True\n                while dq:\n                    y, x = dq.popleft()\n                    # 4-neighbors\n                    if y > 0 and mask[y-1, x] and not visited[y-1, x]:\n                        visited[y-1, x] = True; dq.append((y-1, x))\n                    if y < h-1 and mask[y+1, x] and not visited[y+1, x]:\n                        visited[y+1, x] = True; dq.append((y+1, x))\n                    if x > 0 and mask[y, x-1] and not visited[y, x-1]:\n                        visited[y, x-1] = True; dq.append((y, x-1))\n                    if x < w-1 and mask[y, x+1] and not visited[y, x+1]:\n                        visited[y, x+1] = True; dq.append((y, x+1))\n    area = float(h * w) + 1e-12\n    result = comps / area\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian energy: mean absolute Laplacian normalized by mean absolute intensity (0..1+)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(img)\n    gyy, gyx = np.gradient(gy)\n    gxy, gxx = np.gradient(gx)\n    lap = gxx + gyy\n    num = float(np.mean(np.abs(lap)))\n    denom = float(np.mean(np.abs(img))) + 1e-12\n    result = num / denom\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram bimodality index: difference between two largest peaks scaled by their separation (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    bins = 16\n    try:\n        hist, edges = np.histogram(a, bins=bins, range=(a.min(), a.max()))\n    except Exception:\n        hist, edges = np.histogram(a, bins=bins)\n    if hist.sum() == 0:\n        return 0.0\n    idx = np.argsort(hist)[::-1]\n    p1_idx = int(idx[0])\n    p1 = float(hist[p1_idx])\n    p2 = float(hist[idx[1]]) if hist.size > 1 else 0.0\n    separation = abs(p1_idx - int(idx[1])) if hist.size > 1 else 0\n    sep_norm = separation / max(1, bins - 1)\n    score = (p1 - p2) / (p1 + p2 + 1e-12)\n    result = float(np.clip(score * sep_norm, 0.0, 1.0))\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Horizontal symmetry: 1.0 = perfect left-right mirror, 0.0 = very asymmetric'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 1:\n        return 0.0\n    flipped = np.fliplr(a)\n    diff = np.abs(a - flipped)\n    denom = float(a.max() - a.min()) + eps\n    # if constant image, treat as perfectly symmetric\n    if denom <= eps:\n        return 1.0\n    norm_diff = float(diff.mean()) / denom\n    result = 1.0 - np.clip(norm_diff, 0.0, 1.0)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Shannon entropy of intensity histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    # choose number of bins relative to image dynamic range but cap\n    nbins = 64\n    try:\n        hist, _ = np.histogram(a.ravel(), bins=nbins, range=(float(a.min()), float(a.max()) + eps))\n    except Exception:\n        return 0.0\n    total = float(hist.sum()) + eps\n    p = hist.astype(float) / total\n    # remove zeros\n    p = p[p > 0]\n    if p.size == 0:\n        return 0.0\n    entropy = -float((p * np.log2(p)).sum())\n    max_ent = np.log2(nbins)\n    result = float(entropy / (max_ent + eps))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio in Fourier domain (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n    except Exception:\n        return 0.0\n    power = np.abs(F) ** 2\n    cy = h // 2\n    cx = w // 2\n    ys, xs = np.ogrid[:h, :w]\n    r = np.hypot(ys - cy, xs - cx)\n    # define low-frequency radius as min dimension / 8\n    r0 = max(1.0, min(h, w) / 8.0)\n    low_mask = r <= r0\n    low_energy = float(power[low_mask].sum())\n    total_energy = float(power.sum()) + eps\n    high_energy = total_energy - low_energy\n    result = float(high_energy / total_energy)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Largest bright connected component area fraction (4-connected)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    area = h * w\n    if area == 0:\n        return 0.0\n    thr = float(a.mean() + 0.5 * a.std())\n    mask = a > thr\n    visited = np.zeros(mask.shape, dtype=bool)\n    max_comp = 0\n    # neighbors (4-connected)\n    for yi in range(h):\n        for xi in range(w):\n            if mask[yi, xi] and not visited[yi, xi]:\n                # flood fill\n                cnt = 0\n                stack = [(yi, xi)]\n                visited[yi, xi] = True\n                while stack:\n                    y, x = stack.pop()\n                    cnt += 1\n                    # neighbors\n                    if y > 0 and mask[y-1, x] and not visited[y-1, x]:\n                        visited[y-1, x] = True\n                        stack.append((y-1, x))\n                    if y+1 < h and mask[y+1, x] and not visited[y+1, x]:\n                        visited[y+1, x] = True\n                        stack.append((y+1, x))\n                    if x > 0 and mask[y, x-1] and not visited[y, x-1]:\n                        visited[y, x-1] = True\n                        stack.append((y, x-1))\n                    if x+1 < w and mask[y, x+1] and not visited[y, x+1]:\n                        visited[y, x+1] = True\n                        stack.append((y, x+1))\n                if cnt > max_comp:\n                    max_comp = cnt\n    result = float(max_comp) / float(area + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio of intensity bounding box (min_dim / max_dim, 0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    mask = a != 0\n    if not np.any(mask):\n        # fallback: use pixels above tiny threshold\n        mask = a > (a.max() * 0.01)\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.where(mask)\n    y0, y1 = ys.min(), ys.max()\n    x0, x1 = xs.min(), xs.max()\n    hh = float(y1 - y0 + 1)\n    ww = float(x1 - x0 + 1)\n    if hh <= eps or ww <= eps:\n        return 0.0\n    mn = min(hh, ww)\n    mx = max(hh, ww)\n    result = float(mn / (mx + eps))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity slope: normalized change of mean intensity with radius (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    maxr = float(r.max()) + eps\n    # bin into 10 radial bins\n    nbins = 10\n    bins = np.linspace(0.0, maxr, nbins + 1)\n    bin_idx = np.digitize(r.ravel(), bins) - 1\n    bin_means = []\n    bin_centers = []\n    vals = a.ravel()\n    for b in range(nbins):\n        sel = bin_idx == b\n        if np.any(sel):\n            bin_means.append(float(vals[sel].mean()))\n            # use center radius value\n            bin_centers.append(float((bins[b] + bins[b+1]) / 2.0))\n    if len(bin_means) < 2:\n        return 0.0\n    y = np.array(bin_means)\n    x = np.array(bin_centers)\n    xm = x.mean()\n    ym = y.mean()\n    denom = (np.sum((x - xm) ** 2) + eps)\n    slope = float(np.sum((x - xm) * (y - ym)) / denom)\n    # normalize slope by mean intensity and max radius to keep scale\n    norm = (y.mean() + eps)\n    slope_norm = slope * maxr / norm\n    # clip to -1..1\n    result = float(np.clip(slope_norm, -1.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean pairwise correlation between RGB channels ([-1..1], 0 for non-RGB)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0 or img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    r = a[:, :, 0].ravel()\n    g = a[:, :, 1].ravel()\n    b = a[:, :, 2].ravel()\n    def corr(x, y):\n        xm = x.mean()\n        ym = y.mean()\n        sx = x.std()\n        sy = y.std()\n        if sx < eps or sy < eps:\n            return 0.0\n        return float(((x - xm) * (y - ym)).mean() / (sx * sy))\n    c1 = corr(r, g)\n    c2 = corr(r, b)\n    c3 = corr(g, b)\n    result = float((c1 + c2 + c3) / 3.0)\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border noise ratio: border std divided by global mean intensity'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    bw = max(1, min(h, w) // 10)\n    top = a[:bw, :].ravel()\n    bot = a[-bw:, :].ravel()\n    left = a[:, :bw].ravel()\n    right = a[:, -bw:].ravel()\n    border = np.concatenate([top, bot, left, right])\n    if border.size == 0:\n        return 0.0\n    bstd = float(border.std())\n    mean_global = float(a.mean()) + eps\n    result = float(bstd / mean_global)\n    # clip to reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels in deep dark tail: below mean - 0.75*std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h * w == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thr = m - 0.75 * s\n    count = int(np.count_nonzero(a < thr))\n    result = float(count) / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Center-to-corner brightness contrast normalized by image std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    ch0, cw0 = h // 4, w // 4\n    center = a[ch0:3*ch0 or h, cw0:3*cw0 or w]\n    # corners of same size as center (or as close as possible)\n    c_h = max(1, center.shape[0])\n    c_w = max(1, center.shape[1])\n    tl = a[0:c_h, 0:c_w]\n    tr = a[0:c_h, w-c_w:w]\n    bl = a[h-c_h:h, 0:c_w]\n    br = a[h-c_h:h, w-c_w:w]\n    center_mean = float(np.mean(center)) if center.size else 0.0\n    corners_mean = float(np.mean(np.concatenate([tl.ravel(), tr.ravel(), bl.ravel(), br.ravel()]))) if a.size else 0.0\n    overall_std = float(np.std(a)) + eps\n    result = (center_mean - corners_mean) / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels that are strong edges (edge density)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    med = float(np.median(mag))\n    std = float(mag.std())\n    thresh = med + 0.5 * std\n    count = float(np.count_nonzero(mag > thresh))\n    result = count / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fill ratio of the bright region in its bounding box (1 = fully filled)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    mean = float(a.mean())\n    std = float(a.std())\n    thresh = mean + 0.5 * std\n    ys, xs = np.nonzero(a > thresh)\n    if ys.size == 0:\n        return 0.0\n    y0, y1 = int(ys.min()), int(ys.max())\n    x0, x1 = int(xs.min()), int(xs.max())\n    bbox_area = max(1, (y1 - y0 + 1) * (x1 - x0 + 1))\n    filled = float(ys.size)\n    result = filled / float(bbox_area + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry measured by normalized cross-correlation (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:]\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    # flip right for comparison\n    right_flipped = np.fliplr(right)\n    # if shapes differ (odd width), crop to smallest\n    min_w = min(left.shape[1], right_flipped.shape[1])\n    left = left[:, :min_w].ravel()\n    right_flipped = right_flipped[:, :min_w].ravel()\n    if left.size == 0:\n        return 0.0\n    left = left - left.mean()\n    right_flipped = right_flipped - right_flipped.mean()\n    denom = (np.sqrt((left**2).sum()) * np.sqrt((right_flipped**2).sum())) + eps\n    corr = float((left * right_flipped).sum() / denom)\n    result = float(np.clip(abs(corr), 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy fraction from 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    # subtract mean\n    a = a - float(a.mean())\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute magnitude spectrum\n    F = np.fft.fftshift(np.fft.fft2(a))\n    mag = np.abs(F)\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    maxr = float(r.max()) + eps\n    cutoff = 0.25 * maxr\n    high = mag[r > cutoff].sum()\n    total = mag.sum() + eps\n    result = float(high) / float(total)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of corner-like pixels normalized by image area'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    ang = np.arctan2(gy, gx)\n    # pad for neighbor comparisons\n    pad_ang = np.pad(ang, ((1, 1), (1, 1)), mode='edge')\n    pad_mag = np.pad(mag, ((1, 1), (1, 1)), mode='constant', constant_values=0.0)\n    center_ang = pad_ang[1:-1, 1:-1]\n    center_mag = pad_mag[1:-1, 1:-1]\n    # compare with 8 neighbors: if several neighbors have very different angle and center is strong edge -> corner\n    diffs = 0\n    count = np.zeros_like(center_ang, dtype=int)\n    neighbors = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n    for dy, dx in neighbors:\n        nbr_ang = pad_ang[1+dy:h+1+dy, 1+dx:w+1+dx]\n        # angular difference wrapped\n        d = np.abs(np.angle(np.exp(1j*(center_ang - nbr_ang))))\n        count += (d > (np.pi / 4)).astype(int)\n    mag_thresh = np.median(mag) + 0.5 * mag.std()\n    corners = ((count >= 3) & (center_mag > mag_thresh))\n    result = float(np.count_nonzero(corners)) / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0..1), 0 for grayscale inputs'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    # Use first three channels as R,G,B\n    try:\n        R = np.nan_to_num(img[:, :, 0].astype(float))\n        G = np.nan_to_num(img[:, :, 1].astype(float))\n        B = np.nan_to_num(img[:, :, 2].astype(float))\n    except Exception:\n        return 0.0\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(rg))\n    mean_yb = float(np.mean(yb))\n    # Hasler and S\u00fcsstrunk colorfulness\n    colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n    # normalize by dynamic range estimate\n    max_chan = max(float(R.max()), float(G.max()), float(B.max()), eps)\n    result = colorfulness / (max_chan + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average absolute Laplacian normalized by image std (sharpness proxy)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    gy, gx = np.gradient(a)\n    gxx = np.gradient(gx, axis=1)\n    gyy = np.gradient(gy, axis=0)\n    lap = gxx + gyy\n    mean_abs_lap = float(np.mean(np.abs(lap)))\n    overall_std = float(np.std(a)) + eps\n    result = mean_abs_lap / overall_std\n    # clamp to a reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized inter-percentile spread ( (p90-p10)/(p90+p10) ) in [0,1]'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(img.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    p10 = float(np.percentile(a, 10))\n    p90 = float(np.percentile(a, 90))\n    denom = (abs(p90) + abs(p10) + eps)\n    result = (p90 - p10) / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels near the dominant intensity mode (background uniformity)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(img.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    bins = 64\n    try:\n        hist, edges = np.histogram(a, bins=bins, range=(float(a.min()), float(a.max())) if a.max() > a.min() else (0.0, 1.0))\n    except Exception:\n        return 0.0\n    if hist.sum() == 0:\n        return 0.0\n    idx = int(np.argmax(hist))\n    bin_low = edges[idx]\n    bin_high = edges[idx+1]\n    # include a small neighborhood around the bin\n    width = (bin_high - bin_low) + eps\n    low = bin_low - 0.5 * width\n    high = bin_high + 0.5 * width\n    count = float(np.count_nonzero((a >= low) & (a <= high)))\n    result = count / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with gradient magnitude above mean+std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + mag.std())\n    count = float(np.count_nonzero(mag > thr))\n    result = count / (float(h * w) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center vs border contrast normalized by overall mean'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ch, cw = h // 4, w // 4\n    center = img[ch:3*ch or None, cw:3*cw or None]\n    if center.size == 0:\n        return 0.0\n    mask = np.ones_like(img, dtype=bool)\n    mask[ch:3*ch or None, cw:3*cw or None] = False\n    border = img[mask]\n    if border.size == 0:\n        return 0.0\n    center_mean = float(center.mean())\n    border_mean = float(border.mean())\n    overall_mean = float(img.mean()) + eps\n    result = (center_mean - border_mean) / overall_mean\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) using 16 bins'\n    import numpy as np\n    bins = 16\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    mn, mx = a.min(), a.max()\n    if mx <= mn:\n        return 0.0\n    hist, _ = np.histogram(a, bins=bins, range=(mn, mx))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p_nonzero = p[p > 0]\n    ent = -float((p_nonzero * np.log(p_nonzero)).sum())\n    max_ent = np.log(float(bins))\n    result = ent / (max_ent + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of sparse bright pixels above mean+1*std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    n = a.size\n    if n == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thr = m + 1.0 * s\n    count = float(np.count_nonzero(a > thr))\n    result = count / (float(n) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Foreground bounding-box area fraction (mask by median threshold)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    med = float(np.median(img)) if img.size else 0.0\n    mask = img > med\n    if not mask.any():\n        return 0.0\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n    rmin = int(np.argmax(rows))\n    rmax = int(len(rows) - 1 - np.argmax(rows[::-1]))\n    cmin = int(np.argmax(cols))\n    cmax = int(len(cols) - 1 - np.argmax(cols[::-1]))\n    bbox_area = float((rmax - rmin + 1) * (cmax - cmin + 1))\n    total_area = float(h * w) + eps\n    result = bbox_area / total_area\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    '180-degree rotational symmetry (1=perfect symmetry, 0=none)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    rot = np.rot90(a, 2)\n    diff = np.abs(a - rot)\n    denom = float(a.max() - a.min()) + eps\n    norm_diff = float(diff.mean()) / denom\n    result = 1.0 - np.clip(norm_diff, 0.0, 1.0)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy fraction in 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n    except Exception:\n        return 0.0\n    power = np.abs(F) ** 2\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    rmax = float(r.max()) + eps\n    high_mask = r >= (0.5 * rmax)\n    high_energy = float(power[high_mask].sum())\n    total_energy = float(power.sum()) + eps\n    result = high_energy / total_energy\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian zero-crossing fraction (edges from sign changes) normalized to [0,1]'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # discrete Laplacian using 4-neighbors\n    lap = np.zeros_like(a)\n    lap += np.roll(a, 1, axis=0)\n    lap += np.roll(a, -1, axis=0)\n    lap += np.roll(a, 1, axis=1)\n    lap += np.roll(a, -1, axis=1)\n    lap -= 4.0 * a\n    # check sign changes with 4-neighbors\n    sign = np.sign(lap)\n    zx = (sign * np.roll(sign, 1, axis=0)) < 0\n    zy = (sign * np.roll(sign, 1, axis=1)) < 0\n    crossings = np.logical_or(zx, zy)\n    count = float(np.count_nonzero(crossings))\n    result = count / (float(h * w) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of total intensity contained in top 5% brightest pixels'\n    import numpy as np\n    frac = 0.05\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    n = vals.size\n    if n == 0:\n        return 0.0\n    k = max(1, int(np.ceil(frac * n)))\n    if k >= n:\n        top_sum = float(vals.sum())\n    else:\n        thresh = np.partition(vals, -k)[-k]\n        top_sum = float(vals[vals >= thresh].sum())\n    total = float(vals.sum()) + eps\n    result = top_sum / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Verticality ratio: vertical gradient energy / total gradient energy (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    vy = float(np.abs(gy).sum())\n    hx = float(np.abs(gx).sum())\n    total = vy + hx + eps\n    result = vy / total\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Horizontal symmetry: normalized correlation between left half and mirrored right half (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w < 2:\n        return 0.0\n    mid = w // 2\n    if w % 2 == 0:\n        left = a[:, :mid].astype(float)\n        right = a[:, mid:].astype(float)\n    else:\n        left = a[:, :mid].astype(float)\n        right = a[:, mid+1:].astype(float)\n    # resize smaller to match if shapes differ\n    minw = min(left.shape[1], right.shape[1])\n    if minw == 0:\n        return 0.0\n    L = left[:, :minw].ravel()\n    R = np.fliplr(right)[:, :minw].ravel()\n    L = L.astype(float); R = R.astype(float)\n    Lm = L.mean() if L.size else 0.0\n    Rm = R.mean() if R.size else 0.0\n    num = np.sum((L - Lm) * (R - Rm))\n    den = np.sqrt(np.sum((L - Lm) ** 2) * np.sum((R - Rm) ** 2)) + eps\n    corr = num / den\n    # map from [-1,1] to [0,1]\n    result = float(np.clip((corr + 1.0) / 2.0, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical edge density: fraction of pixels with strong vertical gradient (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag_v = np.abs(gx)\n    mean = float(mag_v.mean())\n    std = float(mag_v.std())\n    thresh = mean + std\n    count = float((mag_v > thresh).sum())\n    total = float(mag_v.size) + eps\n    result = float(np.clip(count / total, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-region compactness: fraction of image occupied by bright pixels concentrated within their bounding box (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thresh = float(a.mean() + a.std())\n    bright = a > thresh\n    bp = int(bright.sum())\n    if bp == 0:\n        return 0.0\n    ys, xs = np.where(bright)\n    miny, maxy = ys.min(), ys.max()\n    minx, maxx = xs.min(), xs.max()\n    bbox_area = float((maxy - miny + 1) * (maxx - minx + 1)) + eps\n    img_area = float(h * w) + eps\n    fill_ratio = float(bp) / bbox_area\n    occupancy = bbox_area / img_area\n    result = float(np.clip(fill_ratio * occupancy, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Foreground-background contrast using median split normalized by image std (>=0)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    thr = float(np.median(flat))\n    fg = flat[flat > thr]\n    bg = flat[flat <= thr]\n    if fg.size == 0 or bg.size == 0:\n        return 0.0\n    mean_fg = float(fg.mean())\n    mean_bg = float(bg.mean())\n    overall_std = float(flat.std()) + eps\n    contrast = (mean_fg - mean_bg) / overall_std\n    result = float(np.clip(abs(contrast), 0.0, 10.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant structure orientation (0..1) where 0=vertical-ish and 1=horizontal-ish (averaged from structure tensor)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    Jxx = (gx * gx).mean()\n    Jyy = (gy * gy).mean()\n    Jxy = (gx * gy).mean()\n    # orientation of dominant eigenvector: 0..pi -> map to 0..1\n    angle = 0.5 * np.arctan2(2.0 * Jxy, (Jxx - Jyy) + eps)\n    # angle in radians in [-pi/2, pi/2]; map to [0,1]\n    result = float(np.clip((angle + np.pi/2.0) / np.pi, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Coarseness proxy: preferred block scale among [1,2,4,8] normalized to [0..1] (higher => coarser)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    scales = [1, 2, 4, 8]\n    valid_scales = [s for s in scales if s <= max(1, min(h, w))]\n    if not valid_scales:\n        return 0.0\n    global_var = float(a.var()) + eps\n    avg_vars = []\n    for s in valid_scales:\n        # reshape into non-overlapping blocks approximate by cropping\n        H = (h // s) * s\n        W = (w // s) * s\n        if H == 0 or W == 0:\n            avg_vars.append(0.0)\n            continue\n        B = a[:H, :W].reshape((H//s, s, W//s, s)).swapaxes(1,2).reshape(-1, s*s)\n        # per-block variance\n        bv = B.var(axis=1)\n        avg_vars.append(float(np.mean(bv)))\n    avg_vars = np.array(avg_vars)\n    if avg_vars.sum() <= 0:\n        return 0.0\n    idx = int(np.argmax(avg_vars))\n    result = float(idx / max(1, (len(valid_scales) - 1)))\n    result = float(np.clip(result, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Harris-corner density: fraction of strong corner responses (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    gy, gx = np.gradient(a)\n    Ixx = gx * gx\n    Iyy = gy * gy\n    Ixy = gx * gy\n    # simple 3x3 box filter using rolls\n    def boxmean(X):\n        s = np.zeros_like(X)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s / 9.0\n    Sxx = boxmean(Ixx)\n    Syy = boxmean(Iyy)\n    Sxy = boxmean(Ixy)\n    det = Sxx * Syy - Sxy * Sxy\n    trace = Sxx + Syy\n    k = 0.04\n    R = det - k * (trace ** 2)\n    Rmax = float(np.max(R)) if R.size else 0.0\n    if Rmax <= 0:\n        return 0.0\n    thresh = Rmax * 0.01\n    count = float((R > thresh).sum())\n    result = float(np.clip(count / (h * w + eps), 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean pairwise distance among top-5 brightest pixels normalized by image diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    flat = a.ravel()\n    if flat.size < 2:\n        return 0.0\n    k = min(5, flat.size)\n    idx = np.argpartition(-flat, k-1)[:k]\n    ys = (idx // w).astype(float)\n    xs = (idx % w).astype(float)\n    pts = np.stack([ys, xs], axis=1)\n    if pts.shape[0] < 2:\n        return 0.0\n    dsum = 0.0\n    pairs = 0\n    for i in range(pts.shape[0]):\n        for j in range(i+1, pts.shape[0]):\n            dsum += np.hypot(pts[i,0]-pts[j,0], pts[i,1]-pts[j,1])\n            pairs += 1\n    avgd = dsum / (pairs + eps)\n    maxd = np.hypot(h-1, w-1) + eps\n    result = float(np.clip(avgd / maxd, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric normalized by dynamic range (0..1); returns 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    imgf = np.nan_to_num(img.astype(float))\n    R = imgf[:, :, 0]\n    G = imgf[:, :, 1]\n    B = imgf[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    colorfulness = np.sqrt(std_rg**2 + std_yb**2) + 0.3 * np.sqrt(mean_rg**2 + mean_yb**2)\n    dyn = float(max(imgf.max() - imgf.min(), eps))\n    result = float(np.clip(colorfulness / dyn, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio from FFT (fraction of power outside central low-pass disk, 0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    try:\n        F = np.fft.fft2(a)\n        Fshift = np.fft.fftshift(F)\n        power = np.abs(Fshift) ** 2\n    except Exception:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    radius = max(1.0, min(h, w) / 8.0)\n    low_mask = r <= radius\n    low_energy = float(power[low_mask].sum())\n    total_energy = float(power.sum()) + eps\n    high_energy = float(total_energy - low_energy)\n    result = float(np.clip(high_energy / total_energy, 0.0, 1.0))\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of columns with strong horizontal transitions (texture/striping indicator)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if w == 0 or h < 2:\n        return 0.0\n    col_diff_means = np.mean(np.abs(np.diff(a, axis=0)), axis=0)\n    overall_mean = float(col_diff_means.mean()) + eps\n    high = col_diff_means > (1.5 * overall_mean)\n    result = float(np.count_nonzero(high)) / float(w)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Shannon entropy of the intensity histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 64\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    mn = float(a.min()); mx = float(a.max())\n    if mx <= mn + eps:\n        return 0.0\n    hist, _ = np.histogram(a.flatten(), bins=bins, range=(mn, mx))\n    total = hist.sum()\n    if total == 0:\n        return 0.0\n    p = hist.astype(float) / float(total)\n    p_nonzero = p[p > 0.0]\n    entropy = -np.sum(p_nonzero * np.log2(p_nonzero))\n    max_entropy = np.log2(bins) + eps\n    result = float(entropy / max_entropy)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized variance of discrete Laplacian (focus / texture measure)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    # discrete 4-neighbor Laplacian via shifts\n    lap = (np.roll(a, 1, axis=0) + np.roll(a, -1, axis=0) +\n           np.roll(a, 1, axis=1) + np.roll(a, -1, axis=1) - 4.0 * a)\n    var_lap = float(np.var(lap))\n    denom = (float(a.std()) ** 2) + eps\n    result = var_lap / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized offset of intensity center-of-mass from geometric center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0 or (h == 0 and w == 0):\n        return 0.0\n    # use nonnegative weights\n    weights = a - float(a.min())\n    s = float(weights.sum())\n    if s <= eps:\n        return 0.0\n    ys = np.arange(h, dtype=float)[:, None]\n    xs = np.arange(w, dtype=float)[None, :]\n    cy = float((weights * ys).sum()) / s\n    cx = float((weights * xs).sum()) / s\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dx = cx - center_x\n    dy = cy - center_y\n    dist = np.hypot(dx, dy)\n    max_dist = np.hypot(center_x, center_y) + eps\n    result = dist / max_dist\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted eccentricity of content (0 = round/uniform, 1 = linear)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    weights = a - float(a.min())\n    S = float(weights.sum())\n    if S <= eps:\n        return 0.0\n    ys = np.arange(h, dtype=float)[:, None]\n    xs = np.arange(w, dtype=float)[None, :]\n    cy = float((weights * ys).sum()) / S\n    cx = float((weights * xs).sum()) / S\n    yc = (ys - cy)\n    xc = (xs - cx)\n    var_y = float((weights * (yc ** 2)).sum()) / S\n    var_x = float((weights * (xc ** 2)).sum()) / S\n    cov_xy = float((weights * xc * yc).sum()) / S\n    # covariance matrix eigenvalues\n    trace = var_x + var_y\n    det = var_x * var_y - cov_xy * cov_xy\n    # numerical stability\n    disc = max(trace * trace / 4.0 - det, 0.0)\n    sqrt_disc = np.sqrt(disc)\n    lam1 = trace / 2.0 + sqrt_disc\n    lam2 = trace / 2.0 - sqrt_disc\n    if lam1 <= eps:\n        return 0.0\n    ratio = lam2 / (lam1 + eps)\n    ecc = 1.0 - float(np.clip(ratio, 0.0, 1.0))\n    return float(np.clip(ecc, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge orientation concentration (1 => edges mostly aligned, 0 => isotropic)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy) + eps\n    ux = gx / mag\n    uy = gy / mag\n    # magnitude-weighted mean direction vector\n    sum_mag = float(mag.sum())\n    if sum_mag <= eps:\n        return 0.0\n    mean_x = float((ux * mag).sum()) / sum_mag\n    mean_y = float((uy * mag).sum()) / sum_mag\n    resultant = np.hypot(mean_x, mean_y)\n    result = float(np.clip(resultant, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric for RGB images (0 for grayscale); normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    arr = np.nan_to_num(img.astype(float))\n    R = arr[..., 0]\n    G = arr[..., 1]\n    B = arr[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(np.abs(rg)))\n    mean_yb = float(np.mean(np.abs(yb)))\n    colorfulness = np.sqrt(std_rg * std_rg + std_yb * std_yb) + 0.3 * np.sqrt(mean_rg * mean_rg + mean_yb * mean_yb)\n    gstd = float(arr.mean(axis=2).std()) + eps\n    result = colorfulness / gstd\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram peakiness: how dominated the image is by a single intensity bin (0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 32\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    mn = float(a.min()); mx = float(a.max())\n    if mx <= mn + eps:\n        return 0.0\n    hist, _ = np.histogram(a.flatten(), bins=bins, range=(mn, mx))\n    if hist.sum() == 0:\n        return 0.0\n    peak_ratio = float(hist.max()) / (float(hist.mean()) + eps)\n    # map to 0..1 smoothly\n    result = float(np.tanh((peak_ratio - 1.0) / 5.0))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score based on mean absolute difference (1 => symmetric)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0 or w == 0:\n        return 0.0\n    left = a[:, :w//2]\n    right = a[:, - (w//2):] if w//2 > 0 else np.empty((h, 0))\n    if left.size == 0 or right.size == 0:\n        # trivial symmetry when one side is empty\n        return 1.0\n    # flip right to align with left\n    right_flipped = np.fliplr(right)\n    # If widths mismatch (odd center column), compare up to min width\n    minw = min(left.shape[1], right_flipped.shape[1])\n    if minw == 0:\n        return 1.0\n    left_c = left[:, :minw]\n    right_c = right_flipped[:, :minw]\n    diff_mean = float(np.mean(np.abs(left_c - right_c)))\n    denom = float(np.mean(np.abs(a))) + eps\n    normalized_error = diff_mean / denom\n    # convert to similarity: 1 - scaled error (use tanh for robustness)\n    score = 1.0 - float(np.tanh(normalized_error))\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels that are strict local maxima in a 3x3 neighborhood (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    pad_val = float(a.min()) - 1.0\n    p = np.pad(a, pad_width=1, mode='constant', constant_values=pad_val)\n    center = p[1:-1, 1:-1]\n    neighbors = [\n        p[0:-2, 0:-2], p[0:-2, 1:-1], p[0:-2, 2:],\n        p[1:-1, 0:-2],               p[1:-1, 2:],\n        p[2:  , 0:-2], p[2:  , 1:-1], p[2:  , 2:],\n    ]\n    mask = np.ones_like(center, dtype=bool)\n    for n in neighbors:\n        mask &= (center > n)\n    count = float(np.count_nonzero(mask))\n    result = count / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio via discrete Laplacian (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    gyy, _ = np.gradient(gy)\n    _, gxx = np.gradient(gx)\n    lap = np.abs(gxx + gyy)\n    high = float(lap.sum())\n    total = float(np.abs(a - a.mean()).sum())\n    result = high / (high + total + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted eccentricity of mass (0=centered circular .. 1=elongated)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    total = float(arr.sum())\n    if total <= eps:\n        return 0.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    cy = float((ys * arr).sum() / (total + eps))\n    cx = float((xs * arr).sum() / (total + eps))\n    rx = (xs - cx)\n    ry = (ys - cy)\n    # broadcast to shape\n    RX = np.repeat(rx, h, axis=0).T if rx.shape[1] == w and ry.shape[0] == h else None\n    # compute centralized moments\n    # more direct computation:\n    dx = (xs - cx)\n    dy = (ys - cy)\n    cov_xx = float(((dx**2) * arr).sum() / (total + eps))\n    cov_yy = float(((dy**2) * arr).sum() / (total + eps))\n    cov_xy = float(((dx * dy) * arr).sum() / (total + eps))\n    trace = cov_xx + cov_yy\n    det = cov_xx * cov_yy - cov_xy * cov_xy\n    disc = max(trace * trace - 4.0 * det, 0.0)\n    l1 = (trace + np.sqrt(disc)) / 2.0\n    l2 = (trace - np.sqrt(disc)) / 2.0\n    if l1 <= eps:\n        return 0.0\n    ecc = float(np.sqrt(max(0.0, 1.0 - max(0.0, l2) / (l1 + eps))))\n    return float(np.clip(ecc, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local 3x3 variance normalized by global variance (>=0)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        # fallback to global variance ratio (0 when flat)\n        glob_var = float(a.var()) + eps\n        return float(np.clip(glob_var / (glob_var + eps), 0.0, 1e6))\n    pad = np.pad(a, 1, mode='reflect')\n    s = np.zeros_like(a, dtype=float)\n    ss = np.zeros_like(a, dtype=float)\n    for dy in (0, 1, 2):\n        for dx in (0, 1, 2):\n            window = pad[dy:dy + h, dx:dx + w]\n            s += window\n            ss += window * window\n    mean_local = s / 9.0\n    mean_sq_local = ss / 9.0\n    var_local = mean_sq_local - mean_local * mean_local\n    avg_var = float(np.mean(var_local))\n    glob_var = float(a.var()) + eps\n    result = avg_var / (glob_var + eps)\n    return float(max(0.0, result))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels within 0.1*std of the mean (flatness 0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    if arr.size == 0:\n        return 0.0\n    m = float(arr.mean())\n    s = float(arr.std()) + eps\n    thresh = 0.1 * s\n    count = float(np.count_nonzero(np.abs(arr - m) <= thresh))\n    result = count / (float(arr.size) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation coherence (0..1), 1 = all gradients aligned'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    total_mag = float(mag.sum()) + eps\n    sx = float(gx.sum())\n    sy = float(gy.sum())\n    coherence = float(np.hypot(sx, sy) / (total_mag + eps))\n    return float(np.clip(coherence, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of local maxima above mean+std (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std()) + eps\n    threshold = m + s\n    padmin = float(a.min()) - 1.0\n    pad = np.pad(a, 1, mode='constant', constant_values=padmin)\n    center = pad[1:1 + h, 1:1 + w]\n    neighs = []\n    for dy in (0, 1, 2):\n        for dx in (0, 1, 2):\n            if dy == 1 and dx == 1:\n                continue\n            neighs.append(pad[dy:dy + h, dx:dx + w])\n    mask = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        mask &= (center > n)\n    mask &= (center > threshold)\n    count = float(np.count_nonzero(mask))\n    result = count / (float(h * w) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical vs horizontal edge bias (-1..1), positive => vertical edges stronger'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    sv = float(np.abs(gx).sum())\n    sh = float(np.abs(gy).sum())\n    result = (sv - sh) / (sv + sh + eps)\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity histogram entropy (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        arr = np.nan_to_num(img.astype(float)).ravel()\n    if arr.size == 0:\n        return 0.0\n    amin = float(arr.min())\n    amax = float(arr.max())\n    if amax <= amin:\n        return 0.0\n    nbins = 64\n    hist, _ = np.histogram(arr, bins=nbins, range=(amin, amax))\n    total = float(hist.sum()) + eps\n    probs = hist.astype(float) / total\n    ent = -float(np.sum(probs * np.log(probs + eps)))\n    norm = float(np.log(nbins) + eps)\n    result = ent / norm\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center block mean minus corner means divided by global std (contrast, can be negative)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ch = max(1, h // 3)\n    cw = max(1, w // 3)\n    r0 = h // 2 - ch // 2\n    c0 = w // 2 - cw // 2\n    center = a[r0:r0 + ch, c0:c0 + cw]\n    qh = ch\n    qw = cw\n    corners = []\n    corners.append(a[0:qh, 0:qw])\n    corners.append(a[0:qh, -qw:])\n    corners.append(a[-qh:, 0:qw])\n    corners.append(a[-qh:, -qw:])\n    center_mean = float(center.mean()) if center.size else 0.0\n    corner_means = [float(x.mean()) if x.size else 0.0 for x in corners]\n    corner_mean = float(np.mean(corner_means)) if len(corner_means) else 0.0\n    gstd = float(a.std()) + eps\n    result = (center_mean - corner_mean) / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Percentile contrast (p90-p10)/(p90+p10) (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    flat = arr.ravel()\n    if flat.size == 0:\n        return 0.0\n    p10, p90 = np.percentile(flat, [10.0, 90.0])\n    denom = (p90 + p10 + eps)\n    result = (p90 - p10) / denom\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian (average second-derivative magnitude) indicating edge/texture strength'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # discrete Laplacian via neighbor rolls (handles edges via wrap which is cheap)\n    lap = -4.0 * a\n    lap += np.roll(a, 1, axis=0)\n    lap += np.roll(a, -1, axis=0)\n    lap += np.roll(a, 1, axis=1)\n    lap += np.roll(a, -1, axis=1)\n    result = float(np.mean(np.abs(lap)))\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio in the 2D FFT (fraction of energy outside low-frequency disk)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute power spectrum\n    F = np.fft.fftshift(np.fft.fft2(a))\n    P = np.abs(F) ** 2\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    # low-frequency radius as 10% of diagonal\n    low_r = max(1.0, 0.1 * np.hypot(h, w))\n    low_mask = (r <= low_r)\n    total = P.sum() + eps\n    high = P[~low_mask].sum()\n    result = float(high / total)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score (0..1) based on normalized correlation between halves'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2 or h == 0:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, w - mid:][:, ::-1]  # flip right to compare\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    # crop to same shape if odd width\n    min_w = min(left.shape[1], right.shape[1])\n    left = left[:, :min_w].ravel()\n    right = right[:, :min_w].ravel()\n    if left.size == 0:\n        return 0.0\n    # correlation\n    left = left - left.mean()\n    right = right - right.mean()\n    num = float((left * right).sum())\n    den = (np.sqrt((left ** 2).sum() * (right ** 2).sum()) + eps)\n    corr = num / den\n    # map from [-1,1] to [0,1]\n    result = (corr + 1.0) / 2.0\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of border pixels with strong gradient (edge activity concentrated near image borders)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    # border width as 1/8 of min dimension, at least 1\n    bw = max(1, min(h, w) // 8)\n    mask = np.zeros_like(a, dtype=bool)\n    mask[:bw, :] = True\n    mask[-bw:, :] = True\n    mask[:, :bw] = True\n    mask[:, -bw:] = True\n    if not np.any(mask):\n        return 0.0\n    # threshold for strong gradient\n    thr = float(mag.mean() + mag.std())\n    strong = (mag > thr) & mask\n    result = float(np.count_nonzero(strong)) / float(np.count_nonzero(mask) + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized contrast between top 10% and bottom 10% intensity pixels (higher => stronger contrast)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    p90 = np.percentile(vals, 90)\n    p10 = np.percentile(vals, 10)\n    top = vals[vals >= p90]\n    bot = vals[vals <= p10]\n    if top.size == 0 or bot.size == 0:\n        return 0.0\n    mean_top = float(top.mean())\n    mean_bot = float(bot.mean())\n    overall_std = float(vals.std()) + eps\n    result = (mean_top - mean_bot) / overall_std\n    # clip to reasonable range\n    return float(np.clip(result, -10.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Approximate number of connected bright components (capped) after thresholding'\n    import numpy as np\n    eps = 1e-12\n    MAX_CC = 1000\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # choose threshold at 75th percentile to focus on bright components\n    thr = float(np.percentile(a.ravel(), 75))\n    mask = a > thr\n    if not np.any(mask):\n        return 0.0\n    visited = np.zeros_like(mask, dtype=bool)\n    cc = 0\n    # 4-connected neighbors\n    for y in range(h):\n        for x in range(w):\n            if mask[y, x] and not visited[y, x]:\n                cc += 1\n                if cc >= MAX_CC:\n                    break\n                # iterative flood fill\n                stack = [(y, x)]\n                visited[y, x] = True\n                while stack:\n                    yy, xx = stack.pop()\n                    # neighbors\n                    if yy > 0 and mask[yy - 1, xx] and not visited[yy - 1, xx]:\n                        visited[yy - 1, xx] = True\n                        stack.append((yy - 1, xx))\n                    if yy + 1 < h and mask[yy + 1, xx] and not visited[yy + 1, xx]:\n                        visited[yy + 1, xx] = True\n                        stack.append((yy + 1, xx))\n                    if xx > 0 and mask[yy, xx - 1] and not visited[yy, xx - 1]:\n                        visited[yy, xx - 1] = True\n                        stack.append((yy, xx - 1))\n                    if xx + 1 < w and mask[yy, xx + 1] and not visited[yy, xx + 1]:\n                        visited[yy, xx + 1] = True\n                        stack.append((yy, xx + 1))\n                # continue scanning\n        if cc >= MAX_CC:\n            break\n    # normalize by image area\n    result = float(min(cc, MAX_CC)) / float(h * w + eps)\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fill ratio of bright pixels within their bounding box (1 => box full, 0 => empty)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(np.median(a.ravel()) + 0.1 * (a.std() + eps))\n    ys, xs = np.nonzero(a > thr)\n    if ys.size == 0:\n        return 0.0\n    y0, y1 = int(ys.min()), int(ys.max())\n    x0, x1 = int(xs.min()), int(xs.max())\n    bbox_area = max(1, (y1 - y0 + 1) * (x1 - x0 + 1))\n    bright_count = ys.size\n    result = float(bright_count) / float(bbox_area + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Multiscale coarseness: ratio of variance of 2x2 pooled image to 8x8 pooled image (higher => finer texture)'\n    import numpy as np\n    eps = 1e-12\n    def block_mean(im, f):\n        h, w = im.shape\n        if f <= 1:\n            return im\n        # pad to multiple of f with edge values\n        ph = (f - (h % f)) % f\n        pw = (f - (w % f)) % f\n        if ph or pw:\n            im = np.pad(im, ((0, ph), (0, pw)), mode='edge')\n        nh, nw = im.shape\n        im2 = im.reshape(nh // f, f, nw // f, f)\n        return im2.mean(axis=(1, 3))\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    small = block_mean(a, 2)\n    large = block_mean(a, 8)\n    var_small = float(np.var(small))\n    var_large = float(np.var(large)) + eps\n    ratio = var_small / var_large\n    # map to [0,1] with a soft cap\n    result = float(1.0 - 1.0 / (1.0 + ratio))\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center-annulus contrast: difference between mean intensity in a small center disk and surrounding annulus (normalized)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(ys - cy, xs - cx)\n    maxr = np.hypot(cy, cx) + eps\n    r_center = max(1.0, 0.12 * maxr)\n    r_outer = max(2.0, 0.3 * maxr)\n    center_mask = r <= r_center\n    ann_mask = (r > r_center) & (r <= r_outer)\n    if not np.any(center_mask) or not np.any(ann_mask):\n        return 0.0\n    mean_center = float(a[center_mask].mean())\n    mean_ann = float(a[ann_mask].mean())\n    overall_std = float(a.std()) + eps\n    result = (mean_center - mean_ann) / overall_std\n    # clip to sensible range\n    return float(np.clip(result, -10.0, 10.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant edge orientation strength: fraction of edge magnitude falling into the most common orientation bin'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    if mag.sum() == 0:\n        return 0.0\n    ang = np.arctan2(gy, gx)  # -pi..pi\n    # convert to 0..180 degrees (orientation ignoring sign)\n    ang_deg = np.degrees(np.abs(ang))\n    ang_deg = ang_deg % 180.0\n    bins = 8\n    hist, _ = np.histogram(ang_deg.ravel(), bins=bins, range=(0.0, 180.0), weights=mag.ravel())\n    total = hist.sum() + eps\n    dominant = hist.max()\n    result = float(dominant) / float(total)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with strong gradient magnitude (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + mag.std())\n    count = float(np.count_nonzero(mag > thr))\n    result = count / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Center-surround contrast normalized by image std (center minus border)'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    ch0, ch1 = h // 4, w // 4\n    cy0, cy1 = ch0, h - ch0\n    cx0, cx1 = ch1, w - ch1\n    center = img[cy0:cy1, cx0:cx1]\n    if center.size == 0:\n        return 0.0\n    mask = np.ones_like(img, dtype=bool)\n    mask[cy0:cy1, cx0:cx1] = False\n    border = img[mask]\n    center_mean = float(np.mean(center)) if center.size else 0.0\n    border_mean = float(np.mean(border)) if border.size else 0.0\n    overall_std = float(np.std(img)) + eps\n    result = (center_mean - border_mean) / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bounding-box area ratio of the top 5% brightest pixels (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h * w == 0:\n        return 0.0\n    try:\n        thr = float(np.percentile(a, 95))\n    except Exception:\n        thr = float(a.mean())\n    mask = a > thr\n    if np.count_nonzero(mask) < 3:\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    y0, y1 = int(ys.min()), int(ys.max())\n    x0, x1 = int(xs.min()), int(xs.max())\n    bbox_area = float((y1 - y0 + 1) * (x1 - x0 + 1))\n    result = bbox_area / float(h * w)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation dispersion (0 coherent -> 1 highly dispersed)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(img)\n    mag = np.hypot(gx, gy)\n    total_mag = float(mag.sum()) + eps\n    if total_mag <= eps:\n        return 0.0\n    cx = (mag * np.cos(np.arctan2(gy, gx))).sum() / total_mag\n    cy = (mag * np.sin(np.arctan2(gy, gx))).sum() / total_mag\n    R = float(np.hypot(cx, cy))\n    R = min(max(R, 0.0), 1.0)\n    dispersion = 1.0 - R\n    return float(np.clip(dispersion, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Saturation fraction: fraction of pixels near data range extremes (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    a = np.nan_to_num(arr.astype(float))\n    if a.ndim == 3 and a.shape[2] >= 1:\n        mins = a.min(axis=(0, 1))\n        maxs = a.max(axis=(0, 1))\n        ranges = maxs - mins\n        small = ranges < eps\n        ranges[small] = eps\n        low_thr = mins + 0.01 * ranges\n        high_thr = maxs - 0.01 * ranges\n        # pixel is saturated if any channel near low or high\n        low_mask = (a <= low_thr[None, None, :]).any(axis=2)\n        high_mask = (a >= high_thr[None, None, :]).any(axis=2)\n        mask = low_mask | high_mask\n        total = float(a.shape[0] * a.shape[1])\n    else:\n        # grayscale\n        mins = a.min()\n        maxs = a.max()\n        if maxs - mins < eps:\n            return 0.0\n        low_thr = mins + 0.01 * (maxs - mins)\n        high_thr = maxs - 0.01 * (maxs - mins)\n        mask = (a <= low_thr) | (a >= high_thr)\n        total = float(a.size)\n    count = float(np.count_nonzero(mask))\n    result = count / (total + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-frequency energy ratio via 2x2 block averaging (0..inf)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    h2 = (h // 2) * 2\n    w2 = (w // 2) * 2\n    crop = img[:h2, :w2]\n    try:\n        blocks = crop.reshape(h2//2, 2, w2//2, 2)\n        low = blocks.mean(axis=(1,3))\n    except Exception:\n        # fallback to simple downsample by slicing\n        low = img[::2, ::2]\n    low_energy = float(np.mean(np.abs(low)))\n    high_energy = float(np.mean(np.abs(img))) + eps\n    result = low_energy / high_energy\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry: normalized correlation between left and mirrored right (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = img[:, :mid]\n    right = img[:, w-mid:]\n    right_flipped = np.fliplr(right)\n    if left.size != right_flipped.size:\n        # ensure same shape by cropping larger\n        min_w = min(left.shape[1], right_flipped.shape[1])\n        left = left[:, :min_w]\n        right_flipped = right_flipped[:, :min_w]\n    x = left.ravel().astype(float)\n    y = right_flipped.ravel().astype(float)\n    if x.size == 0:\n        return 0.0\n    xm = x.mean()\n    ym = y.mean()\n    num = float(((x - xm) * (y - ym)).sum())\n    den = float(np.sqrt(((x - xm) ** 2).sum() * ((y - ym) ** 2).sum())) + eps\n    corr = num / den\n    corr = max(min(corr, 1.0), -1.0)\n    result = (corr + 1.0) / 2.0\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Global gradient coherence from structure tensor (0..1, higher = more coherent)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(img)\n    Sxx = float((gx * gx).sum())\n    Syy = float((gy * gy).sum())\n    Sxy = float((gx * gy).sum())\n    # structure tensor eigenvalues\n    trace = Sxx + Syy\n    det = Sxx * Syy - Sxy * Sxy\n    tmp = max(trace * trace / 4.0 - det, 0.0)\n    diff = float(np.sqrt(tmp))\n    lam1 = trace / 2.0 + diff\n    lam2 = trace / 2.0 - diff\n    if lam1 + lam2 <= eps:\n        return 0.0\n    coherence = (lam1 - lam2) / (lam1 + lam2 + eps)\n    return float(np.clip(coherence, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized histogram entropy of intensities (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    a_flat = a.ravel()\n    mn = float(a_flat.min())\n    mx = float(a_flat.max())\n    if mx - mn < eps:\n        return 0.0\n    bins = 32\n    counts, _ = np.histogram(a_flat, bins=bins, range=(mn, mx))\n    total = float(counts.sum()) + eps\n    p = counts.astype(float) / total\n    p_nonzero = p[p > 0.0]\n    ent = -float((p_nonzero * np.log(p_nonzero)).sum())\n    # normalize by log(bins)\n    norm = float(np.log(bins)) + eps\n    result = ent / norm\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Otsu foreground fraction: proportion of pixels above an automatically found threshold (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    a_flat = a.ravel()\n    mn = float(a_flat.min())\n    mx = float(a_flat.max())\n    if mx - mn < eps:\n        return 0.0\n    counts, bin_edges = np.histogram(a_flat, bins=256, range=(mn, mx))\n    bins = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n    total = counts.sum()\n    if total == 0:\n        return 0.0\n    prob = counts.astype(float) / float(total)\n    omega = np.cumsum(prob)\n    mu = np.cumsum(prob * bins)\n    mu_t = mu[-1]\n    # between-class variance\n    denom = omega * (1.0 - omega) + eps\n    sigma_b = (mu_t * omega - mu) ** 2 / denom\n    idx = int(np.argmax(sigma_b))\n    thresh = bins[idx]\n    fg_count = int((a_flat > thresh).sum())\n    result = float(fg_count) / float(total)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Shannon entropy of intensity histogram (0..1, higher => more texture/variety)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    bins = 64\n    vmin, vmax = float(a.min()), float(a.max())\n    if vmax <= vmin:\n        return 0.0\n    hist, _ = np.histogram(a, bins=bins, range=(vmin, vmax))\n    p = hist.astype(float) + eps\n    p = p / p.sum()\n    entropy = -np.sum(p * np.log2(p))\n    norm = np.log2(bins) + eps\n    result = float(entropy / norm)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score via normalized cross-correlation ( -1..1, 1 => perfect symmetric)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:]\n    # mirror right to align\n    right_mirror = np.fliplr(right)\n    # crop to same shape\n    min_w = min(left.shape[1], right_mirror.shape[1])\n    left_c = left[:, :min_w].ravel()\n    right_c = right_mirror[:, :min_w].ravel()\n    if left_c.size == 0 or right_c.size == 0:\n        return 0.0\n    left_c = left_c - left_c.mean()\n    right_c = right_c - right_c.mean()\n    denom = (np.linalg.norm(left_c) * np.linalg.norm(right_c) + eps)\n    corr = float(np.dot(left_c, right_c) / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge pixel fraction using gradient magnitude (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    thr = mag.mean() + 0.5 * mag.std()\n    edges = mag > thr\n    result = float(np.count_nonzero(edges)) / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized offset of intensity centroid from image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    y_idx, x_idx = np.indices((h, w))\n    total = float(a.sum()) + eps\n    cx = float((a * x_idx).sum()) / total\n    cy = float((a * y_idx).sum()) / total\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    diag = np.hypot(w, h) / 2.0 + eps\n    result = float(dist / diag)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio (squareness) of the bounding box of bright pixels (0..1, 1 => square)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean() + 0.5 * a.std())\n    mask = a > thr\n    if not mask.any():\n        return 0.0\n    ys, xs = np.where(mask)\n    miny, maxy = ys.min(), ys.max()\n    minx, maxx = xs.min(), xs.max()\n    box_h = float(max(1, maxy - miny + 1))\n    box_w = float(max(1, maxx - minx + 1))\n    ratio = min(box_h, box_w) / max(box_h, box_w)\n    result = float(np.clip(ratio, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean chromatic saturation estimate for color images (0..1), grayscale => 0'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    a = np.nan_to_num(img.astype(float))\n    R = a[..., 0]\n    G = a[..., 1]\n    B = a[..., 2]\n    mx = np.maximum(np.maximum(R, G), B)\n    mn = np.minimum(np.minimum(R, G), B)\n    sat = (mx - mn) / (mx + eps)  # in [0,1]\n    result = float(np.mean(sat))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variation of local block contrasts: std of block stds normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # choose 4x4 blocks (or fewer if small)\n    bh = max(1, h // 4)\n    bw = max(1, w // 4)\n    block_stds = []\n    for y in range(0, h, bh):\n        for x in range(0, w, bw):\n            block = a[y:min(h, y + bh), x:min(w, x + bw)]\n            if block.size:\n                block_stds.append(block.std())\n    if not block_stds:\n        return 0.0\n    block_stds = np.array(block_stds, dtype=float)\n    global_std = float(a.std()) + eps\n    result = float(block_stds.std() / global_std)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant gradient orientation strength (0..1): fraction of gradient energy in the peak angle bin'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    ang = np.arctan2(gy, gx)  # -pi..pi\n    bins = 36\n    # shift angles to 0..2pi\n    ang2 = (ang + np.pi) % (2 * np.pi)\n    hist, _ = np.histogram(ang2.ravel(), bins=bins, range=(0.0, 2 * np.pi), weights=mag.ravel())\n    total = hist.sum() + eps\n    peak = float(hist.max())\n    result = float(peak / total)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average nearest-neighbor distance among top-brightness pixels normalized by image diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # select top 5% brightest pixels\n    flat = a.ravel()\n    k = max(1, int(0.05 * flat.size))\n    if flat.size <= k:\n        thresh = flat.min() - 1.0\n    else:\n        # partition to get threshold efficiently\n        idx = np.argpartition(flat, -k)[-k:]\n        thresh = flat[idx].min()\n    ys, xs = np.where(a >= thresh)\n    n = ys.size\n    if n <= 1:\n        return 0.0\n    # sample if too many\n    max_pts = 300\n    if n > max_pts:\n        sel = np.random.choice(n, size=max_pts, replace=False)\n        ys = ys[sel]; xs = xs[sel]; n = max_pts\n    pts = np.stack([ys.astype(float), xs.astype(float)], axis=1)\n    # compute nearest neighbor distances\n    diffs = pts[:, None, :] - pts[None, :, :]  # n x n x 2\n    d2 = np.sum(diffs * diffs, axis=2)\n    np.fill_diagonal(d2, np.inf)\n    nn = np.sqrt(np.min(d2, axis=1))\n    avg_nn = float(np.mean(nn))\n    diag = np.hypot(h, w) + eps\n    result = float(avg_nn / diag)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-frequency energy ratio from FFT (0..1), higher => smoother / large-scale structure'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # compute centered FFT power spectrum\n    F = np.fft.fft2(a - a.mean())\n    Fshift = np.fft.fftshift(F)\n    power = np.abs(Fshift) ** 2\n    # low-frequency square window around center: size = min(h,w)//4\n    cx = w // 2\n    cy = h // 2\n    r = max(1, min(h, w) // 8)\n    y0 = max(0, cy - r)\n    y1 = min(h, cy + r + 1)\n    x0 = max(0, cx - r)\n    x1 = min(w, cx + r + 1)\n    low = power[y0:y1, x0:x1].sum()\n    total = power.sum() + eps\n    result = float(low / total)\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of horizontal vs vertical edge energy (0..1 where >0.5 => more horizontal)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    # grayscale\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # compute simple gradients\n    gy, gx = np.gradient(a)\n    horiz_energy = float(np.sum(np.abs(gx)))\n    vert_energy = float(np.sum(np.abs(gy)))\n    # convert to normalized ratio 0..1: horiz/(horiz+vert)\n    denom = horiz_energy + vert_energy + eps\n    result = horiz_energy / denom\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Standard deviation of block-wise local contrast normalized by global std'\n    import numpy as np\n    eps = 1e-8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # choose block size relative to image\n    blk_h = max(1, h // 8)\n    blk_w = max(1, w // 8)\n    stds = []\n    for y in range(0, h, blk_h):\n        for x in range(0, w, blk_w):\n            block = a[y:y+blk_h, x:x+blk_w]\n            if block.size:\n                stds.append(float(block.std()))\n    if not stds:\n        return 0.0\n    stds = np.array(stds, dtype=float)\n    global_std = float(a.std()) + eps\n    result = float(stds.std()) / global_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized offset of bright-pixel centroid from image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean() + 0.5 * a.std())\n    mask = a > thr\n    if not mask.any():\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    cy = ys.mean()\n    cx = xs.mean()\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    maxdist = np.hypot(center_y, center_x) + eps\n    result = float(dist / maxdist)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Robust percentile contrast: (P90 - P10) / (median + eps)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        flat = np.nan_to_num(img.mean(axis=2).ravel().astype(float))\n    else:\n        flat = np.nan_to_num(img.ravel().astype(float))\n    if flat.size == 0:\n        return 0.0\n    p90 = float(np.percentile(flat, 90))\n    p10 = float(np.percentile(flat, 10))\n    med = float(np.median(flat))\n    result = (p90 - p10) / (abs(med) + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio using discrete Laplacian (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    # Laplacian via neighbors (4-connected)\n    up = a[:-2, 1:-1]\n    down = a[2:, 1:-1]\n    left = a[1:-1, :-2]\n    right = a[1:-1, 2:]\n    center = a[1:-1, 1:-1]\n    lap = (up + down + left + right) - 4.0 * center\n    hf = float(np.sum(np.abs(lap)))\n    total = float(np.sum(np.abs(center))) + eps\n    result = hf / (hf + total + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Orientation bias: normalized difference between column-mean and row-mean variances (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    col_means = a.mean(axis=0)\n    row_means = a.mean(axis=1)\n    var_c = float(np.var(col_means))\n    var_r = float(np.var(row_means))\n    result = (var_c - var_r) / (var_c + var_r + eps)\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of local bright peaks (strict local maxima above adaptive thresh) normalized by image area'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean() + 0.5 * a.std())\n    # pad with very small values so edges can be peaks\n    pad = np.pad(a, pad_width=1, mode='constant', constant_values=a.min() - 1.0)\n    center = pad[1:-1, 1:-1]\n    neighs = [\n        pad[0:-2, 0:-2], pad[0:-2, 1:-1], pad[0:-2, 2:],\n        pad[1:-1, 0:-2],                 pad[1:-1, 2:],\n        pad[2:, 0:-2],   pad[2:, 1:-1],  pad[2:, 2:]\n    ]\n    comp = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        comp &= (center > n)\n    comp &= (center > thr)\n    count = int(np.count_nonzero(comp))\n    result = float(count) / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Balance between dark and bright pixels: fraction_above_median - fraction_below_median (-1..1)'\n    import numpy as np\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        flat = np.nan_to_num(img.mean(axis=2).ravel().astype(float))\n    else:\n        flat = np.nan_to_num(img.ravel().astype(float))\n    if flat.size == 0:\n        return 0.0\n    med = float(np.median(flat))\n    above = float(np.count_nonzero(flat > med))\n    below = float(np.count_nonzero(flat < med))\n    total = float(flat.size)\n    result = (above - below) / (total + 1e-12)\n    return float(np.clip(result, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Compactness of top-intensity region: area(top P%) / bounding-box-area (0..1)'\n    import numpy as np\n    eps = 1e-12\n    P = 5.0  # top percent\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    cutoff = float(np.percentile(flat, 100.0 - P))\n    mask = a >= cutoff\n    if not mask.any():\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    miny, maxy = ys.min(), ys.max()\n    minx, maxx = xs.min(), xs.max()\n    area = float(mask.sum())\n    bbox_area = float(max(1, maxy - miny + 1)) * float(max(1, maxx - minx + 1))\n    result = area / (bbox_area + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant gradient orientation strength (0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 36\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(image.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(image.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= 0:\n        return 0.0\n    # orientation in [0, pi)\n    ang = np.arctan2(gy, gx)\n    ang = np.abs(ang)  # map to [0, pi]\n    ang = ang % np.pi\n    flat_ang = ang.ravel()\n    flat_mag = mag.ravel()\n    # compute weighted histogram\n    bin_edges = np.linspace(0.0, np.pi, bins + 1)\n    hist, _ = np.histogram(flat_ang, bins=bin_edges, weights=flat_mag)\n    total = hist.sum() + eps\n    maxbin = float(hist.max())\n    result = maxbin / total\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry score (1.0 = perfectly symmetric, 0.0 = very asymmetric)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 1.0\n    mid = w // 2\n    left = a[:, :mid]\n    if w % 2 == 0:\n        right = a[:, mid:]\n    else:\n        right = a[:, mid+1:]\n    # make same shape by cropping larger side\n    minw = min(left.shape[1], right.shape[1])\n    if minw == 0:\n        return 1.0\n    l = left[:, :minw]\n    r = right[:, :minw][:, ::-1]  # mirror right side\n    num = float(np.sum(np.abs(l - r)))\n    den = float(np.sum(np.abs(l) + np.abs(r)) + eps)\n    diff = num / den\n    sym = 1.0 - diff\n    return float(np.clip(sym, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of image energy in high-frequency residual after a 3x3 blur (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # simple 3x3 mean blur via shifts with edge padding\n    p = np.pad(a, pad_width=1, mode='edge')\n    s = np.zeros_like(a, dtype=float)\n    s += p[0:h, 0:w]\n    s += p[0:h, 1:w+1]\n    s += p[0:h, 2:w+2]\n    s += p[1:h+1, 0:w]\n    s += p[1:h+1, 1:w+1]\n    s += p[1:h+1, 2:w+2]\n    s += p[2:h+2, 0:w]\n    s += p[2:h+2, 1:w+1]\n    s += p[2:h+2, 2:w+2]\n    blur = s / 9.0\n    residual = a - blur\n    energy_res = float(np.sum(residual * residual))\n    total_var = float(np.sum((a - np.mean(a)) ** 2))\n    if total_var <= eps:\n        return 0.0\n    frac = energy_res / (total_var + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio (width/height) of bounding box of bright region (>=0), 1.0 if none'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 1.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 1.0\n    thr = float(np.mean(a) + 0.5 * np.std(a))\n    mask = a > thr\n    if not np.any(mask):\n        return 1.0\n    rows = np.where(mask.any(axis=1))[0]\n    cols = np.where(mask.any(axis=0))[0]\n    if rows.size == 0 or cols.size == 0:\n        return 1.0\n    h_bb = float(rows[-1] - rows[0] + 1)\n    w_bb = float(cols[-1] - cols[0] + 1)\n    if h_bb <= 0:\n        return 1.0\n    ratio = w_bb / (h_bb + eps)\n    return float(np.clip(ratio, 0.0, 100.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of center-region pixels that are strict local maxima compared to 8 neighbors (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    ch = max(1, h // 4)\n    cw = max(1, w // 4)\n    r0 = max(1, h//2 - ch//2)\n    r1 = min(h-1, r0 + ch)\n    c0 = max(1, w//2 - cw//2)\n    c1 = min(w-1, c0 + cw)\n    region = a[r0:r1, c0:c1]\n    if region.size == 0:\n        return 0.0\n    # compare to 8 neighbors using shifted slices on the original array\n    center = a[r0:r1, c0:c1]\n    n0 = a[r0-1:r1-1, c0-1:c1-1]  # up-left\n    n1 = a[r0-1:r1-1, c0:c1]    # up\n    n2 = a[r0-1:r1-1, c0+1:c1+1]  # up-right\n    n3 = a[r0:r1, c0-1:c1-1]     # left\n    n4 = a[r0:r1, c0+1:c1+1]     # right\n    n5 = a[r0+1:r1+1, c0-1:c1-1]  # down-left\n    n6 = a[r0+1:r1+1, c0:c1]    # down\n    n7 = a[r0+1:r1+1, c0+1:c1+1]  # down-right\n    is_peak = (center > n0) & (center > n1) & (center > n2) & (center > n3) & (center > n4) & (center > n5) & (center > n6) & (center > n7)\n    frac = float(np.count_nonzero(is_peak)) / (float(center.size) + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Color channel imbalance: normalized std of channel means (0..1), 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    img = np.nan_to_num(arr.astype(float))\n    ch = img[..., :3]\n    means = ch.reshape(-1, 3).mean(axis=0)\n    max_mean = float(np.max(np.abs(means)) + eps)\n    imbalance = float(np.std(means) / max_mean)\n    return float(np.clip(imbalance, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Pearson correlation between radial distance from center and intensity (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy).ravel()\n    vals = a.ravel()\n    if vals.size < 2:\n        return 0.0\n    rv = r - r.mean()\n    vv = vals - vals.mean()\n    denom = (np.sqrt(np.sum(rv * rv) * np.sum(vv * vv)) + eps)\n    corr = float(np.sum(rv * vv) / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of edge orientation histogram (0..1 where 1 = maximal orientation disorder)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    ang = np.arctan2(gy, gx)  # [-pi, pi]\n    # map orientation to [0, pi) since opposite directions equivalent for orientation\n    ang = np.mod(ang, np.pi)\n    nbins = 36\n    flat = ang.ravel()\n    hist, _ = np.histogram(flat, bins=nbins, range=(0.0, np.pi))\n    total = float(hist.sum()) + eps\n    probs = hist.astype(float) / total\n    probs = probs[probs > 0]\n    ent = -float(np.sum(probs * np.log2(probs + eps)))\n    max_ent = float(np.log2(nbins))\n    score = ent / (max_ent + eps)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bounding-box compactness: (bbox area) / (region pixel count) for pixels > mean (>=1), 0 if none'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    thr = float(np.mean(a))\n    mask = a > thr\n    region_area = int(np.count_nonzero(mask))\n    if region_area == 0:\n        return 0.0\n    rows = np.where(mask.any(axis=1))[0]\n    cols = np.where(mask.any(axis=0))[0]\n    if rows.size == 0 or cols.size == 0:\n        return 0.0\n    h_bb = rows[-1] - rows[0] + 1\n    w_bb = cols[-1] - cols[0] + 1\n    bbox_area = float(h_bb * w_bb)\n    compactness = bbox_area / (float(region_area) + eps)\n    return float(np.clip(compactness, 1.0, 100.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram bimodality score (0..1); high when two distinct peaks separated across intensities'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    nbins = 64\n    amin, amax = float(flat.min()), float(flat.max())\n    if amax <= amin:\n        return 0.0\n    hist, edges = np.histogram(flat, bins=nbins, range=(amin, amax))\n    if hist.sum() == 0:\n        return 0.0\n    # find two highest peaks\n    idx = np.argsort(hist)\n    i1 = int(idx[-1])\n    i2 = int(idx[-2]) if hist.size >= 2 else int(idx[-1])\n    h1 = float(hist[i1])\n    h2 = float(hist[i2])\n    if h1 <= eps or h2 <= eps:\n        return 0.0\n    separation = abs(i1 - i2) / float(nbins - 1)\n    peak_balance = min(h1, h2) / (max(h1, h2) + eps)\n    score = separation * peak_balance\n    return float(np.clip(score, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score (1.0 = perfect symmetry, 0 = very asymmetric)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 1.0 if a.size == 0 else 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:] if mid > 0 else np.zeros_like(left)\n    right_flipped = np.fliplr(right)\n    # ensure same width\n    minw = min(left.shape[1], right_flipped.shape[1])\n    if minw == 0:\n        return 0.0\n    left_s = left[:, :minw]\n    right_s = right_flipped[:, :minw]\n    diff = np.mean(np.abs(left_s - right_s))\n    norm = np.mean(np.abs(a)) + eps\n    score = 1.0 - (diff / norm)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels significantly brighter than image mean (foreground presence)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    if h * w == 0:\n        return 0.0\n    m = float(arr.mean())\n    s = float(arr.std())\n    thr = m + 0.5 * s\n    mask = arr > thr\n    frac = float(np.count_nonzero(mask)) / float(h * w)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Entropy of edge orientation histogram (0..1), weighted by gradient magnitude'\n    import numpy as np\n    eps = 1e-12\n    bins = 8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    total_mag = mag.sum()\n    if total_mag < eps:\n        return 0.0\n    ang = np.arctan2(gy, gx)\n    # orientation modulo pi (ignore direction)\n    ang = np.mod(ang, np.pi)\n    hist, _ = np.histogram(ang, bins=bins, range=(0.0, np.pi), weights=mag)\n    p = hist / (hist.sum() + eps)\n    ppos = p[p > 0]\n    entropy = -float(np.sum(ppos * np.log(ppos)))\n    norm = float(np.log(bins) + eps)\n    return float(np.clip(entropy / norm, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance (0..1) between intensity-weighted centroid and image center'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    total = float(a.sum()) + eps\n    cx = float((xs * a).sum()) / total\n    cy = float((ys * a).sum()) / total\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = float(np.hypot(cx - center_x, cy - center_y))\n    maxdist = float(np.hypot(center_x, center_y)) + eps\n    return float(np.clip(dist / maxdist, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency (Laplacian) energy divided by total absolute energy (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # discrete Laplacian via 4-neighbor stencil\n    lap = 4.0 * a\n    lap -= np.roll(a, 1, axis=0)\n    lap -= np.roll(a, -1, axis=0)\n    lap -= np.roll(a, 1, axis=1)\n    lap -= np.roll(a, -1, axis=1)\n    high_e = float(np.sum(np.abs(lap)))\n    total_e = float(np.sum(np.abs(a))) + eps\n    ratio = high_e / total_e\n    return float(np.clip(ratio, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels significantly darker than image mean (shadow presence)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        arr = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(arr.astype(float))\n    h, w = arr.shape\n    if h * w == 0:\n        return 0.0\n    m = float(arr.mean())\n    s = float(arr.std())\n    thr = m - 0.5 * s\n    mask = arr < thr\n    frac = float(np.count_nonzero(mask)) / float(h * w)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio of bounding box of bright region (>= mean), mapped to 0..1'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean())\n    mask = a >= thr\n    if not mask.any():\n        return 0.0\n    ys, xs = np.where(mask)\n    ymin, ymax = ys.min(), ys.max()\n    xmin, xmax = xs.min(), xs.max()\n    bh = float(max(1, ymax - ymin + 1))\n    bw = float(max(1, xmax - xmin + 1))\n    ratio = bw / bh\n    # compress to (0..1) via ratio/(1+ratio)\n    val = ratio / (1.0 + ratio + eps)\n    return float(np.clip(val, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness for RGB images (0..1); returns 0 for grayscale'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim != 3 or img.shape[2] < 3:\n        return 0.0\n    arr = np.nan_to_num(img[..., :3].astype(float))\n    R = arr[..., 0]\n    G = arr[..., 1]\n    B = arr[..., 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(np.std(rg))\n    std_yb = float(np.std(yb))\n    mean_rg = float(np.mean(rg))\n    mean_yb = float(np.mean(yb))\n    metric = np.sqrt(std_rg ** 2 + std_yb ** 2) + 0.3 * np.sqrt(mean_rg ** 2 + mean_yb ** 2)\n    # compress to 0..1\n    val = float(metric / (1.0 + metric + eps))\n    return float(np.clip(val, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant global gradient orientation strength (0..1): anisotropy * alignment to horizontal'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    Sxx = float(np.sum(gx * gx))\n    Syy = float(np.sum(gy * gy))\n    Sxy = float(np.sum(gx * gy))\n    trace = Sxx + Syy\n    diff = np.sqrt(max((Sxx - Syy) ** 2 + 4.0 * Sxy ** 2, 0.0))\n    lam1 = 0.5 * (trace + diff)\n    lam2 = 0.5 * (trace - diff)\n    anis = (lam1 - lam2) / (lam1 + lam2 + eps)\n    angle = 0.5 * np.arctan2(2.0 * Sxy, (Sxx - Syy))\n    align = abs(np.cos(angle))\n    result = anis * align\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border uniformity (0..1): higher means border pixels are more uniform'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    borders = []\n    # top and bottom rows\n    borders.append(a[0, :])\n    if h > 1:\n        borders.append(a[-1, :])\n    # left and right columns (excluding corners already added)\n    if w > 1:\n        if h > 2:\n            borders.append(a[1:-1, 0].ravel())\n            borders.append(a[1:-1, -1].ravel())\n        else:\n            borders.append(a[:, 0].ravel())\n            borders.append(a[:, -1].ravel())\n    border_pixels = np.concatenate([b.ravel() for b in borders]) if borders else np.array([])\n    if border_pixels.size == 0:\n        return 0.0\n    mean_abs = float(np.mean(np.abs(border_pixels)))\n    std = float(np.std(border_pixels))\n    # uniformity measure: mean_abs / (mean_abs + std) -> close to 1 if std small relative to mean\n    uniformity = mean_abs / (mean_abs + std + eps)\n    return float(np.clip(uniformity, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Proportion of strong edges (edge density) using gradient magnitude'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    thr = float(np.mean(mag) + 0.5 * mag.std())\n    strong = (mag > thr)\n    result = float(np.count_nonzero(strong)) / float(a.size + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant gradient orientation coherence (0..1): proportion of gradient energy in main orientation bin'\n    import numpy as np\n    eps = 1e-12\n    bins = 12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy) + eps\n    ang = np.arctan2(gy, gx)  # -pi..pi\n    # map angles to 0..pi (orientation, not direction)\n    ang = np.abs(ang)\n    # histogram weighted by magnitude\n    hist, _ = np.histogram(ang.ravel(), bins=bins, range=(0.0, np.pi), weights=mag.ravel())\n    total = hist.sum() + eps\n    result = float(hist.max() / total)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian variance normalized by global std (focus/high-frequency measure)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    gy, gx = np.gradient(a)\n    gyy, gyx = np.gradient(gy)\n    gxy, gxx = np.gradient(gx)\n    lap = gxx + gyy\n    lap_var = float(np.var(lap))\n    gstd = float(a.std()) + eps\n    result = lap_var / (gstd + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect ratio of bounding box of nonzero pixels (height/width), returns 0 if no foreground'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2))\n    else:\n        a = np.nan_to_num(arr)\n    nz = np.count_nonzero(a)\n    if nz == 0:\n        return 0.0\n    rows = np.any(a != 0, axis=1)\n    cols = np.any(a != 0, axis=0)\n    r_idx = np.where(rows)[0]\n    c_idx = np.where(cols)[0]\n    if r_idx.size == 0 or c_idx.size == 0:\n        return 0.0\n    height = float(r_idx[-1] - r_idx[0] + 1)\n    width = float(c_idx[-1] - c_idx[0] + 1)\n    if width == 0.0:\n        return 0.0\n    result = height / width\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Nonzero compactness: fraction of bbox area occupied by nonzero pixels (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2))\n    else:\n        a = np.nan_to_num(arr)\n    nz_count = int(np.count_nonzero(a))\n    if nz_count == 0:\n        return 0.0\n    rows = np.any(a != 0, axis=1)\n    cols = np.any(a != 0, axis=0)\n    r_idx = np.where(rows)[0]\n    c_idx = np.where(cols)[0]\n    if r_idx.size == 0 or c_idx.size == 0:\n        return 0.0\n    bbox_area = float((r_idx[-1] - r_idx[0] + 1) * (c_idx[-1] - c_idx[0] + 1)) + eps\n    result = float(nz_count) / bbox_area\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean pairwise distance among top bright pixels normalized by half-diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    k = min(50, flat.size)\n    # pick top-k indices\n    idx = np.argpartition(-flat, k-1)[:k]\n    coords = np.vstack(np.unravel_index(idx, (h, w))).T.astype(float)\n    if coords.shape[0] < 2:\n        return 0.0\n    # pairwise distances\n    dif = coords[:, None, :] - coords[None, :, :]\n    d = np.hypot(dif[..., 0], dif[..., 1])\n    # take upper triangle without diagonal\n    i_upper = np.triu_indices(d.shape[0], k=1)\n    dvals = d[i_upper]\n    mean_d = float(dvals.mean()) if dvals.size else 0.0\n    half_diag = (np.hypot(h, w) / 2.0) + eps\n    result = mean_d / half_diag\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry score (0..1): 1 = perfectly symmetric left-right'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    if w % 2 == 0:\n        right = a[:, mid:]\n    else:\n        right = a[:, mid+1:]\n    # flip right horizontally\n    right_flipped = np.fliplr(right)\n    # align sizes\n    min_w = min(left.shape[1], right_flipped.shape[1])\n    if min_w == 0:\n        return 0.0\n    left = left[:, :min_w]\n    right_flipped = right_flipped[:, :min_w]\n    diff = np.abs(left - right_flipped)\n    denom = np.mean(np.abs(a)) + eps\n    score = 1.0 - (diff.mean() / denom)\n    return float(np.clip(score, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local patch std (texture) normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # choose patch size relative to image\n    ps = max(1, min(h, w) // 8)\n    stds = []\n    for i in range(0, h, ps):\n        for j in range(0, w, ps):\n            patch = a[i:i+ps, j:j+ps]\n            if patch.size:\n                stds.append(patch.std())\n    if len(stds) == 0:\n        return 0.0\n    mean_patch_std = float(np.mean(stds))\n    global_std = float(a.std()) + eps\n    result = mean_patch_std / global_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average peak sharpness: mean (peak - neighbor mean) normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    k = min(20, flat.size)\n    top_idx = np.argpartition(-flat, k-1)[:k]\n    ys, xs = np.unravel_index(top_idx, (h, w))\n    diffs = []\n    for y, x in zip(ys, xs):\n        y0 = max(0, y-1); y1 = min(h, y+2)\n        x0 = max(0, x-1); x1 = min(w, x+2)\n        neigh = a[y0:y1, x0:x1].astype(float)\n        if neigh.size <= 1:\n            continue\n        neigh_mean = (neigh.sum() - a[y, x]) / (neigh.size - 1)\n        diffs.append(max(0.0, float(a[y, x]) - neigh_mean))\n    if len(diffs) == 0:\n        return 0.0\n    global_std = float(a.std()) + eps\n    result = float(np.mean(diffs)) / global_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels within half a std of the mean (concentration around mean)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    mean = float(a.mean())\n    std = float(a.std()) + eps\n    thr = 0.5 * std\n    mask = np.abs(a - mean) <= thr\n    result = float(np.count_nonzero(mask)) / float(a.size + eps)\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity contrast: (max-min) divided by mean intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    mn = float(a.min())\n    mx = float(a.max())\n    mean = float(np.mean(np.abs(a))) + eps\n    result = (mx - mn) / mean\n    return float(np.clip(result, 0.0, 100.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Shannon entropy of intensity distribution (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(img.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    # use fixed 256 bins for robustness\n    try:\n        hist, _ = np.histogram(a, bins=256, density=False)\n    except Exception:\n        return 0.0\n    p = hist.astype(float)\n    p_sum = p.sum()\n    if p_sum <= 0:\n        return 0.0\n    p = p / (p_sum + eps)\n    ent = -float(np.sum(np.where(p > 0, p * np.log2(p), 0.0)))\n    max_ent = float(np.log2(len(p)))\n    result = ent / (max_ent + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Magnitude-weighted gradient orientation coherence (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(image.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(image.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy) + eps\n    ang = np.arctan2(gy, gx)\n    vec = np.exp(1j * ang) * mag\n    s = vec.sum()\n    total = mag.sum() + eps\n    coherence = np.abs(s) / total\n    return float(np.clip(coherence, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted centroid offset from image center normalized by diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys = np.arange(h)[:, None].astype(float)\n    xs = np.arange(w)[None, :].astype(float)\n    weights = np.maximum(arr, 0.0)\n    total = weights.sum()\n    if total <= eps:\n        # fallback to unweighted centroid (uniform)\n        cy = (h - 1) / 2.0\n        cx = (w - 1) / 2.0\n    else:\n        cy = float((weights * ys).sum() / (total + eps))\n        cx = float((weights * xs).sum() / (total + eps))\n    cy_center = (h - 1) / 2.0\n    cx_center = (w - 1) / 2.0\n    dist = np.hypot(cx - cx_center, cy - cy_center)\n    diag_half = 0.5 * np.hypot(h, w) + eps\n    result = dist / diag_half\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Ratio of edge density in center region to edge density in border (>=0)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    thr = float(np.median(mag))\n    edges = mag > thr\n    # center region\n    r0, r1 = h // 4, 3 * (h // 4)\n    c0, c1 = w // 4, 3 * (w // 4)\n    # ensure at least 1 pixel\n    r1 = max(r1, r0 + 1)\n    c1 = max(c1, c0 + 1)\n    center_mask = np.zeros_like(edges, dtype=bool)\n    center_mask[r0:r1, c0:c1] = True\n    border_mask = ~center_mask\n    center_edges = float(np.count_nonzero(edges & center_mask))\n    border_edges = float(np.count_nonzero(edges & border_mask))\n    center_area = float(center_mask.sum()) + eps\n    border_area = float(border_mask.sum()) + eps\n    center_density = center_edges / center_area\n    border_density = border_edges / border_area\n    if border_density < eps:\n        result = center_density\n    else:\n        result = center_density / (border_density + eps)\n    return float(np.clip(result, 0.0, 100.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local contrast: average absolute difference from 3x3 local mean normalized by mean intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # sum over 3x3 neighborhood using rolls (wrap-around, efficient)\n    s = np.zeros_like(a, dtype=float)\n    shifts = [(0,0),(1,0),(-1,0),(0,1),(0,-1),(1,1),(1,-1),(-1,1),(-1,-1)]\n    for dy, dx in shifts:\n        s += np.roll(np.roll(a, dy, axis=0), dx, axis=1)\n    local_mean = s / 9.0\n    local_contrast = np.mean(np.abs(a - local_mean))\n    denom = float(np.mean(np.abs(a))) + eps\n    result = local_contrast / denom\n    return float(np.clip(result, 0.0, 100.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of gradient energy in the dominant edge orientation bin (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    total = mag.sum()\n    if total <= eps:\n        return 0.0\n    ang = np.arctan2(gy, gx)\n    # orientation modulo pi (unsigned)\n    ang = np.mod(ang, np.pi)\n    bins = 12\n    hist, _ = np.histogram(ang, bins=bins, range=(0.0, np.pi), weights=mag)\n    max_bin = float(hist.max())\n    result = max_bin / (total + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute left-right mirror difference normalized by mean absolute intensity'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = img[:, :mid]\n    right = img[:, w-mid:]\n    right_flipped = np.fliplr(right)\n    # make shapes equal by cropping to min width\n    min_w = min(left.shape[1], right_flipped.shape[1])\n    left = left[:, :min_w]\n    right_flipped = right_flipped[:, :min_w]\n    if left.size == 0:\n        return 0.0\n    diff = np.abs(left - right_flipped)\n    mean_diff = float(diff.mean())\n    denom = float(np.mean(np.abs(img))) + eps\n    result = mean_diff / denom\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gini coefficient of intensity magnitudes (0..1, higher => more sparse/concentrated)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    vals = np.abs(vals)\n    if vals.size == 0:\n        return 0.0\n    vals = vals + 0.0  # ensure float\n    total = vals.sum()\n    if total <= eps:\n        return 0.0\n    # sort ascending\n    sorted_vals = np.sort(vals)\n    n = sorted_vals.size\n    idx = np.arange(1, n + 1, dtype=float)\n    gini = (2.0 * (idx * sorted_vals).sum()) / ((n * total) + eps) - (n + 1.0) / n\n    result = float(np.clip(gini, 0.0, 1.0))\n    return result\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio: energy in high-pass (3x3) divided by total energy (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # local 3x3 mean via rolling\n    s = np.zeros_like(a, dtype=float)\n    shifts = [(0,0),(1,0),(-1,0),(0,1),(0,-1),(1,1),(1,-1),(-1,1),(-1,-1)]\n    for dy, dx in shifts:\n        s += np.roll(np.roll(a, dy, axis=0), dx, axis=1)\n    low = s / 9.0\n    high = a - low\n    energy_high = float((high ** 2).sum())\n    energy_total = float((a ** 2).sum()) + eps\n    result = energy_high / energy_total\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of strong edges: proportion of pixels with gradient magnitude above local threshold'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + 0.5 * mag.std())\n    strong = (mag > thr)\n    # avoid counting border artifacts specially; just normalize by area\n    result = float(np.count_nonzero(strong)) / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Shannon entropy of intensity histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 32\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    hist, _ = np.histogram(a, bins=bins, range=(a.min(), a.max() if a.max() > a.min() else a.min()+1.0))\n    total = float(hist.sum()) + eps\n    p = hist.astype(float) / total\n    p_nonzero = p[p > 0.0]\n    ent = -float((p_nonzero * np.log(p_nonzero)).sum())\n    # normalize by max possible entropy = log(bins)\n    max_ent = float(np.log(bins) + eps)\n    result = ent / max_ent\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fill fraction of mean-thresholded foreground inside its bounding box (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        gray = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        gray = np.nan_to_num(arr.astype(float))\n    h, w = gray.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(gray.mean())\n    mask = (gray > thr)\n    if not mask.any():\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    y0, y1 = ys.min(), ys.max()\n    x0, x1 = xs.min(), xs.max()\n    bbox_area = float((y1 - y0 + 1) * (x1 - x0 + 1))\n    if bbox_area <= 0.0:\n        return 0.0\n    fill = float(mask.sum()) / bbox_area\n    return float(np.clip(fill, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Global gradient orientation coherence: normalized magnitude of vector sum of gradients (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy) + eps\n    sum_gx = float(gx.sum())\n    sum_gy = float(gy.sum())\n    denom = float(mag.sum()) + eps\n    coherence = np.hypot(sum_gx, sum_gy) / denom\n    return float(np.clip(coherence, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of low-variance patches in a 4x4 grid (indicates smoothness / texture sparsity)'\n    import numpy as np\n    eps = 1e-12\n    GRID = 4\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    global_var = float(a.var()) + eps\n    ph = max(1, h // GRID)\n    pw = max(1, w // GRID)\n    low_count = 0\n    total = 0\n    for y in range(0, h, ph):\n        for x in range(0, w, pw):\n            patch = a[y:min(h, y+ph), x:min(w, x+pw)]\n            if patch.size == 0:\n                continue\n            total += 1\n            if float(patch.var()) < 0.5 * global_var:\n                low_count += 1\n    if total == 0:\n        return 0.0\n    result = float(low_count) / float(total)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average fraction of nearly-uniform rows and columns (rows/cols with very small std relative to image)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    img_std = float(a.std()) + eps\n    row_stds = a.std(axis=1)\n    col_stds = a.std(axis=0)\n    thr = max(1e-6, 0.08 * img_std)\n    row_frac = float((row_stds < thr).sum()) / float(max(1, h))\n    col_frac = float((col_stds < thr).sum()) / float(max(1, w))\n    result = 0.5 * (row_frac + col_frac)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant histogram bin ratio: fraction of pixels in the largest intensity bin (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    bins = 16\n    hist, _ = np.histogram(vals, bins=bins, range=(vals.min(), vals.max() if vals.max() > vals.min() else vals.min()+1.0))\n    top = float(hist.max())\n    total = float(hist.sum()) + 1e-12\n    result = top / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian zero-crossing rate: normalized count of sign changes in Laplacian (edge complexity)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    gyy = np.gradient(gy, axis=0)\n    gxx = np.gradient(gx, axis=1)\n    lap = gxx + gyy\n    s = np.sign(lap)\n    # treat zeros as no sign to avoid false crossings\n    s[s == 0] = 0\n    hz = (s[:, :-1] * s[:, 1:]) < 0\n    vt = (s[:-1, :] * s[1:, :]) < 0\n    count = int(np.count_nonzero(hz)) + int(np.count_nonzero(vt))\n    result = float(count) / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized distance of foreground centroid (mean-thresholded) from image center (0=center..1=edge)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        gray = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        gray = np.nan_to_num(arr.astype(float))\n    h, w = gray.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(gray.mean())\n    mask = (gray > thr)\n    if not mask.any():\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    cy = float(ys.mean())\n    cx = float(xs.mean())\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    max_dist = np.hypot(center_x, center_y) + eps\n    result = dist / max_dist\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border vs interior gradient contrast normalized by mean gradient (positive => borders stronger)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    bw = max(1, int(min(h, w) * 0.1))\n    mask_border = np.zeros_like(a, dtype=bool)\n    mask_border[:bw, :] = True\n    mask_border[-bw:, :] = True\n    mask_border[:, :bw] = True\n    mask_border[:, -bw:] = True\n    mask_interior = ~mask_border\n    border_mean = float(mag[mask_border].mean()) if mask_border.any() else 0.0\n    interior_mean = float(mag[mask_interior].mean()) if mask_interior.any() else 0.0\n    denom = float(mag.mean()) + eps\n    result = (border_mean - interior_mean) / denom\n    # clip to a reasonable symmetric range\n    return float(np.clip(result, -5.0, 5.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Centroid offset magnitude: distance between intensity centroid and image center normalized by diagonal (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    total = float(a.sum())\n    if total == 0.0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (xs * a).sum() / total\n    cy = (ys * a).sum() / total\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    diag = np.hypot(w, h) + 1e-12\n    result = float(dist / diag)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy ratio from 2D FFT (fraction of power outside low-frequency disk)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    # FFT power\n    F = np.fft.fft2(a)\n    P = np.abs(np.fft.fftshift(F)) ** 2\n    ys, xs = np.indices((h, w))\n    cy = (h - 1) / 2.0\n    cx = (w - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    # define low-frequency radius as 1/8 of smaller dimension\n    r0 = max(1.0, min(h, w) / 8.0)\n    total = float(P.sum()) + 1e-12\n    high = float(P[r > r0].sum())\n    result = high / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with strong gradient magnitude relative to image stats'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.size == 0:\n        return 0.0\n    m = float(mag.mean())\n    s = float(mag.std())\n    thr = m + s  # strong edges\n    strong = (mag > thr).sum()\n    result = float(strong) / float(mag.size)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Local peak count: normalized count of strict 3x3 local maxima (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    center = a\n    # compare to 8 neighbors using rolls\n    neighs = []\n    for dy in (-1, 0, 1):\n        for dx in (-1, 0, 1):\n            if dy == 0 and dx == 0:\n                continue\n            neighs.append(np.roll(np.roll(a, dy, axis=0), dx, axis=1))\n    is_max = np.ones_like(a, dtype=bool)\n    for n in neighs:\n        is_max &= (center > n)\n    # avoid counting border artifacts caused by roll (treat rolled comparisons as valid but normalize)\n    count = int(is_max.sum())\n    area = float(h * w)\n    result = float(count) / (area + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation concentration: fraction of gradient energy in the dominant orientation bin (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    if mag.sum() == 0:\n        return 0.0\n    ang = np.arctan2(gy, gx)  # -pi..pi\n    nbins = 12\n    # map angles to [0, nbins)\n    bins = ((ang + np.pi) / (2 * np.pi) * nbins).astype(int) % nbins\n    energies = np.zeros(nbins, dtype=float)\n    for b in range(nbins):\n        energies[b] = mag[bins == b].sum()\n    max_frac = float(energies.max()) / (energies.sum() + 1e-12)\n    return float(np.clip(max_frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity correlation: absolute Pearson correlation between radius and mean ring intensity (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    r = r.ravel()\n    vals = a.ravel()\n    # bin radii into integer bins\n    rbins = np.unique(np.floor(r).astype(int))\n    if rbins.size < 2:\n        return 0.0\n    ring_means = []\n    ring_centers = []\n    for b in rbins:\n        mask = (np.floor(r) == b)\n        if mask.sum() == 0:\n            continue\n        ring_means.append(vals[mask].mean())\n        ring_centers.append(float(b))\n    ring_means = np.array(ring_means, dtype=float)\n    ring_centers = np.array(ring_centers, dtype=float)\n    if ring_means.size < 2:\n        return 0.0\n    # Pearson correlation\n    rm = ring_means.mean()\n    rc = ring_centers.mean()\n    num = ((ring_means - rm) * (ring_centers - rc)).sum()\n    den = np.sqrt(((ring_means - rm) ** 2).sum() * ((ring_centers - rc) ** 2).sum()) + 1e-12\n    corr = num / den\n    return float(np.clip(abs(corr), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Border vs inner variability: ratio of border standard deviation to inner standard deviation'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    t = max(1, min(h, w) // 8)\n    mask = np.ones_like(a, dtype=bool)\n    mask[t:-t, t:-t] = False\n    border = a[mask]\n    inner = a[~mask]\n    if border.size == 0 or inner.size == 0:\n        return 0.0\n    bstd = float(border.std())\n    istd = float(inner.std()) + 1e-12\n    result = bstd / istd\n    # reasonable clipping\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-region compactness: normalized compactness = (perimeter^2) / (4*pi*area) (1 => compact like circle)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(a.mean() + 0.5 * a.std())\n    mask = (a > thr)\n    area = float(mask.sum())\n    if area < 1.0:\n        return 0.0\n    # approximate perimeter: mask pixels that have at least one 4-neighbor False\n    up = np.roll(mask, 1, axis=0)\n    down = np.roll(mask, -1, axis=0)\n    left = np.roll(mask, 1, axis=1)\n    right = np.roll(mask, -1, axis=1)\n    neighbor_all = up & down & left & right\n    # perimeter pixels are mask True with any neighbor False\n    perimeter = float(((mask) & (~neighbor_all)).sum())\n    compactness = (perimeter ** 2) / (4.0 * np.pi * area + 1e-12)\n    return float(np.clip(compactness, 0.0, 100.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Multi-scale contrast: mean absolute difference between image and coarse block-averaged version normalized by global std'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h == 0 or w == 0:\n        return 0.0\n    k = max(1, min(h, w) // 4)\n    bh = h // k\n    bw = w // k\n    if bh < 1 or bw < 1:\n        # fallback to simple local mean subtraction\n        coarse = np.full_like(img, img.mean())\n    else:\n        # crop to multiple of k\n        ch = bh * k\n        cw = bw * k\n        cropped = img[:ch, :cw]\n        # reshape to blocks and compute mean per block\n        blocks = cropped.reshape(bh, k, bw, k)\n        block_means = blocks.mean(axis=(1, 3))\n        # expand back\n        coarse = np.repeat(np.repeat(block_means, k, axis=0), k, axis=1)\n        # pad if needed to original size\n        if coarse.shape != img[:coarse.shape[0], :coarse.shape[1]].shape:\n            coarse = coarse[:img.shape[0], :img.shape[1]]\n        if coarse.shape != img.shape:\n            # pad with global mean\n            pad = np.full(img.shape, img.mean())\n            pad[:coarse.shape[0], :coarse.shape[1]] = coarse\n            coarse = pad\n    diff = np.abs(img - coarse)\n    gstd = float(img.std()) + 1e-12\n    result = float(diff.mean()) / gstd\n    # clip to reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical linearity score: how consistently bright centroids align vertically across columns (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    col_means = a.mean(axis=0)\n    thr = float(col_means.mean() + 0.25 * col_means.std())\n    ys = np.arange(h).reshape(h, 1)\n    centroids = []\n    for j in range(w):\n        col = a[:, j]\n        mask = col > thr\n        if mask.sum() < max(1, h // 20):\n            continue\n        weights = col[mask].astype(float)\n        ys_sel = ys[mask, 0].astype(float)\n        c = (weights * ys_sel).sum() / (weights.sum() + 1e-12)\n        centroids.append(c)\n    if len(centroids) < 2:\n        return 0.0\n    centroids = np.array(centroids, dtype=float)\n    stdc = centroids.std()\n    # normalize: if stdc small relative to height, score high\n    score = 1.0 - (stdc / (h / 2.0 + 1e-12))\n    return float(np.clip(score, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right brightness bias normalized by image std (positive => left brighter)'\n    import numpy as np\n    eps = 1e-8\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return float(0.0)\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return float(0.0)\n    mid = w // 2\n    left_mean = float(np.mean(a[:, :mid])) if a[:, :mid].size else 0.0\n    right_mean = float(np.mean(a[:, -mid:])) if a[:, -mid:].size else 0.0\n    overall_std = float(np.std(a)) + eps\n    result = (left_mean - right_mean) / overall_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted centroid distance from geometric center normalized by diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return float(0.0)\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return float(0.0)\n    weights = a - float(a.min())\n    S = float(weights.sum())\n    if S <= eps:\n        return float(0.0)\n    ys = np.arange(h, dtype=float)[:, None]\n    xs = np.arange(w, dtype=float)[None, :]\n    cy = float((weights * ys).sum()) / S\n    cx = float((weights * xs).sum()) / S\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = float(np.hypot(cx - center_x, cy - center_y))\n    norm = float(np.hypot(center_x, center_y)) + eps\n    result = dist / norm\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge pixel density: fraction of pixels with gradient magnitude above local threshold'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return float(0.0)\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return float(0.0)\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    mean_mag = float(np.mean(mag))\n    std_mag = float(np.std(mag))\n    threshold = mean_mag + std_mag\n    count = float((mag > threshold).sum())\n    total = float(mag.size) + eps\n    result = count / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) using 32 bins (0 = pure, 1 = max entropy)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return float(0.0)\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return float(0.0)\n    bins = 32\n    counts, _ = np.histogram(vals, bins=bins)\n    total = counts.sum()\n    if total == 0:\n        return float(0.0)\n    p = counts.astype(float) / float(total)\n    p_nonzero = p[p > 0]\n    entropy = -float((p_nonzero * np.log2(p_nonzero)).sum())\n    max_ent = np.log2(bins)\n    result = entropy / (max_ent + 1e-12)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-region spread: fraction of rows and columns touched by pixels > mean (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return float(0.0)\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return float(0.0)\n    med = float(np.mean(a))\n    mask = a > med\n    if not np.any(mask):\n        return float(0.0)\n    rows_with = float(np.any(mask, axis=1).sum())\n    cols_with = float(np.any(mask, axis=0).sum())\n    result = (rows_with / float(h)) * (cols_with / float(w))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical mirror symmetry correlation between left and right halves (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return float(1.0)\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return float(1.0)\n    mid = w // 2\n    if w % 2 == 0:\n        left = a[:, :mid]\n        right = a[:, mid:]\n    else:\n        left = a[:, :mid]\n        right = a[:, mid+1:]\n    # mirror right horizontally\n    right_mirror = np.fliplr(right)\n    # resize if shapes differ\n    if left.shape != right_mirror.shape:\n        try:\n            right_mirror = np.resize(right_mirror, left.shape)\n        except Exception:\n            min_h = min(left.shape[0], right_mirror.shape[0])\n            min_w = min(left.shape[1], right_mirror.shape[1])\n            left = left[:min_h, :min_w]\n            right_mirror = right_mirror[:min_h, :min_w]\n    L = left.ravel()\n    R = right_mirror.ravel()\n    Lz = L - L.mean()\n    Rz = R - R.mean()\n    num = float((Lz * Rz).sum())\n    den = float(np.sqrt((Lz ** 2).sum() * (Rz ** 2).sum()) + eps)\n    corr = num / den\n    return float(np.clip(abs(corr), 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient orientation coherence (0..1): how aligned gradient directions are'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return float(0.0)\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return float(0.0)\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy) + eps\n    sum_gx = float(gx.sum())\n    sum_gy = float(gy.sum())\n    vec_mag = float(np.hypot(sum_gx, sum_gy))\n    total_mag = float(mag.sum()) + eps\n    result = vec_mag / total_mag\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Corner variance ratio: mean corner variance divided by global variance (>=0)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return float(0.0)\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return float(0.0)\n    cs = max(1, min(h, w) // 8)\n    corners = []\n    corners.append(a[:cs, :cs])\n    corners.append(a[:cs, -cs:])\n    corners.append(a[-cs:, :cs])\n    corners.append(a[-cs:, -cs:])\n    corner_vars = []\n    for c in corners:\n        if c.size:\n            corner_vars.append(float(c.var()))\n        else:\n            corner_vars.append(0.0)\n    mean_corner_var = float(np.mean(corner_vars))\n    global_var = float(a.var()) + eps\n    result = mean_corner_var / global_var\n    return float(max(result, 0.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram peakiness: proportion of pixels in the largest intensity bin (0..1)'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return float(0.0)\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return float(0.0)\n    bins = 64\n    counts, _ = np.histogram(vals, bins=bins)\n    total = float(counts.sum())\n    if total <= 0.0:\n        return float(0.0)\n    peak = float(counts.max())\n    result = peak / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-frequency dominance via 2x2 block smoothing: fraction of energy in low frequencies (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return float(0.0)\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h0, w0 = a.shape\n    if h0 == 0 or w0 == 0:\n        return float(0.0)\n    # pad to even dimensions by reflecting last row/col if needed\n    pad_h = 0 if (h0 % 2 == 0) else 1\n    pad_w = 0 if (w0 % 2 == 0) else 1\n    if pad_h or pad_w:\n        a_pad = np.pad(a, ((0, pad_h), (0, pad_w)), mode='reflect')\n    else:\n        a_pad = a\n    h, w = a_pad.shape\n    # compute 2x2 block means\n    a_blocks = a_pad.reshape((h//2, 2, w//2, 2)).mean(axis=(1,3))\n    # upsample back to padded size\n    low = np.repeat(np.repeat(a_blocks, 2, axis=0), 2, axis=1)\n    low = low[:h0, :w0]\n    residual = a - low\n    low_energy = float(np.sum(np.abs(low)))\n    high_energy = float(np.sum(np.abs(residual)))\n    denom = low_energy + high_energy + eps\n    result = low_energy / denom\n    return float(np.clip(result, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge density: fraction of pixels with gradient magnitude above mean+std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    if img.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(img)\n        mag = np.hypot(gx, gy)\n    except Exception:\n        return 0.0\n    thr = float(mag.mean() + mag.std())\n    count = float(np.count_nonzero(mag > thr))\n    result = count / float(img.size + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Horizontal symmetry: normalized correlation between left and right halves (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:] if mid > 0 else np.empty((h, 0))\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    # flip right to compare mirror\n    right_flipped = right[:, ::-1]\n    # crop to identical shape if needed\n    mh = min(left.shape[0], right_flipped.shape[0])\n    mw = min(left.shape[1], right_flipped.shape[1])\n    L = left[:mh, :mw].ravel()\n    R = right_flipped[:mh, :mw].ravel()\n    if L.size == 0:\n        return 0.0\n    Lc = L - L.mean()\n    Rc = R - R.mean()\n    denom = (np.sqrt((Lc ** 2).sum() * (Rc ** 2).sum()) + eps)\n    corr = float((Lc * Rc).sum() / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity entropy normalized to [0,1] (Shannon entropy of intensity histogram)'\n    import numpy as np\n    eps = 1e-12\n    bins = 256\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    vmin = float(vals.min())\n    vmax = float(vals.max())\n    if vmax <= vmin:\n        return 0.0\n    hist, _ = np.histogram(vals, bins=bins, range=(vmin, vmax))\n    p = hist.astype(float) / (hist.sum() + eps)\n    ppos = p[p > 0]\n    ent = -float((ppos * np.log2(ppos)).sum())\n    norm = np.log2(bins)\n    result = ent / (norm + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Local texture coarseness: mean 5x5 patch std normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h * w == 0:\n        return 0.0\n    # compute local mean and mean of squares over 5x5 using rolled sums\n    offsets = range(-2, 3)\n    sum1 = np.zeros_like(img, dtype=float)\n    sum2 = np.zeros_like(img, dtype=float)\n    for dy in offsets:\n        for dx in offsets:\n            shifted = np.roll(np.roll(img, dy, axis=0), dx, axis=1)\n            sum1 += shifted\n            sum2 += shifted * shifted\n    kernel_area = 25.0\n    local_mean = sum1 / kernel_area\n    local_mean_sq = sum2 / kernel_area\n    local_var = local_mean_sq - (local_mean ** 2)\n    local_var = np.where(local_var > 0, local_var, 0.0)\n    local_std = np.sqrt(local_var)\n    avg_local_std = float(local_std.mean())\n    global_std = float(img.std()) + eps\n    result = avg_local_std / global_std\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright-spot compactness: 1 - (mean distance of top 2% brightest pixels to center normalized by diagonal)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h * w == 0:\n        return 0.0\n    thr = float(np.percentile(img.ravel(), 98))\n    mask = (img >= thr)\n    if not mask.any():\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    d = np.hypot(xs - cx, ys - cy)\n    mean_d = float(d.mean())\n    diag = np.hypot(w, h) + eps\n    norm = mean_d / diag\n    result = 1.0 - float(np.clip(norm, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy fraction from 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image, dtype=float)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2))\n    else:\n        a = np.nan_to_num(arr)\n    h, w = a.shape\n    if h * w == 0:\n        return 0.0\n    try:\n        F = np.fft.fftshift(np.fft.fft2(a))\n    except Exception:\n        return 0.0\n    mag2 = (np.abs(F) ** 2)\n    total = float(mag2.sum()) + eps\n    k = max(1, min(h, w) // 8)\n    cy = h // 2\n    cx = w // 2\n    low = mag2[cy - k:cy + k + 1, cx - k:cx + k + 1]\n    low_energy = float(low.sum())\n    hf_frac = 1.0 - (low_energy / total)\n    return float(np.clip(hf_frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram peak contrast: prominence of top peak vs second peak weighted by their separation (0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 64\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    vmin = float(vals.min())\n    vmax = float(vals.max())\n    if vmax <= vmin:\n        return 0.0\n    hist, edges = np.histogram(vals, bins=bins, range=(vmin, vmax))\n    if hist.sum() == 0:\n        return 0.0\n    idx = np.argsort(hist)[::-1]\n    p1_idx = int(idx[0])\n    p1 = float(hist[p1_idx])\n    p2 = float(hist[idx[1]]) if idx.size > 1 else 0.0\n    sep = abs(p1_idx - (idx[1] if idx.size > 1 else p1_idx))\n    sep_norm = float(sep) / float(bins)\n    score = (p1 - p2) / (float(hist.sum()) + eps)\n    result = float(np.clip(score * sep_norm, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean absolute Laplacian normalized by mean absolute intensity'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image, dtype=float)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2))\n    else:\n        a = np.nan_to_num(arr)\n    h, w = a.shape\n    if h * w == 0:\n        return 0.0\n    up = np.roll(a, -1, axis=0)\n    down = np.roll(a, 1, axis=0)\n    left = np.roll(a, -1, axis=1)\n    right = np.roll(a, 1, axis=1)\n    lap = (4.0 * a) - up - down - left - right\n    mean_abs_lap = float(np.abs(lap).mean())\n    mean_abs = float(np.abs(a).mean()) + eps\n    result = mean_abs_lap / mean_abs\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bright local maxima fraction: fraction of pixels that are local 3x3 maxima above mean+std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        img = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        img = np.nan_to_num(arr.astype(float))\n    h, w = img.shape\n    if h * w == 0:\n        return 0.0\n    # smooth with 3x3 average\n    offsets = (-1, 0, 1)\n    sm = np.zeros_like(img, dtype=float)\n    for dy in offsets:\n        for dx in offsets:\n            sm += np.roll(np.roll(img, dy, axis=0), dx, axis=1)\n    sm /= 9.0\n    # compare to 8 neighbors\n    is_max = np.ones_like(sm, dtype=bool)\n    for dy in offsets:\n        for dx in offsets:\n            if dy == 0 and dx == 0:\n                continue\n            neigh = np.roll(np.roll(sm, dy, axis=0), dx, axis=1)\n            is_max &= (sm > neigh)\n    thr = float(sm.mean() + sm.std())\n    peaks = is_max & (sm > thr)\n    count = float(np.count_nonzero(peaks))\n    result = count / float(img.size + eps)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical stroke ratio: fraction of columns with column-std above mean+0.5*std of column stds'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w == 0 or h == 0:\n        return 0.0\n    col_std = a.std(axis=0)\n    mean_cs = float(col_std.mean())\n    std_cs = float(col_std.std())\n    thr = mean_cs + 0.5 * std_cs\n    count = float(np.count_nonzero(col_std > thr))\n    result = count / float(w + eps)\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized image entropy (0..1) computed from a 32-bin intensity histogram'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).ravel().astype(float))\n    else:\n        vals = np.nan_to_num(arr.ravel().astype(float))\n    if vals.size == 0:\n        return 0.0\n    nbins = 32\n    vmin, vmax = float(vals.min()), float(vals.max())\n    if vmax <= vmin + eps:\n        return 0.0\n    hist, _ = np.histogram(vals, bins=nbins, range=(vmin, vmax))\n    p = hist.astype(float) / (hist.sum() + eps)\n    p = p[p > 0.0]\n    if p.size == 0:\n        return 0.0\n    entropy = -float((p * np.log(p + eps)).sum())\n    norm = float(np.log(nbins) + eps)\n    result = float(np.clip(entropy / norm, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Vertical symmetry score (0..1) comparing left and mirrored right halves'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, w - mid:]\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    right_flipped = np.fliplr(right)\n    # align shapes (if odd width)\n    minw = min(left.shape[1], right_flipped.shape[1])\n    left = left[:, :minw]\n    right_flipped = right_flipped[:, :minw]\n    diff = np.abs(left - right_flipped)\n    denom = np.mean(np.abs(a)) + eps\n    score = 1.0 - float(diff.mean()) / float(denom)\n    result = float(np.clip(score, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Principal orientation of intensity (0..1) as normalized angle of major axis (0..180 deg)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    total = float(a.sum())\n    if total == 0.0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (a * xs).sum() / total\n    cy = (a * ys).sum() / total\n    xcen = xs.ravel().astype(float) - cx\n    ycen = ys.ravel().astype(float) - cy\n    wts = a.ravel().astype(float)\n    if wts.sum() == 0.0:\n        return 0.0\n    xx = (wts * (xcen * xcen)).sum() / (wts.sum() + eps)\n    yy = (wts * (ycen * ycen)).sum() / (wts.sum() + eps)\n    xy = (wts * (xcen * ycen)).sum() / (wts.sum() + eps)\n    cov = np.array([[xx, xy], [xy, yy]])\n    try:\n        vals, vecs = np.linalg.eigh(cov)\n    except Exception:\n        return 0.0\n    # principal eigenvector corresponds to largest eigenvalue\n    idx = int(np.argmax(vals))\n    vx, vy = vecs[0, idx], vecs[1, idx]\n    angle = np.arctan2(vy, vx)  # radians between -pi..pi\n    angle_deg = np.degrees(angle)\n    # normalize to 0..180\n    angle_deg = abs((angle_deg + 180.0) % 180.0)\n    result = float(np.clip(angle_deg / 180.0, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'High-frequency energy fraction (0..1) using discrete Laplacian squared energy'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # discrete 4-neighbor Laplacian via rolls (no boundary loss)\n    up = np.roll(a, -1, axis=0)\n    down = np.roll(a, 1, axis=0)\n    left = np.roll(a, -1, axis=1)\n    right = np.roll(a, 1, axis=1)\n    lap = (4.0 * a) - (up + down + left + right)\n    lap_energy = float((lap ** 2).sum())\n    total_var = float(((a - a.mean()) ** 2).sum()) + eps\n    result = lap_energy / (total_var + eps)\n    # normalize to reasonable 0..1\n    result = float(np.clip(result / (1.0 + result), 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels near intensity extremes (within 5%% of min or max) (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    vmin = float(a.min())\n    vmax = float(a.max())\n    if vmax <= vmin + eps:\n        return 0.0\n    rng = vmax - vmin\n    low_th = vmin + 0.05 * rng\n    high_th = vmax - 0.05 * rng\n    mask = (a <= low_th) | (a >= high_th)\n    result = float(mask.sum()) / float(max(1, a.size))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Bounding-box aspect ratio of foreground (0..1) using median threshold'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    thr = float(np.median(a))\n    mask = a > thr\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.where(mask)\n    ymin, ymax = ys.min(), ys.max()\n    xmin, xmax = xs.min(), xs.max()\n    bh = float(ymax - ymin + 1)\n    bw = float(xmax - xmin + 1)\n    if bh <= 0.0 or bw <= 0.0:\n        return 0.0\n    ratio = min(bh / bw, bw / bh)\n    result = float(np.clip(ratio, 0.0, 1.0))\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Central intensity fraction (0..1): mean intensity in central box vs whole image'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ch0 = h // 4\n    cw0 = w // 4\n    ch1 = 3 * h // 4\n    cw1 = 3 * w // 4\n    center = a[ch0:ch1, cw0:cw1]\n    if center.size == 0:\n        return 0.0\n    center_sum = float(center.sum())\n    total = float(a.sum()) + eps\n    result = center_sum / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Directional edge density (0..1): fraction of pixels with gradient magnitude above mean'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    mean_mag = float(mag.mean())\n    mask = mag > (mean_mag + eps)\n    result = float(mask.sum()) / float(max(1, mag.size))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Count of local intensity maxima normalized by image area (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    padval = float(a.min() - 1.0)\n    p = np.pad(a, pad_width=1, mode='constant', constant_values=padval)\n    center = p[1:-1, 1:-1]\n    neighbors = [\n        p[0:-2, 0:-2], p[0:-2, 1:-1], p[0:-2, 2:],\n        p[1:-1, 0:-2],               p[1:-1, 2:],\n        p[2:,   0:-2], p[2:,   1:-1], p[2:,   2:]\n    ]\n    is_max = np.ones_like(center, dtype=bool)\n    for n in neighbors:\n        is_max &= (center > n)\n    # suppress tiny noise: require peak above mean + 0.5*std\n    thr = float(a.mean() + 0.5 * a.std())\n    is_max &= (center > thr)\n    count = int(is_max.sum())\n    result = float(count) / float(max(1, a.size))\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Radial intensity correlation (-1..1): correlation between radius and pixel intensity (negative => decays outward)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy).ravel()\n    vals = a.ravel().astype(float)\n    if vals.size == 0:\n        return 0.0\n    rx = r - r.mean()\n    vx = float((rx ** 2).sum())\n    vy = float(((vals - vals.mean()) ** 2).sum())\n    if vx <= 0.0 or vy <= 0.0:\n        return 0.0\n    cov = float((rx * (vals - vals.mean())).sum())\n    corr = cov / (np.sqrt(vx * vy) + eps)\n    result = float(np.clip(corr, -1.0, 1.0))\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Inter-percentile contrast: (90th - 10th percentile) normalized by median'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    p90 = float(np.percentile(flat, 90))\n    p10 = float(np.percentile(flat, 10))\n    p50 = float(np.percentile(flat, 50)) + eps\n    result = (p90 - p10) / p50\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels near the image maximum (indicates saturation/highlights)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    if a.size == 0:\n        return 0.0\n    mn = float(a.min())\n    mx = float(a.max())\n    rng = mx - mn + eps\n    # consider pixels within top 2% of range as near-saturated\n    thr = mx - 0.02 * rng\n    count = int(np.count_nonzero(a >= thr))\n    frac = float(count) / float(a.size)\n    return float(frac)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Density of local bright peaks (local maxima stronger than neighbors)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape if arr.size else (0, 0)\n    if h < 3 or w < 3:\n        return 0.0\n    center = arr\n    # compare to 8 neighbors using rolls\n    neigh_max = np.maximum.reduce([\n        np.roll(center, 1, axis=0),\n        np.roll(center, -1, axis=0),\n        np.roll(center, 1, axis=1),\n        np.roll(center, -1, axis=1),\n        np.roll(np.roll(center, 1, axis=0), 1, axis=1),\n        np.roll(np.roll(center, 1, axis=0), -1, axis=1),\n        np.roll(np.roll(center, -1, axis=0), 1, axis=1),\n        np.roll(np.roll(center, -1, axis=0), -1, axis=1),\n    ])\n    peaks = (center > neigh_max)\n    # threshold peaks to be reasonably bright\n    thr = center.mean() + center.std()\n    peaks = peaks & (center > thr)\n    count = int(np.count_nonzero(peaks))\n    density = count / float(max(1, h * w))\n    return float(density)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized entropy of edge orientation histogram (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    total = mag.sum()\n    if total <= 0:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # -pi..pi\n    theta = theta.ravel()\n    weights = mag.ravel()\n    # map to 0..2pi\n    theta = theta + np.pi\n    bins = 16\n    hist, _ = np.histogram(theta, bins=bins, range=(0.0, 2.0 * np.pi), weights=weights)\n    probs = hist / (hist.sum() + eps)\n    # entropy normalized by log(bins)\n    ent = -np.sum(np.where(probs > 0, probs * np.log(probs), 0.0))\n    norm_ent = ent / (np.log(bins) + eps)\n    return float(norm_ent)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity-weighted centroid offset from image center normalized by diagonal (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    weights = a - a.min()\n    wsum = float(weights.sum())\n    if wsum <= 0:\n        # fallback to binary centroid using above-median mask\n        mask = a > np.median(a)\n        if not np.any(mask):\n            return 0.0\n        ys_m, xs_m = np.nonzero(mask)\n        cy = float(ys_m.mean())\n        cx = float(xs_m.mean())\n    else:\n        cx = float((weights * xs).sum()) / (wsum + eps)\n        cy = float((weights * ys).sum()) / (wsum + eps)\n    center_x = (w - 1) / 2.0\n    center_y = (h - 1) / 2.0\n    dist = np.hypot(cx - center_x, cy - center_y)\n    diag = np.hypot(w, h) + eps\n    result = dist / diag\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels belonging to the dominant intensity bin (background uniformity)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    # use 64 bins between min and max\n    mn = float(flat.min())\n    mx = float(flat.max())\n    if mx <= mn:\n        return 1.0\n    bins = 64\n    hist, edges = np.histogram(flat, bins=bins, range=(mn, mx))\n    idx = int(np.argmax(hist))\n    low = edges[idx]\n    high = edges[idx + 1]\n    count = int(np.count_nonzero((a >= low) & (a < high)))\n    frac = float(count) / float(flat.size)\n    return float(frac)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Annular contrast: normalized difference between center and surrounding ring'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    maxr = min(cx, cy)\n    if maxr <= 0:\n        return 0.0\n    r1 = maxr * 0.1\n    r2 = maxr * 0.3\n    center_mask = r <= r1\n    ring_mask = (r > r1) & (r <= r2)\n    if not np.any(center_mask) or not np.any(ring_mask):\n        return 0.0\n    center_mean = float(np.mean(a[center_mask]))\n    ring_mean = float(np.mean(a[ring_mask]))\n    denom = (abs(center_mean) + abs(ring_mean) + eps)\n    result = abs(center_mean - ring_mean) / denom\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Multi-scale Laplacian energy ratio (fine / coarse)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    # discrete Laplacian\n    pad = np.pad(a, pad_width=1, mode='edge').astype(float)\n    center = pad[1:-1, 1:-1]\n    up = pad[0:-2, 1:-1]\n    down = pad[2:, 1:-1]\n    left = pad[1:-1, 0:-2]\n    right = pad[1:-1, 2:]\n    lap = (up + down + left + right) - 4.0 * center\n    fine_energy = float(np.mean(np.abs(lap))) + eps\n    # coarse: smooth by 3x3 average then laplacian\n    sm = (center + up + down + left + right +\n          np.roll(np.roll(center, 1, axis=0), 1, axis=1) +\n          np.roll(np.roll(center, 1, axis=0), -1, axis=1) +\n          np.roll(np.roll(center, -1, axis=0), 1, axis=1) +\n          np.roll(np.roll(center, -1, axis=0), -1, axis=1)) / 9.0\n    pad2 = np.pad(sm, pad_width=1, mode='edge')\n    c2 = pad2[1:-1, 1:-1]\n    u2 = pad2[0:-2, 1:-1]\n    d2 = pad2[2:, 1:-1]\n    l2 = pad2[1:-1, 0:-2]\n    r2 = pad2[1:-1, 2:]\n    lap2 = (u2 + d2 + l2 + r2) - 4.0 * c2\n    coarse_energy = float(np.mean(np.abs(lap2))) + eps\n    result = fine_energy / coarse_energy\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Compactness-aspect score: how box-filling and square the main bright region is (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(np.median(a))\n    mask = a > thr\n    if not np.any(mask):\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    ymin, ymax = int(ys.min()), int(ys.max())\n    xmin, xmax = int(xs.min()), int(xs.max())\n    bh = max(1, ymax - ymin + 1)\n    bw = max(1, xmax - xmin + 1)\n    bbox_area = float(bh * bw)\n    mask_area = float(mask.sum())\n    # compactness: how much of bbox is filled by mask (0..1)\n    fill = mask_area / (bbox_area + eps)\n    # aspect: closeness to square (1..0), compute min(width/height, height/width)\n    aspect = float(min(bh, bw) / (max(bh, bw) + eps))\n    result = fill * aspect\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right Pearson correlation between left half and flipped right half (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:] if mid > 0 else a[:, :mid]\n    if left.size == 0 or right.size == 0:\n        return 0.0\n    # flip right horizontally to compare symmetry\n    right_flipped = np.fliplr(right)\n    # if shapes differ (odd width), crop to min columns\n    mincols = min(left.shape[1], right_flipped.shape[1])\n    left_c = left[:, :mincols].ravel()\n    right_c = right_flipped[:, :mincols].ravel()\n    if left_c.size < 2:\n        return 0.0\n    lv = left_c - left_c.mean()\n    rv = right_c - right_c.mean()\n    denom = (np.sqrt((lv * lv).sum() * (rv * rv).sum()) + eps)\n    corr = float((lv * rv).sum() / denom)\n    return float(np.clip(corr, -1.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized intensity entropy (0..1) using 32 histogram bins'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    flat = a.ravel()\n    if flat.size == 0:\n        return 0.0\n    mn, mx = float(flat.min()), float(flat.max())\n    if mx <= mn + eps:\n        return 0.0\n    bins = 32\n    hist, _ = np.histogram(flat, bins=bins, range=(mn, mx))\n    total = hist.sum()\n    if total <= 0:\n        return 0.0\n    p = hist.astype(float) / (total + eps)\n    p = p[p > 0]\n    ent = -np.sum(p * np.log(p))\n    # normalize by log(number of bins)\n    norm = np.log(bins)\n    result = ent / (norm + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels considered foreground (intensity > mean + 0.5*std)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    m = float(a.mean())\n    s = float(a.std())\n    thr = m + 0.5 * s\n    mask = a > thr\n    frac = float(np.count_nonzero(mask)) / (a.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized offset of intensity-weighted center-of-mass from image center (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    # use nonnegative weights\n    weights = a - float(a.min())\n    wsum = float(weights.sum())\n    if wsum <= eps:\n        return 0.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    cy = float((weights * ys).sum()) / wsum\n    cx = float((weights * xs).sum()) / wsum\n    center_y = (h - 1) / 2.0\n    center_x = (w - 1) / 2.0\n    dist = np.hypot(cy - center_y, cx - center_x)\n    maxrad = np.hypot(h, w) / 2.0 + eps\n    result = dist / maxrad\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Horizontal-vs-vertical edge dominance ratio (-1..1), positive => horizontal stronger'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    magx = np.mean(np.abs(gx))\n    magy = np.mean(np.abs(gy))\n    denom = magx + magy + eps\n    ratio = (magx - magy) / denom\n    return float(np.clip(ratio, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variance of radial ring means normalized by overall variance (0..inf)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        img = img.mean(axis=2)\n    img = np.nan_to_num(img.astype(float))\n    h, w = img.shape\n    if img.size == 0:\n        return 0.0\n    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n    ys = np.arange(h)[:, None]\n    xs = np.arange(w)[None, :]\n    dist = np.hypot(ys - cy, xs - cx)\n    maxd = float(dist.max()) if dist.size else 1.0\n    if maxd <= eps:\n        return 0.0\n    nbins = min(30, max(4, int(round(maxd))))\n    bins = np.linspace(0, maxd, nbins + 1)\n    ring_means = []\n    for i in range(nbins):\n        mask = (dist >= bins[i]) & (dist < bins[i + 1])\n        if np.any(mask):\n            ring_means.append(np.mean(img[mask]))\n    if len(ring_means) <= 1:\n        return 0.0\n    rm = np.array(ring_means)\n    var_rm = float(rm.var())\n    overall_var = float(img.var()) + eps\n    result = var_rm / overall_var\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local 3x3 variance normalized by global variance (0..inf)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if a.size == 0:\n        return 0.0\n    # local mean and local mean of squares via shift-sum\n    def local_mean(X):\n        s = np.zeros_like(X, dtype=float)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s / 9.0\n    M = local_mean(a)\n    M2 = local_mean(a * a)\n    local_var = M2 - M * M\n    mean_local_var = float(np.mean(local_var))\n    overall_var = float(a.var()) + eps\n    result = mean_local_var / overall_var\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Aspect difference of bright-region bounding box (0..1), 0=circular/square, 1=very elongated'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    thr = float(a.mean() + a.std())\n    mask = a > thr\n    if np.count_nonzero(mask) < 3:\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    w_box = int(xs.max() - xs.min() + 1)\n    h_box = int(ys.max() - ys.min() + 1)\n    denom = max(w_box, h_box, eps)\n    result = abs(w_box - h_box) / denom\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge pixel density: fraction of pixels with gradient magnitude > mean+std (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    thr = float(mag.mean() + mag.std())\n    count = int(np.count_nonzero(mag > thr))\n    result = float(count) / (mag.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of positive Harris-like corner energy (0..1)'\n    import numpy as np\n    eps = 1e-12\n    k = 0.04\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    Jxx = gx * gx\n    Jyy = gy * gy\n    Jxy = gx * gy\n    # local average 3x3\n    def local_mean(X):\n        s = np.zeros_like(X, dtype=float)\n        s += X\n        s += np.roll(X, 1, axis=0)\n        s += np.roll(X, -1, axis=0)\n        s += np.roll(X, 1, axis=1)\n        s += np.roll(X, -1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, 1, axis=0), -1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), 1, axis=1)\n        s += np.roll(np.roll(X, -1, axis=0), -1, axis=1)\n        return s / 9.0\n    Sxx = local_mean(Jxx)\n    Syy = local_mean(Jyy)\n    Sxy = local_mean(Jxy)\n    det = Sxx * Syy - Sxy * Sxy\n    trace = Sxx + Syy\n    R = det - k * (trace ** 2)\n    pos_energy = np.sum(R * (R > 0))\n    total_energy = np.sum(np.abs(R)) + eps\n    result = float(pos_energy / total_energy)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Colorfulness metric (Hasler-S\u00fcsstrunk); returns 0 for grayscale or flat images'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim != 3 or arr.shape[2] < 3:\n        return 0.0\n    img = np.nan_to_num(arr.astype(float))\n    R = img[:, :, 0]\n    G = img[:, :, 1]\n    B = img[:, :, 2]\n    rg = R - G\n    yb = 0.5 * (R + G) - B\n    std_rg = float(rg.std())\n    std_yb = float(yb.std())\n    mean_rg = float(rg.mean())\n    mean_yb = float(yb.mean())\n    colorfulness = np.sqrt(std_rg * std_rg + std_yb * std_yb) + 0.3 * np.sqrt(mean_rg * mean_rg + mean_yb * mean_yb)\n    # normalize by dynamic range\n    dyn = float(max(1.0, img.max() - img.min()))\n    result = colorfulness / (dyn + eps)\n    return float(max(0.0, result))\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized aspect ratio: (height/width - 1), clipped to [-5,5]'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        h, w = a.shape\n    except Exception:\n        return 0.0\n    if w == 0:\n        return 0.0\n    val = float(h) / float(w) - 1.0\n    return float(np.clip(val, -5.0, 5.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of pixels in the top 1% intensity band (saturation/clipping indicator)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    amin = a.min()\n    amax = a.max()\n    rng = amax - amin\n    if rng <= 0:\n        return 0.0\n    thresh = amax - 0.01 * rng\n    frac = float(np.count_nonzero(a >= thresh)) / (a.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Contrast of bright tail: (mean of top 5% - median) normalized by std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size == 0:\n        return 0.0\n    thr = float(np.percentile(a, 95))\n    top = a[a >= thr]\n    if top.size == 0:\n        top_mean = float(thr)\n    else:\n        top_mean = float(top.mean())\n    med = float(np.median(a))\n    sd = float(a.std()) + eps\n    result = (top_mean - med) / sd\n    return float(np.clip(result, -10.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram bimodality index (0..1): two peaks separated by a deep valley and distance'\n    import numpy as np\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        a = np.nan_to_num(arr.astype(float)).ravel()\n    if a.size < 3:\n        return 0.0\n    bins = 16\n    try:\n        hist, edges = np.histogram(a, bins=bins, range=(a.min(), a.max()))\n    except Exception:\n        hist, edges = np.histogram(a, bins=bins)\n    if hist.sum() == 0:\n        return 0.0\n    # find two largest peaks\n    idxs = np.argsort(hist)\n    if idxs.size < 2:\n        return 0.0\n    p1, p2 = idxs[-1], idxs[-2]\n    lo = min(p1, p2)\n    hi = max(p1, p2)\n    valley = float(hist[lo:hi + 1].min()) if hi >= lo else float(hist[lo])\n    maxcount = float(hist.max()) if hist.max() > 0 else 1.0\n    peaksum = float(hist[p1] + hist[p2])\n    distance_norm = float(abs(p1 - p2)) / float(max(1, bins - 1))\n    bim = (peaksum - 2.0 * valley) / (2.0 * maxcount)\n    result = np.clip(bim * distance_norm, 0.0, 1.0)\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Mean local variance (3x3) normalized by global variance (texture energy)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    pad = np.pad(a, 1, mode='reflect')\n    s = np.zeros_like(a, dtype=float)\n    s2 = np.zeros_like(a, dtype=float)\n    for dy in (-1, 0, 1):\n        for dx in (-1, 0, 1):\n            patch = pad[1 + dy:1 + dy + h, 1 + dx:1 + dx + w]\n            s += patch\n            s2 += patch * patch\n    mean_local = s / 9.0\n    mean_sq_local = s2 / 9.0\n    var_local = mean_sq_local - mean_local * mean_local\n    energy = float(np.mean(var_local))\n    global_var = float(a.var()) + eps\n    result = energy / global_var\n    return float(np.clip(result, 0.0, 10.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Edge orientation coherence (0..1): how aligned edge orientations are'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= 0:\n        return 0.0\n    theta = np.arctan2(gy, gx)  # orientation in [-pi,pi]\n    # orientation periodicity pi -> use 2*theta\n    vec = np.exp(1j * 2.0 * theta)\n    weighted = vec * mag\n    mean_vec = weighted.sum() / (mag.sum() + eps)\n    coherence = float(np.abs(mean_vec))\n    return float(np.clip(coherence, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Adaptive foreground fraction: fraction of pixels above mean+0.5*std'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    mu = float(np.mean(a))\n    sd = float(np.std(a))\n    thresh = mu + 0.5 * sd\n    frac = float(np.count_nonzero(a > thresh)) / (a.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Pearson correlation between intensity and distance from center (-1..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    h, w = a.shape\n    ys, xs = np.indices((h, w))\n    cx = (w - 1.0) / 2.0\n    cy = (h - 1.0) / 2.0\n    r = np.hypot(xs - cx, ys - cy).ravel()\n    v = a.ravel()\n    if r.size < 2 or v.size < 2:\n        return 0.0\n    r_mean = r.mean()\n    v_mean = v.mean()\n    r_std = r.std() + eps\n    v_std = v.std() + eps\n    cov = ((r - r_mean) * (v - v_mean)).mean()\n    corr = cov / (r_std * v_std)\n    return float(np.clip(corr, -1.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of strict local maxima (8-neighborhood) normalized by image size'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    h, w = a.shape\n    pad_min = float(a.min()) - 1.0\n    p = np.pad(a, 1, mode='constant', constant_values=pad_min)\n    center = p[1:-1, 1:-1]\n    greater = np.ones_like(center, dtype=bool)\n    for dy in (-1, 0, 1):\n        for dx in (-1, 0, 1):\n            if dy == 0 and dx == 0:\n                continue\n            neigh = p[1 + dy:1 + dy + h, 1 + dx:1 + dx + w]\n            greater &= (center > neigh)\n    count = float(np.count_nonzero(greater))\n    frac = count / (center.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Low-frequency energy fraction from 2D FFT (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # zero-mean to focus on structural energy\n    a0 = a - a.mean()\n    # compute power spectrum\n    F = np.fft.fft2(a0)\n    P = np.abs(F) ** 2\n    Psum = float(P.sum()) + eps\n    # select central low-frequency square after fftshift\n    Pshift = np.fft.fftshift(P)\n    ch = max(1, int(max(1, round(0.1 * h))))\n    cw = max(1, int(max(1, round(0.1 * w))))\n    cy = h // 2\n    cx = w // 2\n    ly = slice(max(0, cy - ch), min(h, cy + ch + 1))\n    lx = slice(max(0, cx - cw), min(w, cx + cw + 1))\n    low_power = float(Pshift[ly, lx].sum())\n    frac = low_power / Psum\n    return float(np.clip(frac, 0.0, 1.0))\n",
    "def feature(image: np.ndarray) -> float:\n    'Left-right symmetry score (1.0 = perfectly symmetric, 0.0 = very asymmetric)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 0.0\n    half = w // 2\n    left = a[:, :half]\n    right = a[:, -half:]\n    right_flipped = np.fliplr(right)\n    minw = min(left.shape[1], right_flipped.shape[1])\n    if minw == 0:\n        return 0.0\n    diff = np.abs(left[:, :minw] - right_flipped[:, :minw])\n    denom = float(np.mean(np.abs(a)) + eps)\n    normalized = float(np.mean(diff)) / denom\n    score = 1.0 - np.clip(normalized, 0.0, 1.0)\n    return float(score)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of strict local intensity maxima (peaks) in 8-neighborhood'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 3 or w < 3:\n        return 0.0\n    minval = a.min() - 1.0\n    p = np.pad(a, 1, mode='constant', constant_values=minval)\n    c = p[1:-1, 1:-1]\n    neighs = [\n        p[:-2, :-2], p[:-2, 1:-1], p[:-2, 2:],\n        p[1:-1, :-2],              p[1:-1, 2:],\n        p[2:, :-2],  p[2:, 1:-1],  p[2:, 2:]\n    ]\n    is_max = np.ones_like(c, dtype=bool)\n    for n in neighs:\n        is_max &= (c > n)\n    count = float(np.count_nonzero(is_max))\n    result = count / float(h * w + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of strong gradient pixels (edge density) using mean+std threshold'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    thresh = float(np.mean(mag) + np.std(mag))\n    strong = np.count_nonzero(mag > thresh)\n    result = strong / float(a.size + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Dominant gradient orientation fraction (0..1), measured in 12 orientation bins'\n    import numpy as np\n    eps = 1e-12\n    bins = 12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    gy, gx = np.gradient(a)\n    mag = np.hypot(gx, gy)\n    if mag.sum() <= eps:\n        return 0.0\n    ang = np.degrees(np.arctan2(gy, gx)) % 180.0  # unsigned orientations\n    hist, _ = np.histogram(ang.ravel(), bins=bins, range=(0.0, 180.0), weights=mag.ravel())\n    total = float(hist.sum()) + eps\n    result = float(hist.max()) / total\n    return float(np.clip(result, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Variance of mean intensities across concentric rings normalized by global variance'\n    import numpy as np\n    eps = 1e-12\n    rings = 8\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    cx = (w - 1) / 2.0\n    cy = (h - 1) / 2.0\n    r = np.hypot(xs - cx, ys - cy)\n    maxr = float(r.max()) + eps\n    bins = np.linspace(0.0, maxr, rings + 1)\n    means = []\n    for i in range(rings):\n        mask = (r >= bins[i]) & (r < bins[i+1])\n        if np.any(mask):\n            means.append(float(a[mask].mean()))\n    if len(means) <= 1:\n        return 0.0\n    mean_arr = np.array(means, dtype=float)\n    var_rings = float(np.var(mean_arr))\n    global_var = float(np.var(a)) + eps\n    result = var_rings / global_var\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Robust contrast: (90th - 10th percentile) normalized by global std'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).ravel().astype(float))\n    else:\n        arr = np.nan_to_num(img.ravel().astype(float))\n    if arr.size == 0:\n        return 0.0\n    p90 = float(np.percentile(arr, 90))\n    p10 = float(np.percentile(arr, 10))\n    gstd = float(arr.std()) + eps\n    result = (p90 - p10) / gstd\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Laplacian sharpness proxy: mean absolute Laplacian normalized by mean absolute intensity'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h < 2 or w < 2:\n        return 0.0\n    center = 4.0 * a\n    up = np.roll(a, 1, axis=0)\n    down = np.roll(a, -1, axis=0)\n    left = np.roll(a, 1, axis=1)\n    right = np.roll(a, -1, axis=1)\n    lap = center - (up + down + left + right)\n    mean_abs_lap = float(np.mean(np.abs(lap)))\n    mean_abs_img = float(np.mean(np.abs(a))) + eps\n    result = mean_abs_lap / mean_abs_img\n    return float(result)\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Average patch entropy (4x4 blocks) normalized to [0..1] (8 bins)'\n    import numpy as np\n    eps = 1e-12\n    bins = 8\n    ph, pw = 4, 4\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        arr = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        arr = np.nan_to_num(img.astype(float))\n    h, w = arr.shape\n    if h < ph or w < pw:\n        return 0.0\n    ch = (h // ph) * ph\n    cw = (w // pw) * pw\n    sub = arr[:ch, :cw]\n    if sub.size == 0:\n        return 0.0\n    blocks = sub.reshape(ch // ph, ph, cw // pw, pw).swapaxes(1,2).reshape(-1, ph*pw)\n    max_ent = np.log2(bins)\n    ent_sum = 0.0\n    for blk in blocks:\n        vmin = float(blk.min())\n        vmax = float(blk.max())\n        if vmax <= vmin:\n            continue\n        hist, _ = np.histogram(blk, bins=bins, range=(vmin, vmax))\n        p = hist.astype(float) / (hist.sum() + eps)\n        pnz = p[p > 0.0]\n        if pnz.size == 0:\n            continue\n        ent = -np.sum(pnz * np.log2(pnz))\n        ent_sum += ent / (max_ent + eps)\n    avg = ent_sum / float(blocks.shape[0] + eps)\n    return float(np.clip(avg, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Binary mask eccentricity (1 - minor/major eigenvalue) of median-based mask (0..1)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    med = float(np.median(a))\n    mask = a > med\n    if mask.sum() < 3:\n        return 0.0\n    ys, xs = np.nonzero(mask)\n    dx = xs.astype(float) - float(xs.mean())\n    dy = ys.astype(float) - float(ys.mean())\n    cov_xx = float((dx * dx).mean())\n    cov_xy = float((dx * dy).mean())\n    cov_yy = float((dy * dy).mean())\n    trace = cov_xx + cov_yy\n    det = cov_xx * cov_yy - cov_xy * cov_xy\n    # eigenvalues of 2x2 covariance\n    disc = max(0.0, trace * trace / 4.0 - det)\n    l1 = trace / 2.0 + np.sqrt(disc)\n    l2 = trace / 2.0 - np.sqrt(disc)\n    major = float(max(l1, l2))\n    minor = float(min(l1, l2)) + eps\n    ecc = 1.0 - (minor / (major + eps))\n    return float(np.clip(ecc, 0.0, 1.0))\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Perimeter-to-area ratio of a median-thresholded mask (shape complexity proxy)'\n    import numpy as np\n    eps = 1e-12\n    img = np.asarray(image)\n    if img.size == 0:\n        return 0.0\n    if img.ndim == 3:\n        a = np.nan_to_num(img.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(img.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    thr = float(np.median(a))\n    mask = a > thr\n    area = float(mask.sum())\n    if area <= 0.0:\n        return 0.0\n    # count neighbors that are also foreground\n    neigh_count = np.zeros_like(mask, dtype=int)\n    neigh_count += np.roll(mask, 1, axis=0)\n    neigh_count += np.roll(mask, -1, axis=0)\n    neigh_count += np.roll(mask, 1, axis=1)\n    neigh_count += np.roll(mask, -1, axis=1)\n    # a foreground pixel with any background neighbor contributes to perimeter\n    perimeter = float(np.count_nonzero(mask & (neigh_count < 4)))\n    result = (perimeter + eps) / (area + eps)\n    return float(result)\n",
    "def feature(image: np.ndarray) -> float:\n    'Horizontal left-right symmetry score (1 = perfect symmetry)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if w < 2:\n        return 1.0\n    mid = w // 2\n    left = a[:, :mid]\n    right = a[:, -mid:]\n    if left.size == 0 or right.size == 0:\n        return 1.0\n    # flip right for comparison\n    right_flipped = np.fliplr(right)\n    # if widths differ (odd), center column ignored\n    min_w = min(left.shape[1], right_flipped.shape[1])\n    left = left[:, :min_w]\n    right_flipped = right_flipped[:, :min_w]\n    diff = np.abs(left - right_flipped)\n    denom = float(np.mean(np.abs(a))) + eps\n    norm_diff = float(np.mean(diff)) / denom\n    score = 1.0 - np.clip(norm_diff, 0.0, 1.0)\n    return float(score)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of strict local intensity maxima among pixels (bright spots density)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # pad with -inf so borders are handled\n    pad = np.pad(a, pad_width=1, mode='constant', constant_values=-np.inf)\n    center = pad[1:-1, 1:-1]\n    neighs = [\n        pad[0:-2, 0:-2], pad[0:-2, 1:-1], pad[0:-2, 2:],\n        pad[1:-1, 0:-2],                 pad[1:-1, 2:],\n        pad[2:  , 0:-2], pad[2:  , 1:-1], pad[2:  , 2:]\n    ]\n    greater_than_all = np.ones_like(center, dtype=bool)\n    for n in neighs:\n        greater_than_all &= (center > n)\n    # threshold to avoid tiny peaks: require value > global mean + std\n    thr = float(np.mean(a)) + float(np.std(a)) * 0.5\n    peaks = greater_than_all & (a > thr)\n    frac = float(np.count_nonzero(peaks)) / float(a.size + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Gradient direction bias: (mean|gx|-mean|gy|) / (mean|gx|+mean|gy|) in [-1..1]'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    magx = float(np.mean(np.abs(gx))) + eps\n    magy = float(np.mean(np.abs(gy))) + eps\n    result = (magx - magy) / (magx + magy)\n    return float(np.clip(result, -1.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Intensity centroid offset normalized by image diagonal (0=centered, 1=corner)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    ys, xs = np.indices((h, w))\n    total = float(a.sum())\n    if total <= eps:\n        return 0.0\n    cx = float((xs * a).sum()) / total\n    cy = float((ys * a).sum()) / total\n    midx = (w - 1) / 2.0\n    midy = (h - 1) / 2.0\n    dist = np.hypot(cx - midx, cy - midy)\n    max_dist = np.hypot(midx, midy)\n    result = float(dist / (max_dist + eps))\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Normalized Laplacian energy: mean(|\u0394I|) / (mean(|I|)+eps)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n        gyy, gyx = np.gradient(gy)\n        gxy, gxx = np.gradient(gx)\n        lap = gxx + gyy\n    except Exception:\n        return 0.0\n    mean_abs_lap = float(np.mean(np.abs(lap)))\n    mean_abs = float(np.mean(np.abs(a))) + eps\n    result = mean_abs_lap / mean_abs\n    return float(result)\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Structure-tensor anisotropy (0=isotropic, 1=strong single orientation)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    if a.size == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    Sxx = float((gx * gx).mean())\n    Syy = float((gy * gy).mean())\n    Sxy = float((gx * gy).mean())\n    T = Sxx + Syy\n    # determinant\n    det = Sxx * Syy - Sxy * Sxy\n    disc = max(0.0, T * T - 4.0 * det)\n    sqrt_disc = np.sqrt(disc)\n    lam1 = 0.5 * (T + sqrt_disc)\n    lam2 = 0.5 * (T - sqrt_disc)\n    result = (lam1 - lam2) / (lam1 + lam2 + eps)\n    return float(np.clip(result, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Fraction of strong edges that lie within the outer 20% border (0..1)'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    try:\n        gy, gx = np.gradient(a)\n    except Exception:\n        return 0.0\n    mag = np.hypot(gx, gy)\n    if mag.size == 0:\n        return 0.0\n    thr = float(np.mean(mag)) + float(np.std(mag)) * 0.5\n    edges = mag > thr\n    if not np.any(edges):\n        return 0.0\n    ys, xs = np.indices((h, w))\n    br = int(max(1, np.floor(0.2 * min(h, w))))\n    border_mask = (xs < br) | (xs >= (w - br)) | (ys < br) | (ys >= (h - br))\n    num_border_edges = int(np.count_nonzero(edges & border_mask))\n    num_edges = int(np.count_nonzero(edges))\n    frac = float(num_border_edges) / float(num_edges + eps)\n    return float(np.clip(frac, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Histogram peakiness: largest histogram bin fraction using 16 bins (0..1)'\n    import numpy as np\n    eps = 1e-12\n    bins = 16\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        vals = np.nan_to_num(arr.mean(axis=2).astype(float)).ravel()\n    else:\n        vals = np.nan_to_num(arr.astype(float)).ravel()\n    if vals.size == 0:\n        return 0.0\n    mn = float(vals.min())\n    mx = float(vals.max())\n    if mx <= mn:\n        return 1.0\n    hist, _ = np.histogram(vals, bins=bins, range=(mn, mx))\n    total = hist.sum() + eps\n    peak = float(hist.max()) / total\n    return float(np.clip(peak, 0.0, 1.0))\n\n\n",
    "def feature(image: np.ndarray) -> float:\n    'Row-wise high-pixel fraction variability: std/mean of per-row hot-pixel fractions'\n    import numpy as np\n    eps = 1e-12\n    arr = np.asarray(image)\n    if arr.size == 0:\n        return 0.0\n    if arr.ndim == 3:\n        a = np.nan_to_num(arr.mean(axis=2).astype(float))\n    else:\n        a = np.nan_to_num(arr.astype(float))\n    h, w = a.shape\n    if h == 0 or w == 0:\n        return 0.0\n    # per-row threshold: row mean + 0.5 * row std\n    row_means = a.mean(axis=1)\n    row_stds = a.std(axis=1)\n    thr = row_means + 0.5 * row_stds\n    # broadcast threshold to compare\n    mask = a > thr[:, None]\n    row_fracs = mask.sum(axis=1) / (w + eps)\n    mean_frac = float(row_fracs.mean())\n    std_frac = float(row_fracs.std())\n    result = std_frac / (mean_frac + eps)\n    # clip to a reasonable range\n    return float(np.clip(result, 0.0, 10.0))\n"
  ]
}