[
  "def feature(text: str) -> float:\n    'Character-level Shannon entropy of the text (bits per character)'\n    import math\n    if not text:\n        return 0.0\n    freq = {}\n    for c in text:\n        freq[c] = freq.get(c, 0) + 1\n    n = float(len(text))\n    entropy = 0.0\n    for count in freq.values():\n        p = count / n\n        entropy -= p * math.log2(p)\n    return float(entropy)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique words divided by total words'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for s in sentences:\n        total_words += len(re.findall(r'\\w+', s))\n    return float(total_words / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Punctuation density: punctuation characters divided by total characters'\n    if not text:\n        return 0.0\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct / len(text))\n\n",
  "def feature(text: str) -> float:\n    'Ratio of uppercase alphabetic characters to all alphabetic characters'\n    if not text:\n        return 0.0\n    letters = sum(1 for c in text if c.isalpha())\n    if letters == 0:\n        return 0.0\n    upper = sum(1 for c in text if c.isalpha() and c.isupper())\n    return float(upper / letters)\n\n",
  "def feature(text: str) -> float:\n    'Ratio of purely numeric tokens to total tokens'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if t.isdigit())\n    return float(num / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Ratio of common English stopwords to total words'\n    import re\n    STOPWORDS = {\n        'the','and','is','in','it','of','to','a','an','that','this','for','on','with',\n        'as','are','was','were','be','by','or','from','at','not','but','have','has','had',\n        'you','i','he','she','they','we','their','them','his','her'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in STOPWORDS)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words longer than 6 characters'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 6)\n    return float(long_count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing a repeated character sequence of length >=3'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1{2,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Approximate Flesch reading ease score using simple syllable heuristic'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    total_words = len(words)\n    total_sentences = len(sentences) if sentences else 0\n    if total_words == 0 or total_sentences == 0:\n        return 0.0\n    total_syllables = 0\n    for w in words:\n        # count vowel groups as syllables\n        syll = len(re.findall(r'[aeiouy]+', w))\n        # simple adjustment for trailing silent 'e'\n        if w.endswith('e') and syll > 1:\n            syll -= 1\n        if syll < 1:\n            syll = 1\n        total_syllables += syll\n    # Flesch reading ease formula\n    score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n    return float(score)\n",
  "def feature(text: str) -> float:\n    'Standard deviation of word lengths'\n    import re, math\n    words = re.findall(r'\\w+', text or '')\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a conjunction or discourse marker'\n    import re\n    if not text:\n        return 0.0\n    CONJ = {'and', 'but', 'or', 'so', 'because', 'however', 'then', 'also', 'yet', 'thus', 'therefore'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if words and words[0].lower() in CONJ:\n            starts += 1\n    return float(starts / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that mix letters and digits (alphanumeric tokens)'\n    import re\n    tokens = re.findall(r'\\w+', text or '')\n    if not tokens:\n        return 0.0\n    mixed = 0\n    for t in tokens:\n        has_alpha = any(c.isalpha() for c in t)\n        has_digit = any(c.isdigit() for c in t)\n        if has_alpha and has_digit:\n            mixed += 1\n    return float(mixed / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of titlecase words that occur NOT at the start of a sentence (possible proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    total = 0\n    title_nonstart = 0\n    for s in sentences:\n        toks = re.findall(r'\\w+', s)\n        for i, t in enumerate(toks):\n            total += 1\n            if i > 0 and t.istitle():\n                title_nonstart += 1\n    if total == 0:\n        return 0.0\n    return float(title_nonstart / total)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    if not text:\n        return 0.0\n    q = text.count('?')\n    total_sent = text.count('.') + text.count('!') + text.count('?')\n    if total_sent == 0:\n        return 0.0\n    return float(q / total_sent)\n\n",
  "def feature(text: str) -> float:\n    'Average syllables per word using a simple vowel-group heuristic'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    total_syll = 0\n    for w in words:\n        syll = len(re.findall(r'[aeiouy]+', w))\n        if w.endswith('e') and syll > 1:\n            syll -= 1\n        if syll < 1:\n            syll = 1\n        total_syll += syll\n    return float(total_syll / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Ratio of unique adjacent word bigrams to total bigrams (bigram diversity)'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if len(words) < 2:\n        return 0.0\n    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n    unique = len(set(bigrams))\n    return float(unique / len(bigrams))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of first/second-person pronouns among all counted personal pronouns'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    first_second = {'i', 'we', 'you', 'me', 'us', 'my', 'our', 'mine', 'yours', 'your'}\n    third = {'he', 'she', 'they', 'them', 'his', 'her', 'their', 'theirs', 'hers'}\n    fs_count = sum(1 for w in words if w in first_second)\n    th_count = sum(1 for w in words if w in third)\n    total = fs_count + th_count\n    if total == 0:\n        return 0.0\n    return float(fs_count / total)\n\n",
  "def feature(text: str) -> float:\n    'Density of explicit links or email addresses (URLs/emails per token)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    emails = re.findall(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', text)\n    urls = re.findall(r'https?://\\S+|www\\.\\S+', text)\n    count = len(emails) + len(urls)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Density of emoticons and common emoji characters (per character)'\n    import re\n    if not text:\n        return 0.0\n    emoticon_pattern = re.compile(r'(:-\\)|:\\)|:-\\(|:\\(|:D|:-D|;-\\)|;\\)|:\\'\\(|<3|:\\*)')\n    emoticons = len(emoticon_pattern.findall(text))\n    emoji_ranges = [\n        (0x1F300, 0x1F5FF), (0x1F600, 0x1F64F), (0x1F680, 0x1F6FF),\n        (0x2600, 0x26FF), (0x2700, 0x27BF)\n    ]\n    emoji_chars = 0\n    for c in text:\n        o = ord(c)\n        if any(start <= o <= end for start, end in emoji_ranges):\n            emoji_chars += 1\n    total_symbols = emoticons + emoji_chars\n    if not text:\n        return 0.0\n    return float(total_symbols / max(1, len(text)))\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence (words / sentences)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = len(sentences) or 1\n    return float(len(words) / sent_count)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are fully uppercase (shouting indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    upper_count = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t) and t.isupper() and len(t) > 1:\n            upper_count += 1\n    return float(upper_count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of words that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    hapax = sum(1 for w, c in freqs.items() if c == 1)\n    return float(hapax / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing any non-ASCII character'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    non_ascii = 0\n    for t in tokens:\n        if any(ord(c) > 127 for c in t):\n            non_ascii += 1\n    return float(non_ascii / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average normalized longest repeated-character run per token (max_run/len(token))'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    total = 0.0\n    for t in tokens:\n        if not t:\n            continue\n        prev = ''\n        run = 0\n        max_run = 0\n        for c in t:\n            if c == prev:\n                run += 1\n            else:\n                run = 1\n                prev = c\n            if run > max_run:\n                max_run = run\n        total += (max_run / len(t))\n    return float(total / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that contain three or more identical characters in a row (elongation)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1')\n    count = sum(1 for w in words if pattern.search(w))\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation characters divided by total punctuation occurrences'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct / len(puncts))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sents:\n        return 0.0\n    q = sum(1 for s in sents if s.endswith('?'))\n    return float(q / len(sents))\n\n",
  "def feature(text: str) -> float:\n    'Average proportion of digits inside each token (digits / token_length)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    tot = 0.0\n    for t in tokens:\n        if not t:\n            continue\n        digits = sum(1 for c in t if c.isdigit())\n        tot += (digits / len(t))\n    return float(tot / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that contain at least one exclamation mark'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sents:\n        return 0.0\n    ex = sum(1 for s in sents if '!' in s)\n    return float(ex / len(sents))\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','and','is','in','it','of','to','a','an','that','this','for','on',\n        'with','as','by','at','from','or','be','are','was','were','has','have'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average Shannon entropy per token (mean of token-level char entropies)'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    total = 0.0\n    for t in tokens:\n        freq = {}\n        for c in t:\n            freq[c] = freq.get(c, 0) + 1\n        n = float(len(t))\n        ent = 0.0\n        for v in freq.values():\n            p = v / n\n            ent -= p * math.log2(p)\n        total += ent\n    return float(total / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of raw tokens composed only of punctuation characters'\n    import re, string\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    punct = set(string.punctuation)\n    count = 0\n    for t in tokens:\n        if t and all(c in punct for c in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Approximate Flesch-Kincaid grade level (0 if insufficient data)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    if sentence_count == 0:\n        return 0.0\n    # syllable heuristic similar to common simple rule\n    syllables = 0\n    for w in words:\n        s = len(re.findall(r'[aeiouy]+', w))\n        if w.endswith('e') and s > 1:\n            s -= 1\n        if s < 1:\n            s = 1\n        syllables += s\n    words_count = len(words)\n    # Flesch-Kincaid Grade Level\n    fk = 0.39 * (words_count / sentence_count) + 11.8 * (syllables / words_count) - 15.59\n    return float(fk)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are titlecase (First upper, rest lower)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) > 1 and t[0].isupper() and t[1:].islower():\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Token repetition rate: 1 - (unique_tokens / total_tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    total = len(tokens)\n    return float(1.0 - (unique / total))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of raw tokens that appear to be URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'://|^www\\.|https?://', re.IGNORECASE)\n    email_re = re.compile(r'\\b[\\w.+-]+@[\\w-]+\\.[\\w.-]+\\b')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens containing a character repeated three or more times consecutively'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    patt = re.compile(r'(.)\\1\\1')\n    count = sum(1 for t in tokens if patt.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of raw tokens that are all-uppercase (contain letters and all letters uppercase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        letters = [c for c in t if c.isalpha()]\n        if letters and all(c.isupper() for c in letters) and len(t) > 1:\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that contain both letters and digits (alphanumeric mix)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        has_digit = any(c.isdigit() for c in t)\n        has_alpha = any(c.isalpha() for c in t)\n        if has_digit and has_alpha:\n            count += 1\n    return float(count / len(tokens))\n",
  "def feature(text: str) -> float:\n    'Average longest repeated-character run length normalized by token length'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    total = 0.0\n    for t in tokens:\n        max_run = 1\n        cur = 1\n        for i in range(1, len(t)):\n            if t[i] == t[i-1]:\n                cur += 1\n                if cur > max_run:\n                    max_run = cur\n            else:\n                cur = 1\n        total += (max_run / len(t)) if len(t) > 0 else 0.0\n    return float(total / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are emoticons or contain emoji-like characters'\n    import re\n    if not text:\n        return 0.0\n    emoticons = {':)', ':-)', ':(', ':-(', ':D', ':-D', ':P', ':-P', ';)', ';-)', ':/', ':-/', \":'(\", ':|'}\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if t in emoticons:\n            count += 1\n            continue\n        for ch in t:\n            oc = ord(ch)\n            if oc >= 0x1F300 and oc <= 0x1FAFF:  # Emoji / pictograph ranges (approx)\n                count += 1\n                break\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words that are capitalized but not sentence-initial (internal titlecase)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    total_words = 0\n    cap_internal = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if not words:\n            continue\n        for i, w in enumerate(words):\n            total_words += 1\n            if i > 0 and w and w[0].isupper():\n                cap_internal += 1\n    if total_words == 0:\n        return 0.0\n    return float(cap_internal / total_words)\n\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with ?)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q_count / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of very short sentences (3 or fewer words)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 3 and len(words) > 0:\n            short += 1\n    return float(short / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like contractions (letters apostrophe letters)'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r\"\\b[a-zA-Z]+['\u2019][a-zA-Z]+\\b\", text)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    return float(len(matches) / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that mix letters and digits (alphanumeric mixed tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    mixed = 0\n    for t in tokens:\n        has_digit = any(c.isdigit() for c in t)\n        has_alpha = any(c.isalpha() for c in t)\n        if has_digit and has_alpha:\n            mixed += 1\n    return float(mixed / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Average vowel fraction per word (vowels / letters) averaged across words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    vowels = set('aeiouy')\n    total = 0.0\n    for w in words:\n        v = sum(1 for c in w if c.isalpha() and c in vowels)\n        cons = sum(1 for c in w if c.isalpha() and c not in vowels)\n        letters = v + cons\n        total += (v / letters) if letters > 0 else 0.0\n    return float(total / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = len(sentences)\n    if sent_count == 0:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count / sent_count)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: ratio of unique word tokens to total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    return float(len(set(tokens)) / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are written in ALL CAPS (contains a letter and is uppercase)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(0.0)\n    caps = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t) and t.upper() == t:\n            caps += 1\n    return float(caps / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens in titlecase (first letter uppercase, rest lowercase)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(0.0)\n    title = 0\n    for t in tokens:\n        if len(t) == 1 and t[0].isupper():\n            title += 1\n        elif len(t) > 1 and t[0].isupper() and t[1:].islower():\n            title += 1\n    return float(title / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that consist only of punctuation characters'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    punct_only = 0\n    for tok in tokens:\n        if tok and all(not c.isalnum() for c in tok):\n            punct_only += 1\n    return float(punct_only / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average length of repeated punctuation runs (e.g., \"!!!\", \"??\") considering runs length>1'\n    import re\n    if not text:\n        return float(0.0)\n    runs = [m.group(0) for m in re.finditer(r'([^\\w\\s])\\1*', text)]\n    long_runs = [len(r) for r in runs if len(r) > 1]\n    if not long_runs:\n        return float(0.0)\n    return float(sum(long_runs) / len(long_runs))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with a question mark)'\n    import re\n    if not text:\n        return float(0.0)\n    # split into sentence-like segments\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    questions = sum(1 for s in sentences if s.endswith('?'))\n    return float(questions / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or common domain references'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    url_pattern = re.compile(r'https?://|www\\.|\\.com\\b|\\.net\\b|\\.org\\b|\\.io\\b|\\.gov\\b|\\.edu\\b', re.IGNORECASE)\n    urls = sum(1 for t in tokens if url_pattern.search(t))\n    return float(urls / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average vowel proportion per token (vowels / alphabetic chars), averaged across tokens with letters'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    totals = 0.0\n    count = 0\n    vowels = set('aeiouy')\n    for t in tokens:\n        letters = [c for c in t if c.isalpha()]\n        if not letters:\n            continue\n        v = sum(1 for c in letters if c in vowels)\n        totals += (v / len(letters))\n        count += 1\n    if count == 0:\n        return float(0.0)\n    return float(totals / count)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of short tokens (word tokens shorter than 3 characters)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(0.0)\n    short = sum(1 for t in tokens if len(t) < 3)\n    return float(short / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Density of bracket/parenthesis characters: fraction of characters that are ()[]{}'\n    if not text:\n        return float(0.0)\n    br = sum(1 for c in text if c in '()[]{}')\n    return float(br / len(text))\n",
  "def feature(text: str) -> float:\n    'Density of emoticons and emoji characters in the text'\n    import re\n    if not text:\n        return float(0.0)\n    # common ASCII emoticons\n    emoticons = re.findall(r'[:;=8][\\-~]?[)D\\]pP/(\\\\]|<3', text)\n    # count characters in common emoji ranges (simple heuristic)\n    emoji_count = sum(1 for c in text if ord(c) >= 0x1F300 and ord(c) <= 0x1FAFF)\n    total_chars = len(text)\n    if total_chars == 0:\n        return float(0.0)\n    return float((len(emoticons) + emoji_count) / total_chars)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric-like (digits, currency, percents)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    num_re = re.compile(r'^[\\d\\.,%+\\-]+$')\n    cur_re = re.compile(r'^[\u00a3$\u20ac]\\s?[\\d\\.,]+$')\n    count = 0\n    for t in tokens:\n        if num_re.match(t) or cur_re.match(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words'\n    import re, math\n    s = text.strip()\n    if not s:\n        return float(0.0)\n    sentences = [seg.strip() for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if len(sentences) <= 1:\n        return float(0.0)\n    lengths = [len(re.findall(r'\\w+', sent)) for sent in sentences]\n    n = len(lengths)\n    mean = sum(lengths) / n\n    if mean == 0:\n        return float(0.0)\n    # sample standard deviation (n-1) for stability when n>1\n    var = sum((x - mean) ** 2 for x in lengths) / (n - 1) if n > 1 else 0.0\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\")'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return float(0.0)\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i - 1])\n    return float(repeats / max(1, (len(words) - 1)))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are capitalized but not at the start of a sentence'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [seg.strip() for seg in re.split(r'(?<=[.!?])\\s+', text.strip()) if seg.strip()]\n    if not sentences:\n        return float(0.0)\n    cap_count = 0\n    mid_tokens = 0\n    for sent in sentences:\n        tokens = re.findall(r'\\S+', sent)\n        if len(tokens) <= 1:\n            continue\n        for t in tokens[1:]:\n            mid_tokens += 1\n            first = t[0]\n            if first.isalpha() and first.isupper():\n                cap_count += 1\n    if mid_tokens == 0:\n        return float(0.0)\n    return float(cap_count / mid_tokens)\n\n",
  "def feature(text: str) -> float:\n    'Character-level Shannon entropy normalized by alphabet size'\n    import math\n    if not text:\n        return float(0.0)\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return float(0.0)\n    freq = {}\n    for c in chars:\n        freq[c] = freq.get(c, 0) + 1\n    n = float(len(chars))\n    ent = 0.0\n    for v in freq.values():\n        p = v / n\n        ent -= p * math.log2(p)\n    unique = len(freq)\n    if unique <= 1:\n        return float(0.0)\n    norm = ent / math.log2(unique)\n    return float(norm)\n\n",
  "def feature(text: str) -> float:\n    'Number of punctuation characters per word (punctuation-to-word ratio)'\n    import re\n    if not text:\n        return float(0.0)\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words = len(re.findall(r'\\w+', text))\n    if words == 0:\n        return float(0.0)\n    return float(punct / words)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing an apostrophe (contractions/possessives)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if \"'\" in t)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words with letter elongation (same letter repeated 3+ times)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    elongated = sum(1 for w in words if re.search(r'(.)\\1{2,}', w))\n    return float(elongated / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Ratio of paragraphs to sentences (paragraph density)'\n    import re\n    if not text:\n        return float(0.0)\n    paragraphs = [p for p in re.split(r'\\n{2,}', text.strip()) if p.strip()]\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    return float(len(paragraphs) / max(1, len(sentences)))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    types = set(words)\n    return float(len(types) / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    stopwords = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that contain at least one digit'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are likely emoticons or contain emoji characters'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    emoticon_re = re.compile(r'[:;=8][\\-\\^]?[)\\(DPp\\\\/]|<3')\n    def has_emoji_char(s):\n        for ch in s:\n            oc = ord(ch)\n            if (0x1F300 <= oc <= 0x1F5FF) or (0x1F600 <= oc <= 0x1F64F) or (0x1F680 <= oc <= 0x1F6FF) or (0x2600 <= oc <= 0x26FF) or (0x2700 <= oc <= 0x27BF):\n                return True\n        return False\n    count = 0\n    for t in tokens:\n        if emoticon_re.search(t) or has_emoji_char(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence'\n    import re\n    s = text.strip()\n    if not s:\n        return float(0.0)\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        return float(0.0)\n    totals = sum(sent.count(',') for sent in sentences)\n    return float(totals / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with a question mark)'\n    import re\n    s = text.strip()\n    if not s:\n        return float(0.0)\n    sentences = [seg.strip() for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        return float(0.0)\n    q = sum(1 for sent in sentences if sent.endswith('?'))\n    return float(q / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens with elongated letter sequences (three or more repeats)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    elongated_re = re.compile(r'(.)\\1\\1', re.IGNORECASE)\n    count = sum(1 for w in words if elongated_re.search(w))\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or domain mentions'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    url_re = re.compile(r'https?://|www\\.|[a-z0-9.-]+\\.(com|org|net|edu|gov|io|co)\\b', re.IGNORECASE)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are quotation marks or quote-like symbols'\n    if not text:\n        return float(0.0)\n    quote_chars = set('\"\\'`\u201c\u201d\u2018\u2019\u00ab\u00bb')\n    total_len = len(text)\n    if total_len == 0:\n        return float(0.0)\n    qcount = sum(1 for c in text if c in quote_chars)\n    return float(qcount / total_len)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of alphabetic characters that are uppercase (signal of emphasis or shouting)'\n    if not text:\n        return float(0.0)\n    letters = [c for c in text if c.isalpha()]\n    if not letters:\n        return float(0.0)\n    upper = sum(1 for c in letters if c.isupper())\n    return float(upper / len(letters))\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are all-uppercase words (length>=2, indicates \"shouting\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) >= 2 and any(c.isalpha() for c in t) and t.upper() == t:\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric (pure numbers, possibly with commas/dots/signs)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:[.-]\\d+)?$')\n    count = sum(1 for t in tokens if num_re.match(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'(?i)^(https?://|www\\.|ftp://)')\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}$')\n    domain_re = re.compile(r'\\.\\w{2,4}(/|$)')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.match(t) or domain_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences by punctuation followed by whitespace (keeps abbreviations coarse)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        total_words += len(words)\n    if total_words == 0:\n        return 0.0\n    return float(total_words / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.strip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain a hyphen (hyphenated words)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if '-' in t and any(c.isalpha() for c in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens matching common ASCII emoticons or heart \"<3\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    patterns = [\n        r'[:;=8][\\-~]?[)DdpP]',  # :-) :D ;) etc\n        r'[\\)\\(][\\-~]?[:;=8]',  # reversed\n        r'<3', r':-?\\|', r':-?/', r':\\'\\(', r'XD', r'xd'\n    ]\n    combined = re.compile('(?:' + '|'.join(patterns) + r')$')\n    count = 0\n    for t in tokens:\n        if combined.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens containing three or more repeated letters in a row (elongation)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    repeat_re = re.compile(r'([a-zA-Z])\\1\\1+')\n    count = sum(1 for w in words if repeat_re.search(w))\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are common English stopwords (simple list)'\n    import re\n    if not text:\n        return 0.0\n    STOP = {'the','and','is','a','an','of','to','in','that','it','for','on','with','as','was','by','at','from','be','this','are','or','which','but','not','have','has','had','I','you','he','she','they','we','their','them','his','her'}\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in STOP)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain double or angled quotation marks (dialogue/quotes)'\n    import re\n    if not text:\n        return 0.0\n    quotes = {'\"', '\u201c', '\u201d', '\u00ab', '\u00bb'}\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        if any(q in s for q in quotes):\n            count += 1\n    return float(count / len(sentences))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: distinct word forms divided by total word count'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    distinct = len(set(words))\n    result = distinct / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing at least one digit'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of Unicode symbol-like characters (currency/math/other symbols)'\n    import unicodedata\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    sym = 0\n    for c in text:\n        try:\n            cat = unicodedata.category(c)\n        except Exception:\n            cat = ''\n        if cat.startswith('So') or cat.startswith('Sc') or cat.startswith('Sm'):\n            sym += 1\n    result = sym / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length measured in words'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid dividing by zero if a sentence has zero words; count it as zero length\n    result = sum(word_counts) / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Longest consecutive run of non-alphanumeric (punctuation/symbol) characters normalized by text length'\n    if not text:\n        return float(0.0)\n    max_run = 0\n    run = 0\n    for c in text:\n        if not c.isalnum() and not c.isspace():\n            run += 1\n            if run > max_run:\n                max_run = run\n        else:\n            run = 0\n    result = max_run / max(1, len(text))\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that look like dialogue (start with quotes or dashes)'\n    import re\n    if not text:\n        return float(0.0)\n    markers = {'\"', '\u201c', '\u201d', '\u2018', '\u2019', \"'\", '-', '\u2014', '\u2013'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    count = 0\n    for s in sentences:\n        s_stripped = s.lstrip()\n        if not s_stripped:\n            continue\n        if s_stripped[0] in markers:\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences whose first word begins with an uppercase letter'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    count = 0\n    for s in sentences:\n        m = re.search(r'\\b\\w+', s)\n        if m:\n            w = m.group(0)\n            if w and w[0].isupper():\n                count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are all-caps (length > 1)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if len(w) > 1 and w.isupper())\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens matching common ASCII emoticons'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    # common simple emoticon pattern (covers many ASCII emoticons)\n    pattern = re.compile(r'(?i)(?:[:;=8Xx][\\-~]?[)D\\]pP/\\\\\\(])|(?:[)D\\]pP/\\\\\\(][\\-~]?[:;=8Xx])|(?:[:;=]\\'\\()|(?:<3)')\n    matches = pattern.findall(text)\n    result = len(matches) / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Byte-level compression ratio using zlib (compressed_size / original_size)'\n    import zlib\n    if not text:\n        return float(0.0)\n    try:\n        data = text.encode('utf-8')\n    except Exception:\n        data = str(text).encode('utf-8', errors='ignore')\n    orig = len(data)\n    if orig == 0:\n        return float(0.0)\n    comp = zlib.compress(data)\n    result = len(comp) / orig\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    uniq = len(set(words))\n    return float(uniq / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Hapax ratio: fraction of word types that occur exactly once in the text'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    counts = Counter(words)\n    hapax = sum(1 for v in counts.values() if v == 1)\n    return float(hapax / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing a character repeated three or more times in a row (elongation)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'(.)\\1\\1', t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of characters that are symbol/emoji-like (Unicode \"So\" category or common emoji ranges)'\n    import unicodedata\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    sym = 0\n    for c in text:\n        try:\n            cat = unicodedata.category(c)\n        except Exception:\n            cat = ''\n        o = ord(c)\n        if cat == 'So' or 0x1F300 <= o <= 0x1F6FF or 0x1F900 <= o <= 0x1F9FF:\n            sym += 1\n    return float(sym / total)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that look like URLs or domain-like references'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pat = re.compile(r'https?://\\S+|www\\.\\S+|\\b[\\w-]+\\.(com|org|net|io|gov|edu|co)(/|:|$)', re.I)\n    count = sum(1 for t in tokens if pat.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are purely numeric (integers or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d+(\\.\\d+)?$')\n    count = sum(1 for t in tokens if num_re.match(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that contain one or more non-ASCII characters'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        for c in w:\n            if ord(c) > 127:\n                count += 1\n                break\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of sentence lengths (stddev/mean of words per sentence)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is lowercase (informal or poorly capitalized starts)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m and m.group(0).islower():\n            count += 1\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are hyphenated compounds (contain at least one hyphen between word characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    hyphenated = re.findall(r'\\b\\w+(?:-\\w+)+\\b', text)\n    return float(len(hyphenated) / len(words))\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence (mean sentence length in words)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    word_counts = [c for c in word_counts if c > 0]\n    if not word_counts:\n        return 0.0\n    result = sum(word_counts) / len(word_counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (words per sentence)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    counts = [c for c in counts if c > 0]\n    if len(counts) < 2:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    result = math.sqrt(var)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are written in ALL CAPS (length>=2)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if len(w) >= 2 and any(c.isalpha() for c in w) and w.upper() == w:\n            count += 1\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Ratio of unique word types to total word tokens (lexical uniqueness)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens (heuristic list)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','a','to','of','in','is','it','you','that','he','was','for','on','are','as','with','his','they','i','at','be','this','have','from','or','one','had','by','word','but','not','what','all','were','we','when','your','can','said','there','use','an','each','which','she','do','how','their'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stop)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that consist only of punctuation characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if all(not c.isalnum() for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', re.I)\n    email_re = re.compile(r'^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$')\n    domain_like_re = re.compile(r'\\.\\w{2,}(?:/|$)')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t) or domain_like_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of simple emoticons (counts of common ASCII emoticons per character)'\n    if not text:\n        return 0.0\n    emoticons = [':)', ':-)', ':(', ':-(', ':D', ':-D', ';)', ';-)', ':P', ':-P', ':/', ':-/', ':|', '<3', ':o', ':O', ':-O', '^-^']\n    lower = text\n    total = 0\n    for e in emoticons:\n        total += lower.count(e)\n    if len(text) == 0:\n        return 0.0\n    result = total / len(text)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of characters in the text that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent word pairs that are exact repeats (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    pairs = len(words) - 1\n    repeats = sum(1 for i in range(pairs) if words[i] == words[i + 1])\n    result = repeats / pairs\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    word_counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if words:\n            word_counts.append(len(words))\n        else:\n            word_counts.append(0)\n    if not word_counts:\n        return float(0.0)\n    return float(sum(word_counts) / len(word_counts))\n\n",
  "def feature(text: str) -> float:\n    'Standard deviation of word lengths'\n    import re, math\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of non-whitespace tokens that are all-caps (>=2 letters)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    def is_all_caps(t):\n        letters = ''.join([c for c in t if c.isalpha()])\n        return len(letters) >= 2 and letters.upper() == letters\n    count = sum(1 for t in tokens if is_all_caps(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating conjunction (and,but,or,so,yet,for,nor)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    conjunctions = {'and', 'but', 'or', 'so', 'yet', 'for', 'nor'}\n    count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s.lower())\n        if words and words[0] in conjunctions:\n            count += 1\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Density of repeated punctuation sequences (characters like !! or ???) as fraction of total chars'\n    import re\n    if not text:\n        return float(0.0)\n    total_chars = len(text)\n    if total_chars == 0:\n        return float(0.0)\n    runs = [m.group(0) for m in re.finditer(r'([^\\w\\s])\\1{1,}', text)]\n    repeated_chars = sum(len(r) for r in runs)\n    return float(repeated_chars / total_chars)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that contain any non-ASCII character'\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    def has_non_ascii(t):\n        return any(ord(c) > 127 for c in t)\n    count = sum(1 for t in tokens if has_non_ascii(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are hyphenated words (letter-hyphen-letter)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = 0\n    for t in tokens:\n        if re.search(r'[A-Za-z]-[A-Za-z]', t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that consist of a single word'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    single = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) == 1:\n            single += 1\n    return float(single / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Average vowel fraction per word (vowels / letters), averaged across words'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if not words:\n        return float(0.0)\n    vowels = set('aeiou')\n    fractions = []\n    for w in words:\n        letters = [c for c in w if c.isalpha()]\n        if not letters:\n            continue\n        vcount = sum(1 for c in letters if c in vowels)\n        fractions.append(vcount / len(letters))\n    if not fractions:\n        return float(0.0)\n    return float(sum(fractions) / len(fractions))\n",
  "def feature(text: str) -> float:\n    'Proportion of alphabetic tokens that are in ALL CAPS (shouting indicator)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return float(0.0)\n    caps = 0\n    alpha_tokens = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t):\n            alpha_tokens += 1\n            if t.isupper():\n                caps += 1\n    if alpha_tokens == 0:\n        return float(0.0)\n    return float(caps / alpha_tokens)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit (numbers, IDs, dates)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that belong to punctuation runs of length >=2 (e.g., \"!!\", \"...\")'\n    import re\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    runs = 0\n    for m in re.finditer(r'([^\\w\\s])\\1+', text):\n        runs += len(m.group(0))\n    return float(runs / total)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count (lexical diversity)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words that occur only once in the text (hapax legomena ratio)'\n    import re\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    counts = Counter(words)\n    hapax = sum(1 for w, c in counts.items() if c == 1)\n    return float(hapax / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that begin with a lowercase alphabetic character (informal/casual style)'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    lower_start = 0\n    for s in sentences:\n        for ch in s:\n            if ch.isalpha():\n                if ch.islower():\n                    lower_start += 1\n                break\n    return float(lower_start / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are likely emoji or miscellaneous symbol characters'\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    ranges = [\n        (0x1F600, 0x1F64F),  # emoticons\n        (0x1F300, 0x1F5FF),  # symbols & pictographs\n        (0x1F680, 0x1F6FF),  # transport & map\n        (0x2600, 0x26FF),    # miscellaneous symbols\n        (0x2700, 0x27BF),    # dingbats\n        (0x1F900, 0x1F9FF),  # supplemental symbols and pictographs\n    ]\n    def is_emoji(cp):\n        return any(start <= cp <= end for start, end in ranges)\n    count = 0\n    for ch in text:\n        if is_emoji(ord(ch)):\n            count += 1\n    return float(count / total)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are common English stopwords (function-word density)'\n    import re\n    if not text:\n        return float(0.0)\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with',\n        'he','as','you','do','at','this','but','his','by','from','they','we','say','her',\n        'she','or','an','will','my','one','all','would','there','their','what','so','up',\n        'out','if','about','who','get','which','go','me'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    stop_count = sum(1 for w in words if w in stopwords)\n    return float(stop_count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that contain at least one numeric token'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    with_num = 0\n    for s in sentences:\n        if re.search(r'\\d', s):\n            with_num += 1\n    return float(with_num / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Normalized character entropy (Shannon entropy divided by log2(unique_chars)) in [0,1]'\n    import math\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    counts = Counter(text)\n    unique = len(counts)\n    if unique <= 1:\n        return float(0.0)\n    total = sum(counts.values())\n    ent = 0.0\n    for c, cnt in counts.items():\n        p = cnt / total\n        ent -= p * math.log2(p)\n    norm = ent / math.log2(unique)\n    return float(norm)\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    if not sents:\n        # treat the whole text as one sentence\n        return float(len(words))\n    lengths = []\n    for s in sents:\n        wl = re.findall(r'\\w+', s)\n        if wl:\n            lengths.append(len(wl))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths) / len(lengths))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with ?)'\n    import re\n    if not text:\n        return 0.0\n    # count sentence boundaries by punctuation\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        return 0.0\n    q_count = sum(1 for s in sents if s.rstrip().endswith('?'))\n    return float(q_count / len(sents))\n\n",
  "def feature(text: str) -> float:\n    'Estimated average syllables per word using vowel-group heuristic'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    total = 0\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        syl = len(groups)\n        # adjust for silent e endings (simple heuristic)\n        if syl > 1 and w.lower().endswith('e'):\n            syl -= 1\n        if syl < 1:\n            syl = 1\n        total += syl\n    return float(total / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (possible emojis/foreign chars)'\n    if not text:\n        return 0.0\n    n = len(text)\n    if n == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / n)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL CAPS (length>=2 to avoid single-letter tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    caps = 0\n    for t in tokens:\n        stripped = t.strip(\"()[]{}\\\"'`.,:;!?\")\n        if len(stripped) >= 2 and stripped.isalpha() and stripped.upper() == stripped:\n            caps += 1\n    return float(caps / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric-like (dates, numbers, amounts)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'^[\\d\\.,:/+-]+$')\n    num = sum(1 for t in tokens if pattern.match(t))\n    return float(num / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing long repeated punctuation sequences (e.g., !!! or ???)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    rep = 0\n    pat = re.compile(r'([^\\w\\s])\\1{2,}')\n    for t in tokens:\n        if pat.search(t):\n            rep += 1\n    return float(rep / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating conjunction (and, but, or, so, because, then)'\n    import re\n    if not text:\n        return 0.0\n    conj = {'and', 'but', 'or', 'so', 'because', 'then', 'also', 'yet'}\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        return 0.0\n    count = 0\n    for s in sents:\n        m = re.match(r'^[\\'\"(\\[]*\\s*([A-Za-z]+)', s)\n        if m:\n            if m.group(1).lower() in conj:\n                count += 1\n    return float(count / len(sents))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters contained inside parentheses/brackets/braces'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    if total_len == 0:\n        return 0.0\n    inside = 0\n    for pat in (r'\\((.*?)\\)', r'\\[(.*?)\\]', r'\\{(.*?)\\}'):\n        for m in re.findall(pat, text, flags=re.S):\n            inside += len(m)\n    return float(min(1.0, inside / total_len))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are content words (not common stopwords)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','is','in','it','to','of','a','an','that','this','for','on','with','as','are','was','were','be','by','or','at','from','but','not','they','their','I','you','we','he','she','his','her'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    content = sum(1 for w in words if w not in stop)\n    return float(content / len(words))\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        total_words += len(words)\n    return float(total_words / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    types = set(tokens)\n    return float(len(types) / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are all-uppercase words of length >=2 (shouting/acronyms)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) >= 2 and any(c.isalpha() for c in t):\n            if all((not c.isalpha()) or c.isupper() for c in t):\n                count += 1\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs (http/www or domain-like patterns)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(?:https?://|www\\.)', flags=re.I)\n    domain_re = re.compile(r'\\w+\\.\\w{2,}', flags=re.I)\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or domain_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are email-like (contain \"@\" and a domain part)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if '@' in t:\n            parts = t.split('@', 1)\n            if len(parts) == 2 and '.' in parts[1] and parts[0]:\n                count += 1\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric (integers or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d+(\\.\\d+)?$')\n    count = sum(1 for t in tokens if num_re.match(t))\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Longest consecutive run of punctuation characters normalized by text length'\n    import re\n    if not text:\n        return 0.0\n    # punctuation = any non-alphanumeric, non-space\n    runs = re.findall(r'[^0-9A-Za-z\\s]+', text)\n    if not runs:\n        return 0.0\n    longest = max(len(r) for r in runs)\n    denom = max(1, len(text))\n    return float(longest / denom)\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of adjacent repeated tokens (e.g., \"the the\") indicating stuttering/repetition'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    repeats = 0\n    pairs = 0\n    for i in range(len(tokens) - 1):\n        pairs += 1\n        if tokens[i] == tokens[i + 1]:\n            repeats += 1\n    return float(repeats / pairs)\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with \"?\")'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common function words (small stopword list) among tokens'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stop)\n    return float(count / len(tokens))\n",
  "def feature(text: str) -> float:\n    'Average number of vowel groups per word (simple syllable proxy)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    total = 0\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        total += max(1, len(groups))  # treat words with no vowel group as 1\n    result = total / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are questions'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q_count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<= 5 words)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 5:\n            short += 1\n    result = short / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized Shannon entropy of non-space characters (0-1)'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    freqs = Counter(chars)\n    total = len(chars)\n    probs = [v / total for v in freqs.values()]\n    entropy = -sum(p * math.log2(p) for p in probs if p > 0)\n    if len(freqs) <= 1:\n        return 0.0\n    max_entropy = math.log2(len(freqs))\n    result = entropy / max_entropy\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one numeric digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = num / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of long repeated punctuation sequences (3+ same punctuation) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    denom = len(tokens) if tokens else max(1, len(text))\n    matches = re.findall(r\"([!?.,;:~`*_'\\-\\\"])\\1{2,}\", text)\n    result = len(matches) / denom\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Ratio of hapax legomena (words that occur only once) to total tokens'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    hapax = sum(1 for _, c in counts.items() if c == 1)\n    result = hapax / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain explicit quotes (double/curly/angle quotes)'\n    import re\n    if not text:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', '\u00ab', '\u00bb')\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    qsent = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            qsent += 1\n    result = qsent / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are emoticons or likely emoji'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticon_re = re.compile(r'(:-\\)|:\\)|:-\\(|:\\(|:D|:-D|;-\\)|;\\)|:P|:-P|:\\'\\(|:\\*|<3|:-O|:O)', re.I)\n    count = 0\n    for t in tokens:\n        if emoticon_re.search(t):\n            count += 1\n            continue\n        for ch in t:\n            oc = ord(ch)\n            # common emoji/unicode pictograph ranges heuristic\n            if oc >= 0x1F300 and oc <= 0x1F6FF:\n                count += 1\n                break\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences matching a simple passive-voice pattern (was/were/... + past-tense)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    pattern = re.compile(r'\\b(was|were|is|are|been|being|be)\\b\\s+\\w+ed\\b', re.I)\n    passive = sum(1 for s in sentences if pattern.search(s))\n    result = passive / len(sentences)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    words_total = len(re.findall(r'\\w+', text))\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if sentences:\n        per_sentence = [len(re.findall(r'\\w+', s)) for s in sentences]\n        # avoid division by zero if some splits produce zero\n        per_sentence = [p for p in per_sentence if p > 0]\n        if not per_sentence:\n            result = 0.0\n        else:\n            result = sum(per_sentence) / len(per_sentence)\n    else:\n        # no clear sentence boundaries; treat whole text as one sentence if words exist\n        result = float(words_total) if words_total > 0 else 0.0\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of short words (<4 characters) among all words'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    short = sum(1 for w in words if len(w) < 4)\n    result = short / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of non-whitespace tokens that look like URLs, emails, or domain-like tokens'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\b\\S+@\\S+\\b|\\b\\w+\\.(?:com|org|net|edu|gov|io|co|me|info)\\b', re.I)\n    matches = sum(1 for t in tokens if pattern.search(t))\n    result = matches / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of characters that are non-ASCII'\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized character Shannon entropy (over non-space characters, 0..1)'\n    import math\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return float(0.0)\n    cnt = Counter(chars)\n    total = sum(cnt.values())\n    ent = 0.0\n    for f in cnt.values():\n        p = f / total\n        ent -= p * math.log2(p)\n    # normalize by log2(number of distinct chars) to bring to 0..1\n    distinct = len(cnt)\n    if distinct <= 1:\n        result = 0.0\n    else:\n        result = ent / math.log2(distinct)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences ending with a question mark'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    q = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average per-token punctuation ratio (punctuation chars divided by token length), averaged across tokens'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    totals = 0.0\n    count = 0\n    for t in tokens:\n        L = len(t)\n        if L == 0:\n            continue\n        punct = sum(1 for c in t if not c.isalnum())\n        totals += (punct / L)\n        count += 1\n    if count == 0:\n        return float(0.0)\n    result = totals / count\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are titlecase (First letter uppercase, rest lowercase) including single-letter \"I\"'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    def is_title(w):\n        if not w:\n            return False\n        if len(w) == 1:\n            return w.isupper()\n        return w[0].isupper() and w[1:].islower()\n    count = sum(1 for w in words if is_title(w))\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of modal verbs (can,could,may,might,must,should,would,shall,will) among words'\n    import re\n    if not text:\n        return float(0.0)\n    MODALS = {'can','could','may','might','must','should','would','shall','will'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if w in MODALS)\n    result = count / len(words)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average sentence length measured in words (words per sentence)'\n    import re\n    if not text:\n        return float(0.0)\n    # split into sentences by punctuation followed by whitespace or linebreak\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid zero-division and ignore empty sentences\n    counts = [c for c in word_counts if c > 0]\n    if not counts:\n        return float(0.0)\n    result = sum(counts) / len(counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are common English stopwords (approximate)'\n    import re\n    if not text:\n        return float(0.0)\n    stopwords = {\n        'the','and','is','in','it','you','that','he','was','for','on','are','with',\n        'as','I','his','they','be','at','one','have','this','from','or','had','by',\n        'not','word','but','what','some','we','can','out','other','were','all','there',\n        'when','up','use','your','how','said','an','each','she'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if t in stopwords)\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens in titlecase (initial capital followed by lowercase)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return float(0.0)\n    count = 0\n    for t in tokens:\n        if len(t) >= 2 and t[0].isupper() and t[1:].islower():\n            count += 1\n        elif len(t) == 1 and t.isupper():\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing a run of the same character repeated 3+ times'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    pattern = re.compile(r'(.)\\1{2,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (useful for detecting foreign scripts/emojis)'\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average proportion of digits inside tokens (mean per-token digit density)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    densities = []\n    for t in tokens:\n        L = len(t)\n        if L == 0:\n            densities.append(0.0)\n        else:\n            digits = sum(1 for c in t if c.isdigit())\n            densities.append(digits / L)\n    result = sum(densities) / len(densities)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized capitalization imbalance: abs(upper - lower) / total_letters'\n    if not text:\n        return float(0.0)\n    upp = sum(1 for c in text if c.isupper())\n    low = sum(1 for c in text if c.islower())\n    total_letters = upp + low\n    if total_letters == 0:\n        return float(0.0)\n    result = abs(upp - low) / total_letters\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of hapax legomena (tokens that occur exactly once)'\n    import re\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    cnt = Counter(tokens)\n    hapax = sum(1 for t, c in cnt.items() if c == 1)\n    result = hapax / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark (question density)'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    qcount = sum(1 for s in sentences if s.endswith('?'))\n    result = qcount / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters contained inside single or double quotes'\n    import re\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    # find both \"...\" and '...' occurrences (non-greedy)\n    matches = re.findall(r'\"(.*?)\"|\\'(.*?)\\'', text, flags=re.S)\n    inside = 0\n    for a, b in matches:\n        if a:\n            inside += len(a)\n        if b:\n            inside += len(b)\n    result = min(1.0, inside / total)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    stops = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there','their'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if w in stops)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens in Titlecase (starts uppercase, not all-caps)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return float(0.0)\n    count = 0\n    for t in tokens:\n        if t[0].isupper() and not t.isupper():\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Normalized Shannon entropy of character distribution (0-1)'\n    import math\n    if not text:\n        return float(0.0)\n    freq = {}\n    for ch in text:\n        freq[ch] = freq.get(ch, 0) + 1\n    total = sum(freq.values())\n    if total == 0:\n        return float(0.0)\n    entropy = 0.0\n    for v in freq.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    unique = len(freq)\n    if unique <= 1:\n        return float(0.0)\n    denom = math.log2(unique)\n    if denom <= 0:\n        return float(0.0)\n    return float(entropy / denom)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    url_re = re.compile(r'^(?:https?://|www\\.)', flags=re.I)\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+', flags=re.I)\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t) or ('@' in t and '.' in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens longer than 12 characters (long-word density)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    long_count = sum(1 for w in words if len(w) > 12)\n    return float(long_count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing repeated punctuation sequences (e.g., !!!, ...)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    rep_re = re.compile(r'([^\\w\\s])\\1{1,}')  # same non-word non-space char repeated at least twice\n    count = sum(1 for t in tokens if rep_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter (informal/fragment style)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    count = 0\n    for s in sentences:\n        s2 = s.lstrip(' \"\\'\u201c\u201d\u2018\u2019(')\n        if s2 and s2[0].islower():\n            count += 1\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens ending with common adjective/adverb suffixes (-ly, -ive, -ous, etc.)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    suffixes = ('ly','ive','ous','able','ible','al','ful','less','ic','ish','ant','ent')\n    count = 0\n    for w in words:\n        for sfx in suffixes:\n            if w.endswith(sfx) and len(w) > len(sfx) + 1:\n                count += 1\n                break\n    return float(count / len(words))\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that contain at least one digit'\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (simple list)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    stopwords = {'the','and','is','in','it','of','to','a','that','i','you','was','for','on','with','as','are','this','be','at','or','by','an','have','not'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total words'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words that occur only once (hapax legomena ratio)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    freq = {}\n    for w in words:\n        freq[w] = freq.get(w, 0) + 1\n    hapax = sum(1 for v in freq.values() if v == 1)\n    return float(hapax / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of punctuation characters that are part of repeated punctuation runs (e.g., \"!!!\" or \"??\")'\n    import re\n    if not text:\n        return float(0.0)\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return float(0.0)\n    repeated_len = 0\n    for m in re.finditer(r'([^\\w\\s])\\1+', text):\n        repeated_len += len(m.group(0))\n    return float(repeated_len / total_punct)\n\n",
  "def feature(text: str) -> float:\n    'Density of common ASCII emoticons per token (e.g., :-) :D ;))'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    emot_re = re.compile(r'(?:(?:[:;=8][-^]?[)DpP\\(/\\\\]|<3|:-?\\|))', flags=re.I)\n    matches = emot_re.findall(text)\n    return float(len(matches) / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Simple lexicon sentiment score normalized by token count: (pos_count - neg_count) / tokens'\n    import re\n    if not text:\n        return float(0.0)\n    pos = {'good','great','excellent','happy','love','nice','fortunate','pleasant','enjoy','awesome','best'}\n    neg = {'bad','poor','sad','hate','terrible','awful','worst','unfortunate','angry','problem','fail'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    pos_count = sum(1 for w in words if w in pos)\n    neg_count = sum(1 for w in words if w in neg)\n    return float((pos_count - neg_count) / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Ratio of contractions (e.g., \"don\\'t\", \"you\\'re\") to total words as an informality signal'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    contractions = re.findall(r\"\\b\\w+'(?:t|re|ve|ll|d|s)\\b\", text.lower())\n    return float(len(contractions) / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are titlecase (Capitalized words like \"London\" or \"Alice\")'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(0.0)\n    titlecase = sum(1 for t in tokens if len(t) > 1 and t[0].isupper() and t[1:].islower())\n    return float(titlecase / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Density of explicit URLs (http(s) or www) per whitespace token'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    urls = re.findall(r'(https?://[^\\s]+|www\\.[^\\s]+)', text, flags=re.I)\n    return float(len(urls) / len(tokens))\n",
  "def feature(text: str) -> float:\n    'Lexical diversity: ratio of unique word forms to total words'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of words that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    counts = Counter(words)\n    hapax = sum(1 for w, c in counts.items() if c == 1)\n    result = hapax / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are in ALL CAPS (words of length>=2)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return float(0.0)\n    caps = sum(1 for t in tokens if any(c.isalpha() for c in t) and len([c for c in t if c.isalpha()])>=2 and t.isupper())\n    result = caps / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    def looks_like_link(tok):\n        t = tok.lower().strip('.,;:!?)(\"\\'')\n        if 'http' in t or t.startswith('www.') or '@' in t:\n            return True\n        for suf in ('.com', '.org', '.net', '.edu', '.gov', '.io', '.co'):\n            if t.endswith(suf):\n                return True\n        return False\n    count = sum(1 for t in tokens if looks_like_link(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of long repeated punctuation sequences (count of char repeats>=3 per character)'\n    import re\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    # match any non-word non-space char repeated at least 3 times\n    repeats = re.findall(r'([^\\w\\s])\\1\\1+', text)\n    count = len(repeats)\n    result = count / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q_count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a lowercase letter (informal/lax capitalization)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    low_start = 0\n    for s in sentences:\n        for ch in s:\n            if ch.isspace():\n                continue\n            if ch.isalpha():\n                if ch.islower():\n                    low_start += 1\n                break\n            else:\n                break\n    result = low_start / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause density)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    comma_count = text.count(',')\n    result = comma_count / max(1, len(sentences))\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are purely numeric (digits-only tokens)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    num = sum(1 for t in tokens if t.isdigit())\n    result = num / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Emoticon density: frequency of common ASCII emoticons per character'\n    import re\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    # common emoticon patterns like :-) :) :D :( :P ; ) XD etc.\n    emoticons = re.findall(r'(?:(?:[:;=8Xx][-^]?[)DdpP\\/\\\\]|[)DdpP\\/\\\\][-^]?[:;=8Xx]|:\\'\\(|:\\-\\||:O|:o))', text)\n    count = len(emoticons)\n    result = count / total\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lens = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lens.append(len(words))\n    if not lens:\n        return 0.0\n    result = sum(lens) / len(lens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    types = len(set(tokens))\n    result = types / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or web addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'https?://|www\\.|[a-z0-9\\-]+\\.(com|net|org|edu|gov|io)(/|$)', re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$')\n    count = sum(1 for t in tokens if email_re.match(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    qcount = sum(1 for s in sentences if s.endswith('?'))\n    result = qcount / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens with character elongation (3+ repeated chars)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    elong_re = re.compile(r'(.)\\1\\1')\n    count = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t) and elong_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are punctuation-only (no alphanumeric characters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    punct_only = sum(1 for t in tokens if all(not c.isalnum() for c in t))\n    result = punct_only / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average estimated syllables per word (simple vowel-group heuristic)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    total = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        syl = max(1, len(groups))\n        # heuristic: trailing silent e often reduces syllable count\n        if w.endswith('e') and len(groups) > 1:\n            syl = max(1, syl - 1)\n        total += syl\n    result = total / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a common coordinating/subordinating conjunction'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = {'and', 'but', 'or', 'so', 'because', 'however', 'then', 'thus', 'yet', 'although', 'since'}\n    count = 0\n    for s in sentences:\n        m = re.findall(r'\\w+', s)\n        if m and m[0].lower() in starts:\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing emoji/high-plane unicode characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def has_emoji(tok):\n        for c in tok:\n            o = ord(c)\n            # common emoji/unicode pictograph ranges\n            if 0x1F300 <= o <= 0x1FAFF or 0x1F600 <= o <= 0x1F64F or 0x1F680 <= o <= 0x1F6FF:\n                return True\n        return False\n    count = sum(1 for t in tokens if has_emoji(t))\n    result = count / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    total_words = sum(word_counts)\n    if total_words == 0:\n        return 0.0\n    result = total_words / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word tokens / total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    result = unique / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio (tokens that occur exactly once / total tokens)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    hapax = sum(1 for t, c in counts.items() if c == 1)\n    result = hapax / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL-CAPS words (2+ letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        return len(letters) >= 2 and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of URLs or email-like tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|ftp://|www\\.)', re.I)\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n    domain_re = re.compile(r'\\b[\\w.-]+\\.(com|org|net|io|gov|edu|co|us|uk|de|fr|ru)\\b', re.I)\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.match(t) or domain_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','to','of','a','an','that','it','for','on','with','as','are','was','were','be','by','this','i','you','he','she','they','we','not','or','but','from','at','his','her','their','which','do','does','did','have','has','had','will','can'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of list-like lines (lines starting with bullets or numbered markers)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    marker_re = re.compile(r'^\\s*(?:[-\\*\\+]|[0-9]+[.)])\\s+')\n    count = sum(1 for l in lines if marker_re.match(l))\n    result = count / len(lines)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not sentences:\n        return float(len(words)) if words else 0.0\n    total_words = 0\n    for s in sentences:\n        total_words += len(re.findall(r'\\w+', s))\n    return float(total_words / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word forms divided by total words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with \"?\")'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    qcount = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(qcount / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Dominance of the most frequent word (max word frequency divided by total words)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    most = freqs.most_common(1)[0][1]\n    return float(most / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', flags=re.I)\n    email_re = re.compile(r'^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Density of emoji-like characters and common emoticons per character'\n    import re, unicodedata\n    if not text:\n        return 0.0\n    # common ASCII emoticons\n    emoticons = re.findall(r'(?:(?:[:;=8][\\-~]?[)D\\(\\]/\\\\OpP]))', text)\n    emot_count = len(emoticons)\n    # count symbols in Unicode category \"So\" (Symbol, other) as emoji-like\n    emoji_count = 0\n    for c in text:\n        try:\n            if unicodedata.category(c) == 'So':\n                emoji_count += 1\n        except Exception:\n            continue\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float((emot_count + emoji_count) / total_chars)\n\n\n",
  "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct / len(puncts))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that contain a repeated character sequence (e.g., \"sooo\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if re.search(r'(.)\\1\\1', w):\n            count += 1\n    return float(count / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Average fraction of characters in each token that are punctuation (token-level punctuation density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    total_frac = 0.0\n    valid_tokens = 0\n    for t in tokens:\n        if not t:\n            continue\n        punct = sum(1 for c in t if not c.isalnum() and not c.isspace())\n        total_frac += (punct / len(t))\n        valid_tokens += 1\n    if valid_tokens == 0:\n        return 0.0\n    return float(total_frac / valid_tokens)\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating conjunction (e.g., \"and\", \"but\", \"so\")'\n    import re\n    if not text:\n        return 0.0\n    CONJ = {'and', 'but', 'or', 'so', 'yet', 'for', 'nor', 'however', 'therefore'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        m = re.match(r'\\s*([A-Za-z\\'-]+)', s)\n        if m and m.group(1).lower() in CONJ:\n            starts += 1\n    return float(starts / len(sentences))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total words'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid division by zero if any sentence has no words\n    word_counts = [wc for wc in word_counts if wc > 0]\n    if not word_counts:\n        return float(0.0)\n    result = sum(word_counts) / len(word_counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are questions'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    q_count = sum(1 for s in sentences if s.strip().endswith('?'))\n    result = q_count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are emoticons or emoji-like'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    emoticon_re = re.compile(r'^(?:[:;=8][\\-^]?[)DdpP\\]\\(\\/\\\\]|[)DdpP\\]\\(\\/\\\\][\\-^]?[:;=8])$')\n    count = 0\n    for tok in tokens:\n        t = tok.strip('.,;:!?)(\"\\'')\n        if not t:\n            continue\n        if emoticon_re.match(t):\n            count += 1\n            continue\n        # treat characters in common emoji/unicode ranges as emoji\n        if any(ord(c) >= 0x1F300 for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Ratio of words in ALL CAPS (length >=2) to total words'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    caps = sum(1 for w in words if len(w) >= 2 and w.isalpha() and w.isupper())\n    result = caps / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like list items (bullets or numbered)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return float(0.0)\n    list_re = re.compile(r'^\\s*(?:[-\\*\\u2022]|\\d+\\.)\\s+')\n    count = sum(1 for l in lines if list_re.match(l))\n    result = count / len(lines)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average estimated syllables per word (simple vowel-group heuristic)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    totals = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        syll = max(1, len(groups))\n        # simple adjustment for trailing silent e\n        if w.endswith('e') and syll > 1 and not w.endswith(('le', 'ye')):\n            syll -= 1\n        totals += syll\n    result = totals / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among all words'\n    import re\n    if not text:\n        return float(0.0)\n    STOP = {'the','and','is','in','it','of','to','a','that','for','on','with','as','are','was','be','by','this','an','or','from','at','but','not','have'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if w in STOP)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are very short (3 words or fewer)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    short = 0\n    for s in sentences:\n        wc = len(re.findall(r'\\w+', s))\n        if wc <= 3:\n            short += 1\n    result = short / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens containing non-ASCII characters (ord>127)'\n    if not text:\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    count = 0\n    for t in tokens:\n        if any(ord(c) > 127 for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        words = re.findall(r'\\w+', s)\n        return float(len(words) / max(1, 1)) if words else 0.0\n    total_words = 0\n    for sent in sentences:\n        total_words += len(re.findall(r'\\w+', sent))\n    return float(total_words / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word forms / total words)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are fully uppercase (shouting-like)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # require at least one letter and more than one character to avoid single-letter caps\n        if any(c.isalpha() for c in t) and len(t) > 1 and t.isupper():\n            count += 1\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence (approximate clause density)'\n    import re\n    if not text:\n        return 0.0\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    comma_count = s.count(',')\n    if not sentences:\n        # if no clear sentences, normalize by 1 to give overall density\n        return float(comma_count / 1)\n    return float(comma_count / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ordinal > 127)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are inside HTML/XML-like tags'\n    import re\n    if not text:\n        return 0.0\n    tags = re.findall(r'<[^>]+>', text)\n    if not tags:\n        return 0.0\n    total_len = len(text)\n    if total_len == 0:\n        return 0.0\n    inside_len = sum(len(m) for m in tags)\n    return float(min(1.0, inside_len / total_len))\n\n\n",
  "def feature(text: str) -> float:\n    'Population variance of word lengths (variance of token lengths)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    n = len(lengths)\n    if n < 2:\n        return 0.0\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    return float(var)\n\n\n",
  "def feature(text: str) -> float:\n    'Density of repeated punctuation runs (total run chars / text length) for runs of length>=2'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    if total_len == 0:\n        return 0.0\n    runs = re.findall(r'([^\\w\\s]{2,})', text)\n    if not runs:\n        return 0.0\n    run_chars = sum(len(r) for r in runs)\n    return float(run_chars / total_len)\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that start with a digit (e.g., \"3rd\", \"2020\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t and t[0].isdigit())\n    return float(count / len(tokens))\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts) / len(word_counts))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are questions'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.endswith('?'))\n    return float(q_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that look like URLs'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)\\S+', flags=re.I)\n    count = sum(1 for t in tokens if url_re.match(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing a character repeated three or more times consecutively'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    repeat_re = re.compile(r'(.)\\1{2,}')\n    count = sum(1 for t in tokens if repeat_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Normalized Shannon entropy of the character distribution (0-1)'\n    import math\n    if not text:\n        return 0.0\n    chars = list(text)\n    n = len(chars)\n    if n == 0:\n        return 0.0\n    freq = {}\n    for c in chars:\n        freq[c] = freq.get(c, 0) + 1\n    probs = [v / n for v in freq.values()]\n    H = -sum(p * math.log2(p) for p in probs if p > 0)\n    uniq = len(freq)\n    if uniq <= 1:\n        return 0.0\n    denom = math.log2(uniq)\n    return float(H / denom)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are titlecase (First letter upper, rest lower)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if len(w) >= 1 and w[0].isupper() and (w[1:].islower() or len(w) == 1):\n            count += 1\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Normalized variance of word lengths (variance / (mean + 1))'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    n = len(lengths)\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    result = var / (mean + 1.0)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating conjunction (and, but, or, so, for, nor, yet)'\n    import re\n    if not text:\n        return 0.0\n    conj = {'and', 'but', 'or', 'so', 'for', 'nor', 'yet'}\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'\\b\\w+\\b', s)\n        if m and m.group(0).lower() in conj:\n            count += 1\n    return float(count / len(sentences))\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text:\n        return 0.0\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [p for p in re.split(r'(?<=[.!?])\\s+', s) if p.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for sent in sentences:\n        total_words += len(re.findall(r'\\w+', sent))\n    return float(total_words / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Ratio of unique word types to total word tokens (type-token ratio)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = set(words)\n    return float(len(unique) / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that occur exactly once (hapax legomena ratio)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    cnt = Counter(words)\n    hapax = sum(1 for w, c in cnt.items() if c == 1)\n    return float(hapax / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(num / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    matches = re.findall(r'https?://\\S+|www\\.\\S+|\\S+@\\S+\\.\\S+', text.lower())\n    return float(len(matches) / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens (len>=2) that are all uppercase'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    up = sum(1 for t in tokens if len(t) >= 2 and t.isupper())\n    return float(up / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text:\n        return 0.0\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [p for p in re.split(r'(?<=[.!?])\\s+', s) if p.strip()]\n    if not sentences:\n        return 0.0\n    q = sum(1 for sent in sentences if sent.rstrip().endswith('?'))\n    return float(q / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Average number of punctuation characters per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    total_punct = 0\n    for t in tokens:\n        total_punct += sum(1 for c in t if not c.isalnum())\n    return float(total_punct / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n\n",
  "def feature(text: str) -> float:\n    'Density of emojis or ASCII emoticons (per character)'\n    import re\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    # count Unicode emoji by common ranges\n    emoji_count = 0\n    for c in text:\n        o = ord(c)\n        if 0x1F300 <= o <= 0x1F5FF or 0x1F600 <= o <= 0x1F64F or 0x1F680 <= o <= 0x1F6FF or 0x2600 <= o <= 0x26FF:\n            emoji_count += 1\n    # ASCII emoticon patterns like :) :-) :D :P ;-)\n    emoticons = re.findall(r'(?:(?:[:;=8][\\-^]?[)DdpP\\(/\\\\|])|(?:[)DdpP\\(/\\\\|][\\-^]?[:;=8]))', text)\n    emoji_count += len(emoticons)\n    return float(emoji_count / total_chars)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    result = unique / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit (numeric token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = num / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing a character repeated three or more times consecutively (elongation)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1')\n    count = sum(1 for t in tokens if pattern.search(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that appear inside single or double quotation marks'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    if total_len == 0:\n        return 0.0\n    inside = 0\n    for pat in (r'\"(.*?)\"', r\"'(.*?)'\"):\n        for m in re.findall(pat, text, flags=re.S):\n            inside += len(m)\n    result = min(1.0, inside / total_len)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Heuristic passive-voice frequency: (was|were|is|are|been|be) + past-participle(ed) per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text]\n    matches = re.findall(r'\\b(?:was|were|is|are|been|be)\\s+\\w+ed\\b', text, flags=re.I)\n    result = len(matches) / max(1, len(sentences))\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation characters'\n    if not text:\n        return 0.0\n    punc = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(punc)\n    if total == 0:\n        return 0.0\n    uniq = len(set(punc))\n    result = uniq / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Complex sentence ratio: fraction of sentences that contain a comma or semicolon'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    complex_count = sum(1 for s in sentences if (',' in s or ';' in s))\n    result = complex_count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens (length>1) that are fully uppercase (ALL CAPS words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    caps = sum(1 for w in words if len(w) > 1 and w.isupper())\n    result = caps / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'@|\\bhttps?://|\\bwww\\.|\\.com\\b|\\.net\\b|\\.org\\b', flags=re.I)\n    count = sum(1 for t in tokens if pattern.search(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Filler word density: proportion of tokens that are common disfluency/filler words'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    fillers = {'um','uh','erm','hmm','oh','ah','like','yeah','yep','huh','mm','okay','ok','youknow'}\n    count = sum(1 for t in tokens if t in fillers)\n    result = count / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    return float(len(set(words)) / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens whose alphabetic characters are all uppercase (shouting tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        letters = [c for c in t if c.isalpha()]\n        if letters and all(c.isupper() for c in letters):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths) / len(lengths))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that end with a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(qcount / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing an apostrophe (contractions/possessives)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\S+\", text)\n    if not tokens:\n        return 0.0\n    apos = 0\n    for t in tokens:\n        if \"'\" in t or \"\u2019\" in t:\n            apos += 1\n    return float(apos / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-space characters that are Unicode symbol characters (category starting with \"S\")'\n    import unicodedata\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    sym = 0\n    for c in chars:\n        try:\n            if unicodedata.category(c).startswith('S'):\n                sym += 1\n        except Exception:\n            continue\n    return float(sym / len(chars))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', flags=re.I)\n    count = 0\n    for t in tokens:\n        if url_re.search(t):\n            count += 1\n        elif '@' in t and '.' in t:\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average token surprisal (mean -log2(token frequency) in this text)'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    n = len(tokens)\n    freq = {}\n    for t in tokens:\n        freq[t] = freq.get(t, 0) + 1\n    total = 0.0\n    for t in tokens:\n        p = freq.get(t, 0) / n\n        if p > 0:\n            total += -math.log2(p)\n    return float(total / n)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain any parentheses or bracket characters'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if any(c in s for c in '()[]{}'))\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words longer than 12 characters (very long-word density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 12)\n    return float(long_count / len(words))\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # split into sentences by punctuation, but treat whole text as one sentence if no delimiters\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    # compute words per sentence\n    lengths = []\n    for s in sentences:\n        w = re.findall(r'\\w+', s)\n        lengths.append(len(w))\n    if not lengths:\n        return 0.0\n    result = sum(lengths) / len(lengths)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a capitalized word'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    starts = 0\n    total = 0\n    for s in sentences:\n        total += 1\n        first = re.findall(r'\\w+', s)\n        if first and first[0][0].isupper():\n            starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts / total)\n\n",
  "def feature(text: str) -> float:\n    'Punctuation characters per word (punctuation-to-word ratio)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = len(words)\n    if word_count == 0:\n        return 0.0\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct_count / word_count)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences longer than 20 words (long-sentence density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    long_count = 0\n    for s in sentences:\n        if len(re.findall(r'\\w+', s)) > 20:\n            long_count += 1\n    return float(long_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing a hyphen (hyphenated-token ratio)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of non-empty lines that begin with a quotation or dash (quoted-line density)'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    starts = 0\n    for ln in lines:\n        s = ln.lstrip()\n        if not s:\n            continue\n        if s[0] in {'\"', \"'\", '\u201c', '\u201d', '\u2014', '-'}:\n            starts += 1\n    return float(starts / len(lines))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain both letters and digits (alphanumeric-token rate)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    mixed = 0\n    for t in tokens:\n        has_digit = any(c.isdigit() for c in t)\n        has_alpha = any(c.isalpha() for c in t)\n        if has_digit and has_alpha:\n            mixed += 1\n    return float(mixed / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens in ALL CAPS (at least two letters) indicating emphasis or acronyms'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    caps = 0\n    for t in tokens:\n        alpha_count = sum(1 for c in t if c.isalpha())\n        if alpha_count >= 2 and all((not c.isalpha()) or c.isupper() for c in t):\n            caps += 1\n    return float(caps / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Adjacent repeated-word rate (fraction of adjacent word pairs that are identical)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = 0\n    pairs = len(words) - 1\n    for i in range(pairs):\n        if words[i] == words[i + 1]:\n            repeats += 1\n    return float(repeats / pairs)\n",
  "def feature(text: str) -> float:\n    'Average sentence length measured in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if words:\n            word_counts.append(len(words))\n        else:\n            word_counts.append(0)\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts) / len(word_counts))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of alphabetic characters that are uppercase'\n    if not text:\n        return 0.0\n    letters = [c for c in text if c.isalpha()]\n    if not letters:\n        return 0.0\n    upper = sum(1 for c in letters if c.isupper())\n    return float(upper / len(letters))\n\n",
  "def feature(text: str) -> float:\n    'Density of common English function words (approximate stopword density)'\n    import re\n    if not text:\n        return 0.0\n    STOP = {'the','be','to','of','and','a','in','that','it','is','for','on','with','as','are','was','at','by','an','this','from','or','but'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in STOP)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word types that occur only once (hapax legomena density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    freq = Counter(tokens)\n    hapax = sum(1 for w, c in freq.items() if c == 1)\n    return float(hapax / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average number of punctuation characters per token'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    punct_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct_chars / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    qcount = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(qcount / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Density of tokens that look like URLs or email addresses'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', flags=re.I)\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', flags=re.I)\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t) or '://' in t:\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of multi-character tokens that are ALL CAPS (shouting/acronym density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 1 and t.isupper())\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing an apostrophe (contraction or possessive density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if (\"'\" in t) or (\"\u2019\" in t))\n    return float(count / len(tokens))\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that are titlecased (start with uppercase then lowercase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.istitle())\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are all-caps (contain at least one letter and all letters uppercase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(t):\n        letters = [c for c in t if c.isalpha()]\n        return bool(letters) and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters in the text that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain a character repeated three or more times in a row (elongation)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1')\n    count = sum(1 for t in tokens if pattern.search(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence (sentences split on .!? boundaries)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    result = sum(word_counts) / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of alphabetic characters that are vowels (a,e,i,o,u)'\n    if not text:\n        return 0.0\n    letters = [c.lower() for c in text if c.isalpha()]\n    if not letters:\n        return 0.0\n    vowels = sum(1 for c in letters if c in 'aeiou')\n    result = vowels / len(letters)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs (start with http/www or contain a TLD-like dot)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re1 = re.compile(r'^(?:https?://|ftp://|www\\.)', re.I)\n    url_re2 = re.compile(r'\\.[a-z]{2,6}(/|$)', re.I)\n    count = 0\n    for t in tokens:\n        if url_re1.search(t) or url_re2.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that consist only of punctuation characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if all(not c.isalnum() for c in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized character-level Shannon entropy (0-1) using unique-character max entropy'\n    import math\n    if not text:\n        return 0.0\n    n = len(text)\n    if n == 0:\n        return 0.0\n    freq = {}\n    for c in text:\n        freq[c] = freq.get(c, 0) + 1\n    ent = 0.0\n    for v in freq.values():\n        p = v / n\n        ent -= p * math.log2(p)\n    max_ent = math.log2(len(freq)) if len(freq) > 1 else 0.0\n    if max_ent <= 0.0:\n        result = 0.0\n    else:\n        result = ent / max_ent\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of token lengths (std / mean) using word tokens'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    variance = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(variance)\n    result = std / mean\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens / total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    types = len(set(words))\n    return float(types / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of words that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    counts = Counter(words)\n    hapax = sum(1 for w, c in counts.items() if c == 1)\n    return float(hapax / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words written in all caps (length>=2)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    allcaps = sum(1 for w in words if len(w) >= 2 and w.isalpha() and w.isupper())\n    return float(allcaps / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    num_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num_tokens / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average vowel-group count per word (proxy for syllable-like units)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return float(0.0)\n    groups = [len(re.findall(r'[aeiouy]+', w.lower())) or 1 for w in words]\n    return float(sum(groups) / len(groups))\n\n",
  "def feature(text: str) -> float:\n    'Normalized character entropy (0-1) measuring char distribution uniformity'\n    import math\n    if not text:\n        return float(0.0)\n    freq = {}\n    for c in text:\n        freq[c] = freq.get(c, 0) + 1\n    n = sum(freq.values())\n    if n == 0 or len(freq) <= 1:\n        return float(0.0)\n    entropy = 0.0\n    for v in freq.values():\n        p = v / n\n        entropy -= p * math.log2(p)\n    # normalize by max entropy log2(|alphabet|)\n    max_e = math.log2(len(freq))\n    if max_e <= 0:\n        return float(0.0)\n    return float(entropy / max_e)\n\n",
  "def feature(text: str) -> float:\n    'Punctuation-to-word ratio (punctuation characters per word)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    word_count = len(words)\n    if word_count == 0:\n        return float(0.0)\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct / word_count)\n\n",
  "def feature(text: str) -> float:\n    'Emoticon density: number of common emoticons per word'\n    if not text:\n        return float(0.0)\n    emoticons = [':)', ':-)', ':(', ':-(', ':d', ':-d', ';)', ';-)', ':p', ':-p', ':/', \":'(\", '<3', '>:(', ':o', ':-o']\n    lt = text.lower()\n    count = 0\n    for e in emoticons:\n        count += lt.count(e)\n    # fallback to words count\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words that contain an uppercase letter after the first character (internal caps)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    mixed = 0\n    for w in words:\n        if len(w) > 1 and any(c.isupper() for c in w[1:]):\n            mixed += 1\n    return float(mixed / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a quotation, bracket, or dash character'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    starters = set(['\"', \"'\", '\u201c', '\u2018', '(', '[', '{', '-', '\u2014', '\u2013', '`'])\n    count = 0\n    for s in sentences:\n        s2 = s.lstrip()\n        if not s2:\n            continue\n        if s2[0] in starters:\n            count += 1\n    return float(count / len(sentences))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    types = set(words)\n    result = len(types) / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of punctuation characters per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # split into sentences but keep fallback to whole text\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    punct_counts = []\n    for s in sentences:\n        punct = sum(1 for c in s if not c.isalnum() and not c.isspace())\n        punct_counts.append(punct)\n    result = sum(punct_counts) / max(1, len(sentences))\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that contain any digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words that are titlecased (start with uppercase followed by lowercase)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        # treat single-letter uppercase (e.g., \"A\") as titlecased as well\n        if w.istitle():\n            count += 1\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of common English stopwords among word tokens (small fixed list)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','of','and','to','a','in','is','it','that','for','on','with','as','are','was','be','by','at','from','an'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stop)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Approximate average syllables per word using vowel-group heuristic'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[ A-Za-z0-9\\'-]*[A-Za-z][A-Za-z0-9\\'-]*\\b', text)\n    # fallback to simpler word extraction if above fails\n    if not words:\n        words = re.findall(r'\\w+', text)\n    words = [w for w in words if any(ch.isalpha() for ch in w)]\n    if not words:\n        return 0.0\n    def syllable_count(w: str) -> int:\n        s = w.lower()\n        groups = re.findall(r'[aeiouy]+', s)\n        count = len(groups)\n        # common heuristic: silent trailing 'e' reduces one syllable if more than one group\n        if s.endswith('e') and count > 1:\n            count -= 1\n        return max(1, count)\n    total = sum(syllable_count(w) for w in words)\n    result = total / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    if not text or not text.strip():\n        return 0.0\n    sentences_count = text.count('.') + text.count('!') + text.count('?')\n    if sentences_count == 0:\n        return 0.0\n    result = text.count('?') / sentences_count\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are long (length >= 12 characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) >= 12)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized Shannon entropy of word distribution (0-1), measures lexical diversity'\n    import re, math\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    total = len(words)\n    probs = [freq / total for freq in freqs.values()]\n    entropy = -sum(p * math.log2(p) for p in probs if p > 0)\n    # normalize by log2(number of types) to keep in [0,1] when more than one type exists\n    types = len(freqs)\n    if types <= 1:\n        return 0.0\n    norm = entropy / math.log2(types)\n    return float(norm)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with an uppercase letter (ignoring leading non-letters)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    starts_upper = 0\n    for s in sentences:\n        first_alpha = None\n        for c in s:\n            if c.isalpha():\n                first_alpha = c\n                break\n        if first_alpha is None:\n            # no alphabetic char in sentence; treat as not starting with uppercase\n            continue\n        if first_alpha.isupper():\n            starts_upper += 1\n    result = starts_upper / len(sentences)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena proportion: words that occur exactly once over total words'\n    import re, collections\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    freq = collections.Counter(tokens)\n    hapax = sum(1 for w, c in freq.items() if c == 1)\n    return float(hapax / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Stopword density using a small common English stopword set'\n    import re\n    if not text:\n        return float(0.0)\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on',\n        'with','he','as','you','do','at','this','but','his','by','from','they','we',\n        'say','her','she','or','an','will','my','one','all','would','there','their',\n        'what','so','up','out','if','about','who','get','which','go','me'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    stop_count = sum(1 for t in tokens if t in stopwords)\n    return float(stop_count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Numeric token fraction: proportion of tokens containing any digit'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\b\\S+\\b', text)\n    if not tokens:\n        return float(0.0)\n    num_count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(num_count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Non-ASCII character fraction (proxy for emojis, foreign scripts, or special symbols)'\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    nonascii = sum(1 for c in text if ord(c) > 127)\n    return float(nonascii / total)\n\n",
  "def feature(text: str) -> float:\n    'Average commas per sentence (commas divided by sentence count, sentences>=1)'\n    import re\n    if not text:\n        return float(0.0)\n    commas = text.count(',')\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    count_sents = max(1, len(sents))\n    return float(commas / count_sents)\n\n",
  "def feature(text: str) -> float:\n    'Short-word ratio: fraction of word tokens with length <= 3'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    short = sum(1 for w in words if len(w) <= 3)\n    return float(short / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with an uppercase letter (ignoring leading quotes/parentheses)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        return float(0.0)\n    def starts_upper(s):\n        for ch in s:\n            if ch.isalpha():\n                return ch.isupper()\n            if ch.isdigit():\n                return False\n        return False\n    count = sum(1 for s in sents if starts_upper(s))\n    return float(count / len(sents))\n\n",
  "def feature(text: str) -> float:\n    'Repeated adjacent word bigram ratio: count of \"word word\" repeats divided by possible bigrams'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return float(0.0)\n    repeats = len(re.findall(r'\\b(\\w+)\\s+\\1\\b', text, flags=re.IGNORECASE))\n    denom = max(1, len(tokens) - 1)\n    return float(repeats / denom)\n\n",
  "def feature(text: str) -> float:\n    'List-item line density: fraction of non-empty lines that appear to be list items (bullets or numbered)'\n    import re\n    if not text:\n        return float(0.0)\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return float(0.0)\n    pattern = re.compile(r'^\\s*(?:[-*+]|(?:\\d+[\\.\\)]))\\s+')\n    list_lines = sum(1 for l in lines if pattern.match(l))\n    return float(list_lines / len(lines))\n",
  "def feature(text: str) -> float:\n    'Ratio of unique words to total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    return float(len(set(words)) / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(words) / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'[\\w.+-]+@[\\w-]+\\.\\w+', re.I)\n    url_re = re.compile(r'https?://|www\\.', re.I)\n    count = 0\n    for t in tokens:\n        if email_re.search(t) or url_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average number of vowel groups per alphabetic word (simple syllable proxy)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'[A-Za-z]+', text)\n    if not words:\n        return 0.0\n    vg = sum(len(re.findall(r'[aeiouyAEIOUY]+', w)) for w in words)\n    return float(vg / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing a repeated character sequence of length >=3 (elongation)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    rep_re = re.compile(r'(.)\\1{2,}', re.I)\n    count = sum(1 for t in tokens if rep_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that contain double or typographic quotation marks'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    quote_chars = set('\"\u201c\u201d')\n    count = sum(1 for s in sentences if any(c in s for c in quote_chars))\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating conjunction (and, but, or, so, because, yet)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    start_re = re.compile(r'^\\s*(and|but|or|so|because|yet)\\b', re.I)\n    count = sum(1 for s in sentences if start_re.search(s))\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Ratio of punctuation characters to alphabetic characters'\n    if not text:\n        return 0.0\n    alpha = sum(1 for c in text if c.isalpha())\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if alpha == 0:\n        return 0.0\n    return float(punct / alpha)\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of token lengths (stdev/mean) for word tokens'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    lengths = [len(t) for t in tokens]\n    if len(lengths) <= 1:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    stdev = math.sqrt(var)\n    return float(stdev / mean)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace tokens that are emoticons or emoji-like'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    # common ASCII emoticons\n    emot_re = re.compile(r'^(?:[:;=8][\\-^]?[)\\]D\\(\\]/Pp3<\\*]|<3|:\\'\\)|:\\'\\()$')\n    count = 0\n    for t in tokens:\n        if emot_re.search(t):\n            count += 1\n            continue\n        # crude emoji detection by codepoint ranges\n        for ch in t:\n            oc = ord(ch)\n            if oc >= 0x1F300 and oc <= 0x1FAFF:\n                count += 1\n                break\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are written in ALL CAPS (2+ letters)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    caps = sum(1 for w in words if len([c for c in w if c.isalpha()]) >= 2 and w.isupper())\n    result = caps / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are very short (3 words or fewer)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        wc = len(re.findall(r'\\w+', s))\n        if wc <= 3:\n            short += 1\n    result = short / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (stddev / mean)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    result = std / mean\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are common English stopwords (simple list)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {\n        'the','and','a','to','of','in','that','is','it','for','on','with','as','are','was','be','by','this','have','or','an','at','from','not','but','they','you','I','we'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters in the text that are digits'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    digits = sum(1 for c in text if c.isdigit())\n    result = digits / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of punctuation characters per word'\n    import re, string\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    punct = sum(1 for c in text if (not c.isalnum()) and (not c.isspace()))\n    result = punct / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence (indicator of clause density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_commas = sum(s.count(',') for s in sentences)\n    result = total_commas / len(sentences)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Shannon character entropy normalized by log2(unique characters)'\n    import math, collections\n    if not text:\n        return 0.0\n    total = len(text)\n    freq = collections.Counter(text)\n    if len(freq) < 2:\n        return 0.0\n    probs = [v / total for v in freq.values()]\n    entropy = -sum(p * math.log2(p) for p in probs if p > 0)\n    # normalize by log2 of number of unique chars\n    norm = entropy / math.log2(len(freq))\n    return float(norm)\n\n\n",
  "def feature(text: str) -> float:\n    'Compressibility score (1 - compressed_size/original_size), clamped to [0,1]'\n    import zlib\n    if not text:\n        return 0.0\n    b = text.encode('utf-8', errors='ignore')\n    orig = len(b)\n    if orig == 0:\n        return 0.0\n    comp = zlib.compress(b)\n    ratio = len(comp) / orig\n    score = max(0.0, 1.0 - ratio)\n    if score > 1.0:\n        score = 1.0\n    return float(score)\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are all-uppercase tokens of length>=2 (ACRONYMS/SHOUT)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    caps = sum(1 for t in tokens if re.fullmatch(r'[A-Z]{2,}', t))\n    return float(caps / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that match common email address patterns'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'^[\\w.+-]+@[\\w-]+\\.[\\w.-]+$', re.IGNORECASE)\n    emails = sum(1 for t in tokens if email_re.match(t))\n    return float(emails / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are pure numeric values (integers or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d[\\d,]*\\.?\\d*$')\n    nums = sum(1 for t in tokens if num_re.match(t.replace(' ', '')))\n    return float(nums / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of sentence lengths (stdev/mean of words per sentence)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if len(sentences) <= 1:\n        return 0.0\n    lens = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lens or sum(lens) == 0:\n        return 0.0\n    mean = sum(lens) / len(lens)\n    var = sum((l - mean) ** 2 for l in lens) / len(lens)\n    stdev = math.sqrt(var)\n    cov = stdev / mean if mean > 0 else 0.0\n    return float(cov)\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(len(words) - 1) if words[i] == words[i + 1])\n    return float(repeats / (len(words) - 1))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like code/identifiers (contain _ or \\\\ or / or camelCase or ::)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    code_like = 0\n    camel_re = re.compile(r'[a-z][A-Z]')\n    for t in tokens:\n        if '_' in t or '\\\\' in t or '/' in t or '::' in t or camel_re.search(t):\n            code_like += 1\n    return float(code_like / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Emoji character density: fraction of characters that are common emoji codepoints'\n    import re\n    if not text:\n        return 0.0\n    # common emoji/unicode pictograph ranges\n    emoji_re = re.compile(\n        '['\n        '\\U0001F300-\\U0001F5FF'\n        '\\U0001F600-\\U0001F64F'\n        '\\U0001F680-\\U0001F6FF'\n        '\\U0001F1E0-\\U0001F1FF'\n        '\\U00002700-\\U000027BF'\n        '\\U00002600-\\U000026FF'\n        ']',\n        flags=re.UNICODE)\n    chars = len(text)\n    if chars == 0:\n        return 0.0\n    emojis = len(emoji_re.findall(text))\n    return float(emojis / chars)\n\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    uniq = len(set(words))\n    return float(uniq / len(words))\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    avg = len(words) / max(1, len(sentences))\n    return float(avg)\n\n",
  "def feature(text: str) -> float:\n    'Normalized standard deviation of sentence lengths (stddev / mean)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lens = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lens:\n        return 0.0\n    mean = sum(lens) / len(lens)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lens) / len(lens)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    types = len(set(tokens))\n    return float(types / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','it','of','to','a','that','i','you','for','on','with','as','was','are','be','at','by','an','this','have','or','not','they','from','but','we','he','she','his','her','them','which','their','were','has','had'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are all-uppercase words (shouting tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # require at least one alpha and more than one character to avoid single-letter capitals\n        if any(c.isalpha() for c in t) and len([c for c in t if c.isalpha()]) >= 2 and t.isupper():\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.|mailto:|[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,})', flags=re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Density of common emoticons per token (smiley/frowny/etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    # common sideways emoticons and hearts\n    emoticon_re = re.compile(r'(?:(?:[:;=8][\\-~]?[)DdpP\\(\\]/\\\\])|(?:[)DdpP\\-\\^][\\-~]?[:;=8])|<3|\\^\\_^|:\\'\\)|:\\'\\(|:\\(|:\\)|;-\\)|;-?\\))', flags=re.I)\n    count = sum(1 for t in tokens if emoticon_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Approximate Flesch Reading Ease score (higher = easier); 0 if not computable'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not words or not sentences:\n        return 0.0\n    # syllable heuristic similar to other feature\n    total_syll = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        syl = max(1, len(groups))\n        if w.endswith('e') and len(groups) > 1:\n            syl = max(1, syl - 1)\n        total_syll += syl\n    words_count = len(words)\n    sent_count = len(sentences)\n    # Flesch Reading Ease\n    score = 206.835 - 1.015 * (words_count / sent_count) - 84.6 * (total_syll / words_count)\n    return float(score)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of text lines that look like list items (bullets or numbered)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    bullet_re = re.compile(r'^\\s*([-*\u2022]|(\\d+[\\).\\s])|([a-zA-Z]\\)))')\n    count = sum(1 for ln in lines if bullet_re.match(ln))\n    return float(count / len(lines))\n\n",
  "def feature(text: str) -> float:\n    'Average number of punctuation characters per word (punctuation-to-word ratio)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    punct_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct_chars / len(words))\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of sentence lengths (std / mean of words per sentence)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are in ALL CAPS (signal of emphasis/shouting)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # require at least two alphabetic chars to avoid counting single-letter tokens like 'I'\n        if any(c.isalpha() for c in t) and t.isupper() and sum(1 for c in t if c.isalpha()) >= 2:\n            count += 1\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that end with an exclamation mark'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    ex = sum(1 for s in sentences if s.rstrip().endswith('!'))\n    return float(ex / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of vocabulary that occurs exactly once'\n    import re, collections\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freq = collections.Counter(words)\n    vocab = len(freq)\n    if vocab == 0:\n        return 0.0\n    hapax = sum(1 for _, c in freq.items() if c == 1)\n    return float(hapax / vocab)\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like common ASCII emoticons or simple hearts (<3)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'^(?:[:;=8][\\-~]?[\\)DdpP/\\(\\\\]|<3|:-?\\(|:-?\\))$', re.I)\n    matches = 0\n    for t in tokens:\n        if pattern.search(t):\n            matches += 1\n    return float(matches / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens considered \"long\" (length >= 7), a proxy for lexical complexity'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 7)\n    return float(long_count / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord>127), capturing emojis or foreign scripts'\n    if not text:\n        return 0.0\n    L = len(text)\n    if L == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / L)\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of non-whitespace tokens that contain a hyphen'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if '-' in t and len(t.strip('-')) > 0:\n            count += 1\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that contain a repeated character sequence of length >=3 (loooove)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1', re.I)\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence (captures clause density and listing style)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_commas = sum(s.count(',') for s in sentences)\n    return float(total_commas / len(sentences))\n",
  "def feature(text: str) -> float:\n    'Average number of word tokens per sentence (words/sentence)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    words_in = lambda seg: re.findall(r'\\w+', seg)\n    if sentences:\n        totals = sum(len(words_in(seg)) for seg in sentences)\n        return float(totals / len(sentences)) if len(sentences) else float(0.0)\n    # fallback: use overall word count\n    all_words = words_in(s)\n    if not all_words:\n        return float(0.0)\n    return float(len(all_words))\n\n\n",
  "def feature(text: str) -> float:\n    'Ratio of hapax legomena (tokens that occur exactly once) to total tokens'\n    import re\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    counts = Counter(tokens)\n    hapax = sum(1 for v in counts.values() if v == 1)\n    return float(hapax / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric-like (contain digits and no letters)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    def is_numeric_token(t):\n        if re.search(r'[A-Za-z]', t):\n            return False\n        return bool(re.search(r'\\d', t))\n    count = sum(1 for t in tokens if is_numeric_token(t))\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Density of URL-like tokens per non-space token (urls / tokens)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    urls = re.findall(r'https?://\\S+|www\\.\\S+|[a-z0-9.-]+\\.(?:com|net|org|io|gov|edu)\\b', text, flags=re.IGNORECASE)\n    return float(len(urls) / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are short (<=3 characters)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    short = sum(1 for w in words if len(w) <= 3)\n    return float(short / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        return float(1.0 if s.endswith('?') else 0.0)\n    questions = sum(1 for seg in sentences if seg.strip().endswith('?'))\n    return float(questions / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is lowercase'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        m = re.search(r'[A-Za-z]', s)\n        return float(1.0 if (m and m.group(0).islower()) else 0.0)\n    count = 0\n    for seg in sentences:\n        m = re.search(r'[A-Za-z]', seg)\n        if m and m.group(0).islower():\n            count += 1\n    return float(count / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are quote characters (single/double/curly quotes)'\n    if not text:\n        return float(0.0)\n    QUOTES = set('\\'\"\u2018\u2019\u201c\u201d')\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    q = sum(1 for c in text if c in QUOTES)\n    return float(q / total)\n\n\n",
  "def feature(text: str) -> float:\n    'Normalized entropy of token length distribution (0..1 approx)'\n    import re, math\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    lengths = [len(t) for t in tokens]\n    counts = Counter(lengths)\n    total = sum(counts.values())\n    entropy = 0.0\n    for c in counts.values():\n        p = c / total\n        entropy -= p * math.log(p, 2)\n    maxlen = max(lengths)\n    denom = math.log(maxlen + 1, 2) if maxlen > 0 else 1.0\n    return float(entropy / denom)\n\n\n",
  "def feature(text: str) -> float:\n    'Average fraction of uppercase letters among letter characters within tokens'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    letter_tokens = []\n    for t in tokens:\n        letters = [c for c in t if c.isalpha()]\n        if letters:\n            upp = sum(1 for c in letters if c.isupper())\n            letter_tokens.append(upp / len(letters))\n    if not letter_tokens:\n        return float(0.0)\n    return float(sum(letter_tokens) / len(letter_tokens))\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (mean words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    return float(sum(word_counts) / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are written in ALL CAPS (shouting density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    all_caps = sum(1 for t in tokens if len(t) > 1 and t.isupper())\n    return float(all_caps / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that look like URLs (http/https/www)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    urls = re.findall(r'(?:https?://|www\\.)\\S+', text, flags=re.I)\n    return float(len(urls) / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Ratio of numeric tokens (numbers, integers, decimals, formatted with commas/dots) to all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d+(?:[.,]\\d+)*$')\n    numeric = sum(1 for t in tokens if num_re.match(t))\n    return float(numeric / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are common English stopwords (approximate stopword density)'\n    import re\n    if not text:\n        return 0.0\n    STOP = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on',\n        'with','he','as','you','do','at','this','but','his','by','from','they','we',\n        'say','her','she','or','an','will','my','one','all','would','there','their',\n        'what','so','up','out','if','about','who','get','which','go','me'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    stop_count = sum(1 for t in tokens if t in STOP)\n    return float(stop_count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Punctuation characters per word (total punctuation chars divided by number of word tokens)'\n    import re, string\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    punct_count = sum(1 for c in text if c in string.punctuation)\n    return float(punct_count / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that appear structurally complex (contain semicolon/colon or subordinating conjunction)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = r'\\b(because|although|though|since|while|unless|whereas|where|which|that)\\b'\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    complex_count = 0\n    for s in sentences:\n        if re.search(r'[;:]', s) or re.search(conj, s, flags=re.I):\n            complex_count += 1\n    return float(complex_count / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Shannon entropy of the character distribution (bits per character)'\n    import math, collections\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    freq = collections.Counter(text)\n    entropy = 0.0\n    for cnt in freq.values():\n        p = cnt / total\n        entropy -= p * math.log2(p)\n    return float(entropy)\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a lowercase letter (indicative of informal/fragment style)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lower_start = 0\n    for s in sentences:\n        s_stripped = s.lstrip(' \\t\\n\"\\'' )\n        first_alpha = None\n        for ch in s_stripped:\n            if ch.isalpha():\n                first_alpha = ch\n                break\n            if ch.isalnum():\n                first_alpha = ch\n                break\n        if first_alpha is not None and first_alpha.isalpha() and first_alpha.islower():\n            lower_start += 1\n    return float(lower_start / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Density of immediate repeated word bigrams (e.g., \"very very\") as fraction of all bigrams'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = len(tokens) - 1\n    repeats = sum(1 for i in range(len(tokens) - 1) if tokens[i] == tokens[i + 1])\n    return float(repeats / bigrams)\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are ALL CAPS (minimum length 2)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    caps = sum(1 for w in words if len(w) >= 2 and w.isupper())\n    return float(caps / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average vowel-to-length ratio per word (vowels / word length)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    vowels = set('aeiou')\n    ratios = []\n    for w in words:\n        L = len(w)\n        if L == 0:\n            continue\n        vcount = sum(1 for c in w if c in vowels)\n        ratios.append(vcount / L)\n    if not ratios:\n        return 0.0\n    return float(sum(ratios) / len(ratios))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(digit_tokens / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average naive syllable count per word (vowel groups, at least 1)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n    if not words:\n        return 0.0\n    sy_counts = []\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        sy = max(1, len(groups))\n        sy_counts.append(sy)\n    return float(sum(sy_counts) / len(sy_counts))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','it','to','of','a','that','for','on','with',\n                 'as','are','was','were','be','by','this','an','or','from','at',\n                 'not','have','has','but','we','they','you','i','he','she','them'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    sw = sum(1 for w in words if w in stopwords)\n    return float(sw / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Normalized character entropy (0-1) excluding whitespace'\n    import math, collections\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    freq = collections.Counter(chars)\n    total = len(chars)\n    entropy = 0.0\n    for count in freq.values():\n        p = count / total\n        entropy -= p * math.log2(p)\n    uniq = len(freq)\n    # normalize by log2(uniq), ensure denominator >=1\n    denom = math.log2(max(2, uniq))\n    return float(entropy / denom)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a lowercase letter (informal starts)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m and m.group(0).islower():\n            count += 1\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs (scheme or common TLD pattern)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re1 = re.compile(r'^(https?://|www\\.)', re.IGNORECASE)\n    url_re2 = re.compile(r'\\.[a-z]{2,3}([/:]|$)', re.IGNORECASE)\n    def is_url(t):\n        if url_re1.search(t):\n            return True\n        if '@' in t:\n            return False\n        return bool(url_re2.search(t))\n    urls = sum(1 for t in tokens if is_url(t))\n    return float(urls / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Highest frequency of a sentence-start word divided by sentence count (repetition measure)'\n    import re, collections\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = []\n    for s in sentences:\n        m = re.findall(r'\\b\\w+\\b', s)\n        if m:\n            starts.append(m[0].lower())\n    if not starts:\n        return 0.0\n    freq = collections.Counter(starts)\n    most = max(freq.values())\n    return float(most / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are very long (length >= 12 characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 12)\n    return float(long_words / len(words))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    return float(len(set(words)) / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts) / len(word_counts))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are all-caps (shouting), requiring at least 2 letters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t) and t.isupper() and sum(1 for c in t if c.isalpha()) >= 2:\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain both letters and digits (alphanumeric tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(c.isdigit() for c in t) and any(c.isalpha() for c in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if s.strip().endswith('?'))\n    return float(qcount / len(sentences)) if sentences else 0.0\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', re.I)\n    email_re = re.compile(r'^[\\w.+-]+@[\\w.-]+\\.\\w+$')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens containing an apostrophe (contractions/possessives)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if \"'\" in t and any(c.isalpha() for c in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Normalized character entropy (Shannon) across distinct characters, in [0,1]'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    freqs = Counter(text)\n    ntypes = len(freqs)\n    if ntypes <= 1:\n        return 0.0\n    entropy = 0.0\n    for v in freqs.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    # normalize by max entropy = log2(ntypes)\n    norm = entropy / math.log2(ntypes) if ntypes > 1 else 0.0\n    return float(norm)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing letter elongations (a letter repeated 3 or more times)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'([A-Za-z])\\1{2,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that lie inside quoted spans (double quotes and standalone single-quote pairs)'\n    import re\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    inside = 0\n    # double-quoted spans\n    for m in re.findall(r'\"(.*?)\"', text, flags=re.S):\n        inside += len(m)\n    # single-quoted spans but avoid contractions/embedded apostrophes by requiring non-word boundaries\n    for m in re.findall(r\"(?<!\\w)'(.*?)'(?!\\w)\", text, flags=re.S):\n        inside += len(m)\n    return float(min(1.0, inside / total))\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are common ASCII emoticons (e.g., :) :-D ;))'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticon_re = re.compile(r'^(?:[:;=8X][-~]?[)DdpP/(\\\\|\\]|[\\]{}]|<3|:\\'[\\)])$|^<3$|^:-?O$|^:-?\\($', re.I)\n    count = 0\n    for t in tokens:\n        if emoticon_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain a run of the same punctuation character of length >=3 (e.g., \"!!!\", \"...\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    rep_re = re.compile(r'([!?\\.,\"\\'-])\\1{2,}')\n    count = sum(1 for t in tokens if rep_re.search(t))\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (lexical diversity): unique word tokens divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = set(tokens)\n    return float(len(unique) / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are in ALL CAPS (and contain at least one letter)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t) and t.isupper():\n            count += 1\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        return 0.0\n    count = sum(1 for s in sents if s.endswith('?'))\n    return float(count / len(sents))\n\n\n",
  "def feature(text: str) -> float:\n    'Variance of token lengths (average squared deviation of word lengths)'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    lengths = [len(t) for t in tokens]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(var)\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like list items (bullets or numbered markers)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    bullet_re = re.compile(r'^\\s*(?:[-\\*\\u2022]|\\d+\\.|[a-zA-Z]\\))\\s+')\n    count = sum(1 for l in lines if bullet_re.match(l))\n    return float(count / len(lines))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words that contain an internal hyphen or apostrophe (e.g., self-esteem, don\\'t)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    hy_ap_re = re.compile(r'\\w+[-\\']\\w+')\n    count = 0\n    for m in re.finditer(r'\\S+', text):\n        token = m.group(0)\n        if hy_ap_re.search(token):\n            count += 1\n    return float(count / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Ratio of punctuation characters to alphabetic letters (punctuation_count / letter_count)'\n    if not text:\n        return 0.0\n    letters = sum(1 for c in text if c.isalpha())\n    if letters == 0:\n        return 0.0\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct / letters)\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of adjacent token pairs that are identical (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(len(tokens) - 1) if tokens[i] == tokens[i + 1])\n    return float(repeats / (len(tokens) - 1))\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q_count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total word tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of non-empty tokens whose alphabetic letters are all uppercase (shouting tokens)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_shout(t):\n        letters = [c for c in t if c.isalpha()]\n        if len(letters) < 2:\n            return False\n        return all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_shout(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Mean per-token character-uniqueness ratio (unique chars / token length) across word tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    ratios = []\n    for t in tokens:\n        L = len(t)\n        if L == 0:\n            continue\n        ratios.append(len(set(t)) / L)\n    if not ratios:\n        return 0.0\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are numeric (integers or decimals)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d+(\\.\\d+)?$')\n    count = sum(1 for t in tokens if num_re.match(t.strip('(),%')))  # strip common wrappers\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are short (fewer than 5 words)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) < 5:\n            short += 1\n    result = short / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (simple fixed list)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    STOPWORDS = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not',\n        'on','with','he','as','you','do','at','this','but','his','by','from',\n        'they','we','say','her','she','or','an','will','my','one','all','would',\n        'there','their','what','so','up','out','if','about','who','get','which',\n        'go','me'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in STOPWORDS)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of common ASCII emoticons among whitespace-separated tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    emoticons = {':)', ':-)', ':(', ':-(', ':D', ':-D', ':P', ':-P', ';)', ';-)', ':/', ':-/', ':-|', ':|', 'XD', 'xD'}\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    def norm(t):\n        return t.strip('.,!?;:').replace('\"','').replace(\"'\", \"\").strip()\n    count = 0\n    for t in tokens:\n        if norm(t) in emoticons or norm(t).upper() in emoticons:\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average count of coordinating conjunctions (and, but, or, so, for, nor, yet) per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj_re = re.compile(r'\\b(?:and|but|or|so|for|nor|yet)\\b', flags=re.I)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = [len(conj_re.findall(s)) for s in sentences]\n    result = sum(counts) / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing an internal hyphen or apostrophe (contractions / hyphenation)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'[A-Za-z0-9][\\'\u2019\\-][A-Za-z0-9]')\n    count = sum(1 for t in tokens if pattern.search(t))\n    result = count / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of tokens that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    hapaxes = sum(1 for t, c in counts.items() if c == 1)\n    return float(hapaxes / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        # fallback: treat whole text as one sentence\n        sentences = [s]\n    total_words = 0\n    for seg in sentences:\n        total_words += len(re.findall(r'\\w+', seg))\n    if not sentences:\n        return 0.0\n    return float(total_words / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are short (length <= 2)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    short = sum(1 for t in tokens if len(t) <= 2)\n    return float(short / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens containing an apostrophe'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if \"'\" in t)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Uppercase-letter ratio: uppercase alphabetic chars divided by total alphabetic chars'\n    if not text:\n        return 0.0\n    alpha_total = sum(1 for c in text if c.isalpha())\n    if alpha_total == 0:\n        return 0.0\n    upper = sum(1 for c in text if c.isalpha() and c.isupper())\n    return float(upper / alpha_total)\n\n",
  "def feature(text: str) -> float:\n    'Punctuation-to-word ratio: punctuation characters per whitespace-separated word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\S+', text)\n    if not words:\n        return 0.0\n    punct = sum(1 for c in text if (not c.isalnum()) and (not c.isspace()))\n    return float(punct / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (small heuristic list)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','is','in','it','to','of','a','for','on','that','this','with','as','are','was','be','by','an','or','at','from','but','not'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stop)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Flesch reading ease score (simple syllable heuristic), clamped to [0,100]'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not words or not sentences:\n        return 0.0\n    # syllable heuristic similar to other feature\n    total_syl = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        syl = max(1, len(groups))\n        if w.endswith('e') and len(groups) > 1:\n            syl = max(1, syl - 1)\n        total_syl += syl\n    words_count = len(words)\n    sentences_count = max(1, len(sentences))\n    # Flesch Reading Ease formula\n    score = 206.835 - 1.015 * (words_count / sentences_count) - 84.6 * (total_syl / words_count)\n    # clamp to 0-100 for stability\n    score = max(0.0, min(100.0, score))\n    return float(score)\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens longer than 6 characters (long-word density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) >= 7)\n    return float(long_count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Standard deviation of word lengths (0.0 for empty or single-token texts)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if len(tokens) <= 1:\n        return 0.0\n    lengths = [len(t) for t in tokens]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL CAPS (length>=2 alphabetic characters)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        if len(letters) < 2:\n            return False\n        return all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are Titlecase (start with uppercase then lowercase)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.istitle())\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation types divided by total punctuation chars'\n    import string\n    if not text:\n        return 0.0\n    puncs = [c for c in text if c in string.punctuation]\n    if not puncs:\n        return 0.0\n    distinct = len(set(puncs))\n    return float(distinct / len(puncs))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like abbreviations/initialisms (e.g., U.S., E.G.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'^[A-Za-z](?:\\.[A-Za-z])+\\.?$')\n    short_dot = re.compile(r'^[A-Za-z]{1,3}\\.$')\n    count = 0\n    for t in tokens:\n        if pattern.match(t) or short_dot.match(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email-like tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'https?://|www\\.|@|\\.\\w{2,4}', flags=re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord(c) > 127) \u2014 proxies emojis/foreign scripts'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of consecutive duplicate word pairs (e.g., \"the the\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    dup_pairs = sum(1 for a, b in zip(tokens, tokens[1:]) if a == b)\n    return float(dup_pairs / (len(tokens) - 1))\n\n",
  "def feature(text: str) -> float:\n    'Average number of punctuation characters per sentence'\n    import re, string\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    punct_counts = [sum(1 for c in s if c in string.punctuation) for s in sentences]\n    if not punct_counts:\n        return 0.0\n    return float(sum(punct_counts) / len(punct_counts))\n",
  "def feature(text: str) -> float:\n    'Proportion of words longer than 6 characters (long-word density)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    long_count = sum(1 for w in words if len(w) > 6)\n    result = long_count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of tokens that contain acronyms/initialisms (>=2 consecutive uppercase letters)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = 0\n    for t in tokens:\n        if re.search(r'[A-Z]{2,}', t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that include at least one digit (numeric token ratio)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of distinct punctuation characters per sentence (punctuation variety)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    def puncts(s):\n        return set(c for c in s if not c.isalnum() and not c.isspace())\n    total = sum(len(puncts(s)) for s in sentences)\n    result = total / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Ratio of longest consecutive repeated word run to total words (detects stuttering/repetition)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    longest = 1\n    current = 1\n    for i in range(1, len(tokens)):\n        if tokens[i] == tokens[i-1]:\n            current += 1\n            if current > longest:\n                longest = current\n        else:\n            current = 1\n    result = longest / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that match common ASCII emoticons or heart (<3)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    emoticon_re = re.compile(r'^(?:[:;=8Xx][-~]?[)\\]D\\(\\]/\\\\OpP]|<3)$', flags=re.I)\n    count = 0\n    for t in tokens:\n        if emoticon_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that start with a capitalized (title-like) first word'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    valid = 0\n    cap = 0\n    for s in sentences:\n        m = re.search(r'\\w+', s)\n        if not m:\n            continue\n        valid += 1\n        word = m.group(0)\n        if word[0].isupper() and (len(word) == 1 or word[1:].islower()):\n            cap += 1\n    if valid == 0:\n        return float(0.0)\n    result = cap / valid\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are hyphenated internal-word forms (e.g., well-known)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if re.search(r'\\w-\\w', t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of punctuation characters per word (punctuation-to-word ratio)'\n    import re\n    if not text:\n        return float(0.0)\n    punct_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words = len(re.findall(r'\\w+', text))\n    if words == 0:\n        return float(0.0)\n    result = punct_chars / words\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are modal verbs (can/could/may/might/must/shall/should/will/would)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for t in tokens if t in modals)\n    result = count / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are written in all uppercase (acronyms, emphasis)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        # consider uppercase words with at least two characters to avoid counting \"I\"\n        if len(w) >= 2 and any(c.isalpha() for c in w) and w.isupper():\n            count += 1\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (stddev / mean)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    stdev = math.sqrt(var)\n    result = stdev / mean\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit (numeric token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'\\d', t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are punctuation-only (emoji-like or standalone punctuation)'\n    import re, string\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    punct_set = set(string.punctuation)\n    count = 0\n    for t in tokens:\n        if t and all((c in punct_set) for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of tokens per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    token_counts = [len(re.findall(r'\\S+', s)) for s in sentences]\n    if not token_counts:\n        return 0.0\n    result = sum(token_counts) / len(token_counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (simple list)'\n    import re\n    if not text:\n        return 0.0\n    STOPWORDS = {\n        'the','and','is','in','it','of','to','a','an','that','this','for','on',\n        'with','as','are','was','were','be','by','at','from','or','but','not',\n        'he','she','they','we','you','i','me','my','our','your','his','her','their'\n    }\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in STOPWORDS)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (ending with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain any non-ASCII character (unicode token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def has_non_ascii(t):\n        for c in t:\n            if ord(c) > 127:\n                return True\n        return False\n    count = sum(1 for t in tokens if has_non_ascii(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of adjacent duplicate word pairs (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if len(words) < 2:\n        return 0.0\n    dup = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    result = dup / (len(words) - 1)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word tokens / total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    STOPWORDS = {\n        'the','and','a','an','in','on','at','for','to','of','is','are','was','were',\n        'it','that','this','with','as','by','from','or','be','have','has','had','not',\n        'but','if','they','you','he','she','we','i','me','my','your','our','their'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in STOPWORDS)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are all-uppercase abbreviations or \"shouting\" (>=2 letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        return len(letters) >= 2 and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (mean words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        words = re.findall(r'\\w+', s)\n        return float(len(words)) if words else 0.0\n    lens = [len(re.findall(r'\\w+', sent)) for sent in sentences]\n    if not lens:\n        return 0.0\n    return float(sum(lens) / len(lens))\n\n",
  "def feature(text: str) -> float:\n    'Relative variation of sentence lengths (coefficient of variation of words per sentence)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        return 0.0\n    lens = [len(re.findall(r'\\w+', sent)) for sent in sentences]\n    if not lens:\n        return 0.0\n    n = len(lens)\n    mean = sum(lens) / n\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lens) / n\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing elongated character sequences (3+ repeated chars)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    elong_re = re.compile(r'(.)\\1{2,}')\n    count = sum(1 for t in tokens if elong_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like social media handles or hashtags (start with @ or #)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) > 1 and (t[0] in ('@', '#') and re.match(r'^[#@][\\w\\-_]+$', t)):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Punctuation-to-word ratio (punctuation characters / number of word tokens)'\n    import re\n    if not text:\n        return 0.0\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words = re.findall(r'\\w+', text)\n    denom = max(1, len(words))\n    return float(punct_count / denom)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of alphabetic characters that are uppercase (overall capitalization intensity)'\n    if not text:\n        return 0.0\n    total_alpha = sum(1 for c in text if c.isalpha())\n    if total_alpha == 0:\n        return 0.0\n    upper = sum(1 for c in text if c.isalpha() and c.isupper())\n    return float(upper / total_alpha)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain double quotation marks (indicates quotes/dialogue)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        sentences = [s]\n    quote_chars = ('\"', '\u201c', '\u201d')\n    count = sum(1 for sent in sentences if any(q in sent for q in quote_chars))\n    return float(count / len(sentences))\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stop = {'a','an','the','and','or','but','if','while','for','to','of','in','on','at','by','with','from','that','this','these','those','is','are','was','were','be','been','being','it','its','as','he','she','they','we','you','I','me','him','her','them','us'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stop)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas/semicolons per sentence (punctuation complexity)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    punct_count = sum(1 for c in text if c == ',' or c == ';')\n    return float(punct_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Density of personal pronouns (I, you, he, she, we, they, etc.) among tokens'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','you','he','him','she','her','we','us','they','them','my','your','his','her','our','their','mine','yours','hers','ours','theirs'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if any(c.isdigit() for c in w))\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Normalized character-level Shannon entropy (0-1)'\n    import math\n    if not text:\n        return 0.0\n    freqs = {}\n    for c in text:\n        freqs[c] = freqs.get(c, 0) + 1\n    n = sum(freqs.values())\n    if n == 0:\n        return 0.0\n    probs = [v / n for v in freqs.values()]\n    import math as _m\n    H = -sum(p * _m.log2(p) for p in probs if p > 0)\n    types = len(probs)\n    if types <= 1:\n        return 0.0\n    return float(H / _m.log2(types))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter (informal style)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    def starts_lower(s):\n        for c in s:\n            if c.isalpha():\n                return c.islower()\n        return False\n    count = sum(1 for s in sentences if starts_lower(s))\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_link(tok):\n        t = tok.lower()\n        if t.startswith('http://') or t.startswith('https://') or t.startswith('www.'):\n            return True\n        if '@' in tok and '.' in tok:\n            return True\n        return False\n    count = sum(1 for t in tokens if is_link(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average estimated syllables per word using vowel-group counts'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    def syllables(w):\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        return max(1, len(groups))\n    total = sum(syllables(w) for w in words)\n    return float(total / len(words))\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are common English stopwords (simple stopword density)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there','their'}\n    toks = re.findall(r'\\w+', text.lower())\n    if not toks:\n        return 0.0\n    count = sum(1 for t in toks if t in stopwords)\n    return float(count / len(toks))\n\n",
  "def feature(text: str) -> float:\n    'Lexical diversity: ratio of unique word tokens to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\w+', text.lower())\n    if not toks:\n        return 0.0\n    uniq = len(set(toks))\n    return float(uniq / len(toks))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-empty tokens that are all-uppercase words (length>1, contains letters)'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\S+', text)\n    if not toks:\n        return 0.0\n    def is_all_caps(t):\n        has_alpha = any(c.isalpha() for c in t)\n        return has_alpha and len([c for c in t if c.isalpha()]) > 1 and t.isupper()\n    count = sum(1 for t in toks if is_all_caps(t))\n    return float(count / len(toks))\n\n",
  "def feature(text: str) -> float:\n    'Variance of word lengths (higher means more variation in word size)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    n = len(words)\n    if n <= 1:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    return float(var)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit (numbers, codes, alphanumerics)'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\S+', text)\n    if not toks:\n        return 0.0\n    count = sum(1 for t in toks if any(c.isdigit() for c in t))\n    return float(count / len(toks))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like list items (bullets or numbered)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    pat = re.compile(r'^\\s*(?:[-*+]|(?:\\d+\\.))\\s+')\n    count = sum(1 for ln in lines if pat.match(ln))\n    return float(count / len(lines))\n\n",
  "def feature(text: str) -> float:\n    'Approximate Flesch reading ease score using vowel-group syllable proxy'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    syllables = sum(len(re.findall(r'[aeiouyAEIOUY]+', w)) or 1 for w in words)\n    words_per_sentence = len(words) / max(1, len(sentences))\n    syl_per_word = syllables / len(words)\n    # Flesch reading ease (approximate)\n    score = 206.835 - 1.015 * words_per_sentence - 84.6 * syl_per_word\n    return float(score)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences whose first alphabetic character is capitalized (sentence-start capitalization)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    def first_alpha_is_upper(s):\n        for ch in s:\n            if ch.isalpha():\n                return ch.isupper()\n        return False\n    count = sum(1 for s in sentences if first_alpha_is_upper(s))\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\S+', text)\n    if not toks:\n        return 0.0\n    url_email_re = re.compile(r'([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})|(https?://)|(www\\.)', re.I)\n    count = sum(1 for t in toks if url_email_re.search(t))\n    return float(count / len(toks))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing a character repeated three or more times consecutively (elongation/punctuation runs)'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\S+', text)\n    if not toks:\n        return 0.0\n    rep_re = re.compile(r'(.)\\1\\1')\n    count = sum(1 for t in toks if rep_re.search(t))\n    return float(count / len(toks))\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts) / len(word_counts))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a quotation or opening quote char'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    open_quotes = set(['\"', '\u201c', '\u00ab', '\u201a', '\u2018', \"'\"])\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    starts = 0\n    for s in sentences:\n        s_strip = s.lstrip()\n        if s_strip and s_strip[0] in open_quotes:\n            starts += 1\n    return float(starts / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Ratio of tokens that contain at least one digit (numeric token density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    numeric = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(numeric / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of nonempty lines that look like list items or bullets'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    pattern = re.compile(r'^\\s*(?:[-*\u2022]|[0-9]+[.\\)]|\\u2022|>\\s+)\\s+')\n    bullets = sum(1 for l in lines if pattern.match(l))\n    return float(bullets / len(lines))\n\n",
  "def feature(text: str) -> float:\n    'Average syllable count per word estimated by vowel groupings'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    syll_counts = []\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        count = len(groups)\n        # ensure at least one syllable\n        syll_counts.append(max(1, count))\n    return float(sum(syll_counts) / len(syll_counts))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are emoticons or contain emoji codepoints'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticon_re = re.compile(r'^(?:[:;=8][\\-^]?[)DdpP(/\\\\]|<3|:-?[\\]|\\\\]|:\\'-?\\(|:\\||;-\\))$')\n    def has_emoji(token):\n        for c in token:\n            o = ord(c)\n            if 0x1F300 <= o <= 0x1FAFF or 0x1F600 <= o <= 0x1F64F or 0x2600 <= o <= 0x26FF:\n                return True\n        return False\n    count = 0\n    for t in tokens:\n        if emoticon_re.search(t) or has_emoji(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of adjacent token pairs that are exact repeats (e.g. \"the the\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    return float(repeats / (len(words) - 1))\n\n",
  "def feature(text: str) -> float:\n    'Normalized standard deviation of word lengths (stdev / mean length)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    stdev = math.sqrt(var)\n    return float(stdev / mean)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    questions = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(questions / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain an apostrophe character (proxy for contractions)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"\\S+\", text)\n    if not tokens:\n        return 0.0\n    apos_chars = set([\"'\", \"\u2019\", \"\u02bc\"])\n    count = sum(1 for t in tokens if any(c in apos_chars for c in t))\n    return float(count / len(tokens))\n",
  "def feature(text: str) -> float:\n    'Std deviation of sentence lengths (in words) divided by mean sentence length'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lens = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lens:\n        return 0.0\n    mean = sum(lens) / len(lens)\n    if mean == 0 or len(lens) == 1:\n        return 0.0\n    var = sum((L - mean) ** 2 for L in lens) / len(lens)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that look like URLs (http(s) or www.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(?:https?://|www\\.)\\S+', flags=re.I)\n    count = sum(1 for t in tokens if url_re.match(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word-like tokens that contain any digit (numeric token density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    num = sum(1 for w in words if any(ch.isdigit() for ch in w))\n    return float(num / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of alphabetic word tokens that are fully uppercase (acronym density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    upper = 0\n    alpha_total = 0\n    for w in words:\n        if any(c.isalpha() for c in w):\n            alpha_total += 1\n            if w.isupper() and len([c for c in w if c.isalpha()]) >= 2:\n                upper += 1\n    if alpha_total == 0:\n        return 0.0\n    return float(upper / alpha_total)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (useful for language/emoji signals)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent word pairs that are exact repeats (e.g., \"the the\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = [w.lower() for w in re.findall(r'\\b\\w+\\b', text)]\n    if len(tokens) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(tokens)) if tokens[i] == tokens[i-1])\n    return float(repeats / max(1, len(tokens) - 1))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace tokens containing an apostrophe (contraction/possessive density)'\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if \"'\" in t)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Normalized entropy of punctuation character distribution (0-1)'\n    import string, math\n    if not text:\n        return 0.0\n    punct_chars = [c for c in text if c in string.punctuation]\n    total = len(punct_chars)\n    if total == 0:\n        return 0.0\n    freq = {}\n    for c in punct_chars:\n        freq[c] = freq.get(c, 0) + 1\n    probs = [v / total for v in freq.values()]\n    entropy = -sum(p * math.log2(p) for p in probs if p > 0)\n    types = len(probs)\n    if types <= 1:\n        return 0.0\n    norm = math.log2(types)\n    return float(entropy / norm)\n\n",
  "def feature(text: str) -> float:\n    'Average vowel fraction per word (vowels per word length), vowels include y'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    vowels = set('aeiouy')\n    ratios = []\n    for w in words:\n        L = len(w)\n        if L == 0:\n            continue\n        vc = sum(1 for ch in w if ch in vowels)\n        ratios.append(vc / L)\n    if not ratios:\n        return 0.0\n    return float(sum(ratios) / len(ratios))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (small built-in list)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stop = {'the','and','of','to','a','in','is','it','you','that','he','she','they','on','for','with','as','i','we','be','was','are','this','by','an','or'}\n    words = [w.lower() for w in re.findall(r'\\b\\w+\\b', text)]\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stop)\n    return float(count / len(words))\n",
  "def feature(text: str) -> float:\n    'Distinct-token ratio: number of unique word tokens divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    distinct = len(set(tokens))\n    result = distinct / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Stopword density: proportion of common function words among all tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','it','you','that','he','she','they','we','a','an','to','of','for','on','with','as','at','by','from','this','be','are','was','were','or','but','not','have','has','had','my','me','your','I','so'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'URL/email token density: fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def looks_like_link(tok):\n        if 'http' in tok.lower() or 'www.' in tok.lower() or '@' in tok:\n            return True\n        if re.search(r'\\.(com|net|org|io|gov|edu|co)(?:\\W|$)', tok.lower()):\n            return True\n        return False\n    count = sum(1 for t in tokens if looks_like_link(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Contraction density: fraction of whitespace-separated tokens containing an apostrophe'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if \"'\" in t)\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'All-caps word density: proportion of words (len>=2) that are entirely uppercase'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) >= 2 and w.isupper())\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Repeated-punctuation-run density: number of repeated punctuation runs (e.g., \"!!!\",\"...\") per character'\n    import re\n    if not text:\n        return 0.0\n    runs = re.findall(r'([!?\\.])\\1{1,}', text)\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    result = len(runs) / total_chars\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average commas/semicolons per sentence (proxy for sentence complexity)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(s.count(',') + s.count(';') for s in sentences)\n    result = count / len(sentences) if sentences else 0.0\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Short-word ratio: proportion of word tokens of length two or less'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) <= 2)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Exclamation sentence ratio: fraction of sentences that end with an exclamation mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if s.strip().endswith('!'))\n    result = count / len(sentences) if sentences else 0.0\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Internal-punctuation token ratio: fraction of tokens that contain punctuation and also alphanumeric chars (e.g., e-mail, contractions)'\n    import re, string\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def has_internal_punct(tok):\n        has_p = any(c in string.punctuation for c in tok)\n        has_alnum = any(c.isalnum() for c in tok)\n        all_punct = all((not c.isalnum()) for c in tok)\n        return has_p and has_alnum and (not all_punct)\n    count = sum(1 for t in tokens if has_internal_punct(t))\n    result = count / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain any digit (numeric content density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are predominantly ALL CAPS (>=2 letters uppercase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        if len(letters) < 2:\n            return False\n        return all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word tokens that occur only once'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    from collections import Counter\n    cnt = Counter(words)\n    hapaxes = sum(1 for w in words if cnt[w] == 1)\n    return float(hapaxes / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing an apostrophe (contractions/possessives)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if \"'\" in t and any(c.isalpha() for c in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Density of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', flags=re.I)\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}$')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t) or ('.com' in t.lower() or '.org' in t.lower()) and '/' in t:\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    s = text.strip()\n    sentences = [p for p in re.split(r'(?<=[.!?])\\s+', s) if p.strip()]\n    if not sentences:\n        sentences = [s]\n    qcount = sum(1 for sent in sentences if sent.rstrip().endswith('?'))\n    return float(qcount / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Average number of vowel groups per word (rough proxy for syllable density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    total = 0\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        total += max(1, len(groups))\n    return float(total / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that belong to consecutive repeated-token runs (e.g., \"no no no\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    run_tokens = 0\n    i = 0\n    while i < n:\n        j = i + 1\n        while j < n and tokens[j].lower() == tokens[i].lower():\n            j += 1\n        if j - i >= 2:\n            run_tokens += (j - i)\n        i = j\n    return float(run_tokens / n)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that end with a punctuation character'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if t and not t[-1].isalnum():\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Density of emoticons and emoji-like chars (text emoticons + high-codepoint chars)'\n    import re\n    if not text:\n        return 0.0\n    # common text emoticon patterns\n    emoticon_re = re.compile(r'(?:(?:[:;=8][-^]?[)DdpP(/\\[\\\\])|(?:<3)|(?:\\^\\_^))')\n    emoticons = len(emoticon_re.findall(text))\n    # count characters with high codepoints as proxy for emoji\n    emoji_like = sum(1 for c in text if ord(c) > 10000)\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float((emoticons + emoji_like) / total_chars)\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for s in sentences:\n        total_words += len(re.findall(r'\\w+', s))\n    return float(total_words / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique words / total words)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    return float(len(set(words)) / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are long (>=7 characters)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 7)\n    return float(long_count / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Ratio of tokens that contain any digit (numeric token density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Density of URLs or domain-like tokens among whitespace-separated tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'https?://\\S+|www\\.\\S+|\\b[\\w.-]+\\.(?:com|org|net|io|gov|edu|me|info)\\b', flags=re.I)\n    url_count = sum(1 for t in tokens if url_re.search(t))\n    return float(url_count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Density of email-like tokens among whitespace-separated tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', flags=re.I)\n    email_count = sum(1 for t in tokens if email_re.search(t))\n    return float(email_count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words in titlecase (initial capital, rest lower)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    title_count = sum(1 for w in words if w.istitle())\n    return float(title_count / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Relative word-length variability (std deviation of lengths divided by mean length)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of adjacent word pairs that are exact repeats (e.g., \"the the\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i - 1])\n    return float(repeats / (len(words) - 1))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing an apostrophe (contraction or possessive density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9'`]+\", text)\n    if not tokens:\n        return 0.0\n    apos_count = sum(1 for t in tokens if \"'\" in t or \"\u2019\" in t or \"`\" in t)\n    return float(apos_count / len(tokens))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total word tokens'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if not words:\n        return 0.0\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|ftp://|www\\.|mailto:|[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,})', flags=re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are likely emoji (common Unicode emoji ranges)'\n    import re\n    if not text:\n        return 0.0\n    try:\n        emoji_re = re.compile('[\\U0001F300-\\U0001F5FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F900-\\U0001F9FF\\U00002600-\\U000026FF\\U00002700-\\U000027BF]', flags=re.UNICODE)\n        matches = emoji_re.findall(text)\n    except re.error:\n        # fallback: treat high-codepoint chars as emoji-like\n        matches = [c for c in text if ord(c) >= 0x1F300]\n    total = len(text)\n    if total == 0:\n        return 0.0\n    return float(len(matches) / total)\n\n",
  "def feature(text: str) -> float:\n    'Longest run of the same character normalized by text length'\n    if not text:\n        return 0.0\n    max_run = 1\n    cur = 1\n    prev = text[0]\n    for c in text[1:]:\n        if c == prev:\n            cur += 1\n            if cur > max_run:\n                max_run = cur\n        else:\n            cur = 1\n            prev = c\n    return float(max_run / len(text))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a quotation mark or dash (dialogue-like starts)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    starts = ('\"', \"'\", '\u201c', '\u201d', '\u2018', '\u2019', '-', '\u2014')\n    count = sum(1 for s in sentences if s.lstrip().startswith(starts))\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that contain at least one digit'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text or '')\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if any(c.isdigit() for c in w))\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words longer than 12 characters (very long word density)'\n    import re\n    words = re.findall(r'\\w+', text or '')\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) > 12)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Normalized imbalance between matching bracket characters () [] {} <> (sum abs differences / total bracket count)'\n    if not text:\n        return 0.0\n    pairs = [('(', ')'), ('[', ']'), ('{', '}'), ('<', '>')]\n    total = 0\n    imbalance = 0\n    for o, c in pairs:\n        oc = text.count(o)\n        cc = text.count(c)\n        total += oc + cc\n        imbalance += abs(oc - cc)\n    if total == 0:\n        return 0.0\n    return float(imbalance / total)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is uppercase (sentence capitalization)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    total = 0\n    count = 0\n    for s in sentences:\n        for ch in s:\n            if ch.isalpha():\n                total += 1\n                if ch.isupper():\n                    count += 1\n                break\n    if total == 0:\n        return 0.0\n    return float(count / total)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent word pairs that are identical (immediate repeated words)'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i - 1])\n    return float(repeats / (len(words) - 1))\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not sentences:\n        return 0.0\n    return float(len(words) / max(1, len(sentences)))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.endswith('?'))\n    return float(q_count / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Ratio of tokens that contain at least one digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are all-caps (length>=2)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.isupper() and len(w) >= 2)\n    return float(count / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio (words that occur exactly once)'\n    import re\n    from collections import Counter\n    if not text or not text.strip():\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not words:\n        return 0.0\n    counts = Counter(words)\n    hapax = sum(1 for w, c in counts.items() if c == 1)\n    return float(hapax / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words that are common English stopwords (small list)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {\n        'the','a','an','and','or','but','if','in','on','at','by','for','with',\n        'to','of','is','it','this','that','these','those','he','she','they',\n        'we','you','i','was','were','be','been','has','have','had','not','as'\n    }\n    words = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas and semicolons per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    punct_count = text.count(',') + text.count(';')\n    return float(punct_count / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain explicit quotation characters'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', '\u00ab', '\u00bb')\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = sum(1 for s in sentences if any(q in s for q in quote_chars))\n    return float(count / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Approximate Flesch reading-ease score (heuristic syllable count)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not sentences or not words:\n        return 0.0\n    # approximate syllables by vowel groups per word\n    syllables = 0\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        syllables += max(1, len(groups))\n    asl = len(words) / len(sentences)  # average sentence length\n    asw = syllables / len(words)       # average syllables per word\n    flesch = 206.835 - 1.015 * asl - 84.6 * asw\n    return float(flesch)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num_tokens / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are entirely uppercase (shouting words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    all_caps = sum(1 for w in words if w.isupper() and len(w) > 1)\n    return float(all_caps / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(https?://\\S+|www\\.\\S+|\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b)', re.I)\n    matches = sum(1 for t in tokens if pattern.search(t))\n    return float(matches / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (stddev / mean), 0 if undefined'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens (simple list)'\n    import re\n    if not text:\n        return 0.0\n    STOP = {'the','is','in','it','and','or','to','a','of','for','on','with','as','by','an','be','this','that','are','was','were','has','have','at','from','but','not','they','you','i','we','he','she','his','her'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    stop_count = sum(1 for w in words if w in STOP)\n    return float(stop_count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent repeated word pairs (e.g., \"the the\") among all word gaps'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    return float(repeats / (len(words) - 1))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words that are titlecase (initial uppercase, rest lowercase) or single-letter uppercase'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    def is_title(w):\n        return w[0].isupper() and (len(w) == 1 or w[1:].islower())\n    title_count = sum(1 for w in words if is_title(w))\n    return float(title_count / len(words))\n",
  "def feature(text: str) -> float:\n    'Average sentence length measured in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    if not sentences:\n        return float(len(words))\n    return float(len(words) / max(1, len(sentences)))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are long (length > 7)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long = sum(1 for w in words if len(w) > 7)\n    return float(long / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that are detected URLs'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'https?://\\S+|www\\.\\S+|\\b\\S+\\.(?:com|org|net|io|gov|edu)(?:[/:]\\S*)?', flags=re.I)\n    matches = url_re.findall(text)\n    # matches may include overlaps; estimate by matching tokens\n    count = 0\n    for t in tokens:\n        if url_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that look like email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}$')\n    count = 0\n    for t in tokens:\n        if email_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters in the text that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are ALL CAPS and length>=2 (shouting tokens)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    caps = sum(1 for w in words if len(w) >= 2 and w.isupper())\n    return float(caps / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average estimated syllable count per word (vowel-group heuristic)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    total = 0\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        s = len(groups)\n        # simple heuristic: words with no vowel groups count as 1\n        if s <= 0:\n            s = 1\n        # reduce by one for trailing silent \"e\" in longer words\n        if len(w) > 2 and w.lower().endswith('e') and s > 1:\n            s -= 1\n        total += max(1, s)\n    return float(total / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of token occurrences that are part of repeated words (frequency>1)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    ctr = Counter(words)\n    repeated_tokens = sum(cnt for w, cnt in ctr.items() if cnt > 1)\n    return float(repeated_tokens / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Ratio of punctuation characters to word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    num_words = len(words)\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if num_words == 0:\n        return 0.0\n    return float(punct / num_words)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of unique word types that occur exactly once (hapax legomena ratio)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    ctr = Counter(words)\n    hapax = sum(1 for w, c in ctr.items() if c == 1)\n    return float(hapax / len(ctr))\n",
  "def feature(text: str) -> float:\n    'Lexical diversity: fraction of unique word tokens (lowercased)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (contain a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    q_count = sum(1 for s in sentences if '?' in s)\n    result = q_count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length measured in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not word_counts:\n        return 0.0\n    result = sum(word_counts) / len(word_counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain any numeric digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    result = num / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of emoticons and emoji (matches common ascii emoticons and basic emoji chars)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticon_re = re.compile(r'^(?:[:;=8][-^]?[)D(Pp/\\|\\\\]|<3|:\\||:\\(|:\\))$')\n    emoticon_count = sum(1 for t in tokens if emoticon_re.search(t))\n    # heuristic emoji count by codepoint ranges\n    emoji_count = 0\n    for ch in text:\n        try:\n            oc = ord(ch)\n        except Exception:\n            continue\n        if 0x1F300 <= oc <= 0x1F9FF:\n            emoji_count += 1\n    total = emoticon_count + emoji_count\n    result = total / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized variability of token lengths (stddev / mean token length)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    result = std / mean\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens (small stopword set)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','it','to','of','a','that','for','on','with','as','are','was','be','by','this','an','or','from','at','which'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    stop_count = sum(1 for w in words if w in stopwords)\n    result = stop_count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of titlecase words that occur not at the start of a sentence (proxy for proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    total_words = 0\n    title_nonstart = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        for i, w in enumerate(words):\n            total_words += 1\n            if i != 0 and w.istitle():\n                title_nonstart += 1\n    if total_words == 0:\n        return 0.0\n    result = title_nonstart / total_words\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of long words (>=8 characters) among all word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 8)\n    result = long_count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-empty tokens that are hyphenated (contain a hyphen between alnum parts)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t for t in re.findall(r'\\S+', text) if t.strip()]\n    if not tokens:\n        return 0.0\n    hyph_re = re.compile(r'[A-Za-z0-9]+-(?:[A-Za-z0-9]+)(?:-[A-Za-z0-9]+)*')\n    hyph_count = sum(1 for t in tokens if hyph_re.search(t))\n    result = hyph_count / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are emojis or common emoticons'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticons = {':)', ':-)', ':(', ':-(', ':D', ':-D', ';)', ';-)', ':P', ':-P', ':/', ':-/', \":'(\", ':-|', ':o', ':-o'}\n    def is_emoji_char(c):\n        try:\n            cp = ord(c)\n        except Exception:\n            return False\n        return (\n            0x1F300 <= cp <= 0x1F5FF or\n            0x1F600 <= cp <= 0x1F64F or\n            0x1F680 <= cp <= 0x1F6FF or\n            0x2600  <= cp <= 0x26FF  or\n            0x2700  <= cp <= 0x27BF  or\n            0x1F900 <= cp <= 0x1F9FF\n        )\n    count = 0\n    for t in tokens:\n        if t in emoticons:\n            count += 1\n            continue\n        if any(is_emoji_char(ch) for ch in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace tokens that are acronyms (all upper-case, length>=2)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # strip surrounding non-alphanumerics\n        tt = re.sub(r'^[^A-Za-z0-9]+|[^A-Za-z0-9]+$', '', t)\n        if re.fullmatch(r'[A-Z]{2,}', tt):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are pure numeric (digits only)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if re.fullmatch(r'\\d+', t))\n    return float(num / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Normalized variance of sentence lengths (words per sentence)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    # normalize by mean+1 to reduce scale sensitivity\n    return float(var / (mean + 1.0))\n\n",
  "def feature(text: str) -> float:\n    'Ratio of common English stopwords to total words (approximate)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','it','of','to','a','that','i','you','for','on','with','as','are','was','be','this','by','an','or','not'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain internal hyphens connecting word/number parts'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphen_count = 0\n    for t in tokens:\n        if '-' in t and re.search(r'[A-Za-z0-9]-[A-Za-z0-9]', t):\n            hyphen_count += 1\n    return float(hyphen_count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (immediate duplicates like \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    return float(repeats / max(1, len(words) - 1))\n\n",
  "def feature(text: str) -> float:\n    'Average syllable count per alphabetic word (simple vowel-group heuristic)'\n    import re\n    if not text:\n        return 0.0\n    words = [w for w in re.findall(r\"[A-Za-z']+\", text) if any(c.isalpha() for c in w)]\n    if not words:\n        return 0.0\n    def syllables(w):\n        w = w.lower()\n        groups = re.findall(r'[aeiouy]+', w)\n        # heuristics: at least one syllable\n        return max(1, len(groups))\n    avg = sum(syllables(w) for w in words) / len(words)\n    return float(avg)\n\n",
  "def feature(text: str) -> float:\n    'Diversity of punctuation: distinct punctuation characters divided by total punctuation count'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    unique = len(set(puncts))\n    return float(unique / total)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with an uppercase letter (formal-capitalization signal)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m and s[m.start()].isupper():\n            count += 1\n    return float(count / len(sentences))\n",
  "def feature(text: str) -> float:\n    'Ratio of unique word tokens to total word tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord > 127)'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    has_digit = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(has_digit / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain a repeated letter run of length >=3 (e.g., \"soooo\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'([A-Za-z])\\1{2,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Variety of punctuation: distinct punctuation characters divided by total punctuation chars'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct / len(puncts))\n\n",
  "def feature(text: str) -> float:\n    'Normalized character-level Shannon entropy (entropy / log2(unique_chars))'\n    import math, collections\n    if not text:\n        return 0.0\n    total = len(text)\n    counts = collections.Counter(text)\n    unique = len(counts)\n    if unique <= 1:\n        return 0.0\n    entropy = 0.0\n    for v in counts.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    norm = math.log2(unique)\n    if norm <= 0:\n        return 0.0\n    return float(entropy / norm)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens whose alphabetic characters are all uppercase (word-level shouting)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_word_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        return len(letters) >= 2 and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_word_all_caps(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is lowercase'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m and s[m.start()].islower():\n            count += 1\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing an apostrophe (contractions or possessives)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if (\"'\" in t) or (\"\\u2019\" in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look code-like (contain characters typical in code: ={}<>/\\\\;#*%|` )'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    code_chars = set('=<>/\\\\{};#*%|`@^~')\n    count = 0\n    for t in tokens:\n        if any(c in code_chars for c in t):\n            count += 1\n    return float(count / len(tokens))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if re.search(r'\\w', s)]\n    if not sentences:\n        # fallback to words-based average if no sentence punctuation\n        words = re.findall(r'\\w+', text)\n        return float(len(words) / max(1, 1)) if words else 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    return float(sum(lengths) / len(lengths))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are ALL CAPS words (>=2 letters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    caps = 0\n    for w in words:\n        # consider alpha-only tokens where letters are uppercase and length>=2\n        if any(c.isalpha() for c in w) and all((not c.isalpha()) or c.isupper() for c in w) and sum(1 for c in w if c.isalpha()) >= 2:\n            caps += 1\n    return float(caps / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are pure numeric tokens (digits only)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    nums = sum(1 for t in tokens if re.fullmatch(r'\\d+', t))\n    return float(nums / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Contraction/apostrophe density: fraction of whitespace tokens containing an apostrophe'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    apos = sum(1 for t in tokens if (\"'\" in t or \"\u2019\" in t))\n    return float(apos / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Pronoun density: fraction of word tokens that are common pronouns'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','you','he','she','it','we','they','me','him','her','us','them','my','your','his','her','our','their','mine','yours','hers'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in pronouns)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Emoticon/emoji token density: fraction of whitespace tokens that are emoticons or contain emoji characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    # common ASCII emoticons (simple set) and a basic emoji unicode block range\n    emot_re = re.compile(r'^(?:[:;=8X][-^]?[)D(Pp/\\\\]|<3|:\\'\\(|:\\'\\))$')\n    emoji_re = re.compile('[\\U0001F300-\\U0001F5FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF]', flags=re.UNICODE)\n    count = 0\n    for t in tokens:\n        if emot_re.search(t) or emoji_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average vowel-group count per word (rough syllable proxy)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+\", text.lower())\n    if not words:\n        return 0.0\n    totals = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        totals += max(1, len(groups))\n    return float(totals / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average number of modal verbs per sentence (can/could/may/might/will/would/shall/should/must/ought)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    modal_re = re.compile(r'\\b(?:can|could|may|might|will|would|shall|should|must|ought)\\b', flags=re.I)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if re.search(r'\\w', s)]\n    if not sentences:\n        # if no sentences, compute per entire text as one sentence\n        return float(len(modal_re.findall(text)) / 1.0)\n    counts = [len(modal_re.findall(s)) for s in sentences]\n    return float(sum(counts) / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Normalized entropy of word length distribution (0-1)'\n    import re, math, collections\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    freq = collections.Counter(lengths)\n    total = len(lengths)\n    ent = 0.0\n    for c in freq.values():\n        p = c / total\n        ent -= p * math.log2(p)\n    # normalize by maximum possible entropy = log2(number of distinct length classes)\n    k = len(freq)\n    if k <= 1:\n        return 0.0\n    max_ent = math.log2(k)\n    return float(ent / max_ent)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average commas+semicolons per sentence'\n    import re\n    if not text:\n        return 0.0\n    count = text.count(',') + text.count(';')\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    return float(count / max(1, len(sentences)))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of hapax legomena (words that occur exactly once)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    hapax = sum(1 for w,c in freqs.items() if c == 1)\n    return float(hapax / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord>127)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
  "def feature(text: str) -> float:\n    'Standard deviation of word lengths (0.0 if <2 words)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if len(words) < 2:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that contain a quoted segment (double quotes or typographic quotes)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = set(['\"', '\u201c', '\u201d', '\u00ab', '\u00bb'])\n    def has_quote(s):\n        return any(c in quote_chars for c in s)\n    count = sum(1 for s in sentences if has_quote(s))\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are Titlecase (start uppercase, not ALLCAPS)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    def is_title(t):\n        return len(t) > 0 and t[0].isupper() and not t.isupper()\n    count = sum(1 for t in tokens if is_title(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs, emails, mentions, or hashtags'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pat = re.compile(r'https?://|www\\.|@[A-Za-z0-9_]+|#[A-Za-z0-9_]+|[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}', flags=re.I)\n    count = sum(1 for t in tokens if pat.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Normalized variance of sentence lengths (variance divided by mean, 0.0 if insufficient data)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences if re.findall(r'\\w+', s)]\n    if len(lengths) < 2:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((L - mean) ** 2 for L in lengths) / len(lengths)\n    return float(var / mean)\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        word_counts.append(len(words))\n    if not word_counts:\n        return 0.0\n    result = sum(word_counts) / len(word_counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio (tokens that appear only once)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    c = Counter(tokens)\n    hapax = sum(1 for tok, cnt in c.items() if cnt == 1)\n    result = hapax / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit (numeric token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\S+\\b', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    result = num_count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of common emoticons/emoji-like tokens (matches per character)'\n    import re\n    if not text:\n        return 0.0\n    # ASCII emoticons and simple heart token\n    ascii_emot_re = re.compile(r'(?:(?:[:;=8][\\-^]?[)D\\(Pp/\\\\])|<3)')\n    matches = ascii_emot_re.findall(text)\n    count = len(matches)\n    # Attempt to count a few emoji ranges; ignore errors if regex engine differs\n    try:\n        emoji_re = re.compile(r'[\\U0001F300-\\U0001F6FF\\U0001F900-\\U0001F9FF\\U00002600-\\U000027BF]')\n        count += len(emoji_re.findall(text))\n    except re.error:\n        # Fallback: count uncommon non-ascii symbols that are neither alnum nor whitespace\n        count += sum(1 for ch in text if ord(ch) > 10000 and not ch.isalnum() and not ch.isspace())\n    total_chars = max(1, len(text))\n    result = count / total_chars\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with an uppercase letter'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m and s[m.start()].isupper():\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of token lengths (std/mean)'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    lengths = [len(t) for t in tokens]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    result = std / mean\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of punctuation characters that appear in long repeated runs (>=3)'\n    import re\n    if not text:\n        return 0.0\n    # Count total punctuation (non-alnum, non-space)\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    repeated = 0\n    for m in re.finditer(r'([^\\w\\s])\\1{2,}', text):\n        repeated += len(m.group(0))\n    result = repeated / total_punct\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average vowel-group count per alphabetic token (simple syllable proxy)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'[A-Za-z]+', text)\n    if not words:\n        return 0.0\n    vg_counts = [len(re.findall(r'[aeiouyAEIOUY]+', w)) for w in words]\n    # allow zero vowel groups (e.g., \"rhythm\"); keep as-is\n    result = sum(vg_counts) / len(vg_counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Ratio of tokens that look like URLs (http/www or contain domain-like pattern)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\S+\\b', text)\n    if not tokens:\n        return 0.0\n    url_count = 0\n    domain_re = re.compile(r'\\.\\w{2,4}(?:[:/]|$)')\n    for t in tokens:\n        if t.lower().startswith(('http://', 'https://', 'www.')) or '://' in t or domain_re.search(t):\n            url_count += 1\n    result = url_count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    result = repeats / (len(words) - 1)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that are all-uppercase words (>=2 letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    up_re = re.compile(r'^[A-Z]{2,}$')\n    count = sum(1 for t in tokens if up_re.match(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that end with an exclamation mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    ex_count = sum(1 for s in sentences if s.rstrip().endswith('!'))\n    return float(ex_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence (words defined by \\\\w+)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if sentences:\n        counts = []\n        for s in sentences:\n            counts.append(len(re.findall(r'\\w+', s)))\n        if not counts:\n            return 0.0\n        return float(sum(counts) / len(counts))\n    # fallback: if no clear sentences, return average words per 1\n    return float(len(words) / max(1, len(sentences) if sentences else 1))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that are numeric (ints or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?(?:\\d{1,3}(?:,\\d{3})+|\\d+)(?:\\.\\d+)?$')\n    count = 0\n    for t in tokens:\n        if num_re.match(t):\n            count += 1\n        else:\n            # allow simple decimals like .5 or percentages like 50%\n            if re.match(r'^[+-]?\\.\\d+$', t) or re.match(r'^[+-]?\\d+(?:\\.\\d+)?%$', t):\n                count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are Titlecase (capital first letter, rest lowercase)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w[0].isupper() and w[1:].islower())\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are short (length <= 2)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    short = sum(1 for w in words if len(w) <= 2)\n    return float(short / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like list items (bulleted or numbered)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    item_re = re.compile(r'^\\s*(?:[-*\u2022]|(?:\\d+[\\.\\)]|\\([a-zA-Z0-9]+\\))|[a-zA-Z]\\))\\s+')\n    count = sum(1 for l in lines if item_re.match(l))\n    return float(count / len(lines))\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not sentences or not words:\n        return 0.0\n    result = len(words) / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of alphabetic words (length>=2) that are all uppercase'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b[A-Za-z]{2,}\\b', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.isupper())\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    comma_count = text.count(',')\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    denom = max(1, len(sentences))\n    result = comma_count / denom\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like common emoticons (e.g. :-) :D <3)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'^(?:[:;=8][\\-~]?[)D\\(Pp3/\\\\]|<3)$')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average vowel fraction per alphabetic word (vowels/word length)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'[A-Za-z]+', text)\n    if not words:\n        return 0.0\n    total_frac = 0.0\n    for w in words:\n        lw = w.lower()\n        vowels = sum(1 for c in lw if c in 'aeiou')\n        total_frac += (vowels / len(lw)) if len(lw) > 0 else 0.0\n    result = total_frac / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of adjacent word pairs that are exact duplicates (case-insensitive)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    dup = sum(1 for i in range(len(tokens)-1) if tokens[i] == tokens[i+1])\n    result = dup / (len(tokens) - 1)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'\\b(?:https?://|www\\.)', re.I)\n    email_re = re.compile(r'\\b[\\w.+-]+@[\\w-]+\\.[\\w.-]+\\b')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord > 127)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that end with a run of two or more punctuation characters'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        # treat entire text as one sentence\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'([^\\w\\s])+$', s.strip())\n        if m and len(m.group(0)) >= 2:\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    result = len(words) / max(1, len(sents))\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that end with an exclamation mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    count = sum(1 for s in sents if s.endswith('!'))\n    result = count / len(sents)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are numeric (integers or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    nums = re.findall(r'\\b\\d+(?:\\.\\d+)?\\b', text)\n    result = len(nums) / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are in all-uppercase (length>=2, contains letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    up = 0\n    for t in tokens:\n        # consider tokens that have at least one letter and are all upper\n        if any(c.isalpha() for c in t) and len(t) > 1 and t.isupper():\n            up += 1\n    result = up / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio: words that appear exactly once divided by total words'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    cnt = Counter(words)\n    hapax = sum(1 for w, c in cnt.items() if c == 1)\n    result = hapax / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Diversity of punctuation used: distinct punctuation chars divided by standard set size'\n    import string\n    if not text:\n        return 0.0\n    puncts = set(c for c in text if c in string.punctuation)\n    total_types = len(string.punctuation) or 1\n    result = len(puncts) / total_types\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (std dev / mean of lengths)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    sd = math.sqrt(var)\n    result = sd / mean\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter (first alphabetic char)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    count = 0\n    for s in sents:\n        m = re.search(r'[A-Za-z]', s)\n        if m and m.group(0).islower():\n            count += 1\n    result = count / len(sents)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'URL/token density: fraction of word-like tokens that look like URLs/domains'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    urls = re.findall(r'https?://\\S+|www\\.\\S+|\\b[\\w-]+\\.(?:com|net|org|io|gov|edu|co)\\b', text, flags=re.I)\n    result = len(urls) / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Emoticon density: fraction of tokens that are common emoticons (e.g., :-) :( :D)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+|\\S', text)\n    if not tokens:\n        return 0.0\n    # common emoticon patterns (simple)\n    emoticons = re.findall(r'(?:(?:[:;=8][\\-^]?[)\\]DdpP(/\\|\\\\])|[)\\]DdpP(/\\|\\\\][\\-^]?[:;=8])', text)\n    result = len(emoticons) / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    words = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not words:\n        return 0.0\n    result = sum(words) / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    result = unique / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = digit_tokens / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Punctuation-to-word ratio (punctuation chars divided by number of words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = max(1, len(words))\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    result = punct / word_count\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = qcount / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a coordinating conjunction (and,but,or,so,yet,for,nor)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = {'and','but','or','so','yet','for','nor'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'^\\s*([A-Za-z]+)', s)\n        if m and m.group(1).lower() in conj:\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of short words (length <= 2 characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    short = sum(1 for w in words if len(w) <= 2)\n    result = short / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing non-ASCII characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    non_ascii = sum(1 for t in tokens if any(ord(c) > 127 for c in t))\n    result = non_ascii / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words estimated to be polysyllabic (>=3 vowel groups)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+\", text)\n    if not words:\n        return 0.0\n    def syllable_groups(w):\n        return len(re.findall(r'[aeiouyAEIOUY]+', w))\n    poly = sum(1 for w in words if syllable_groups(w) >= 3)\n    result = poly / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'([A-Za-z])', s)\n        if m and m.group(1).islower():\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    types = len(set(words))\n    result = types / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    result = num_tokens / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are all-caps (ignore single-letter tokens like \"I\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    def is_all_caps(w):\n        return any(ch.isalpha() for ch in w) and w.isupper() and sum(1 for ch in w if ch.isalpha()) >= 2\n    caps = sum(1 for w in words if is_all_caps(w))\n    result = caps / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of common English stopwords among word tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','a','an','and','or','but','if','to','of','in','for','on','with','as','at','by','from',\n                 'that','this','it','is','are','was','were','be','been','have','has','had','do','does','did',\n                 'not','no','so','will','would','can','could','should','may','might','he','she','they','we',\n                 'you','i','me','my','your','our','their','his','her','its'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    cnt = sum(1 for w in words if w in stopwords)\n    result = cnt / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-delimited tokens that contain hyphens, slashes, or underscores'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    special = sum(1 for t in tokens if ('-' in t) or ('_' in t) or ('/' in t))\n    result = special / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\") normalized by adjacent pairs'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    n = len(words)\n    if n < 2:\n        return 0.0\n    repeats = sum(1 for a, b in zip(words, words[1:]) if a == b)\n    result = repeats / max(1, n - 1)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if '?' in s)\n    result = qcount / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Diversity of punctuation characters used (distinct punctuation types normalized)'\n    import string\n    if not text:\n        return 0.0\n    punc = set(c for c in text if c in string.punctuation)\n    if not punc:\n        return 0.0\n    result = len(punc) / len(string.punctuation)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens longer than 7 characters (long-word density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) > 7)\n    result = long_words / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are hapax legomena (appear exactly once in the text)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    counts = Counter(words)\n    hapax_tokens = sum(1 for w in words if counts[w] == 1)\n    result = hapax_tokens / len(words)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that are predominantly numeric (dates, currencies, numbers)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = 0\n    num_re = re.compile(r'^[\\d\\-,./%\u20ac$\u00a3\u00a5]+$')\n    for t in tokens:\n        t2 = t.strip('.,:;!?()[]{}\"\\'')\n        if t2 and num_re.match(t2) and re.search(r'\\d', t2):\n            num += 1\n    return float(num / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are in ALL CAPS (length>=2), indicating emphasis/shouting'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        t2 = t.strip('.,:;!?()[]{}\"\\'')\n        if len(t2) >= 2 and any(c.isalpha() for c in t2) and t2 == t2.upper():\n            count += 1\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are hapax legomena (occur exactly once) relative to total tokens'\n    import re\n    from collections import Counter\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    freqs = Counter(tokens)\n    hapax = sum(1 for t in tokens if freqs[t] == 1)\n    return float(hapax / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that contain non-ASCII characters (accents, symbols)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if any(ord(ch) > 127 for ch in w):\n            count += 1\n    return float(count / len(words))\n\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentence count)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    total_commas = sum(s.count(',') for s in sentences)\n    return float(total_commas / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain at least one common emoticon (:-), :D, :(, <3, etc.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    emoticon_re = re.compile(r'(?:[:;=8][\\-~]?[)DdpP\\(/\\\\]|<3|:\\'\\)|:\\(|:\\||:-?\\/|:\\-?\\|)', flags=re.I)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if emoticon_re.search(s))\n    return float(count / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like dates (dd/mm, mm-dd-yyyy, yyyy-mm-dd, month names)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    date_patterns = [\n        re.compile(r'^\\d{1,2}[/-]\\d{1,2}(?:[/-]\\d{2,4})?$'),  # 1/2, 01-02-2000\n        re.compile(r'^\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}$'),        # 2000-01-02\n        re.compile(r'^(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[a-z]*\\.?$', re.I)\n    ]\n    count = 0\n    for t in tokens:\n        t2 = t.strip('.,;()[]{}\"\\'')\n        for p in date_patterns:\n            if p.match(t2):\n                count += 1\n                break\n    return float(count / len(tokens))\n\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (stddev / mean) to capture lexical length variability'\n    import re, math\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0 or len(lengths) < 2:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain quoted spans (paired double or paired single quotes or typographic quotes)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    quote_pairs = 0\n    for s in sentences:\n        dq = s.count('\"')\n        sq = s.count(\"'\")\n        lq = s.count('\u201c') + s.count('\u201d') + s.count('\u2018') + s.count('\u2019')\n        if dq >= 2 or lq >= 2 or sq >= 2:\n            quote_pairs += 1\n    return float(quote_pairs / len(sentences))\n\n\n",
  "def feature(text: str) -> float:\n    'Ratio of punctuation characters to alphabetic letters (punctuation_count / max(1, letter_count))'\n    if not text:\n        return 0.0\n    letters = sum(1 for c in text if c.isalpha())\n    punctuation = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punctuation / max(1, letters))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (approximate stopword density)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','to','of','a','that','it','for','on','with','as','are','was','be','by','this','an','or','from','at','not','but','have','has','had','were','they','you','i','he','she','we','their','them','his','her','its','which','who','what','when','where','why','how'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = qcount / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length measured in word tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    result = sum(lengths) / len(lengths)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word-like tokens that are all-caps (two or more letters uppercase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(t):\n        letters = [c for c in t if c.isalpha()]\n        return len(letters) >= 2 and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average per-token punctuation proportion (punctuation chars / token length)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    ratios = []\n    for t in tokens:\n        L = len(t)\n        if L == 0:\n            ratios.append(0.0)\n        else:\n            punct = sum(1 for c in t if not c.isalnum())\n            ratios.append(punct / L)\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain any non-ASCII character (accents, emoji, foreign script)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(ord(c) > 127 for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average vowel-group count per word (simple syllable proxy using vowel runs)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    counts = []\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        cnt = len(groups)\n        if cnt < 1:\n            cnt = 1\n        counts.append(cnt)\n    result = sum(counts) / len(counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that are numeric-like (contain digits and typical numeric punctuation)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[\\d\\.,%:\\-/]+$')\n    count = 0\n    for t in tokens:\n        if re.search(r'\\d', t) and num_re.match(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent word tokens that are immediate repeats (case-insensitive)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    result = repeats / (len(words) - 1)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL CAPS (useful for shouting/abbrev detection)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # consider token as ALL CAPS if it has at least one letter and all letters are uppercase\n        letters = [c for c in t if c.isalpha()]\n        if letters and all(c.isupper() for c in letters):\n            # ignore single-letter tokens like 'I'\n            if len(letters) >= 2:\n                count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (lexical diversity): unique word forms / total words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing any digit (numbers, dates, codes)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    totals = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        totals += len(words)\n    return float(totals / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of words with elongated letters (e.g., \"sooo\", \"loooove\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if re.search(r'(.)\\1{2,}', w))\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Distinct punctuation variety normalized (distinct punctuation characters / 10, capped at 1.0)'\n    if not text:\n        return 0.0\n    puncts = set(ch for ch in text if not ch.isalnum() and not ch.isspace())\n    if not puncts:\n        return 0.0\n    result = min(len(puncts) / 10.0, 1.0)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are titlecase words (Initial capital then lowercase)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    all_tokens = re.findall(r'\\w+', text)\n    if not all_tokens:\n        return 0.0\n    title_tokens = re.findall(r'\\b[A-Z][a-z]+\\b', text)\n    return float(len(title_tokens) / len(all_tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average length of alphabetic tokens (letters only)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    alpha_tokens = re.findall(r'[A-Za-z]+', text)\n    if not alpha_tokens:\n        return 0.0\n    avg = sum(len(t) for t in alpha_tokens) / len(alpha_tokens)\n    return float(avg)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a WH-question word (who, what, when, where, why, how)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    qwords = {'who', 'what', 'when', 'where', 'why', 'how'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        s2 = re.sub(r'^[^A-Za-z0-9]+', '', s).lower()\n        parts = re.findall(r'\\w+', s2)\n        if parts and parts[0] in qwords:\n            count += 1\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens (simple stopword density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there','their'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n",
  "def feature(text: str) -> float:\n    'Stopword token density: fraction of word tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there','their'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are all uppercase (>=2 letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if t.isalpha() and len(t) >= 2 and t.upper() == t:\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are long (>=12 characters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) >= 12)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average approximate syllable count per word (vowel-group heuristic)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    def syls(w):\n        groups = re.findall(r'[aeiouy]+', w.lower())\n        return max(1, len(groups))\n    totals = sum(syls(w) for w in words)\n    return float(totals / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences containing quotation marks or quotes'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    quote_chars = set(['\"', '\u201c', '\u201d', \"\u00ab\", \"\u00bb\"])\n    count = 0\n    for s in sentences:\n        if any(c in s for c in quote_chars):\n            count += 1\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace tokens that look like URLs or domain mentions'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'(https?://|www\\.|\\.\\w{2,4}(/|$))', flags=re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace tokens that contain digits (numeric tokens)'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing a character repeated three or more times consecutively'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    pat = re.compile(r'(.)\\1{2,}', flags=re.I)\n    count = sum(1 for t in tokens if pat.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Personal pronoun token density (first/second/third person pronouns)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','you','your','yours','he','him','his','she','her','hers','they','them','their','theirs','it','its'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating conjunction (and, but, or, so, yet, for, nor)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = {'and','but','or','so','yet','for','nor'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'\\b([A-Za-z]+)\\b', s)\n        if m and m.group(1).lower() in conj:\n            count += 1\n    return float(count / len(sentences))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of distinct word types divided by total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    types = set(words)\n    result = len(types) / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord > 127)'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        if s.rstrip().endswith('?'):\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are hyphenated (contain a hyphen)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hy = sum(1 for t in tokens if '-' in t)\n    result = hy / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Variance of word lengths (population variance of lengths of alphanumeric words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    n = len(words)\n    if n == 0:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / n\n    mean_sq = sum(l * l for l in lengths) / n\n    var = mean_sq - mean * mean\n    if var < 0:\n        var = 0.0\n    return float(var)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words longer than 12 characters (long-word density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 12)\n    result = long_count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Ratio of punctuation characters to alphabetic characters'\n    if not text:\n        return 0.0\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    letters = sum(1 for c in text if c.isalpha())\n    if letters == 0:\n        return 0.0\n    result = punct / letters\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Longest run of the same character divided by text length (captures repeated characters)'\n    import re\n    if not text:\n        return 0.0\n    longest = 0\n    for m in re.finditer(r'(.)\\1*', text, flags=re.S):\n        ln = len(m.group(0))\n        if ln > longest:\n            longest = ln\n    denom = max(1, len(text))\n    result = longest / denom\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of first-person pronouns (I, me, my, we, us, our, mine, ours)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    pronouns = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for w in words if w in pronouns)\n    result = count / len(words)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    types = len(set(words))\n    result = types / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    total_words = 0\n    for s in sents:\n        total_words += len(re.findall(r'\\w+', s))\n    if not total_words:\n        return 0.0\n    result = total_words / len(sents)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    q = sum(1 for s in sents if s.rstrip().endswith('?'))\n    result = q / len(sents)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are contractions/contain internal apostrophes'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    ct = 0\n    for t in tokens:\n        if re.search(r\"[A-Za-z]+'[A-Za-z]+\", t):\n            ct += 1\n    result = ct / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    commas = text.count(',')\n    result = commas / len(sents) if len(sents) > 0 else 0.0\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that contain a hyphen'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hy = sum(1 for t in tokens if '-' in t and re.search(r'[A-Za-z0-9]', t))\n    result = hy / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average syllable-like count per word (approx: vowel groups)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+\", text)\n    if not words:\n        return 0.0\n    tot = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w.lower())\n        count = len(groups)\n        if count == 0:\n            count = 1\n        tot += count\n    result = tot / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words that are repeated within their sentence (duplicate tokens)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    total_words = 0\n    repeated = 0\n    for s in sents:\n        words = re.findall(r'\\w+', s.lower())\n        total_words += len(words)\n        if words:\n            from collections import Counter\n            c = Counter(words)\n            for v in c.values():\n                if v > 1:\n                    repeated += (v - 1)\n    if total_words == 0:\n        return 0.0\n    result = repeated / total_words\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized character entropy (Shannon) of the text excluding whitespace'\n    import math\n    from collections import Counter\n    if not text or not text.strip():\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    cnt = Counter(chars)\n    total = len(chars)\n    entropy = 0.0\n    for v in cnt.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    unique = len(cnt)\n    if unique <= 1:\n        return 0.0\n    max_entropy = math.log2(unique)\n    result = entropy / max_entropy if max_entropy > 0 else 0.0\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation chars divided by total punctuation count'\n    import string\n    if not text or not text.strip():\n        return 0.0\n    puncts = [c for c in text if c in string.punctuation]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    result = distinct / total\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    words_per_sentence = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        words_per_sentence.append(len(words))\n    if not words_per_sentence:\n        return 0.0\n    result = sum(words_per_sentence) / len(words_per_sentence)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if s.strip().endswith('?'))\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are all-caps words (shouting), length >=2'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    cap_count = 0\n    for t in tokens:\n        # strip common surrounding punctuation\n        t2 = re.sub(r'^[^\\w]+|[^\\w]+$', '', t)\n        if len(t2) >= 2 and any(c.isalpha() for c in t2) and t2.isupper():\n            cap_count += 1\n    result = cap_count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Count of URLs or email-like tokens per 100 words (normalized density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    urls = re.findall(r'https?://\\S+|www\\.\\S+|\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', text, flags=re.I)\n    # normalize to per-100-words\n    result = (len(urls) / len(words)) * 100.0\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = num_tokens / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word types that occur only once (hapax legomena ratio)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    from collections import Counter\n    freqs = Counter(words)\n    hapax = sum(1 for w, c in freqs.items() if c == 1)\n    result = hapax / len(freqs)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average vowel-to-consonant ratio per word (letters only)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    vowels = set('aeiou')\n    ratios = []\n    for w in words:\n        letters = [c for c in w if c.isalpha()]\n        if not letters:\n            continue\n        v = sum(1 for c in letters if c in vowels)\n        c = sum(1 for c in letters if c.isalpha() and c not in vowels)\n        # avoid division by zero: if no consonants, treat ratio as v (large)\n        if c == 0:\n            ratios.append(float(v))\n        else:\n            ratios.append(v / c)\n    if not ratios:\n        return 0.0\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain quotation marks (dialogue or quotes)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    quotes = set('\"\\'\u201c\u201d\u2018\u2019')\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        if any(q in s for q in quotes):\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of distinct punctuation characters per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    counts = []\n    for s in sentences:\n        pset = set(c for c in s if not c.isalnum() and not c.isspace())\n        counts.append(len(pset))\n    if not counts:\n        return 0.0\n    result = sum(counts) / len(counts)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    result = len(set(tokens)) / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens containing a run of the same letter three or more times (elongation)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if re.search(r'(.)\\1\\1', w, flags=re.IGNORECASE):\n            count += 1\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of emoticons and emoji characters measured per character length'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    if total_len == 0:\n        return 0.0\n    # common ASCII emoticons\n    ascii_emots = re.findall(r'(?:(?:[:;=8][\\-~]?[)DdpP\\(\\\\/])|<3)', text)\n    ascii_count = len(ascii_emots)\n    # count emoji-range characters (several common Unicode blocks)\n    emoji_count = 0\n    for c in text:\n        o = ord(c)\n        if (0x1F300 <= o <= 0x1F5FF) or (0x1F600 <= o <= 0x1F64F) or (0x1F680 <= o <= 0x1F6FF) or (0x2600 <= o <= 0x26FF) or (0x2700 <= o <= 0x27BF):\n            emoji_count += 1\n    result = (ascii_count + emoji_count) / total_len\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is lowercase (informal starts)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        for ch in s:\n            if ch.isalpha():\n                if ch.islower():\n                    count += 1\n                break\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain internal punctuation like apostrophes or hyphens (e-mail, you\\'re)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'\\w[\\'\\-]\\w', t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Ratio of average alphabetic-token length to average token length (alpha-token length bias)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    alpha_tokens = [t for t in tokens if t.isalpha()]\n    if not alpha_tokens:\n        return 0.0\n    avg_alpha = sum(len(t) for t in alpha_tokens) / len(alpha_tokens)\n    avg_all = sum(len(t) for t in tokens) / len(tokens)\n    if avg_all == 0:\n        return 0.0\n    result = avg_alpha / avg_all\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        # if no clear sentences, treat whole text as one and check\n        return float(text.strip().endswith('?'))\n    count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized character-level Shannon entropy over letters a-z (0-1)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    letters = re.findall(r'[a-z]', text.lower())\n    if not letters:\n        return 0.0\n    from collections import Counter\n    freq = Counter(letters)\n    total = len(letters)\n    ent = 0.0\n    for v in freq.values():\n        p = v / total\n        if p > 0:\n            ent -= p * math.log2(p)\n    # normalize by log2(26)\n    max_ent = math.log2(26)\n    result = ent / max_ent if max_ent > 0 else 0.0\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are long (12 or more characters)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 12)\n    result = long_count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of multi-character tokens that are written in ALL CAPS (shouting indicator)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    caps = sum(1 for t in tokens if len(t) >= 2 and t.isupper())\n    result = caps / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return 0.0\n    # small common stopword set for efficiency\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not',\n        'on','with','he','as','you','do','at','this','but','his','by','from',\n        'they','we','say','her','she','or','an','will','my','one','all','would',\n        'there','their','what','so','up','out','if','about','who','get','which',\n        'go','me'\n    }\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are written in ALL CAPS (len>=2 and contain letters)'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\b\\S+\\b', text)\n    if not toks:\n        return 0.0\n    def is_all_caps(t):\n        has_alpha = any(ch.isalpha() for ch in t)\n        return has_alpha and t.upper() == t and sum(ch.isalpha() for ch in t) >= 2\n    count = sum(1 for t in toks if is_all_caps(t))\n    return float(count / len(toks))\n\n",
  "def feature(text: str) -> float:\n    'Normalized variance of sentence lengths (word counts) = var(lengths)/(mean+1)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((L - mean) ** 2 for L in lengths) / len(lengths)\n    return float(var / (mean + 1.0))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of characters that are alphabetic letters (letters / total chars)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    letters = sum(1 for ch in text if ch.isalpha())\n    return float(letters / total)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that match common contraction pattern (e.g., don\\'t, I\\'m)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+'\\w+\\b\", text)\n    all_toks = re.findall(r'\\b\\w+\\b', text)\n    if not all_toks:\n        return 0.0\n    return float(len(tokens) / len(all_toks))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is uppercase (start-cap consistency)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    def starts_upper(s):\n        for ch in s:\n            if ch.isalpha():\n                return ch.isupper()\n        return False\n    count = sum(1 for s in sentences if starts_upper(s))\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of word tokens that occur exactly once in the text'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freq = {}\n    for w in words:\n        freq[w] = freq.get(w, 0) + 1\n    hapax = sum(1 for w in words if freq.get(w, 0) == 1)\n    return float(hapax / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens containing a character repeated three or more times in a row (elongation)'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\b\\S+\\b', text)\n    if not toks:\n        return 0.0\n    rep_re = re.compile(r'(.)\\1\\1+')\n    count = sum(1 for t in toks if rep_re.search(t))\n    return float(count / len(toks))\n",
  "def feature(text: str) -> float:\n    'Lexical diversity: fraction of unique word tokens (type/token ratio)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    uniq = len(set(tokens))\n    result = uniq / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit (numbers, codes, dates)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (accents, emoji, other scripts)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized sentence-length variance (variance of words per sentence, divided by mean+1)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if len(sentences) <= 1:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    # normalize by mean+1 to keep scale reasonable\n    result = var / (mean + 1.0)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Contraction density: fraction of tokens that match common contractions (e.g., don\\'t, I\\'m)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    contractions = re.findall(r\"\\b[a-zA-Z]+['\u2019][a-zA-Z]+\\b\", text)\n    result = len(contractions) / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of single-character tokens (isolated letters or punctuation tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) == 1)\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Pronoun density: fraction of word tokens that are common personal pronouns (I, you, he, she, we, they, etc.)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','you','he','she','we','they','me','him','her','us','them','it','its','my','your','our','their','mine','yours','hers','his'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in pronouns)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    result = distinct / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Uppercase-token ratio: fraction of tokens that are all uppercase and length>=2 (shouting tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) >= 2 and t.isupper())\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of long alphabetic words (words with >12 letters) as a measure of lexical complexity'\n    import re\n    if not text:\n        return 0.0\n    alpha_words = re.findall(r'[A-Za-z]+', text)\n    if not alpha_words:\n        return 0.0\n    long_count = sum(1 for w in alpha_words if len(w) > 12)\n    result = long_count / len(alpha_words)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    total_words = 0\n    for s in sentences:\n        total_words += len(re.findall(r'\\w+', s))\n    if not sentences:\n        return 0.0\n    return float(total_words / max(1, len(sentences)))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are fully uppercase (shouting)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.isupper() and len(t) > 1)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','a','an','of','to','in','is','it','that','this','for','on','with','as','are','was','were','be','by','or','from','at','not','but','you','i','he','she','they','we','his','her','their'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Approximate lexical density: fraction of tokens likely to be content words'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','a','an','of','to','in','is','it','that','this','for','on','with','as','are','was','were','be','by','or','from','at','not','but','you','i'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    content = sum(1 for t in tokens if len(t) > 3 and t not in stopwords)\n    return float(content / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain digits (numbers, dates, codes)'\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of immediate adjacent word pairs that are exact repeats (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    pairs = sum(1 for i in range(len(tokens)-1) if tokens[i] == tokens[i+1])\n    return float(pairs / max(1, len(tokens)))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are inside quoted spans (double or substantial single quotes)'\n    import re\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    inside = 0\n    # double-quoted spans\n    for m in re.findall(r'\"(.*?)\"', text, flags=re.S):\n        inside += len(m)\n    # single-quoted spans that contain a space (avoid contractions)\n    for m in re.findall(r\"'([^']+\\s[^']+)'\", text, flags=re.S):\n        inside += len(m)\n    return float(min(1.0, inside / total))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(https?://|www\\.|@)')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Diversity of punctuation: distinct punctuation symbols divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct / total)\n\n",
  "def feature(text: str) -> float:\n    'Average number of clause separators (commas/semicolons/colons) per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    sep_count = text.count(',') + text.count(';') + text.count(':')\n    return float(sep_count / max(1, len(sentences)))\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths) / len(lengths))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that are questions'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    q = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are ALL CAPS (>=2 letters and contain a letter)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_shouting(t):\n        return any(c.isalpha() for c in t) and len([c for c in t if c.isalpha()]) >= 2 and t.upper() == t\n    count = sum(1 for t in tokens if is_shouting(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    nonascii = sum(1 for c in text if ord(c) > 127)\n    return float(nonascii / total)\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of word tokens that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    hapaxes = sum(1 for t in tokens if counts[t] == 1)\n    return float(hapaxes / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average punctuation density per sentence (punctuation chars / sentence length)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    densities = []\n    for s in sentences:\n        length = len(s)\n        if length == 0:\n            continue\n        punct = sum(1 for c in s if not c.isalnum() and not c.isspace())\n        densities.append(punct / length)\n    if not densities:\n        return 0.0\n    return float(sum(densities) / len(densities))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Normalized character entropy (0-1) of non-space characters'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    counts = Counter(chars)\n    total = len(chars)\n    entropy = -sum((v/total) * math.log(v/total, 2) for v in counts.values() if v > 0)\n    k = len(counts)\n    if k <= 1:\n        return 0.0\n    norm = entropy / math.log(k, 2)\n    return float(norm)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of non-initial tokens that start with an uppercase letter (possible proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    noninitial_total = 0\n    cap_count = 0\n    for s in sentences:\n        tokens = re.findall(r'\\w+', s)\n        if len(tokens) <= 1:\n            continue\n        for t in tokens[1:]:\n            noninitial_total += 1\n            if t and t[0].isupper():\n                cap_count += 1\n    if noninitial_total == 0:\n        return 0.0\n    return float(cap_count / noninitial_total)\n\n",
  "def feature(text: str) -> float:\n    'Overall vowel density among alphabetic characters (vowels / letters)'\n    if not text:\n        return 0.0\n    letters = [c for c in text if c.isalpha()]\n    if not letters:\n        return 0.0\n    vowels = sum(1 for c in letters if c.lower() in 'aeiou')\n    return float(vowels / len(letters))\n",
  "def feature(text: str) -> float:\n    'Lexical diversity: unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths) / len(lengths))\n\n",
  "def feature(text: str) -> float:\n    'Digit character density: fraction of characters that are digits'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    digits = sum(1 for c in text if c.isdigit())\n    return float(digits / total)\n\n",
  "def feature(text: str) -> float:\n    'Hyphenated token ratio: fraction of whitespace-separated tokens containing a hyphen'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if '-' in t and re.search(r'[A-Za-z]', t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Titlecase word density: fraction of words that are Titlecase (First upper, rest lower) or single uppercase letter'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if w[0].isupper() and (len(w) == 1 or w[1:].islower()):\n            count += 1\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Elongated word density: fraction of tokens containing a character repeated three or more times consecutively'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pat = re.compile(r'(.)\\1\\1+', flags=re.I)\n    count = sum(1 for t in tokens if pat.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Quoted content fraction: fraction of characters contained inside single or double quotes'\n    import re\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    inside = 0\n    # matches either \"...\" or '...'\n    for a, b in re.findall(r'\"(.*?)\"|\\'(.*?)\\'', text, flags=re.S):\n        if a:\n            inside += len(a)\n        if b:\n            inside += len(b)\n    return float(min(1.0, inside / total))\n\n",
  "def feature(text: str) -> float:\n    'Stopword density: fraction of word tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'a','an','the','and','or','but','if','while','with','without','to','of','in','on','for',\n        'is','are','was','were','be','been','being','at','by','from','as','that','this','these',\n        'those','it','its','he','she','they','them','we','us','you','your','I','me','my','mine'\n    }\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Character entropy (normalized): Shannon entropy of character distribution normalized to [0,1]'\n    import math\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    freq = {}\n    for c in text:\n        freq[c] = freq.get(c, 0) + 1\n    n_types = len(freq)\n    if n_types <= 1:\n        return 0.0\n    ent = 0.0\n    for v in freq.values():\n        p = v / total\n        ent -= p * math.log2(p)\n    # normalize by max entropy log2(n_types)\n    max_ent = math.log2(n_types)\n    return float(ent / max_ent)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a lowercase letter (informal/fragment starts)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        # find first alphabetic character\n        m = re.search(r'[A-Za-z]', s)\n        if m:\n            ch = m.group(0)\n            if ch.islower():\n                count += 1\n    return float(count / len(sentences))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    types = len(set(tokens))\n    result = types / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (simple stopword density)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'if', 'is', 'are', 'was', 'were', 'in', 'on', 'at', 'for', 'to', 'of', 'with', 'by', 'from', 'that', 'this', 'these', 'those', 'it', 'i', 'you', 'he', 'she', 'they', 'we'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that start with an uppercase letter (capitalized-word ratio)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    cap_count = sum(1 for t in tokens if t[0].isupper())\n    result = cap_count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that contain at least one digit (numeric-token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = num_count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized character-level Shannon entropy (0-1), measuring character diversity'\n    from collections import Counter\n    import math\n    if not text:\n        return 0.0\n    counts = Counter(text)\n    total = sum(counts.values())\n    if total == 0:\n        return 0.0\n    entropy = 0.0\n    for ccount in counts.values():\n        p = ccount / total\n        entropy -= p * math.log2(p)\n    unique = len(counts)\n    if unique <= 1:\n        return 0.0\n    max_entropy = math.log2(unique)\n    result = entropy / max_entropy\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of punctuation characters per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    punct_counts = []\n    for s in sentences:\n        punct_counts.append(sum(1 for c in s if not c.isalnum() and not c.isspace()))\n    result = sum(punct_counts) / len(sentences) if sentences else 0.0\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are modal verbs (can/could/may/might/must/shall/should/will/would)'\n    import re\n    if not text:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in modals)\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of space-separated tokens that look like URLs or email addresses'\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        t_low = t.lower()\n        if 'http' in t_low or t_low.startswith('www.') or '@' in t:\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of adjacent sentences that start with the same word (repeated sentence-starts)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    starts = []\n    for s in sentences:\n        m = re.findall(r'\\w+', s)\n        starts.append(m[0].lower() if m else '')\n    if len(starts) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(len(starts) - 1) if starts[i] and starts[i] == starts[i + 1])\n    result = repeats / (len(starts) - 1)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of common ASCII emoticons (count per token), e.g., :) :( :D :P ;)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    pattern = r'(?:(?:[:;=8][\\-^]?[)DPp\\(\\]/\\\\])|<3|:-?\\|)'\n    emoticons = re.findall(pattern, text)\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    result = len(emoticons) / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        # fallback to words in whole text\n        words = re.findall(r'\\w+', text)\n        return float(len(words)) if words else 0.0\n    words_per_sent = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        words_per_sent.append(len(words))\n    if not words_per_sent:\n        return 0.0\n    result = sum(words_per_sent) / len(words_per_sent)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 1.0 if text.strip().endswith('?') else 0.0\n    q_count = sum(1 for s in sentences if s.strip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL-CAPS words (length>=2)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # strip surrounding punctuation\n        clean = re.sub(r'^[^\\w]+|[^\\w]+$', '', t)\n        if len(clean) >= 2 and any(c.isalpha() for c in clean) and clean.isupper():\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)\\S+|^\\S+\\.(com|net|org|io|gov|edu)(/.*)?$', re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that look like email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'^[^@\\s]+@[^@\\s]+\\.[A-Za-z]{2,}$')\n    count = sum(1 for t in tokens if email_re.match(t.strip('.,;:()[]{}<>')))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of token lengths (std/mean) using word tokens'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    result = std / mean\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word-tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'a','an','the','and','or','but','if','then','because','of','to','in','on','for','with',\n        'as','at','by','from','that','this','these','those','is','are','was','were','be','been',\n        'being','have','has','had','do','does','did','will','would','can','could','should',\n        'not','no','yes','so','into','about','over','under','up','down','out','only','than'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word-tokens that are first-person pronouns (I/me/we/us/my/our...)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','we','us','my','mine','our','ours'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in pronouns)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of characters that are non-ASCII (likely non-Latin or special chars)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that match common emoticon patterns'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emo_re = re.compile(r'^[<>]?[;:=8][\\-o\\*\\']?[\\)\\]\\(dDpP/\\\\|]|[;\\:\\-\\=][\\)D\\(\\]/\\\\Pp]$')\n    # also accept simple :-) :-( :D :P ;)\n    count = 0\n    for t in tokens:\n        if emo_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if words:\n            word_counts.append(len(words))\n    if not word_counts:\n        return 0.0\n    result = sum(word_counts) / len(word_counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized character Shannon entropy (0-1)'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    cnt = Counter(text)\n    total = sum(cnt.values())\n    if total == 0:\n        return 0.0\n    entropy = 0.0\n    for v in cnt.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    # normalize by log2 of alphabet size (at least 1 to avoid div by zero)\n    alph = max(1, len(cnt))\n    norm = entropy / math.log2(alph) if alph > 1 else 0.0\n    return float(min(1.0, max(0.0, norm)))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that contain a digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of common ASCII emoticons per token (smile/frown/wink/heart etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    # common ASCII emoticon patterns and heart <3\n    emot_re = re.compile(r'(:-\\)|:\\)|:-\\(|:\\(|:D|:d|:P|:p|;-?\\)|<3|:\\||:/|:\\\\|:O|:o|=\\)|=\\()')\n    count = 0\n    for t in tokens:\n        if emot_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of capitalized words (Titlecase) excluding sentence-initial words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words_all = []\n    non_initial = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if not words:\n            continue\n        words_all.extend(words)\n        # exclude first token of the sentence\n        for w in words[1:]:\n            non_initial.append(w)\n    if not non_initial:\n        return 0.0\n    cap_count = sum(1 for w in non_initial if w[0].isupper() and w[1:].islower())\n    result = cap_count / len(non_initial)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain modal verbs (can/could/may/might/must/shall/should/will/would)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    modals = r'\\b(?:can|could|may|might|must|shall|should|will|would)\\b'\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    cre = re.compile(modals, flags=re.I)\n    for s in sentences:\n        if cre.search(s):\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Heuristic passive-voice sentence fraction (forms of \"be\" near past participles)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # look for be-verb within a few tokens of a word ending with ed/en (simple heuristic)\n    pattern = re.compile(r'\\b(?:am|is|are|was|were|be|been|being)\\b(?:\\s+\\w+){0,3}\\s+\\w+(?:ed|en)\\b', flags=re.I)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if pattern.search(s))\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words that are long (length >= 7 characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 7)\n    result = long_count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\") among all adjacent pairs'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    pairs = len(words) - 1\n    repeat = sum(1 for i in range(pairs) if words[i] == words[i+1])\n    result = repeat / pairs\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average number of word tokens per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    total_tokens = 0\n    for s in sentences:\n        total_tokens += len(re.findall(r'\\w+', s))\n    if not sentences:\n        return 0.0\n    result = total_tokens / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens longer than 12 characters'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 12)\n    result = long_count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized Shannon entropy of character distribution (0-1)'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    counts = Counter(chars)\n    total = sum(counts.values())\n    if total == 0:\n        return 0.0\n    entropy = -sum((v/total) * math.log2(v/total) for v in counts.values() if v > 0)\n    distinct = len(counts)\n    if distinct <= 1:\n        return 0.0\n    # normalize by maximum entropy log2(distinct)\n    result = entropy / math.log2(distinct)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average fraction of digits inside whitespace-separated tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    ratios = []\n    for t in tokens:\n        ln = len(t)\n        if ln == 0:\n            continue\n        digit_count = sum(1 for c in t if c.isdigit())\n        ratios.append(digit_count / ln)\n    if not ratios:\n        return 0.0\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of exclamation marks per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    exclam = text.count('!')\n    result = exclam / len(sentences) if sentences else 0.0\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are common personal pronouns'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','you','your','yours',\n                'he','him','his','she','her','hers','they','them','their','theirs',\n                'himself','herself','themselves','ourselves'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in pronouns)\n    result = count / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing a character repeated three or more times consecutively'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    repeated = 0\n    for t in tokens:\n        if re.search(r'(.)\\1\\1', t):\n            repeated += 1\n    result = repeated / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average proportion of uppercase letters within word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    ratios = []\n    for w in words:\n        if len(w) == 0:\n            continue\n        up = sum(1 for c in w if c.isupper())\n        ratios.append(up / len(w))\n    if not ratios:\n        return 0.0\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating conjunction (and, but, or, so, yet, for, nor)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = {'and','but','or','so','yet','for','nor'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    total = 0\n    for s in sentences:\n        m = re.search(r'\\b(\\w+)\\b', s.strip())\n        if not m:\n            continue\n        total += 1\n        if m.group(1).lower() in conj:\n            count += 1\n    if total == 0:\n        return 0.0\n    result = count / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are purely numeric'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    nums = sum(1 for t in tokens if t.isdigit())\n    result = nums / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Proportion of digit characters among all alphanumeric characters (digits / (digits+letters))'\n    if not text:\n        return 0.0\n    digits = sum(1 for c in text if c.isdigit())\n    letters = sum(1 for c in text if c.isalpha())\n    denom = digits + letters\n    if denom == 0:\n        return 0.0\n    return float(digits / denom)\n\n",
  "def feature(text: str) -> float:\n    'Lexical diversity measured as unique word tokens divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence (words measured with \\\\w+), 0 for empty'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    counts = []\n    for s in sentences:\n        w = re.findall(r'\\w+', s)\n        counts.append(len(w))\n    if not counts:\n        return 0.0\n    return float(sum(counts) / len(counts))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of whitespace-delimited tokens that look like URLs (http/https/www)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)\\S+', re.IGNORECASE)\n    count = sum(1 for t in tokens if url_re.match(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens containing an elongated letter run (same letter repeated 3+ times)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'([A-Za-z])\\1{2,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are all-uppercase words (at least two letters and contain letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        letters = [c for c in t if c.isalpha()]\n        if len(letters) >= 2 and ''.join(letters).upper() == ''.join(letters) and any(c.isalpha() for c in t):\n            # consider as ALL CAPS token\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Normalized character entropy (Shannon entropy divided by log2(unique_chars)), 0 for short texts'\n    import math\n    if not text:\n        return 0.0\n    counts = {}\n    for c in text:\n        counts[c] = counts.get(c, 0) + 1\n    n = sum(counts.values())\n    if n == 0 or len(counts) <= 1:\n        return 0.0\n    entropy = 0.0\n    for v in counts.values():\n        p = v / n\n        entropy -= p * math.log2(p)\n    # normalize by log2(k) where k = number of unique symbols\n    k = len(counts)\n    norm = math.log2(k) if k > 1 else 1.0\n    return float(entropy / norm)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain no alphabetic characters (pure numbers/symbols)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if not any(c.isalpha() for c in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating/subordinating conjunction (e.g., and, but, or, so)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = {'and', 'but', 'or', 'so', 'because', 'however', 'then', 'also', 'although', 'though'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'\\b([A-Za-z]+)\\b', s)\n        if m and m.group(1).lower() in conj:\n            count += 1\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Relative imbalance of bracket/parenthesis characters (abs(opens-closes) / total bracket chars)'\n    if not text:\n        return 0.0\n    opens = sum(1 for c in text if c in '([{')\n    closes = sum(1 for c in text if c in ')]}')\n    total = opens + closes\n    if total == 0:\n        return 0.0\n    imbalance = abs(opens - closes)\n    return float(imbalance / total)\n",
  "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid division by zero if sentences exist but have no words\n    valid_counts = [c for c in word_counts if c > 0]\n    if not valid_counts:\n        return 0.0\n    result = sum(valid_counts) / len(valid_counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Ratio of hapax legomena (words that occur exactly once) to total words'\n    import re, collections\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    counts = collections.Counter(words)\n    hapax = sum(1 for w in words if counts[w] == 1)\n    result = hapax / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain any digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(c.isdigit() for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average number of punctuation characters per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    punct_counts = []\n    for s in sentences:\n        pc = sum(1 for c in s if not c.isalnum() and not c.isspace())\n        punct_counts.append(pc)\n    if not punct_counts:\n        return 0.0\n    result = sum(punct_counts) / len(punct_counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of alphabetic words that are in ALL CAPS (length >=2)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    alpha_words = re.findall(r'\\b[A-Za-z]{2,}\\b', text)\n    if not alpha_words:\n        return 0.0\n    all_caps = sum(1 for w in alpha_words if w.isupper())\n    result = all_caps / len(alpha_words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with \"?\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    q_count = sum(1 for s in sentences if s.strip().endswith('?'))\n    result = q_count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens containing internal non-alphanumeric characters'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # consider token containing any char that is not alnum as internal punctuation/symbol\n        if any((not c.isalnum()) for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences containing adjacent duplicate words (e.g., \"the the\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    dup_sentences = 0\n    for s in sentences:\n        tokens = re.findall(r'\\w+', s.lower())\n        if len(tokens) < 2:\n            continue\n        for i in range(1, len(tokens)):\n            if tokens[i] == tokens[i-1]:\n                dup_sentences += 1\n                break\n    result = dup_sentences / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words (length>=3) that are palindromes'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    palins = sum(1 for w in words if len(w) >= 3 and w == w[::-1])\n    result = palins / len(words)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord>127)'\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that are all-uppercase (length>=2)'\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    def is_shout(t):\n        letters = [c for c in t if c.isalpha()]\n        return len(letters) >= 2 and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_shout(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that are URLs or email addresses'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    url_re = re.compile(r'^(https?://|www\\.)', re.I)\n    email_re = re.compile(r'^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Normalized entropy of token length distribution (0-1)'\n    import re, math\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    lengths = {}\n    for w in words:\n        l = len(w)\n        lengths[l] = lengths.get(l, 0) + 1\n    total = len(words)\n    probs = [v / total for v in lengths.values()]\n    entropy = -sum(p * math.log(p) for p in probs if p > 0)\n    k = len(probs)\n    if k <= 1:\n        return float(0.0)\n    max_entropy = math.log(k)\n    result = entropy / max_entropy\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average vowel-to-consonant ratio across words (letters only)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'[A-Za-z]+', text)\n    if not words:\n        return float(0.0)\n    vowels = set('aeiouAEIOU')\n    ratios = []\n    for w in words:\n        v = sum(1 for c in w if c in vowels)\n        c = sum(1 for c in w if c.isalpha()) - v\n        ratios.append((v / c) if c > 0 else float(v))\n    if not ratios:\n        return float(0.0)\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        sentences = [s]\n    count = 0\n    for sent in sentences:\n        s2 = sent.lstrip()\n        if s2 and s2[0].isalpha() and s2[0].islower():\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent token pairs that are identical (consecutive repeats)'\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if len(tokens) < 2:\n        return float(0.0)\n    repeats = sum(1 for i in range(1, len(tokens)) if tokens[i].lower() == tokens[i-1].lower())\n    transitions = len(tokens) - 1\n    result = repeats / transitions\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Estimated average syllable count per word (vowel-group heuristic)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'[A-Za-z]+', text)\n    if not words:\n        return float(0.0)\n    vowel_group_re = re.compile(r'[aeiouyAEIOUY]+')\n    counts = []\n    for w in words:\n        groups = vowel_group_re.findall(w)\n        counts.append(max(1, len(groups)))\n    result = sum(counts) / len(counts)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain any digit characters'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        sentences = [s]\n    count = sum(1 for sent in sentences if any(c.isdigit() for c in sent))\n    result = count / len(sentences)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence (words defined by \\\\w+)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid zero-division if all sentences empty\n    nonzero = [c for c in word_counts if c > 0]\n    if not nonzero:\n        return 0.0\n    result = sum(word_counts) / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    result = unique / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric (integers or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d+(\\.\\d+)?$')\n    count = sum(1 for t in tokens if num_re.match(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of non-empty lines that begin with a list marker (bullets or numbered)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    marker_re = re.compile(r'^\\s*(?:[-*\u2022]|\\d+[\\.\\)])\\s+')\n    count = sum(1 for ln in lines if marker_re.match(ln))\n    result = count / len(lines)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are written in all-caps (at least two letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        if len(letters) < 2:\n            return False\n        return all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'(https?://|www\\.)', re.I)\n    email_re = re.compile(r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.match(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of ASCII emoticons and simple heart markers per token (e.g. :), :(, :D, <3)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticon_patterns = [\n        r'[:;=8][\\-~]?[)\\]\\(DdpP/\\\\]',  # smiles/frowns\n        r'<3',                          # heart\n        r'\\^\\_^',                       # cute face\n        r'[xX][-~]?[D)]'                # laughing variants\n    ]\n    combined = re.compile('|'.join('(?:%s)' % p for p in emoticon_patterns))\n    # count occurrences across text (not just token match) but normalize by token count\n    matches = combined.findall(text)\n    count = len(matches)\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average fraction of punctuation characters inside tokens (per-token punctuation ratio)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    ratios = []\n    for t in tokens:\n        if not t:\n            continue\n        punct = sum(1 for c in t if (not c.isalnum()))\n        ratios.append(punct / len(t))\n    if not ratios:\n        return 0.0\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of word tokens that are common stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','and','a','an','of','to','in','is','it','that','this','for','on','with','as','are','was','were','be','by',\n        'or','from','at','not','but','you','i','he','she','they','we','his','her','their','our','your','us','them','so','if'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Average estimated syllable count per word using vowel-group heuristic'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[a-zA-Z]+\", text.lower())\n    if not words:\n        return 0.0\n    syll_counts = []\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        count = len(groups)\n        # ensure at least one syllable\n        syll_counts.append(max(1, count))\n    result = sum(syll_counts) / len(syll_counts)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Fraction of characters that are emoji/unicode pictographs'\n    import re\n    if not text:\n        return 0.0\n    try:\n        pattern = re.compile(\n            r'['\n            r'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n            r'\\U0001F600-\\U0001F64F'  # emoticons\n            r'\\U0001F680-\\U0001F6FF'  # transport & map\n            r'\\U0001F700-\\U0001F77F'  # alchemical\n            r'\\U0001F780-\\U0001F7FF'\n            r'\\U0001F800-\\U0001F8FF'\n            r'\\U0001F900-\\U0001F9FF'\n            r'\\U0001FA00-\\U0001FA6F'\n            r'\\u2600-\\u26FF'          # misc symbols\n            r'\\u2700-\\u27BF'          # dingbats\n            r']', flags=re.UNICODE)\n        emojis = pattern.findall(text)\n        total = len(text)\n        return float(len(emojis) / total) if total > 0 else 0.0\n    except re.error:\n        # fallback: simple heuristic for surrogate ranges\n        count = sum(1 for c in text if ord(c) > 0x2600)\n        return float(count / len(text)) if text else 0.0\n\n",
  "def feature(text: str) -> float:\n    'Average estimated syllables per word (vowel-group heuristic)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    def syllables(w):\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        return max(1, len(groups))\n    totals = sum(syllables(w) for w in words)\n    return float(totals / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a coordinating conjunction/connector'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    connectors = {'and', 'but', 'or', 'so', 'because', 'then', 'also', 'however', 'yet', 'thus', 'therefore'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r\"\\b([A-Za-z']+)\", s)\n        if m and m.group(1).lower() in connectors:\n            count += 1\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Flesch\u2013Kincaid grade level (approximate; 0.0 when not computable)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words or not sentences:\n        return 0.0\n    def syllables(w):\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        return max(1, len(groups))\n    total_syllables = sum(syllables(w) for w in words)\n    words_per_sentence = len(words) / max(1, len(sentences))\n    syllables_per_word = total_syllables / len(words)\n    score = 0.39 * words_per_sentence + 11.8 * syllables_per_word - 15.59\n    return float(score)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Normalized Shannon entropy of character distribution (0-1)'\n    from collections import Counter\n    import math\n    if not text:\n        return 0.0\n    counts = Counter(text)\n    total = sum(counts.values())\n    if total == 0 or len(counts) <= 1:\n        return 0.0\n    entropy = 0.0\n    for v in counts.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    denom = math.log2(len(counts))\n    return float(entropy / denom) if denom > 0 else 0.0\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that contain a character repeated three or more times consecutively'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (small set)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {\n        'the','a','an','in','on','of','to','is','are','was','were','and','or','but',\n        'for','with','without','by','at','from','that','this','it','as','be','has','have'\n    }\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in stopwords)\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Ratio of average length of longest 10% tokens to shortest 10% tokens (lexical skew)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    n = len(words)\n    if n < 2:\n        return 0.0\n    lengths = sorted([len(w) for w in words])\n    k = max(1, int(max(1, n * 0.1)))\n    top = lengths[-k:]\n    bottom = lengths[:k]\n    avg_top = sum(top) / len(top)\n    avg_bottom = sum(bottom) / len(bottom) if sum(bottom) > 0 else 0.0\n    if avg_bottom <= 0.0:\n        return float(avg_top)\n    return float(avg_top / avg_bottom)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(count / len(sentences))\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    result = unique / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Coefficient of variation of sentence lengths (std/mean of words per sentence)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    result = std / mean\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that contain double-quote style characters (dialogue indicator)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    quote_chars = set(['\"', '\u201c', '\u201d', '\u00ab', '\u00bb'])\n    count = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL-CAPS words of length>=2 (shouting/acronym signal)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z]+\", text)\n    if not tokens:\n        return 0.0\n    caps = sum(1 for t in tokens if t.isupper() and len(t) >= 2)\n    result = caps / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common emoticons present (smileys, hearts, XD, etc.) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    # common emoticon pattern (simple, case-insensitive)\n    emoticon_pat = re.compile(r'(?:(?:[:;=xX8][-^]?[)DdpP/\\\\])|(?:<3)|(?::-?\\()|(?:\\bXD\\b))', re.IGNORECASE)\n    emoticons = sum(1 for t in tokens if emoticon_pat.search(t))\n    result = emoticons / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of short words (<=3 characters) among all words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    short = sum(1 for w in words if len(w) <= 3)\n    result = short / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of common English stopwords (simple small stopword list) among tokens'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','a','of','to','in','is','it','that','for','on','with','as','are','was','be','by','an','this','which','or','from'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stop)\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of long words (length >= 8) among all words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    longw = sum(1 for w in words if len(w) >= 8)\n    result = longw / len(words)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Density of abbreviations/acronyms: counts of common dotted abbreviations or uppercase acronyms per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+|\\S', text)\n    if not tokens:\n        return 0.0\n    # dotted abbreviations like e.g., i.e., etc. (with trailing dot) and common abbreviations\n    abbr_pat = re.compile(r'\\b(?:e\\.g|i\\.e|etc|vs|mr|mrs|dr|prof|inc|ltd)\\.', re.IGNORECASE)\n    dotted = len(re.findall(abbr_pat, text))\n    # uppercase acronyms of length >=2\n    acronyms = len(re.findall(r'\\b[A-Z]{2,}\\b', text))\n    count = dotted + acronyms\n    result = count / len(tokens)\n    return float(result)\n\n",
  "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\") among total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    repeats = len(re.findall(r'\\b(\\w+)\\s+\\1\\b', text.lower()))\n    result = repeats / len(tokens)\n    return float(result)\n",
  "def feature(text: str) -> float:\n    'Average sentence length measured in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths) / len(lengths))\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio: unique words divided by total words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word-tokens that are URLs or email addresses'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\S+', text)\n    if not words:\n        return 0.0\n    url_pattern = re.compile(r'(?i)\\b(?:https?://|www\\.)\\S+')\n    email_pattern = re.compile(r'\\b\\S+@\\S+\\.\\S+\\b')\n    count = 0\n    for w in words:\n        if url_pattern.search(w) or email_pattern.search(w):\n            count += 1\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words that contain at least one digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    digit_words = sum(1 for w in words if any(ch.isdigit() for ch in w))\n    return float(digit_words / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of word tokens that are in ALL CAPS (length >= 2)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    caps = sum(1 for t in tokens if len(t) >= 2 and t.isupper())\n    return float(caps / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words containing a repeated character run of length >= 3 (e.g., \"sooo\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1', flags=re.IGNORECASE)\n    count = sum(1 for w in words if pattern.search(w))\n    return float(count / len(words))\n\n",
  "def feature(text: str) -> float:\n    'Normalized Shannon entropy of characters (0..1)'\n    import math, collections\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    freq = collections.Counter(chars)\n    total = len(chars)\n    entropy = -sum((v/total) * math.log2(v/total) for v in freq.values() if v > 0)\n    uniq = len(freq)\n    if uniq <= 1:\n        return 0.0\n    # normalize by max entropy log2(uniq)\n    return float(entropy / math.log2(uniq))\n\n",
  "def feature(text: str) -> float:\n    'Average number of commas per sentence (comma density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        # treat entire text as one sentence\n        sentences = [text.strip()]\n    commas = sum(s.count(',') for s in sentences)\n    return float(commas / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating/subordinating conjunction (approx.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = {'and','but','or','so','because','although','though','however','yet','then','also','since','when','while'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        # remove leading non-letter characters like quotes or parentheses\n        s2 = re.sub(r'^[^A-Za-z0-9]+', '', s).strip()\n        m = re.match(r'([A-Za-z]+)', s2)\n        if m and m.group(1).lower() in conj:\n            starts += 1\n    return float(starts / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of words that are capitalized mid-sentence (approximate proper noun density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = []\n    mid_cap = 0\n    total = 0\n    for s in sentences:\n        toks = re.findall(r'\\w+', s)\n        for i, t in enumerate(toks):\n            total += 1\n            if i > 0 and len(t) > 0 and t[0].isupper():\n                # treat as mid-sentence capitalized\n                mid_cap += 1\n    if total == 0:\n        return 0.0\n    return float(mid_cap / total)\n",
  "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not counts:\n        return 0.0\n    return float(sum(counts) / len(counts))\n\n",
  "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that are common emoticons (e.g., :-) , :D, <3)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    toks = re.findall(r'\\S+', text)\n    if not toks:\n        return 0.0\n    emoticon_re = re.compile(r'(?:(?:[:;=8][\\-^\\'`]?[\\)D\\(\\]/\\\\OpP])|<3)', re.I)\n    count = sum(1 for t in toks if emoticon_re.search(t))\n    return float(count / len(toks))\n\n",
  "def feature(text: str) -> float:\n    'Hapax legomena ratio (words that occur exactly once divided by total tokens)'\n    import re\n    from collections import Counter\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    cnt = Counter(tokens)\n    hapax = sum(1 for w, c in cnt.items() if c == 1)\n    return float(hapax / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of sentences that contain a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if '?' in s)\n    return float(qcount / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Punctuation character variety: distinct punctuation types divided by total punctuation characters'\n    import string\n    if not text:\n        return 0.0\n    puncs = [c for c in text if c in string.punctuation]\n    if not puncs:\n        return 0.0\n    distinct = len(set(puncs))\n    return float(distinct / len(puncs))\n\n",
  "def feature(text: str) -> float:\n    'Variance of word lengths (population variance of token lengths)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    lengths = [len(w) for w in re.findall(r'\\w+', text)]\n    if not lengths or len(lengths) == 1:\n        return 0.0\n    n = len(lengths)\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    return float(var)\n\n",
  "def feature(text: str) -> float:\n    'Fraction of tokens that are personal pronouns (common English pronouns)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    pronouns = {'i','me','you','he','him','she','her','we','us','they','them','my','your','yours','his','hers','our','their','theirs','mine','its','it'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count / len(tokens))\n\n",
  "def feature(text: str) -> float:\n    'Fraction of sentences that include double-quote characters (indicative of quotations)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    quote_chars = ('\"', '\u201c', '\u201d')\n    count = sum(1 for s in sentences if any(q in s for q in quote_chars))\n    return float(count / len(sentences))\n\n",
  "def feature(text: str) -> float:\n    'Proportion of non-whitespace tokens that contain at least one digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num / len(tokens))\n"
]