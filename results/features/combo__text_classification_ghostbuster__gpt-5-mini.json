{
  "combined_from": [
    "results/features/funsearch__text_classification_ghostbuster__gpt-5-mini.json",
    "results/features/did3__text_classification_ghostbuster__gpt-5-mini.json"
  ],
  "used_features": [
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        # if no clear sentences, treat whole text as one and check\n        return float(text.strip().endswith('?'))\n    count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated (contain a hyphen), normalized by token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    word_tokens = re.findall(r'\\w+', text)\n    denom = max(1, len(word_tokens))\n    hyphen_count = sum(1 for t in tokens if '-' in t and any(ch.isalpha() for ch in t))\n    return float(hyphen_count) / denom\n\n",
    "def feature(text: str) -> float:\n    'Frequency of common author-year citation patterns (e.g., \"(Smith, 2019)\" or \"Smith et al., 2019\") per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    patterns = [\n        r'\\([A-Za-z][A-Za-z0-9\\-\\s\\.]+,\\s*\\d{4}\\)',   # (Lastname, 2019) or (Lastname et al., 2019)\n        r'\\b[A-Z][A-Za-z\\-]+ et al\\.,?\\s*\\d{4}\\b',   # Lastname et al., 2019\n        r'\\[\\s*[A-Za-z][A-Za-z\\-\\s\\.]*\\d{4}\\s*\\]'    # [Lastname 2019] etc.\n    ]\n    combined = '|'.join('(?:%s)' % p for p in patterns)\n    matches = re.findall(combined, text)\n    return float(len(matches)) / float(len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Ratio of average length of longest 10% tokens to shortest 10% tokens (lexical skew)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    n = len(words)\n    if n < 2:\n        return 0.0\n    lengths = sorted([len(w) for w in words])\n    k = max(1, int(max(1, n * 0.1)))\n    top = lengths[-k:]\n    bottom = lengths[:k]\n    avg_top = sum(top) / len(top)\n    avg_bottom = sum(bottom) / len(bottom) if sum(bottom) > 0 else 0.0\n    if avg_bottom <= 0.0:\n        return float(avg_top)\n    return float(avg_top / avg_bottom)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid division by zero if any sentence has no words\n    word_counts = [wc for wc in word_counts if wc > 0]\n    if not word_counts:\n        return float(0.0)\n    result = sum(word_counts) / len(word_counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of word tokens per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    total_tokens = 0\n    for s in sentences:\n        total_tokens += len(re.findall(r'\\w+', s))\n    if not sentences:\n        return 0.0\n    result = total_tokens / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one numeric digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = num / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are short (<=3 characters)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    short = sum(1 for w in words if len(w) <= 3)\n    return float(short / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens longer than 6 characters (long-word density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) >= 7)\n    return float(long_count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Mean per-token character-uniqueness ratio (unique chars / token length) across word tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    ratios = []\n    for t in tokens:\n        L = len(t)\n        if L == 0:\n            continue\n        ratios.append(len(set(t)) / L)\n    if not ratios:\n        return 0.0\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of paragraph word counts (paragraphs split on blank lines)'\n    import re, math\n    if not text:\n        return 0.0\n    paras = [p for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n    counts = [len(re.findall(r'\\w+', p)) for p in paras if re.findall(r'\\w+', p)]\n    if len(counts) < 2:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(var)\n    return std / mean\n\n",
    "def feature(text: str) -> float:\n    'Density of URLs or email-like tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|ftp://|www\\.)', re.I)\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n    domain_re = re.compile(r'\\b[\\w.-]+\\.(com|org|net|io|gov|edu|co|us|uk|de|fr|ru)\\b', re.I)\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.match(t) or domain_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Heuristic passive-voice frequency: (was|were|is|are|been|be) + past-participle(ed) per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text]\n    matches = re.findall(r'\\b(?:was|were|is|are|been|be)\\s+\\w+ed\\b', text, flags=re.I)\n    result = len(matches) / max(1, len(sentences))\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a quotation mark or dash (dialogue-like starts)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    starts = ('\"', \"'\", '\u201c', '\u201d', '\u2018', '\u2019', '-', '\u2014')\n    count = sum(1 for s in sentences if s.lstrip().startswith(starts))\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (contain a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    q_count = sum(1 for s in sentences if '?' in s)\n    result = q_count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens with character elongation (3+ repeated chars)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    elong_re = re.compile(r'(.)\\1\\1')\n    count = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t) and elong_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Complex sentence ratio: fraction of sentences that contain a comma or semicolon'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    complex_count = sum(1 for s in sentences if (',' in s or ';' in s))\n    result = complex_count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of adjacent sentences that start with the same word (repeated sentence-starts)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    starts = []\n    for s in sentences:\n        m = re.findall(r'\\w+', s)\n        starts.append(m[0].lower() if m else '')\n    if len(starts) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(len(starts) - 1) if starts[i] and starts[i] == starts[i + 1])\n    result = repeats / (len(starts) - 1)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Comma density: fraction of characters that are commas (complex, clause-heavy prose)'\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count) / max(1.0, float(len(text)))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of space-separated tokens that look like URLs or email addresses'\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        t_low = t.lower()\n        if 'http' in t_low or t_low.startswith('www.') or '@' in t:\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of short words (<4 characters) among all words'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    short = sum(1 for w in words if len(w) < 4)\n    result = short / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Diversity of punctuation: distinct punctuation characters divided by total punctuation count'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    unique = len(set(puncts))\n    return float(unique / total)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of words that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    hapax = sum(1 for w, c in freqs.items() if c == 1)\n    return float(hapax / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique words divided by total words'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for s in sentences:\n        total_words += len(re.findall(r'\\w+', s))\n    return float(total_words / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of modal verbs (can/could/will/would/shall/should/may/might/must) to all tokens \u2014 signals speculative or advisory tone'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are written in ALL CAPS (2+ letters)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    caps = sum(1 for w in words if len([c for c in w if c.isalpha()]) >= 2 and w.isupper())\n    result = caps / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Parentheses density: fraction of characters that are parentheses (common in academic/case documents with citations)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    return float(paren_count) / float(total_chars)\n\n\n",
    "def feature(text: str) -> float:\n    'Ratio of hedge/modality words (may, might, could, appears, suggests, likely, etc.)'\n    import re\n    if not text:\n        return 0.0\n    hedges = {'may','might','could','can','would','should','possibly','likely',\n              'suggests','suggest','appears','appear','seems','seem','tends','tend','probable','probably'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are Titlecase (start uppercase, not ALLCAPS)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    def is_title(t):\n        return len(t) > 0 and t[0].isupper() and not t.isupper()\n    count = sum(1 for t in tokens if is_title(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are content words (not common stopwords)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','is','in','it','to','of','a','an','that','this','for','on','with','as','are','was','were','be','by','or','at','from','but','not','they','their','I','you','we','he','she','his','her'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    content = sum(1 for w in words if w not in stop)\n    return float(content / len(words))\n",
    "def feature(text: str) -> float:\n    'Proportion of characters in the text that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of long words (>=8 characters) among all word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 8)\n    result = long_count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are common English stopwords (simple list)'\n    import re\n    if not text:\n        return 0.0\n    STOP = {'the','and','is','a','an','of','to','in','that','it','for','on','with','as','was','by','at','from','be','this','are','or','which','but','not','have','has','had','I','you','he','she','they','we','their','them','his','her'}\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in STOP)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Stopword density: proportion of common function words among all tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','it','you','that','he','she','they','we','a','an','to','of','for','on','with','as','at','by','from','this','be','are','was','were','or','but','not','have','has','had','my','me','your','I','so'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing a run of the same character repeated 3+ times'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    pattern = re.compile(r'(.)\\1{2,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain modal verbs (can/could/may/might/must/shall/should/will/would)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    modals = r'\\b(?:can|could|may|might|must|shall|should|will|would)\\b'\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    cre = re.compile(modals, flags=re.I)\n    for s in sentences:\n        if cre.search(s):\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Frequency of ellipses per sentence (counts \"...\" and single-character ellipsis \"\u2026\")'\n    import re\n    if not text:\n        return 0.0\n    ellipses = text.count('...') + text.count('\u2026')\n    # approximate sentence count by splitting on sentence-ending punctuation; ensure at least 1\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    return float(ellipses) / max(1, len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words \u2014 high variability can indicate dialog or informal style'\n    import re, math\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    lengths = [len(re.findall(r'\\b[\\w\\']+\\b', s)) for s in sents]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n",
    "def feature(text: str) -> float:\n    'Non-ASCII character fraction (proxy for emojis, foreign scripts, or special symbols)'\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    nonascii = sum(1 for c in text if ord(c) > 127)\n    return float(nonascii / total)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        return float(1.0 if s.endswith('?') else 0.0)\n    questions = sum(1 for seg in sentences if seg.strip().endswith('?'))\n    return float(questions / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word tokens / total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentence-ending punctuation that are question marks or exclamation points'\n    import re\n    if not text:\n        return 0.0\n    ends = re.findall(r'[.!?]', text)\n    if not ends:\n        return 0.0\n    q_e = sum(1 for c in ends if c in ('?', '!'))\n    return float(q_e) / len(ends)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of long words (length >= 7), a proxy for formality and vocabulary complexity'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 7)\n    return float(long_words) / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens / total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    types = len(set(words))\n    return float(types / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'([A-Za-z])', s)\n        if m and m.group(1).islower():\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing an apostrophe (contraction or possessive density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if (\"'\" in t) or (\"\u2019\" in t))\n    return float(count / len(tokens))\n",
    "def feature(text: str) -> float:\n    'Density of emoticons and emoji-like chars (text emoticons + high-codepoint chars)'\n    import re\n    if not text:\n        return 0.0\n    # common text emoticon patterns\n    emoticon_re = re.compile(r'(?:(?:[:;=8][-^]?[)DdpP(/\\[\\\\])|(?:<3)|(?:\\^\\_^))')\n    emoticons = len(emoticon_re.findall(text))\n    # count characters with high codepoints as proxy for emoji\n    emoji_like = sum(1 for c in text if ord(c) > 10000)\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float((emoticons + emoji_like) / total_chars)\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, we, me, us, my, our, mine, ours)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    first_person = {'i','we','me','us','my','our','mine','ours','myself','ourselves'}\n    count = sum(1 for t in tokens if t in first_person)\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Emoticon/emoji token density: fraction of whitespace tokens that are emoticons or contain emoji characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    # common ASCII emoticons (simple set) and a basic emoji unicode block range\n    emot_re = re.compile(r'^(?:[:;=8X][-^]?[)D(Pp/\\\\]|<3|:\\'\\(|:\\'\\))$')\n    emoji_re = re.compile('[\\U0001F300-\\U0001F5FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF]', flags=re.UNICODE)\n    count = 0\n    for t in tokens:\n        if emot_re.search(t) or emoji_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are all uppercase (>=2 letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if t.isalpha() and len(t) >= 2 and t.upper() == t:\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average fraction of characters in each token that are punctuation (token-level punctuation density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    total_frac = 0.0\n    valid_tokens = 0\n    for t in tokens:\n        if not t:\n            continue\n        punct = sum(1 for c in t if not c.isalnum() and not c.isspace())\n        total_frac += (punct / len(t))\n        valid_tokens += 1\n    if valid_tokens == 0:\n        return 0.0\n    return float(total_frac / valid_tokens)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (small anchored list) \u2014 can indicate function-word heavy prose vs descriptive'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {'the', 'and', 'of', 'to', 'a', 'in', 'that', 'is', 'was', 'it', 'for', 'on', 'with', 'as', 'by', 'an', 'be'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / float(len(tokens))\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing an apostrophe (contraction or possessive density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9'`]+\", text)\n    if not tokens:\n        return 0.0\n    apos_count = sum(1 for t in tokens if \"'\" in t or \"\u2019\" in t or \"`\" in t)\n    return float(apos_count / len(tokens))\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words; high values indicate variable sentence sizing'\n    import re, math\n    if not text:\n        return 0.0\n    # Split on sentence end punctuation sequences\n    parts = [p.strip() for p in re.split(r'[.!?]+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', p)) for p in parts]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(var)\n    return std / mean\n\n",
    "def feature(text: str) -> float:\n    'Ratio of contractions (e.g., \"don\\'t\", \"you\\'re\") to total words as an informality signal'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    contractions = re.findall(r\"\\b\\w+'(?:t|re|ve|ll|d|s)\\b\", text.lower())\n    return float(len(contractions) / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average punctuation density per sentence (punctuation chars / sentence length)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    densities = []\n    for s in sentences:\n        length = len(s)\n        if length == 0:\n            continue\n        punct = sum(1 for c in s if not c.isalnum() and not c.isspace())\n        densities.append(punct / length)\n    if not densities:\n        return 0.0\n    return float(sum(densities) / len(densities))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of apostrophe uses that look like possessives (e.g., owner\\'s or goods\u2019 ) among all apostrophe-like characters'\n    if not text:\n        return 0.0\n    # curly and straight apostrophes\n    apost_count = text.count(\"'\") + text.count(\"\u2019\")\n    if apost_count == 0:\n        return 0.0\n    possessive_patterns = re.findall(r\"\\b\\w+(?:'|\u2019)\\s?s\\b\", text)  # captures patterns like word's (with optional stray space)\n    # also capture plural possessive like words' or words\u2019 (word + s + apostrophe)\n    possessive_patterns += re.findall(r\"\\b\\w+s(?:'|\u2019)\\b\", text)\n    possessive_count = len(possessive_patterns)\n    return float(possessive_count) / apost_count\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs (start with http/www or contain a TLD-like dot)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re1 = re.compile(r'^(?:https?://|ftp://|www\\.)', re.I)\n    url_re2 = re.compile(r'\\.[a-z]{2,6}(/|$)', re.I)\n    count = 0\n    for t in tokens:\n        if url_re1.search(t) or url_re2.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent repeated word pairs (e.g., \"the the\") among all word gaps'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    return float(repeats / (len(words) - 1))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\") among all adjacent pairs'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    pairs = len(words) - 1\n    repeat = sum(1 for i in range(pairs) if words[i] == words[i+1])\n    result = repeat / pairs\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct / len(puncts))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are purely numeric (integers or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d+(\\.\\d+)?$')\n    count = sum(1 for t in tokens if num_re.match(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Estimated clauses per sentence: (coordinating/subordinating conjunctions + semicolons + half the commas) per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj_pattern = r'\\b(?:and|but|because|although|while|since|when|which|that|however|so|therefore)\\b'\n    conj_count = len(re.findall(conj_pattern, text, flags=re.IGNORECASE))\n    semicolons = text.count(';')\n    commas = text.count(',')\n    # sentence count robust split\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    n_sent = max(1, len(sentences))\n    clause_estimate = conj_count + semicolons + 0.5 * commas\n    return float(clause_estimate) / float(n_sent)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of words in titlecase (initial capital, rest lower)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    title_count = sum(1 for w in words if w.istitle())\n    return float(title_count / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are hyphenated compounds (contain at least one hyphen between word characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    hyphenated = re.findall(r'\\b\\w+(?:-\\w+)+\\b', text)\n    return float(len(hyphenated) / len(words))\n",
    "def feature(text: str) -> float:\n    'Proportion of non-whitespace tokens that contain at least one digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num / len(tokens))\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that belong to punctuation runs of length >=2 (e.g., \"!!\", \"...\")'\n    import re\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    runs = 0\n    for m in re.finditer(r'([^\\w\\s])\\1+', text):\n        runs += len(m.group(0))\n    return float(runs / total)\n\n",
    "def feature(text: str) -> float:\n    'Density of time expressions like \"7:00\", \"07:30 am\" per token (captures logs/diaries)'\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    times = re.findall(r'\\b\\d{1,2}:\\d{2}\\b(?:\\s?(?:am|pm))?|\\b\\d{1,2}\\s?(?:am|pm)\\b', text, flags=re.I)\n    return float(len(times)) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Approximate Flesch reading ease score using simple syllable heuristic'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    total_words = len(words)\n    total_sentences = len(sentences) if sentences else 0\n    if total_words == 0 or total_sentences == 0:\n        return 0.0\n    total_syllables = 0\n    for w in words:\n        # count vowel groups as syllables\n        syll = len(re.findall(r'[aeiouy]+', w))\n        # simple adjustment for trailing silent 'e'\n        if w.endswith('e') and syll > 1:\n            syll -= 1\n        if syll < 1:\n            syll = 1\n        total_syllables += syll\n    # Flesch reading ease formula\n    score = 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n    return float(score)\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    totals = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        totals += len(words)\n    return float(totals / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return unique / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Discourse-marker density: frequency of common academic/connective phrases (however, therefore, moreover, on the other hand) per sentence'\n    import re\n    if not text:\n        return 0.0\n    lc = text.lower()\n    markers = ['however', 'therefore', 'moreover', 'furthermore', 'in addition', 'on the other hand', 'conversely', 'nevertheless', 'as a result', 'in contrast']\n    count = 0\n    for m in markers:\n        # count word/phrase occurrences\n        count += len(re.findall(r'\\b' + re.escape(m) + r'\\b', lc))\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(count) / float(sentences)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words that contain an internal hyphen or apostrophe (e.g., self-esteem, don\\'t)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    hy_ap_re = re.compile(r'\\w+[-\\']\\w+')\n    count = 0\n    for m in re.finditer(r'\\S+', text):\n        token = m.group(0)\n        if hy_ap_re.search(token):\n            count += 1\n    return float(count / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Average estimated syllables per word (vowel-group heuristic)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    def syllables(w):\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        return max(1, len(groups))\n    totals = sum(syllables(w) for w in words)\n    return float(totals / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL CAPS (useful for shouting/abbrev detection)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # consider token as ALL CAPS if it has at least one letter and all letters are uppercase\n        letters = [c for c in t if c.isalpha()]\n        if letters and all(c.isupper() for c in letters):\n            # ignore single-letter tokens like 'I'\n            if len(letters) >= 2:\n                count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Normalized count of 4-digit years (1000-2099) appearing in the text'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(?:1[0-9]{3}|20[0-9]{2})\\b', text)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(years)) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters divided by total punctuation count'\n    import re\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    unique = len(set(puncts))\n    return float(unique) / float(total)\n\n",
    "def feature(text: str) -> float:\n    'Variance of word lengths (population variance of lengths of alphanumeric words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    n = len(words)\n    if n == 0:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / n\n    mean_sq = sum(l * l for l in lengths) / n\n    var = mean_sq - mean * mean\n    if var < 0:\n        var = 0.0\n    return float(var)\n\n",
    "def feature(text: str) -> float:\n    'Normalized capitalization imbalance: abs(upper - lower) / total_letters'\n    if not text:\n        return float(0.0)\n    upp = sum(1 for c in text if c.isupper())\n    low = sum(1 for c in text if c.islower())\n    total_letters = upp + low\n    if total_letters == 0:\n        return float(0.0)\n    result = abs(upp - low) / total_letters\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average proportion of digits inside each token (digits / token_length)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    tot = 0.0\n    for t in tokens:\n        if not t:\n            continue\n        digits = sum(1 for c in t if c.isdigit())\n        tot += (digits / len(t))\n    return float(tot / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of words containing a repeated character run of length >= 3 (e.g., \"sooo\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1', flags=re.IGNORECASE)\n    count = sum(1 for w in words if pattern.search(w))\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of text lines that look like list items (bullets or numbered)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    bullet_re = re.compile(r'^\\s*([-*\u2022]|(\\d+[\\).\\s])|([a-zA-Z]\\)))')\n    count = sum(1 for ln in lines if bullet_re.match(ln))\n    return float(count / len(lines))\n\n",
    "def feature(text: str) -> float:\n    'Average number of vowel groups per word (rough proxy for syllable density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    total = 0\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        total += max(1, len(groups))\n    return float(total / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of non-empty lines that begin with a quotation or dash (quoted-line density)'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    starts = 0\n    for ln in lines:\n        s = ln.lstrip()\n        if not s:\n            continue\n        if s[0] in {'\"', \"'\", '\u201c', '\u201d', '\u2014', '-'}:\n            starts += 1\n    return float(starts / len(lines))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of alphabetic tokens that are in ALL CAPS (shouting indicator)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return float(0.0)\n    caps = 0\n    alpha_tokens = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t):\n            alpha_tokens += 1\n            if t.isupper():\n                caps += 1\n    if alpha_tokens == 0:\n        return float(0.0)\n    return float(caps / alpha_tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a WH-question word (who, what, when, where, why, how)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    qwords = {'who', 'what', 'when', 'where', 'why', 'how'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        s2 = re.sub(r'^[^A-Za-z0-9]+', '', s).lower()\n        parts = re.findall(r'\\w+', s2)\n        if parts and parts[0] in qwords:\n            count += 1\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are the second-person pronoun \"you\" (case-insensitive)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    you_count = sum(1 for t in tokens if t == 'you')\n    return you_count / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are fully uppercase (shouting)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.isupper() and len(t) > 1)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating conjunction (e.g., \"and\", \"but\", \"so\")'\n    import re\n    if not text:\n        return 0.0\n    CONJ = {'and', 'but', 'or', 'so', 'yet', 'for', 'nor', 'however', 'therefore'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        m = re.match(r'\\s*([A-Za-z\\'-]+)', s)\n        if m and m.group(1).lower() in CONJ:\n            starts += 1\n    return float(starts / len(sentences))\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not sentences or not words:\n        return 0.0\n    result = len(words) / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that match common emoticon patterns'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emo_re = re.compile(r'^[<>]?[;:=8][\\-o\\*\\']?[\\)\\]\\(dDpP/\\\\|]|[;\\:\\-\\=][\\)D\\(\\]/\\\\Pp]$')\n    # also accept simple :-) :-( :D :P ;)\n    count = 0\n    for t in tokens:\n        if emo_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences by punctuation followed by whitespace (keeps abbreviations coarse)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        total_words += len(words)\n    if total_words == 0:\n        return 0.0\n    return float(total_words / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that end with an exclamation mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    count = sum(1 for s in sents if s.endswith('!'))\n    result = count / len(sents)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Approximate Flesch reading ease score using vowel-group syllable proxy'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    syllables = sum(len(re.findall(r'[aeiouyAEIOUY]+', w)) or 1 for w in words)\n    words_per_sentence = len(words) / max(1, len(sentences))\n    syl_per_word = syllables / len(words)\n    # Flesch reading ease (approximate)\n    score = 206.835 - 1.015 * words_per_sentence - 84.6 * syl_per_word\n    return float(score)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are all-uppercase words (at least two letters and contain letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        letters = [c for c in t if c.isalpha()]\n        if len(letters) >= 2 and ''.join(letters).upper() == ''.join(letters) and any(c.isalpha() for c in t):\n            # consider as ALL CAPS token\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of very short sentences (3 or fewer words)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 3 and len(words) > 0:\n            short += 1\n    return float(short / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total tokens (lexical diversity)'\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / n\n\n\n",
    "def feature(text: str) -> float:\n    'Density of violent/visceral lexical items (small curated lexicon) per token, useful for gore/violence signal'\n    import re\n    if not text:\n        return 0.0\n    lexicon = ['kill', 'killed', 'murder', 'axe', 'axe', 'head', 'decap', 'decapit', 'blood', 'bleed', 'stab', 'knife', 'shot', 'rifle', 'thud', 'gore', 'cut', 'swing', 'chop']\n    tokens = re.findall(r\"[A-Za-z0-9']+\", text.lower())\n    if not tokens:\n        return 0.0\n    cnt = 0\n    for t in tokens:\n        for root in lexicon:\n            if root in t:\n                cnt += 1\n                break\n    return float(cnt) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of common emoticons per token (smiley/frowny/etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    # common sideways emoticons and hearts\n    emoticon_re = re.compile(r'(?:(?:[:;=8][\\-~]?[)DdpP\\(\\]/\\\\])|(?:[)DdpP\\-\\^][\\-~]?[:;=8])|<3|\\^\\_^|:\\'\\)|:\\'\\(|:\\(|:\\)|;-\\)|;-?\\))', flags=re.I)\n    count = sum(1 for t in tokens if emoticon_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of alphabetic characters that are uppercase (overall capitalization intensity)'\n    if not text:\n        return 0.0\n    total_alpha = sum(1 for c in text if c.isalpha())\n    if total_alpha == 0:\n        return 0.0\n    upper = sum(1 for c in text if c.isalpha() and c.isupper())\n    return float(upper / total_alpha)\n\n",
    "def feature(text: str) -> float:\n    'Standard deviation of word lengths (0.0 if <2 words)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if len(words) < 2:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of line lengths in words \u2014 captures poetic/line-oriented structure'\n    import re, math\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    counts = []\n    for L in lines:\n        if L.strip():\n            counts.append(len(re.findall(r'\\w+', L)))\n    if not counts:\n        return 0.0\n    mean = sum(counts) / float(len(counts))\n    if mean == 0:\n        return 0.0\n    variance = sum((c - mean) ** 2 for c in counts) / float(len(counts))\n    std = math.sqrt(variance)\n    return float(std) / float(mean)\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        # fallback to words in whole text\n        words = re.findall(r'\\w+', text)\n        return float(len(words)) if words else 0.0\n    words_per_sent = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        words_per_sent.append(len(words))\n    if not words_per_sent:\n        return 0.0\n    result = sum(words_per_sent) / len(words_per_sent)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing a character repeated three or more times consecutively (elongation/punctuation runs)'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\S+', text)\n    if not toks:\n        return 0.0\n    rep_re = re.compile(r'(.)\\1\\1')\n    count = sum(1 for t in toks if rep_re.search(t))\n    return float(count / len(toks))\n",
    "def feature(text: str) -> float:\n    'Pronoun variety: distinct pronouns used divided by total pronoun occurrences (higher = more varied pronoun use)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','you','your','yours','he','him','his','she','her','hers','they','them','their','theirs','it','its','who','whom','whose'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    found = []\n    for w in tokens:\n        lw = w.lower().strip(\"'-\")\n        if lw in pronouns:\n            found.append(lw)\n    if not found:\n        return 0.0\n    distinct = len(set(found))\n    return float(distinct) / len(found)\n\n",
    "def feature(text: str) -> float:\n    'Character entropy (normalized): Shannon entropy of character distribution normalized to [0,1]'\n    import math\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    freq = {}\n    for c in text:\n        freq[c] = freq.get(c, 0) + 1\n    n_types = len(freq)\n    if n_types <= 1:\n        return 0.0\n    ent = 0.0\n    for v in freq.values():\n        p = v / total\n        ent -= p * math.log2(p)\n    # normalize by max entropy log2(n_types)\n    max_ent = math.log2(n_types)\n    return float(ent / max_ent)\n\n",
    "def feature(text: str) -> float:\n    'Density of common English stopwords among word tokens (small fixed list)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','of','and','to','a','in','is','it','that','for','on','with','as','are','was','be','by','at','from','an'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stop)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of common English stopwords among word tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','a','an','and','or','but','if','to','of','in','for','on','with','as','at','by','from',\n                 'that','this','it','is','are','was','were','be','been','have','has','had','do','does','did',\n                 'not','no','so','will','would','can','could','should','may','might','he','she','they','we',\n                 'you','i','me','my','your','our','their','his','her','its'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    cnt = sum(1 for w in words if w in stopwords)\n    result = cnt / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of newline characters (newlines per character) to detect line-oriented formats'\n    if not text:\n        return 0.0\n    newlines = text.count('\\n')\n    return float(newlines) / float(max(1, len(text)))\n\n",
    "def feature(text: str) -> float:\n    'Character-level Shannon entropy of the text (bits per character)'\n    import math\n    if not text:\n        return 0.0\n    freq = {}\n    for c in text:\n        freq[c] = freq.get(c, 0) + 1\n    n = float(len(text))\n    entropy = 0.0\n    for count in freq.values():\n        p = count / n\n        entropy -= p * math.log2(p)\n    return float(entropy)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are modal verbs (can/could/may/might/must/shall/should/will/would)'\n    import re\n    if not text:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in modals)\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that contain a hyphen'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hy = sum(1 for t in tokens if '-' in t and re.search(r'[A-Za-z0-9]', t))\n    result = hy / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Heading density: fraction of lines that look like short title/headings (one-4 words, title-case)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    heading_count = 0\n    for ln in lines:\n        words = ln.split()\n        if 1 <= len(words) <= 4:\n            # consider title-case or a single word starting with uppercase\n            if all(w[0].isupper() for w in words if w):\n                heading_count += 1\n    return heading_count / len(lines)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of tokens that occur only once (lexical variety proxy)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    counts = Counter(tokens)\n    hapaxes = sum(1 for c in counts.values() if c == 1)\n    return float(hapaxes) / float(len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing repeated punctuation sequences (e.g., !!!, ...)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    rep_re = re.compile(r'([^\\w\\s])\\1{1,}')  # same non-word non-space char repeated at least twice\n    count = sum(1 for t in tokens if rep_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are unusually long (>12 characters) \u2014 may indicate complex vocabulary or concatenated tokens typical in some generated text'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) > 12)\n    return float(long_count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word tokens / total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    result = unique / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are hapax legomena (appear exactly once in the text)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    counts = Counter(words)\n    hapax_tokens = sum(1 for w in words if counts[w] == 1)\n    result = hapax_tokens / len(words)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain internal hyphens connecting word/number parts'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphen_count = 0\n    for t in tokens:\n        if '-' in t and re.search(r'[A-Za-z0-9]-[A-Za-z0-9]', t):\n            hyphen_count += 1\n    return float(hyphen_count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of long punctuation sequences like ellipses or double-dashes (count per character)'\n    if not text:\n        return 0.0\n    long_seqs = text.count('...') + text.count('--') + text.count('\u2014')\n    return long_seqs / max(1, len(text))\n\n",
    "def feature(text: str) -> float:\n    'Simple passive-voice indicator: occurrences of forms like \"was/ were/ is/ are\" followed by an -ed/-en token, normalized by sentences'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r'\\b(?:was|were|is|are|been|being|be)\\b\\s+\\w+(?:ed|en)\\b', text.lower())\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    denom = float(len(sentences)) if sentences else 1.0\n    return float(len(matches)) / denom\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of non-initial tokens that start with an uppercase letter (possible proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    noninitial_total = 0\n    cap_count = 0\n    for s in sentences:\n        tokens = re.findall(r'\\w+', s)\n        if len(tokens) <= 1:\n            continue\n        for t in tokens[1:]:\n            noninitial_total += 1\n            if t and t[0].isupper():\n                cap_count += 1\n    if noninitial_total == 0:\n        return 0.0\n    return float(cap_count / noninitial_total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'@|\\bhttps?://|\\bwww\\.|\\.com\\b|\\.net\\b|\\.org\\b', flags=re.I)\n    count = sum(1 for t in tokens if pattern.search(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of words that are capitalized but not the first word of their sentence (proxy for named-entity or mid-sentence capitalization)'\n    if not text:\n        return 0.0\n    import re\n    # Split into sentences heuristically\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    total_words = 0\n    mid_capitalized = 0\n    for sent in sentences:\n        sent_words = re.findall(r\"\\b[\\w'-]+\\b\", sent)\n        if not sent_words:\n            continue\n        total_words += len(sent_words)\n        # skip first word of sentence\n        for w in sent_words[1:]:\n            if w[0].isupper() and not w.isupper():  # ignore full acronyms\n                mid_capitalized += 1\n    if total_words == 0:\n        return 0.0\n    return float(mid_capitalized) / float(total_words)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        word_counts.append(len(words))\n    if not word_counts:\n        return 0.0\n    result = sum(word_counts) / len(word_counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that appear structurally complex (contain semicolon/colon or subordinating conjunction)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = r'\\b(because|although|though|since|while|unless|whereas|where|which|that)\\b'\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    complex_count = 0\n    for s in sentences:\n        if re.search(r'[;:]', s) or re.search(conj, s, flags=re.I):\n            complex_count += 1\n    return float(complex_count / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word types that occur only once (hapax legomena ratio)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    from collections import Counter\n    freqs = Counter(words)\n    hapax = sum(1 for w, c in freqs.items() if c == 1)\n    result = hapax / len(freqs)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that contain a repeated character sequence of length >=3 (loooove)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1', re.I)\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Ellipsis density: count of sequences of 3+ dots normalized by number of sentences (0 if none)'\n    import re\n    if not text:\n        return 0.0\n    ellipses = re.findall(r'\\.{3,}', text)\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(ellipses)) / float(sentences)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'^[^@\\s]+@[^@\\s]+\\.[A-Za-z]{2,}$')\n    count = sum(1 for t in tokens if email_re.match(t.strip('.,;:()[]{}<>')))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Normalized Shannon entropy of character distribution (0-1)'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    counts = Counter(chars)\n    total = sum(counts.values())\n    if total == 0:\n        return 0.0\n    entropy = -sum((v/total) * math.log2(v/total) for v in counts.values() if v > 0)\n    distinct = len(counts)\n    if distinct <= 1:\n        return 0.0\n    # normalize by maximum entropy log2(distinct)\n    result = entropy / math.log2(distinct)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words; returns 0.0 for a single or empty sentence'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+|\\n+' , text) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()] if text.strip() else []\n    lens = []\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s)\n        lens.append(len(words))\n    if len(lens) <= 1:\n        return 0.0\n    mean = sum(lens) / float(len(lens))\n    if mean == 0:\n        return 0.0\n    variance = sum((l - mean) ** 2 for l in lens) / float(len(lens))\n    std = math.sqrt(variance)\n    return std / mean\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are fully uppercase (shouting-like)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # require at least one letter and more than one character to avoid single-letter caps\n        if any(c.isalpha() for c in t) and len(t) > 1 and t.isupper():\n            count += 1\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Relative word-length variability (std deviation of lengths divided by mean length)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters contained inside parentheses/brackets/braces'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    if total_len == 0:\n        return 0.0\n    inside = 0\n    for pat in (r'\\((.*?)\\)', r'\\[(.*?)\\]', r'\\{(.*?)\\}'):\n        for m in re.findall(pat, text, flags=re.S):\n            inside += len(m)\n    return float(min(1.0, inside / total_len))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of possessive apostrophe tokens (\\'s or \u2019s) to total apostrophe occurrences (possessive vs contraction tendency)'\n    import re\n    if not text:\n        return 0.0\n    # count possessive forms 's and \u2019s (lowercase or uppercase)\n    possessive = len(re.findall(r\"(?:'s|\u2019s)\\b\", text, flags=re.IGNORECASE))\n    total_apost = text.count(\"'\") + text.count('\\u2019') + text.count('\\u2018')\n    if total_apost == 0:\n        return 0.0\n    return float(possessive) / float(total_apost)\n",
    "def feature(text: str) -> float:\n    'Lexical diversity: unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', flags=re.I)\n    count = 0\n    for t in tokens:\n        if url_re.search(t):\n            count += 1\n        elif '@' in t and '.' in t:\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that look \"definition-like\" (linking verb such as \"is/are/can be/are used to/is defined as\" followed by \"that\" or a defining construction)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    pattern = re.compile(r'\\b(?:is|are|can be|are used to|serves to|serve to|refers to|is defined as|are defined as|acts as|can serve as)\\b.*\\bthat\\b', flags=re.I)\n    count = 0\n    for s in sentences:\n        if pattern.search(s):\n            count += 1\n    return float(count) / float(len(sentences)) if sentences else 0.0\n\n",
    "def feature(text: str) -> float:\n    'Fraction of multi-character tokens that are ALL CAPS (shouting/acronym density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 1 and t.isupper())\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of adjacent word pairs that are exact duplicates (case-insensitive)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    dup = sum(1 for i in range(len(tokens)-1) if tokens[i] == tokens[i+1])\n    result = dup / (len(tokens) - 1)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent word pairs that are exact repeats (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    pairs = len(words) - 1\n    repeats = sum(1 for i in range(pairs) if words[i] == words[i + 1])\n    result = repeats / pairs\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Hyphenated token ratio: fraction of whitespace-separated tokens containing a hyphen'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if '-' in t and re.search(r'[A-Za-z]', t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adverbs approximated by tokens ending with \"ly\" (case-insensitive)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[^\\s]+\\b\", text)\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if w.lower().endswith('ly'))\n    return float(ly_count) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence'\n    import re\n    s = text.strip()\n    if not s:\n        return float(0.0)\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        return float(0.0)\n    totals = sum(sent.count(',') for sent in sentences)\n    return float(totals / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Density of common ASCII emoticons (count per token), e.g., :) :( :D :P ;)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    pattern = r'(?:(?:[:;=8][\\-^]?[)DPp\\(\\]/\\\\])|<3|:-?\\|)'\n    emoticons = re.findall(pattern, text)\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    result = len(emoticons) / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Overall vowel density among alphabetic characters (vowels / letters)'\n    if not text:\n        return 0.0\n    letters = [c for c in text if c.isalpha()]\n    if not letters:\n        return 0.0\n    vowels = sum(1 for c in letters if c.lower() in 'aeiou')\n    return float(vowels / len(letters))\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that include double-quote characters (indicative of quotations)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    quote_chars = ('\"', '\u201c', '\u201d')\n    count = sum(1 for s in sentences if any(q in s for q in quote_chars))\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens containing digits (numeric token ratio), catches years and enumerations'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    numeric = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(numeric) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average per-token punctuation ratio (punctuation chars divided by token length), averaged across tokens'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    totals = 0.0\n    count = 0\n    for t in tokens:\n        L = len(t)\n        if L == 0:\n            continue\n        punct = sum(1 for c in t if not c.isalnum())\n        totals += (punct / L)\n        count += 1\n    if count == 0:\n        return float(0.0)\n    result = totals / count\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Past-tense-looking word ratio: fraction of words ending in \"ed\" (simple proxy for past-tense narration)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    past = sum(1 for w in words if len(w) > 2 and w.endswith('ed'))\n    return past / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of a small set of formal/academic transition words (therefore, however, thus, furthermore, moreover) to total words'\n    import re\n    if not text:\n        return 0.0\n    transitions = {'therefore','however','thus','moreover','furthermore','consequently','additionally','hence','nevertheless','notwithstanding'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in transitions)\n    return float(count) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain a quoted span that looks like a title (starts with a capitalized word and is short)'\n    import re\n    if not text:\n        return 0.0\n    # extract sentences\n    sentences = re.findall(r'[^.!?]+[.!?]?', text, flags=re.S)\n    if not sentences:\n        return 0.0\n    # find all quoted spans and mark sentences that contain a likely title\n    quoted_spans = []\n    for m in re.finditer(r'[\u201c\"\\'\\u2018\\u201C](.+?)[\u201d\"\\'\\u2019\\u201D]', text, flags=re.S):\n        inner = m.group(1).strip()\n        # limit length and word count to likely title length\n        words = re.findall(r'\\w+', inner)\n        if words and len(inner) <= 60 and 1 <= len(words) <= 6 and words[0][0].isupper():\n            # record span indices\n            quoted_spans.append((m.start(), m.end(), inner))\n    if not quoted_spans:\n        return 0.0\n    # assign each sentence if it contains any title-like quoted span\n    sentence_contains = 0\n    for s in sentences:\n        s_start = text.find(s)\n        if s_start == -1:\n            continue\n        s_end = s_start + len(s)\n        for (qs, qe, _) in quoted_spans:\n            if qs >= s_start and qe <= s_end:\n                sentence_contains += 1\n                break\n    return float(sentence_contains) / max(1.0, float(len(sentences)))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace tokens that are emoticons or emoji-like'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    # common ASCII emoticons\n    emot_re = re.compile(r'^(?:[:;=8][\\-^]?[)\\]D\\(\\]/Pp3<\\*]|<3|:\\'\\)|:\\'\\()$')\n    count = 0\n    for t in tokens:\n        if emot_re.search(t):\n            count += 1\n            continue\n        # crude emoji detection by codepoint ranges\n        for ch in t:\n            oc = ord(ch)\n            if oc >= 0x1F300 and oc <= 0x1FAFF:\n                count += 1\n                break\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain a digit or look like numeric measurements (percent, mg, ml, cm) \u2014 numeric density'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    num_patterns = re.compile(r'\\d|%|mg\\b|ml\\b|cm\\b|km\\b|mm\\b|kg\\b|g\\b|\u00b5g\\b|ml\\)|\\b\\(\\d')\n    count = 0\n    for t in tokens:\n        if num_patterns.search(t.lower()):\n            count += 1\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Approximate Flesch reading-ease score (heuristic syllable count)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not sentences or not words:\n        return 0.0\n    # approximate syllables by vowel groups per word\n    syllables = 0\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        syllables += max(1, len(groups))\n    asl = len(words) / len(sentences)  # average sentence length\n    asw = syllables / len(words)       # average syllables per word\n    flesch = 206.835 - 1.015 * asl - 84.6 * asw\n    return float(flesch)\n",
    "def feature(text: str) -> float:\n    'Fraction of adjacent word pairs that are exact repeats (e.g., \"the the\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i - 1])\n    return float(repeats / (len(words) - 1))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that contain a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if '?' in s)\n    return float(qcount / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Progressive -ing token fraction: fraction of tokens ending with \"ing\" (approx. continuous actions or gerunds)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    ing_count = sum(1 for t in tokens if t.endswith('ing'))\n    return ing_count / n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences whose first alphabetic character is lowercase (informal/lax capitalization)'\n    import re\n    if not text:\n        return 0.0\n    # Split on sentence terminators or newlines to get candidates\n    parts = re.split(r'(?<=[.!?])\\s+|\\n+', text)\n    starts = 0\n    total = 0\n    for p in parts:\n        s = p.strip()\n        if not s:\n            continue\n        m = re.search(r'[A-Za-z]', s)\n        if not m:\n            continue\n        total += 1\n        if m.group(0).islower():\n            starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts) / float(total)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are inside quoted spans (double or substantial single quotes)'\n    import re\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    inside = 0\n    # double-quoted spans\n    for m in re.findall(r'\"(.*?)\"', text, flags=re.S):\n        inside += len(m)\n    # single-quoted spans that contain a space (avoid contractions)\n    for m in re.findall(r\"'([^']+\\s[^']+)'\", text, flags=re.S):\n        inside += len(m)\n    return float(min(1.0, inside / total))\n\n",
    "def feature(text: str) -> float:\n    'Repeated adjacent word bigram ratio: count of \"word word\" repeats divided by possible bigrams'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return float(0.0)\n    repeats = len(re.findall(r'\\b(\\w+)\\s+\\1\\b', text, flags=re.IGNORECASE))\n    denom = max(1, len(tokens) - 1)\n    return float(repeats / denom)\n\n",
    "def feature(text: str) -> float:\n    'Emoticon density: number of common emoticons per word'\n    if not text:\n        return float(0.0)\n    emoticons = [':)', ':-)', ':(', ':-(', ':d', ':-d', ';)', ';-)', ':p', ':-p', ':/', \":'(\", '<3', '>:(', ':o', ':-o']\n    lt = text.lower()\n    count = 0\n    for e in emoticons:\n        count += lt.count(e)\n    # fallback to words count\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Density of list-like lines (lines starting with bullets or numbered markers)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    marker_re = re.compile(r'^\\s*(?:[-\\*\\+]|[0-9]+[.)])\\s+')\n    count = sum(1 for l in lines if marker_re.match(l))\n    result = count / len(lines)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Proportion of words with letter elongation (same letter repeated 3+ times)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    elongated = sum(1 for w in words if re.search(r'(.)\\1{2,}', w))\n    return float(elongated / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Compressibility score (1 - compressed_size/original_size), clamped to [0,1]'\n    import zlib\n    if not text:\n        return 0.0\n    b = text.encode('utf-8', errors='ignore')\n    orig = len(b)\n    if orig == 0:\n        return 0.0\n    comp = zlib.compress(b)\n    ratio = len(comp) / orig\n    score = max(0.0, 1.0 - ratio)\n    if score > 1.0:\n        score = 1.0\n    return float(score)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    nonascii = sum(1 for c in text if ord(c) > 127)\n    return float(nonascii / total)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total tokens (lexical diversity)'\n    import re\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Shannon character entropy normalized by log2(unique characters)'\n    import math, collections\n    if not text:\n        return 0.0\n    total = len(text)\n    freq = collections.Counter(text)\n    if len(freq) < 2:\n        return 0.0\n    probs = [v / total for v in freq.values()]\n    entropy = -sum(p * math.log2(p) for p in probs if p > 0)\n    # normalize by log2 of number of unique chars\n    norm = entropy / math.log2(len(freq))\n    return float(norm)\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    matches = re.findall(r'https?://\\S+|www\\.\\S+|\\S+@\\S+\\.\\S+', text.lower())\n    return float(len(matches) / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Density of typographic (non-ASCII) quotation/apostrophe/dash characters per word (curly quotes, em-dash, ellipsis, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    nwords = max(1, len(tokens))\n    special_chars = ['\\u2018', '\\u2019', '\\u201c', '\\u201d', '\\u2013', '\\u2014', '\\u2026']\n    count = sum(text.count(ch) for ch in special_chars)\n    return count / float(nwords)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord > 127)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-delimited tokens that look like URLs (http/https/www)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)\\S+', re.IGNORECASE)\n    count = sum(1 for t in tokens if url_re.match(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Estimate of passive-voice constructions: occurrences of be-forms followed by an -ed token per sentence'\n    import re\n    if not text:\n        return 0.0\n    be_forms = r'\\b(?:is|are|was|were|be|been|being|am)\\b'\n    # look for patterns like 'is produced', 'were observed', etc.\n    matches = re.findall(be_forms + r'\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    # normalize by number of sentences (approx)\n    sentences = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(matches)) / sentences\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length measured in words (words per sentence)'\n    import re\n    if not text:\n        return float(0.0)\n    # split into sentences by punctuation followed by whitespace or linebreak\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid zero-division and ignore empty sentences\n    counts = [c for c in word_counts if c > 0]\n    if not counts:\n        return float(0.0)\n    result = sum(counts) / len(counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of hapax legomena (tokens that occur exactly once)'\n    import re\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    cnt = Counter(tokens)\n    hapax = sum(1 for t, c in cnt.items() if c == 1)\n    result = hapax / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'(https?://|www\\.)', re.I)\n    email_re = re.compile(r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.match(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average vowel fraction per word (vowels per word length), vowels include y'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    vowels = set('aeiouy')\n    ratios = []\n    for w in words:\n        L = len(w)\n        if L == 0:\n            continue\n        vc = sum(1 for ch in w if ch in vowels)\n        ratios.append(vc / L)\n    if not ratios:\n        return 0.0\n    return float(sum(ratios) / len(ratios))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sents:\n        return 0.0\n    q = sum(1 for s in sents if s.endswith('?'))\n    return float(q / len(sents))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ordinal > 127)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    return float(len(set(words)) / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of lines that look like short Title-Case headings (e.g., \"Case Summary\")'\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_title_case_line(ln: str) -> bool:\n        parts = ln.split()\n        if len(parts) < 1 or len(parts) > 6:\n            return False\n        # require most words to start with uppercase and have at least one lowercase letter after\n        good = 0\n        for p in parts:\n            if len(p) >= 2 and p[0].isupper() and any(ch.islower() for ch in p[1:]):\n                good += 1\n        return good >= max(1, len(parts) - 1)\n    count = sum(1 for ln in lines if is_title_case_line(ln))\n    return float(count) / float(len(lines))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are short (<= 5 words)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 5:\n            short += 1\n    result = short / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain both letters and digits (alphanumeric tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(c.isdigit() for c in t) and any(c.isalpha() for c in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average vowel-group count per alphabetic token (simple syllable proxy)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'[A-Za-z]+', text)\n    if not words:\n        return 0.0\n    vg_counts = [len(re.findall(r'[aeiouyAEIOUY]+', w)) for w in words]\n    # allow zero vowel groups (e.g., \"rhythm\"); keep as-is\n    result = sum(vg_counts) / len(vg_counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is lowercase (informal starts)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        for ch in s:\n            if ch.isalpha():\n                if ch.islower():\n                    count += 1\n                break\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized Shannon entropy of characters (0..1)'\n    import math, collections\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    freq = collections.Counter(chars)\n    total = len(chars)\n    entropy = -sum((v/total) * math.log2(v/total) for v in freq.values() if v > 0)\n    uniq = len(freq)\n    if uniq <= 1:\n        return 0.0\n    # normalize by max entropy log2(uniq)\n    return float(entropy / math.log2(uniq))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a quotation, bracket, or dash character'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    starters = set(['\"', \"'\", '\u201c', '\u2018', '(', '[', '{', '-', '\u2014', '\u2013', '`'])\n    count = 0\n    for s in sentences:\n        s2 = s.lstrip()\n        if not s2:\n            continue\n        if s2[0] in starters:\n            count += 1\n    return float(count / len(sentences))\n",
    "def feature(text: str) -> float:\n    'Average fraction of digits inside whitespace-separated tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    ratios = []\n    for t in tokens:\n        ln = len(t)\n        if ln == 0:\n            continue\n        digit_count = sum(1 for c in t if c.isdigit())\n        ratios.append(digit_count / ln)\n    if not ratios:\n        return 0.0\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens that are uppercase acronyms (2+ consecutive ASCII uppercase letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    acr = 0\n    for t in tokens:\n        if len(t) >= 2 and re.fullmatch(r'[A-Z]{2,}', t):\n            acr += 1\n    return float(acr) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    word_counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if words:\n            word_counts.append(len(words))\n        else:\n            word_counts.append(0)\n    if not word_counts:\n        return float(0.0)\n    return float(sum(word_counts) / len(word_counts))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing non-ASCII characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    non_ascii = sum(1 for t in tokens if any(ord(c) > 127 for c in t))\n    result = non_ascii / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a lowercase letter (informal/fragment starts)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        # find first alphabetic character\n        m = re.search(r'[A-Za-z]', s)\n        if m:\n            ch = m.group(0)\n            if ch.islower():\n                count += 1\n    return float(count / len(sentences))\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that are numeric (ints or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?(?:\\d{1,3}(?:,\\d{3})+|\\d+)(?:\\.\\d+)?$')\n    count = 0\n    for t in tokens:\n        if num_re.match(t):\n            count += 1\n        else:\n            # allow simple decimals like .5 or percentages like 50%\n            if re.match(r'^[+-]?\\.\\d+$', t) or re.match(r'^[+-]?\\d+(?:\\.\\d+)?%$', t):\n                count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Heuristic passive-voice score: occurrences of a \"be\" form followed by an -ed token per sentence'\n    import re\n    if not text:\n        return 0.0\n    # look for common \"be\" auxiliaries followed by a past-participial-looking word\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|wasn\\'t|weren\\'t|has been|have been|had been)\\b\\s+\\b\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    # normalize by number of sentences (to be robust across lengths)\n    sent_count = max(1, len(re.findall(r'[.!?]', text)))\n    return float(len(matches) / sent_count)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is lowercase'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m and s[m.start()].islower():\n            count += 1\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain any digit characters'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        sentences = [s]\n    count = sum(1 for sent in sentences if any(c.isdigit() for c in sent))\n    result = count / len(sentences)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens in titlecase (first letter uppercase, rest lowercase)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(0.0)\n    title = 0\n    for t in tokens:\n        if len(t) == 1 and t[0].isupper():\n            title += 1\n        elif len(t) > 1 and t[0].isupper() and t[1:].islower():\n            title += 1\n    return float(title / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are written in ALL CAPS (shouting density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    all_caps = sum(1 for t in tokens if len(t) > 1 and t.isupper())\n    return float(all_caps / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Ratio of curly (smart) apostrophes/quotes (\u2019 \u2018 \u201c \u201d) to all apostrophe/quote characters'\n    if not text:\n        return 0.0\n    curly = sum(text.count(ch) for ch in ['\u2019', '\u2018', '\u201c', '\u201d'])\n    straight = sum(text.count(ch) for ch in [\"'\", '\"'])\n    total = curly + straight\n    if total == 0:\n        return 0.0\n    return float(curly) / total\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace tokens that look like URLs or domain mentions'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'(https?://|www\\.|\\.\\w{2,4}(/|$))', flags=re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that contain both letters and digits (alphanumeric mix)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        has_digit = any(c.isdigit() for c in t)\n        has_alpha = any(c.isalpha() for c in t)\n        if has_digit and has_alpha:\n            count += 1\n    return float(count / len(tokens))\n",
    "def feature(text: str) -> float:\n    'Rate of ellipses (\\'...\\', unicode ellipsis) per token \u2014 indicates trailing thoughts/truncation or dramatic pauses'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    ellipsis_count = text.count('...') + text.count('\u2026')\n    return float(ellipsis_count) / float(max(1, len(tokens)))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are extremely long (length > 12 characters) \u2014 catches technical names and compound terms'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) > 12)\n    return float(long_count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL-CAPS words (2+ letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        return len(letters) >= 2 and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    total_words = 0\n    for s in sents:\n        total_words += len(re.findall(r'\\w+', s))\n    if not total_words:\n        return 0.0\n    result = total_words / len(sents)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of word tokens that occur only once'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    from collections import Counter\n    cnt = Counter(words)\n    hapaxes = sum(1 for w in words if cnt[w] == 1)\n    return float(hapaxes / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is lowercase (informal or poorly capitalized starts)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m and m.group(0).islower():\n            count += 1\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of words (length>=3) that are palindromes'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    palins = sum(1 for w in words if len(w) >= 3 and w == w[::-1])\n    result = palins / len(words)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Stopword token density: fraction of word tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there','their'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause density)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    comma_count = text.count(',')\n    result = comma_count / max(1, len(sentences))\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio (first-person tokens +1) / (third-person tokens +1) to capture narrative perspective bias'\n    import re\n    if not text:\n        return 1.0\n    tokens = re.findall(r'\\w+', text.lower())\n    first = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    third = {'he', 'she', 'they', 'him', 'her', 'them', 'his', 'hers', 'their', 'theirs'}\n    fcount = sum(1 for t in tokens if t in first)\n    tcount = sum(1 for t in tokens if t in third)\n    return float(fcount + 1) / float(tcount + 1)\n\n",
    "def feature(text: str) -> float:\n    'Average vowel fraction per word (vowels / letters), averaged across words'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if not words:\n        return float(0.0)\n    vowels = set('aeiou')\n    fractions = []\n    for w in words:\n        letters = [c for c in w if c.isalpha()]\n        if not letters:\n            continue\n        vcount = sum(1 for c in letters if c in vowels)\n        fractions.append(vcount / len(letters))\n    if not fractions:\n        return float(0.0)\n    return float(sum(fractions) / len(fractions))\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common function words (small stopword list) among tokens'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stop)\n    return float(count / len(tokens))\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences matching a simple passive-voice pattern (was/were/... + past-tense)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    pattern = re.compile(r'\\b(was|were|is|are|been|being|be)\\b\\s+\\w+ed\\b', re.I)\n    passive = sum(1 for s in sentences if pattern.search(s))\n    result = passive / len(sentences)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (captures clause density / syntactic complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    comma_count = text.count(',')\n    if not sentences:\n        # if no clear sentence split, normalize by word count instead\n        words = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text)\n        return float(comma_count) / max(1.0, len(words))\n    return float(comma_count) / max(1.0, len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that are titlecased (start with uppercase then lowercase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.istitle())\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like adverbs (ending in \"ly\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and any(c.isalpha() for c in t))\n    return float(ly_count) / float(len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Density of parenthetical years/citation-style years (e.g., (2001)) per sentence, proxy for academic citations'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.findall(r'[.!?]', text)\n    num_sent = max(1, len(sentences))\n    # detect 4-digit years in parentheses (1900-2099)\n    year_paren_matches = re.findall(r'\\(\\s*(?:19|20)\\d{2}\\b[^\\)]*\\)', text)\n    return float(len(year_paren_matches)) / float(num_sent)\n\n",
    "def feature(text: str) -> float:\n    'Normalized character entropy (0-1) excluding whitespace'\n    import math, collections\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    freq = collections.Counter(chars)\n    total = len(chars)\n    entropy = 0.0\n    for count in freq.values():\n        p = count / total\n        entropy -= p * math.log2(p)\n    uniq = len(freq)\n    # normalize by log2(uniq), ensure denominator >=1\n    denom = math.log2(max(2, uniq))\n    return float(entropy / denom)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique lowercase word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"[A-Za-z']+\", text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Normalized entropy of word length distribution (0-1)'\n    import re, math, collections\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    freq = collections.Counter(lengths)\n    total = len(lengths)\n    ent = 0.0\n    for c in freq.values():\n        p = c / total\n        ent -= p * math.log2(p)\n    # normalize by maximum possible entropy = log2(number of distinct length classes)\n    k = len(freq)\n    if k <= 1:\n        return 0.0\n    max_ent = math.log2(k)\n    return float(ent / max_ent)\n",
    "def feature(text: str) -> float:\n    'Average number of punctuation characters per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    punct_counts = []\n    for s in sentences:\n        pc = sum(1 for c in s if not c.isalnum() and not c.isspace())\n        punct_counts.append(pc)\n    if not punct_counts:\n        return 0.0\n    result = sum(punct_counts) / len(punct_counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of token occurrences that are part of repeated words (frequency>1)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    ctr = Counter(words)\n    repeated_tokens = sum(cnt for w, cnt in ctr.items() if cnt > 1)\n    return float(repeated_tokens / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    types = len(set(tokens))\n    return float(types / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of capitalized words (Titlecase) excluding sentence-initial words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words_all = []\n    non_initial = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if not words:\n            continue\n        words_all.extend(words)\n        # exclude first token of the sentence\n        for w in words[1:]:\n            non_initial.append(w)\n    if not non_initial:\n        return 0.0\n    cap_count = sum(1 for w in non_initial if w[0].isupper() and w[1:].islower())\n    result = cap_count / len(non_initial)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized variance of sentence lengths (variance / (mean+1)) to capture sentence-length variability'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(var) / (mean + 1.0)\n\n",
    "def feature(text: str) -> float:\n    'Density of personal pronouns (I, you, he, she, we, they, etc.) among tokens'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','you','he','him','she','her','we','us','they','them','my','your','his','her','our','their','mine','yours','hers','ours','theirs'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that are numeric-like (contain digits and typical numeric punctuation)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[\\d\\.,%:\\-/]+$')\n    count = 0\n    for t in tokens:\n        if re.search(r'\\d', t) and num_re.match(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    url_re = re.compile(r'^(?:https?://|www\\.)', flags=re.I)\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+', flags=re.I)\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t) or ('@' in t and '.' in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with a question mark)'\n    import re\n    s = text.strip()\n    if not s:\n        return float(0.0)\n    sentences = [seg.strip() for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        return float(0.0)\n    q = sum(1 for sent in sentences if sent.endswith('?'))\n    return float(q / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of words that are long (length >= 7 characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 7)\n    result = long_count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    words_per_sentence = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        words_per_sentence.append(len(words))\n    if not words_per_sentence:\n        return 0.0\n    result = sum(words_per_sentence) / len(words_per_sentence)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are questions'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q_count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized imbalance between matching bracket characters () [] {} <> (sum abs differences / total bracket count)'\n    if not text:\n        return 0.0\n    pairs = [('(', ')'), ('[', ']'), ('{', '}'), ('<', '>')]\n    total = 0\n    imbalance = 0\n    for o, c in pairs:\n        oc = text.count(o)\n        cc = text.count(c)\n        total += oc + cc\n        imbalance += abs(oc - cc)\n    if total == 0:\n        return 0.0\n    return float(imbalance / total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (immediate duplicates like \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    return float(repeats / max(1, len(words) - 1))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of characters that are alphabetic letters (letters / total chars)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    letters = sum(1 for ch in text if ch.isalpha())\n    return float(letters / total)\n\n",
    "def feature(text: str) -> float:\n    'Diversity of punctuation used: distinct punctuation chars divided by standard set size'\n    import string\n    if not text:\n        return 0.0\n    puncts = set(c for c in text if c in string.punctuation)\n    total_types = len(string.punctuation) or 1\n    result = len(puncts) / total_types\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain any numeric digit (numbers, years, measurements)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain any digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(c.isdigit() for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain internal punctuation like apostrophes or hyphens (e-mail, you\\'re)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'\\w[\\'\\-]\\w', t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','to','of','a','an','that','it','for','on','with','as','are','was','were','be','by','this','i','you','he','she','they','we','not','or','but','from','at','his','her','their','which','do','does','did','have','has','had','will','can'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word-tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'a','an','the','and','or','but','if','then','because','of','to','in','on','for','with',\n        'as','at','by','from','that','this','these','those','is','are','was','were','be','been',\n        'being','have','has','had','do','does','did','will','would','can','could','should',\n        'not','no','yes','so','into','about','over','under','up','down','out','only','than'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Normalized variance of word lengths (variance / (mean + 1))'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    n = len(lengths)\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    result = var / (mean + 1.0)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of words that are repeated within their sentence (duplicate tokens)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    total_words = 0\n    repeated = 0\n    for s in sents:\n        words = re.findall(r'\\w+', s.lower())\n        total_words += len(words)\n        if words:\n            from collections import Counter\n            c = Counter(words)\n            for v in c.values():\n                if v > 1:\n                    repeated += (v - 1)\n    if total_words == 0:\n        return 0.0\n    result = repeated / total_words\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with \"?\")'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    qcount = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(qcount / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark (question density)'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    qcount = sum(1 for s in sentences if s.endswith('?'))\n    result = qcount / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Stopword density using a small common English stopword set'\n    import re\n    if not text:\n        return float(0.0)\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on',\n        'with','he','as','you','do','at','this','but','his','by','from','they','we',\n        'say','her','she','or','an','will','my','one','all','would','there','their',\n        'what','so','up','out','if','about','who','get','which','go','me'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    stop_count = sum(1 for t in tokens if t in stopwords)\n    return float(stop_count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are all-uppercase abbreviations or \"shouting\" (>=2 letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        return len(letters) >= 2 and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average longest repeated-character run length normalized by token length'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    total = 0.0\n    for t in tokens:\n        max_run = 1\n        cur = 1\n        for i in range(1, len(t)):\n            if t[i] == t[i-1]:\n                cur += 1\n                if cur > max_run:\n                    max_run = cur\n            else:\n                cur = 1\n        total += (max_run / len(t)) if len(t) > 0 else 0.0\n    return float(total / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (accents, emoji, other scripts)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of distinct punctuation characters per sentence (punctuation variety)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    def puncts(s):\n        return set(c for c in s if not c.isalnum() and not c.isspace())\n    total = sum(len(puncts(s)) for s in sentences)\n    result = total / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain double quotation marks (indicates quotes/dialogue)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        sentences = [s]\n    quote_chars = ('\"', '\u201c', '\u201d')\n    count = sum(1 for sent in sentences if any(q in sent for q in quote_chars))\n    return float(count / len(sentences))\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a coordinating conjunction/connector'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    connectors = {'and', 'but', 'or', 'so', 'because', 'then', 'also', 'however', 'yet', 'thus', 'therefore'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r\"\\b([A-Za-z']+)\", s)\n        if m and m.group(1).lower() in connectors:\n            count += 1\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    result = len(set(tokens)) / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized character-level Shannon entropy (entropy / log2(unique_chars))'\n    import math, collections\n    if not text:\n        return 0.0\n    total = len(text)\n    counts = collections.Counter(text)\n    unique = len(counts)\n    if unique <= 1:\n        return 0.0\n    entropy = 0.0\n    for v in counts.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    norm = math.log2(unique)\n    if norm <= 0:\n        return 0.0\n    return float(entropy / norm)\n\n",
    "def feature(text: str) -> float:\n    'Dominance of the most frequent word (max word frequency divided by total words)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    most = freqs.most_common(1)[0][1]\n    return float(most / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing an apostrophe (contractions/possessives)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if \"'\" in t and any(c.isalpha() for c in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of word tokens that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    hapaxes = sum(1 for t in tokens if counts[t] == 1)\n    return float(hapaxes / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common emoticons present (smileys, hearts, XD, etc.) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    # common emoticon pattern (simple, case-insensitive)\n    emoticon_pat = re.compile(r'(?:(?:[:;=xX8][-^]?[)DdpP/\\\\])|(?:<3)|(?::-?\\()|(?:\\bXD\\b))', re.IGNORECASE)\n    emoticons = sum(1 for t in tokens if emoticon_pat.search(t))\n    result = emoticons / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = qcount / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio (words that occur exactly once)'\n    import re\n    from collections import Counter\n    if not text or not text.strip():\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not words:\n        return 0.0\n    counts = Counter(words)\n    hapax = sum(1 for w, c in counts.items() if c == 1)\n    return float(hapax / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are short (fewer than 5 words)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) < 5:\n            short += 1\n    result = short / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are very long (length >= 12 characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 12)\n    return float(long_words / len(words))\n",
    "def feature(text: str) -> float:\n    'Density of repeated punctuation sequences (characters like !! or ???) as fraction of total chars'\n    import re\n    if not text:\n        return float(0.0)\n    total_chars = len(text)\n    if total_chars == 0:\n        return float(0.0)\n    runs = [m.group(0) for m in re.finditer(r'([^\\w\\s])\\1{1,}', text)]\n    repeated_chars = sum(len(r) for r in runs)\n    return float(repeated_chars / total_chars)\n\n",
    "def feature(text: str) -> float:\n    'Average vowel-to-length ratio per word (vowels / word length)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    vowels = set('aeiou')\n    ratios = []\n    for w in words:\n        L = len(w)\n        if L == 0:\n            continue\n        vcount = sum(1 for c in w if c in vowels)\n        ratios.append(vcount / L)\n    if not ratios:\n        return 0.0\n    return float(sum(ratios) / len(ratios))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of titlecase words that occur not at the start of a sentence (proxy for proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    total_words = 0\n    title_nonstart = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        for i, w in enumerate(words):\n            total_words += 1\n            if i != 0 and w.istitle():\n                title_nonstart += 1\n    if total_words == 0:\n        return 0.0\n    result = title_nonstart / total_words\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord > 127)'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of hapax legomena (words that occur exactly once) to total words'\n    import re, collections\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    counts = collections.Counter(words)\n    hapax = sum(1 for w in words if counts[w] == 1)\n    result = hapax / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per quoted segment (mean quoted-turn length)'\n    import re\n    if not text:\n        return 0.0\n    quotes = re.findall(r'\"([^\"]+)\"', text)\n    if not quotes:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', q)) for q in quotes]\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n",
    "def feature(text: str) -> float:\n    'Average number of vowel groups per word (simple syllable proxy)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    total = 0\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        total += max(1, len(groups))  # treat words with no vowel group as 1\n    result = total / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not sentences:\n        return 0.0\n    return float(len(words) / max(1, len(sentences)))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain a repeated letter run of length >=3 (e.g., \"soooo\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'([A-Za-z])\\1{2,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of mid-sentence capitalized tokens (likely proper nouns) excluding sentence-initial capitals'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    total = 0\n    for m in re.finditer(r'\\b[A-Z][a-z]+\\b', text):\n        start = m.start()\n        # look back one char to see if this is sentence start\n        if start == 0:\n            continue\n        prev_char = text[start-1]\n        # treat as mid-sentence if previous char is not sentence terminator or newline\n        if prev_char not in '.!? \\n\\r\\t':\n            count += 1\n        total += 1\n    # normalize by token count to be comparable across lengths\n    return float(count) / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Average proportion of digits inside tokens (mean per-token digit density)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    densities = []\n    for t in tokens:\n        L = len(t)\n        if L == 0:\n            densities.append(0.0)\n        else:\n            digits = sum(1 for c in t if c.isdigit())\n            densities.append(digits / L)\n    result = sum(densities) / len(densities)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Contraction/apostrophe density: fraction of whitespace tokens containing an apostrophe'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    apos = sum(1 for t in tokens if (\"'\" in t or \"\u2019\" in t))\n    return float(apos / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of 4-digit year mentions (1900-2099) per word, common in academic citations'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = text.split()\n    if not words:\n        return 0.0\n    years = re.findall(r'\\b(?:19|20)\\d{2}\\b', text)\n    return float(len(years)) / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # split into sentences by punctuation, but treat whole text as one sentence if no delimiters\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    # compute words per sentence\n    lengths = []\n    for s in sentences:\n        w = re.findall(r'\\w+', s)\n        lengths.append(len(w))\n    if not lengths:\n        return 0.0\n    result = sum(lengths) / len(lengths)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (std / mean) \u2014 captures variability in lexical shape'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = float(sum(lengths)) / len(lengths)\n    if mean == 0.0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std) / float(mean)\n\n",
    "def feature(text: str) -> float:\n    'Average syllable count per word estimated by vowel groupings'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    syll_counts = []\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        count = len(groups)\n        # ensure at least one syllable\n        syll_counts.append(max(1, count))\n    return float(sum(syll_counts) / len(syll_counts))\n\n",
    "def feature(text: str) -> float:\n    'Density of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', flags=re.I)\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}$')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t) or ('.com' in t.lower() or '.org' in t.lower()) and '/' in t:\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that contain double quotes (dialogue sentence ratio)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    dialogue_sent = sum(1 for s in sentences if '\"' in s)\n    return float(dialogue_sent) / len(sentences)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are all-caps (ignore single-letter tokens like \"I\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    def is_all_caps(w):\n        return any(ch.isalpha() for ch in w) and w.isupper() and sum(1 for ch in w if ch.isalpha()) >= 2\n    caps = sum(1 for w in words if is_all_caps(w))\n    result = caps / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of distinct punctuation characters per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    counts = []\n    for s in sentences:\n        pset = set(c for c in s if not c.isalnum() and not c.isspace())\n        counts.append(len(pset))\n    if not counts:\n        return 0.0\n    result = sum(counts) / len(counts)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = qcount / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','and','is','in','it','of','to','a','an','that','this','for','on',\n        'with','as','by','at','from','or','be','are','was','were','has','have'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL CAPS (length>=2 to avoid single-letter tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    caps = 0\n    for t in tokens:\n        stripped = t.strip(\"()[]{}\\\"'`.,:;!?\")\n        if len(stripped) >= 2 and stripped.isalpha() and stripped.upper() == stripped:\n            caps += 1\n    return float(caps / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is uppercase (sentence capitalization)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    total = 0\n    count = 0\n    for s in sentences:\n        for ch in s:\n            if ch.isalpha():\n                total += 1\n                if ch.isupper():\n                    count += 1\n                break\n    if total == 0:\n        return 0.0\n    return float(count / total)\n\n",
    "def feature(text: str) -> float:\n    'Unique word ratio: number of unique words divided by total words (low values indicate repetition, might reveal certain generation patterns)'\n    if not text:\n        return 0.0\n    words = [w.strip(\".,;:\\\"'()[]{}\").lower() for w in text.split() if w.strip(\".,;:\\\"'()[]{}\")]\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(len(words))\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that contain any digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Function-word density: fraction of common function words present among all tokens (proxy for grammatical/stopword usage)'\n    import re\n    if not text:\n        return 0.0\n    stopset = {'the','and','of','to','a','in','that','it','is','was','for','as','with','on','at','by','an','be','this','which','or','from','but','not','are','his','her','their','they','he','she','you','i','we','my','your','our'}\n    words = re.findall(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?\", text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in stopset)\n    return float(count) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        return 0.0\n    count = sum(1 for s in sents if s.endswith('?'))\n    return float(count / len(sents))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are common speech/dialogue verbs (said, asked, shouted, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    speech_verbs = {'said','asked','replied','whispered','muttered','shouted','exclaimed','cried',\n                    'sighed','yelled','answered','answered','beckoned','laughed','snapped','observed'}\n    count = sum(1 for t in tokens if t in speech_verbs)\n    return count / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    words_total = len(re.findall(r'\\w+', text))\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if sentences:\n        per_sentence = [len(re.findall(r'\\w+', s)) for s in sentences]\n        # avoid division by zero if some splits produce zero\n        per_sentence = [p for p in per_sentence if p > 0]\n        if not per_sentence:\n            result = 0.0\n        else:\n            result = sum(per_sentence) / len(per_sentence)\n    else:\n        # no clear sentence boundaries; treat whole text as one sentence if words exist\n        result = float(words_total) if words_total > 0 else 0.0\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of interior words (not the first word of a sentence) that start with an uppercase letter (proper nouns / title-casing irregularity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'([.!?]+\\s*)', text)\n    # reconstruct sentences to ensure splitting keeps content\n    chunks = []\n    temp = ''\n    for part in sentences:\n        temp += part\n        if re.search(r'[.!?]$', part.strip()):\n            chunks.append(temp.strip())\n            temp = ''\n    if temp:\n        chunks.append(temp.strip())\n    interior_capital = 0\n    interior_total = 0\n    for s in chunks:\n        words = re.findall(r'\\b\\w+\\b', s)\n        for w in words[1:]:\n            interior_total += 1\n            if w and w[0].isupper():\n                interior_capital += 1\n    if interior_total == 0:\n        return 0.0\n    return float(interior_capital) / float(interior_total)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain at least one common emoticon (:-), :D, :(, <3, etc.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    emoticon_re = re.compile(r'(?:[:;=8][\\-~]?[)DdpP\\(/\\\\]|<3|:\\'\\)|:\\(|:\\||:-?\\/|:\\-?\\|)', flags=re.I)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if emoticon_re.search(s))\n    return float(count / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words that are adverbs ending with \"ly\" (indicative of descriptive/flowery prose)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[\\w'-]+\\b\", text.lower())\n    if not words:\n        return 0.0\n    ly_count = sum(1 for w in words if w.endswith('ly') and len(w) > 3)\n    return float(ly_count) / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    types = len(set(tokens))\n    result = types / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens in Titlecase (starts uppercase, not all-caps)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return float(0.0)\n    count = 0\n    for t in tokens:\n        if t[0].isupper() and not t.isupper():\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Filler word density: proportion of tokens that are common disfluency/filler words'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    fillers = {'um','uh','erm','hmm','oh','ah','like','yeah','yep','huh','mm','okay','ok','youknow'}\n    count = sum(1 for t in tokens if t in fillers)\n    result = count / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are ALL CAPS (minimum length 2)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    caps = sum(1 for w in words if len(w) >= 2 and w.isupper())\n    return float(caps / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look code-like (contain characters typical in code: ={}<>/\\\\;#*%|` )'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    code_chars = set('=<>/\\\\{};#*%|`@^~')\n    count = 0\n    for t in tokens:\n        if any(c in code_chars for c in t):\n            count += 1\n    return float(count / len(tokens))\n",
    "def feature(text: str) -> float:\n    'Elongated word density: fraction of tokens containing a character repeated three or more times consecutively'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pat = re.compile(r'(.)\\1\\1+', flags=re.I)\n    count = sum(1 for t in tokens if pat.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of explicit links or email addresses (URLs/emails per token)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    emails = re.findall(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', text)\n    urls = re.findall(r'https?://\\S+|www\\.\\S+', text)\n    count = len(emails) + len(urls)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Character-level Shannon entropy normalized by alphabet size'\n    import math\n    if not text:\n        return float(0.0)\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return float(0.0)\n    freq = {}\n    for c in chars:\n        freq[c] = freq.get(c, 0) + 1\n    n = float(len(chars))\n    ent = 0.0\n    for v in freq.values():\n        p = v / n\n        ent -= p * math.log2(p)\n    unique = len(freq)\n    if unique <= 1:\n        return float(0.0)\n    norm = ent / math.log2(unique)\n    return float(norm)\n\n",
    "def feature(text: str) -> float:\n    'Density of double-quote style dialogue markers per sentence (counts \" and smart-quotes per sentence)'\n    import re\n    if not text:\n        return 0.0\n    quote_chars = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    # sentence count (fallback)\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(quote_chars) / float(sentences)\n\n",
    "def feature(text: str) -> float:\n    'Density of subordinating conjunctions (because, although, while, since, unless, etc.) per sentence'\n    import re\n    if not text:\n        return 0.0\n    conj_pattern = r'\\b(because|although|though|while|since|whereas|unless|after|before|provided|if|when|whenever)\\b'\n    matches = re.findall(conj_pattern, text, flags=re.IGNORECASE)\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(matches)) / float(sentence_count)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    num_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num_tokens / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Repeated trigram fraction: proportion of word-trigrams that occur more than once (0 if none)'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 3:\n        return 0.0\n    trigrams = [' '.join(words[i:i+3]) for i in range(len(words)-2)]\n    from collections import Counter\n    c = Counter(trigrams)\n    repeated_count = sum(count for trigram, count in c.items() if count > 1)\n    # measure repeated trigram occurrences relative to total trigrams\n    return float(repeated_count) / float(len(trigrams))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord>127)'\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens (simple list)'\n    import re\n    if not text:\n        return 0.0\n    STOP = {'the','is','in','it','and','or','to','a','of','for','on','with','as','by','an','be','this','that','are','was','were','has','have','at','from','but','not','they','you','i','we','he','she','his','her'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    stop_count = sum(1 for w in words if w in STOP)\n    return float(stop_count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence (words / sentences)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = len(sentences) or 1\n    return float(len(words) / sent_count)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing an internal hyphen or apostrophe (contractions / hyphenation)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'[A-Za-z0-9][\\'\u2019\\-][A-Za-z0-9]')\n    count = sum(1 for t in tokens if pattern.search(t))\n    result = count / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Immediate repeated-word rate: fraction of adjacent word pairs that are identical (e.g., \"the the\")'\n    import re\n    words = re.findall(r\"\\b\\w+\\b\", (text or '').lower())\n    if not words:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    return repeats / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that include at least one digit (numeric token ratio)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Number of distinct punctuation characters used (variety of punctuation marks)'\n    if not text:\n        return 0.0\n    puncts = {c for c in text if not c.isalnum() and not c.isspace()}\n    return float(len(puncts))\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like common ASCII emoticons or simple hearts (<3)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'^(?:[:;=8][\\-~]?[\\)DdpP/\\(\\\\]|<3|:-?\\(|:-?\\))$', re.I)\n    matches = 0\n    for t in tokens:\n        if pattern.search(t):\n            matches += 1\n    return float(matches / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Adjacent repeated-word rate (fraction of adjacent word pairs that are identical)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = 0\n    pairs = len(words) - 1\n    for i in range(pairs):\n        if words[i] == words[i + 1]:\n            repeats += 1\n    return float(repeats / pairs)\n",
    "def feature(text: str) -> float:\n    'Proportion of single-character tokens (isolated letters or punctuation tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) == 1)\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of titlecase words that occur NOT at the start of a sentence (possible proper nouns)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    total = 0\n    title_nonstart = 0\n    for s in sentences:\n        toks = re.findall(r'\\w+', s)\n        for i, t in enumerate(toks):\n            total += 1\n            if i > 0 and t.istitle():\n                title_nonstart += 1\n    if total == 0:\n        return 0.0\n    return float(title_nonstart / total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are likely emoticons or contain emoji characters'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    emoticon_re = re.compile(r'[:;=8][\\-\\^]?[)\\(DPp\\\\/]|<3')\n    def has_emoji_char(s):\n        for ch in s:\n            oc = ord(ch)\n            if (0x1F300 <= oc <= 0x1F5FF) or (0x1F600 <= oc <= 0x1F64F) or (0x1F680 <= oc <= 0x1F6FF) or (0x2600 <= oc <= 0x26FF) or (0x2700 <= oc <= 0x27BF):\n                return True\n        return False\n    count = 0\n    for t in tokens:\n        if emoticon_re.search(t) or has_emoji_char(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\S+', text)\n    if not toks:\n        return 0.0\n    url_email_re = re.compile(r'([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})|(https?://)|(www\\.)', re.I)\n    count = sum(1 for t in toks if url_email_re.search(t))\n    return float(count / len(toks))\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (indicator of clause density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_commas = sum(s.count(',') for s in sentences)\n    result = total_commas / len(sentences)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric (integers or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d+(\\.\\d+)?$')\n    count = sum(1 for t in tokens if num_re.match(t))\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','a','an','of','to','in','is','it','that','this','for','on','with','as','are','was','were','be','by','or','from','at','not','but','you','i','he','she','they','we','his','her','their'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a capitalized word'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    starts = 0\n    total = 0\n    for s in sentences:\n        total += 1\n        first = re.findall(r'\\w+', s)\n        if first and first[0][0].isupper():\n            starts += 1\n    if total == 0:\n        return 0.0\n    return float(starts / total)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters in the text that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of internal capitalized tokens (probable proper-nouns) excluding sentence-initial tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+\\s*', text) if s.strip()]\n    total_tokens = 0\n    proper_tokens = 0\n    for s in sentences:\n        tokens = [t for t in re.findall(r'\\S+', s)]\n        if not tokens:\n            continue\n        # ignore first token of the sentence (may be capitalized by sentence start)\n        for t in tokens[1:]:\n            cleaned = t.strip('\"\\',:;()[]{}')\n            if not cleaned:\n                continue\n            total_tokens += 1\n            # consider as proper noun if starts with uppercase letter and has at least one lowercase letter following\n            if cleaned[0].isupper() and any(c.islower() for c in cleaned[1:]):\n                proper_tokens += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(proper_tokens) / float(total_tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of long words (length >= 8 characters) as a proxy for vocabulary complexity'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) >= 8)\n    return float(long_words) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent token pairs that are identical (consecutive repeats)'\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if len(tokens) < 2:\n        return float(0.0)\n    repeats = sum(1 for i in range(1, len(tokens)) if tokens[i].lower() == tokens[i-1].lower())\n    transitions = len(tokens) - 1\n    result = repeats / transitions\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are long (>=7 characters)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 7)\n    return float(long_count / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Ratio of common English stopwords to total tokens (a proxy for function-word density and formal prose)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[^\\d\\W_][\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    stopwords = {'the','is','and','of','to','a','in','that','it','for','with','as','on','are','was','by','an','be','this','which','or','from','at','their','these','they','has','have','had'}\n    sw_count = sum(1 for t in tokens if t in stopwords)\n    return float(sw_count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of long alphabetic words (words with >12 letters) as a measure of lexical complexity'\n    import re\n    if not text:\n        return 0.0\n    alpha_words = re.findall(r'[A-Za-z]+', text)\n    if not alpha_words:\n        return 0.0\n    long_count = sum(1 for w in alpha_words if len(w) > 12)\n    result = long_count / len(alpha_words)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts) / len(word_counts))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing a hyphen (hyphenated-token ratio)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences whose first alphabetic character is capitalized (sentence-start capitalization)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    def first_alpha_is_upper(s):\n        for ch in s:\n            if ch.isalpha():\n                return ch.isupper()\n        return False\n    count = sum(1 for s in sentences if first_alpha_is_upper(s))\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Numeric token fraction: proportion of tokens containing any digit'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\b\\S+\\b', text)\n    if not tokens:\n        return float(0.0)\n    num_count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(num_count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens that look like URLs (http/www or contain domain-like pattern)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\S+\\b', text)\n    if not tokens:\n        return 0.0\n    url_count = 0\n    domain_re = re.compile(r'\\.\\w{2,4}(?:[:/]|$)')\n    for t in tokens:\n        if t.lower().startswith(('http://', 'https://', 'www.')) or '://' in t or domain_re.search(t):\n            url_count += 1\n    result = url_count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized entropy of punctuation character distribution (0-1)'\n    import string, math\n    if not text:\n        return 0.0\n    punct_chars = [c for c in text if c in string.punctuation]\n    total = len(punct_chars)\n    if total == 0:\n        return 0.0\n    freq = {}\n    for c in punct_chars:\n        freq[c] = freq.get(c, 0) + 1\n    probs = [v / total for v in freq.values()]\n    entropy = -sum(p * math.log2(p) for p in probs if p > 0)\n    types = len(probs)\n    if types <= 1:\n        return 0.0\n    norm = math.log2(types)\n    return float(entropy / norm)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if s.strip().endswith('?'))\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        words = re.findall(r'\\w+', s)\n        return float(len(words) / max(1, 1)) if words else 0.0\n    total_words = 0\n    for sent in sentences:\n        total_words += len(re.findall(r'\\w+', sent))\n    return float(total_words / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Normalized density of multi-word capitalized sequences (e.g., \"New York\", \"Lord Voldemort\") per sentence'\n    import re\n    if not text:\n        return 0.0\n    # find sequences of two or more consecutive Titlecase words\n    matches = re.findall(r'\\b(?:[A-Z][a-z]+(?:\\s+|$)){2,}', text)\n    sents = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    scount = max(1, len(sents))\n    return float(len(matches)) / float(scount)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of words longer than 12 characters (very long word density)'\n    import re\n    words = re.findall(r'\\w+', text or '')\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) > 12)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like time expressions (e.g., \"5:01\", \"12:30\", tokens containing \":\" or standalone AM/PM)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    time_re = re.compile(r'\\b\\d{1,2}:\\d{2}\\b')\n    ampm_re = re.compile(r'(?i)\\b(?:am|pm)\\b')\n    count = 0\n    for t in tokens:\n        if time_re.search(t) or ampm_re.search(t) or (':' in t and any(ch.isdigit() for ch in t)):\n            count += 1\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are all-uppercase words of length >=2 (shouting/acronyms)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) >= 2 and any(c.isalpha() for c in t):\n            if all((not c.isalpha()) or c.isupper() for c in t):\n                count += 1\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length measured in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    if not sentences:\n        return float(len(words))\n    return float(len(words) / max(1, len(sentences)))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of first/second-person pronouns among all counted personal pronouns'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    first_second = {'i', 'we', 'you', 'me', 'us', 'my', 'our', 'mine', 'yours', 'your'}\n    third = {'he', 'she', 'they', 'them', 'his', 'her', 'their', 'theirs', 'hers'}\n    fs_count = sum(1 for w in words if w in first_second)\n    th_count = sum(1 for w in words if w in third)\n    total = fs_count + th_count\n    if total == 0:\n        return 0.0\n    return float(fs_count / total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words that contain an uppercase letter after the first character (internal caps)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    mixed = 0\n    for w in words:\n        if len(w) > 1 and any(c.isupper() for c in w[1:]):\n            mixed += 1\n    return float(mixed / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Normalized standard deviation of sentence lengths (stddev / mean)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lens = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lens:\n        return 0.0\n    mean = sum(lens) / len(lens)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lens) / len(lens)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
    "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    result = distinct / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of long words (words longer than 7 characters) \u2014 lexical complexity indicator'\n    import re\n    tokens = re.findall(r\"\\b\\w+\\b\", text)\n    if not tokens:\n        return 0.0\n    long_words = sum(1 for t in tokens if len(t) > 7)\n    return long_words / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain an apostrophe character (proxy for contractions)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"\\S+\", text)\n    if not tokens:\n        return 0.0\n    apos_chars = set([\"'\", \"\u2019\", \"\u02bc\"])\n    count = sum(1 for t in tokens if any(c in apos_chars for c in t))\n    return float(count / len(tokens))\n",
    "def feature(text: str) -> float:\n    'Fraction of lines that look like short title lines (1-6 words, each word title-cased), e.g. \"The Colors Fade\"'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip() != '']\n    if not lines:\n        return 0.0\n    title_like = 0\n    for ln in lines:\n        tokens = re.findall(r'\\b\\w+\\b', ln)\n        if 1 <= len(tokens) <= 6:\n            alpha_tokens = [t for t in tokens if any(c.isalpha() for c in t)]\n            if alpha_tokens and all(t[0].isupper() for t in alpha_tokens):\n                title_like += 1\n    return float(title_like) / float(len(lines))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that match common contraction pattern (e.g., don\\'t, I\\'m)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w+'\\w+\\b\", text)\n    all_toks = re.findall(r'\\b\\w+\\b', text)\n    if not all_toks:\n        return 0.0\n    return float(len(tokens) / len(all_toks))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens ending with -ed (simple past/participial marker density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z'-]+\", text.lower())\n    if not tokens:\n        return 0.0\n    ed_count = sum(1 for t in tokens if t.endswith('ed') and len(t) > 3)\n    return float(ed_count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing an apostrophe (contractions/possessives)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\S+\", text)\n    if not tokens:\n        return 0.0\n    apos = 0\n    for t in tokens:\n        if \"'\" in t or \"\u2019\" in t:\n            apos += 1\n    return float(apos / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Flesch reading ease score (simple syllable heuristic), clamped to [0,100]'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not words or not sentences:\n        return 0.0\n    # syllable heuristic similar to other feature\n    total_syl = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        syl = max(1, len(groups))\n        if w.endswith('e') and len(groups) > 1:\n            syl = max(1, syl - 1)\n        total_syl += syl\n    words_count = len(words)\n    sentences_count = max(1, len(sentences))\n    # Flesch Reading Ease formula\n    score = 206.835 - 1.015 * (words_count / sentences_count) - 84.6 * (total_syl / words_count)\n    # clamp to 0-100 for stability\n    score = max(0.0, min(100.0, score))\n    return float(score)\n",
    "def feature(text: str) -> float:\n    'Fraction of words estimated to be polysyllabic (>=3 vowel groups)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+\", text)\n    if not words:\n        return 0.0\n    def syllable_groups(w):\n        return len(re.findall(r'[aeiouyAEIOUY]+', w))\n    poly = sum(1 for w in words if syllable_groups(w) >= 3)\n    result = poly / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of simple past auxiliaries to present auxiliaries (was/were/had/did vs is/are/am/have/do) to hint at tense usage'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    past_set = {'was', 'were', 'had', 'did'}\n    present_set = {'is', 'are', 'am', 'have', 'do', 'does'}\n    past = sum(1 for w in words if w in past_set)\n    present = sum(1 for w in words if w in present_set)\n    # add small smoothing to avoid division by zero\n    return float(past + 0.5) / float(present + 0.5)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of modal verbs (can/could/may/might/shall/should/will/would/must/ought) among tokens'\n    import re\n    modals = {'can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would', 'must', 'ought'}\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in modals)\n    return float(count) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of short title-like lines: lines of 1-6 words where each word starts with an uppercase letter (heuristic for headings)'\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_title_like(line):\n        if any(line.endswith(p) for p in '.:!?'):\n            return False\n        words = line.split()\n        if not (1 <= len(words) <= 6):\n            return False\n        for w in words:\n            if not w[0].isupper():\n                return False\n        return True\n    title_lines = sum(1 for ln in lines if is_title_like(ln))\n    return float(title_lines) / len(lines)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of lines that look like headings (short line starting with capitalized word)'\n    import re\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    heading_pattern = re.compile(r'^[A-Z][A-Za-z0-9 \\-]{0,60}\\s*$')\n    heading_count = sum(1 for l in lines if heading_pattern.match(l.strip()))\n    return float(heading_count) / max(1.0, len(lines))\n\n",
    "def feature(text: str) -> float:\n    'Lexical diversity: ratio of unique word forms to total words'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average estimated syllables per word using vowel-group counts'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    def syllables(w):\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        return max(1, len(groups))\n    total = sum(syllables(w) for w in words)\n    return float(total / len(words))\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens ending with common adjective/adverb suffixes (-ly, -ive, -ous, etc.)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    suffixes = ('ly','ive','ous','able','ible','al','ful','less','ic','ish','ant','ent')\n    count = 0\n    for w in words:\n        for sfx in suffixes:\n            if w.endswith(sfx) and len(w) > len(sfx) + 1:\n                count += 1\n                break\n    return float(count / len(words))\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that end with an exclamation mark'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    ex = sum(1 for s in sentences if s.rstrip().endswith('!'))\n    return float(ex / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain digits (numbers, dates, codes)'\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of immediate adjacent word pairs that are exact repeats (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    pairs = sum(1 for i in range(len(tokens)-1) if tokens[i] == tokens[i+1])\n    return float(pairs / max(1, len(tokens)))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that contain a repeated character sequence (e.g., \"sooo\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if re.search(r'(.)\\1\\1', w):\n            count += 1\n    return float(count / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Average number of punctuation characters per word (punctuation-to-word ratio)'\n    import re\n    if not text:\n        return float(0.0)\n    punct_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words = len(re.findall(r'\\w+', text))\n    if words == 0:\n        return float(0.0)\n    result = punct_chars / words\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens containing any digit (years, quantities, case-study numbers), useful for formal/business texts'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(digit_tokens) / float(len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Variance of word lengths (population variance of token lengths)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    lengths = [len(w) for w in re.findall(r'\\w+', text)]\n    if not lengths or len(lengths) == 1:\n        return 0.0\n    n = len(lengths)\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    return float(var)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are questions'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.endswith('?'))\n    return float(q_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Normalized character-level Shannon entropy (0-1) using unique-character max entropy'\n    import math\n    if not text:\n        return 0.0\n    n = len(text)\n    if n == 0:\n        return 0.0\n    freq = {}\n    for c in text:\n        freq[c] = freq.get(c, 0) + 1\n    ent = 0.0\n    for v in freq.values():\n        p = v / n\n        ent -= p * math.log2(p)\n    max_ent = math.log2(len(freq)) if len(freq) > 1 else 0.0\n    if max_ent <= 0.0:\n        result = 0.0\n    else:\n        result = ent / max_ent\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of common academic/Latin abbreviations (e.g., \"e.g.\", \"i.e.\", \"et al.\") per token'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    patterns = ['e.g.', 'eg,', 'i.e.', 'ie,', 'et al', 'cf.', 'viz.', 'ibid', 'op. cit.']\n    count = 0\n    for p in patterns:\n        count += lower.count(p)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths) / len(lengths))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that begin with a lowercase alphabetic character (informal/casual style)'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    lower_start = 0\n    for s in sentences:\n        for ch in s:\n            if ch.isalpha():\n                if ch.islower():\n                    lower_start += 1\n                break\n    return float(lower_start / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Diversity of punctuation: distinct punctuation symbols divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct / total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of raw tokens that are all-uppercase (contain letters and all letters uppercase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        letters = [c for c in t if c.isalpha()]\n        if letters and all(c.isupper() for c in letters) and len(t) > 1:\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation density: punctuation characters divided by total characters'\n    if not text:\n        return 0.0\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct / len(text))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that belong to a small narrative genre vocabulary (sci-fi/horror words like \"galaxy\",\"ship\",\"alien\",\"void\",\"corpse\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text.lower())\n    if not tokens:\n        return 0.0\n    genre_words = {'galaxy','ship','alien','void','captain','orbit','planet','surface','readings','gelatinous',\n                   'mortician','corpse','slumber','woke','body','glowing','game','void','orb','sci','science','engine',\n                   'ship','star','world','alien','creature','spaceship','colony','astronaut','synthetic','android','cyber'}\n    count = sum(1 for t in tokens if t in genre_words)\n    return float(count) / len(tokens)\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are all-caps (length > 1)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if len(w) > 1 and w.isupper())\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique words divided by total words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation characters divided by total punctuation occurrences'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct / len(puncts))\n\n",
    "def feature(text: str) -> float:\n    'Average number of word tokens per sentence (words/sentence)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    words_in = lambda seg: re.findall(r'\\w+', seg)\n    if sentences:\n        totals = sum(len(words_in(seg)) for seg in sentences)\n        return float(totals / len(sentences)) if len(sentences) else float(0.0)\n    # fallback: use overall word count\n    all_words = words_in(s)\n    if not all_words:\n        return float(0.0)\n    return float(len(all_words))\n\n\n",
    "def feature(text: str) -> float:\n    'Ratio of punctuation tokens that are preceded by a space (e.g., \"word .\"), which flags odd spacing'\n    import re\n    if not text:\n        return 0.0\n    spaced_before = len(re.findall(r'\\s[.,;:!?]', text))\n    total_punct = len(re.findall(r'[.,;:!?]', text))\n    if total_punct == 0:\n        return 0.0\n    return float(spaced_before) / float(total_punct)\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count), 0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = max(1, len(sentences))\n    return float(comma_count) / float(num_sent)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    def looks_like_link(tok):\n        t = tok.lower().strip('.,;:!?)(\"\\'')\n        if 'http' in t or t.startswith('www.') or '@' in t:\n            return True\n        for suf in ('.com', '.org', '.net', '.edu', '.gov', '.io', '.co'):\n            if t.endswith(suf):\n                return True\n        return False\n    count = sum(1 for t in tokens if looks_like_link(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts) / len(word_counts))\n\n",
    "def feature(text: str) -> float:\n    'Short-word ratio: proportion of word tokens of length two or less'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) <= 2)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = digit_tokens / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Approximate passive-voice score: number of \"be\" + past-participles patterns per sentence'\n    import re\n    if not text:\n        return 0.0\n    # Look for patterns like \"was created\", \"has been transformed\", \"is discussed\"\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|being|has been|have been|had been|will be|would be|should be)\\b\\s+\\w+(?:ed|en)\\b', re.I)\n    matches = pattern.findall(text)\n    sentences = max(1.0, float(text.count('.') + text.count('!') + text.count('?')))\n    return float(len(matches)) / sentences\n\n",
    "def feature(text: str) -> float:\n    'Normalized character entropy (Shannon entropy divided by log2(unique_chars)) in [0,1]'\n    import math\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    counts = Counter(text)\n    unique = len(counts)\n    if unique <= 1:\n        return float(0.0)\n    total = sum(counts.values())\n    ent = 0.0\n    for c, cnt in counts.items():\n        p = cnt / total\n        ent -= p * math.log2(p)\n    norm = ent / math.log2(unique)\n    return float(norm)\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace tokens that are acronyms (all upper-case, length>=2)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # strip surrounding non-alphanumerics\n        tt = re.sub(r'^[^A-Za-z0-9]+|[^A-Za-z0-9]+$', '', t)\n        if re.fullmatch(r'[A-Z]{2,}', tt):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like list items (bullets or numbered)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return float(0.0)\n    list_re = re.compile(r'^\\s*(?:[-\\*\\u2022]|\\d+\\.)\\s+')\n    count = sum(1 for l in lines if list_re.match(l))\n    result = count / len(lines)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens ending with -ly (heuristic adverb density)'\n    import re, string\n    if not text:\n        return 0.0\n    raw = re.findall(r\"\\S+\", text)\n    cleaned = []\n    for tok in raw:\n        tok = tok.strip(string.punctuation).lower()\n        if tok:\n            cleaned.append(tok)\n    if not cleaned:\n        return 0.0\n    ly_count = sum(1 for w in cleaned if w.endswith('ly'))\n    return float(ly_count) / len(cleaned)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are common stopwords (the, and, of, to, a, in, is, that)'\n    import re\n    stopwords = {'the','and','of','to','a','in','is','that','it','for','on','with','as','are','was','be','by','an','this'}\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of email-like tokens among whitespace-separated tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', flags=re.I)\n    email_count = sum(1 for t in tokens if email_re.search(t))\n    return float(email_count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-space characters that are Unicode symbol characters (category starting with \"S\")'\n    import unicodedata\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    sym = 0\n    for c in chars:\n        try:\n            if unicodedata.category(c).startswith('S'):\n                sym += 1\n        except Exception:\n            continue\n    return float(sym / len(chars))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that look like URLs (http(s) or www.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(?:https?://|www\\.)\\S+', flags=re.I)\n    count = sum(1 for t in tokens if url_re.match(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of explicit URLs (http(s) or www) per whitespace token'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    urls = re.findall(r'(https?://[^\\s]+|www\\.[^\\s]+)', text, flags=re.I)\n    return float(len(urls) / len(tokens))\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = num_tokens / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized Shannon entropy of the character distribution (0-1)'\n    import math\n    if not text:\n        return 0.0\n    chars = list(text)\n    n = len(chars)\n    if n == 0:\n        return 0.0\n    freq = {}\n    for c in chars:\n        freq[c] = freq.get(c, 0) + 1\n    probs = [v / n for v in freq.values()]\n    H = -sum(p * math.log2(p) for p in probs if p > 0)\n    uniq = len(freq)\n    if uniq <= 1:\n        return 0.0\n    denom = math.log2(uniq)\n    return float(H / denom)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are pure numeric (digits only)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if re.fullmatch(r'\\d+', t))\n    return float(num / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of ellipses (\"...\" or longer) per token (indicates trailing/reflective style)'\n    if not text:\n        return 0.0\n    ellipses = len(re.findall(r'\\.{3,}', text))\n    tokens = re.findall(r'\\w+|\\S', text)\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    return float(ellipses) / float(total)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that look like URLs or domain-like references'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pat = re.compile(r'https?://\\S+|www\\.\\S+|\\b[\\w-]+\\.(com|org|net|io|gov|edu|co)(/|:|$)', re.I)\n    count = sum(1 for t in tokens if pat.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Distinct punctuation variety normalized (distinct punctuation characters / 10, capped at 1.0)'\n    if not text:\n        return 0.0\n    puncts = set(ch for ch in text if not ch.isalnum() and not ch.isspace())\n    if not puncts:\n        return 0.0\n    result = min(len(puncts) / 10.0, 1.0)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of abbreviations/acronyms: counts of common dotted abbreviations or uppercase acronyms per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+|\\S', text)\n    if not tokens:\n        return 0.0\n    # dotted abbreviations like e.g., i.e., etc. (with trailing dot) and common abbreviations\n    abbr_pat = re.compile(r'\\b(?:e\\.g|i\\.e|etc|vs|mr|mrs|dr|prof|inc|ltd)\\.', re.IGNORECASE)\n    dotted = len(re.findall(abbr_pat, text))\n    # uppercase acronyms of length >=2\n    acronyms = len(re.findall(r'\\b[A-Z]{2,}\\b', text))\n    count = dotted + acronyms\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\")'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return float(0.0)\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i - 1])\n    return float(repeats / max(1, (len(words) - 1)))\n\n",
    "def feature(text: str) -> float:\n    'Density of long repeated punctuation sequences (3+ same punctuation) per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    denom = len(tokens) if tokens else max(1, len(text))\n    matches = re.findall(r\"([!?.,;:~`*_'\\-\\\"])\\1{2,}\", text)\n    result = len(matches) / denom\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of words that appear only once in the text'\n    import re\n    words = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not words:\n        return 0.0\n    from collections import Counter\n    cnt = Counter(words)\n    hapax = sum(1 for w, c in cnt.items() if c == 1)\n    return hapax / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio (tokens that appear only once)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    c = Counter(tokens)\n    hapax = sum(1 for tok, cnt in c.items() if cnt == 1)\n    result = hapax / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    commas = text.count(',')\n    result = commas / len(sents) if len(sents) > 0 else 0.0\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain no alphabetic characters (pure numbers/symbols)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if not any(c.isalpha() for c in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are Titlecase (capital first letter, rest lowercase)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w[0].isupper() and w[1:].islower())\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that contain a digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens containing a character repeated three or more times consecutively'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    patt = re.compile(r'(.)\\1\\1')\n    count = sum(1 for t in tokens if patt.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation (std/mean) of sentence lengths in words'\n    import re, math\n    s = text.strip()\n    if not s:\n        return float(0.0)\n    sentences = [seg.strip() for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if len(sentences) <= 1:\n        return float(0.0)\n    lengths = [len(re.findall(r'\\w+', sent)) for sent in sentences]\n    n = len(lengths)\n    mean = sum(lengths) / n\n    if mean == 0:\n        return float(0.0)\n    # sample standard deviation (n-1) for stability when n>1\n    var = sum((x - mean) ** 2 for x in lengths) / (n - 1) if n > 1 else 0.0\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
    "def feature(text: str) -> float:\n    'Standard deviation of word lengths'\n    import re, math\n    words = re.findall(r'\\w+', text or '')\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that end with a run of two or more punctuation characters'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        # treat entire text as one sentence\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'([^\\w\\s])+$', s.strip())\n        if m and len(m.group(0)) >= 2:\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Average number of commas and semicolons per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    punct_count = text.count(',') + text.count(';')\n    return float(punct_count / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Emoji character density: fraction of characters that are common emoji codepoints'\n    import re\n    if not text:\n        return 0.0\n    # common emoji/unicode pictograph ranges\n    emoji_re = re.compile(\n        '['\n        '\\U0001F300-\\U0001F5FF'\n        '\\U0001F600-\\U0001F64F'\n        '\\U0001F680-\\U0001F6FF'\n        '\\U0001F1E0-\\U0001F1FF'\n        '\\U00002700-\\U000027BF'\n        '\\U00002600-\\U000026FF'\n        ']',\n        flags=re.UNICODE)\n    chars = len(text)\n    if chars == 0:\n        return 0.0\n    emojis = len(emoji_re.findall(text))\n    return float(emojis / chars)\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are titlecase (Capitalized words like \"London\" or \"Alice\")'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(0.0)\n    titlecase = sum(1 for t in tokens if len(t) > 1 and t[0].isupper() and t[1:].islower())\n    return float(titlecase / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average length in words of quoted segments (0.0 if no quoted segments)'\n    import re\n    if not text:\n        return 0.0\n    # capture content between common quote characters\n    segments = re.findall(r'[\"\u201c\u201d](.*?)[\"\u201c\u201d]', text, flags=re.DOTALL)\n    if not segments:\n        # try single quotes as fallback\n        segments = re.findall(r\"[\u2018\u2019'](.*?)[\u2018\u2019']\", text, flags=re.DOTALL)\n    if not segments:\n        return 0.0\n    word_counts = []\n    for seg in segments:\n        words = re.findall(r\"\\b[\\w']+\\b\", seg)\n        word_counts.append(len(words))\n    return float(sum(word_counts)) / float(len(word_counts))\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), 0 if no sentence delimiters'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence end punctuation, but keep robust for no punctuation\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback: use entire text as one sentence if there are words\n        words = re.findall(r'\\w+', text)\n        return float(len(words)) if words else 0.0\n    words_per_sent = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not words_per_sent:\n        return 0.0\n    return float(sum(words_per_sent)) / float(len(words_per_sent))\n\n",
    "def feature(text: str) -> float:\n    'Binary-ish score: detects title-like first non-empty line (short, title-case words) to indicate headings'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    first = lines[0].strip()\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", first)\n    if not (1 <= len(words) <= 6):\n        return 0.0\n    # Check many words start with uppercase letter\n    upper_count = sum(1 for w in words if w[0].isupper())\n    if upper_count >= max(1, int(0.6 * len(words))):\n        return 1.0\n    return 0.0\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens (len>=2) that are all uppercase'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    up = sum(1 for t in tokens if len(t) >= 2 and t.isupper())\n    return float(up / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that are detected URLs'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'https?://\\S+|www\\.\\S+|\\b\\S+\\.(?:com|org|net|io|gov|edu)(?:[/:]\\S*)?', flags=re.I)\n    matches = url_re.findall(text)\n    # matches may include overlaps; estimate by matching tokens\n    count = 0\n    for t in tokens:\n        if url_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that are common emoticons (e.g., :-) , :D, <3)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    toks = re.findall(r'\\S+', text)\n    if not toks:\n        return 0.0\n    emoticon_re = re.compile(r'(?:(?:[:;=8][\\-^\\'`]?[\\)D\\(\\]/\\\\OpP])|<3)', re.I)\n    count = sum(1 for t in toks if emoticon_re.search(t))\n    return float(count / len(toks))\n\n",
    "def feature(text: str) -> float:\n    'Max run length of consecutive sentences that start with the same first 3-word phrase, normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentences and extract normalized starts\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    m = len(sentences)\n    if m == 0:\n        return 0.0\n    starts = []\n    for s in sentences:\n        words = re.findall(r'\\b\\w+\\b', s.lower())\n        start = ' '.join(words[:3])\n        starts.append(start)\n    max_run = 1\n    cur_run = 1\n    for i in range(1, len(starts)):\n        if starts[i] and starts[i] == starts[i-1]:\n            cur_run += 1\n            if cur_run > max_run:\n                max_run = cur_run\n        else:\n            cur_run = 1\n    return float(max_run) / m\n",
    "def feature(text: str) -> float:\n    'Ratio of hapax legomena (tokens that occur exactly once) to total tokens'\n    import re\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    counts = Counter(tokens)\n    hapax = sum(1 for v in counts.values() if v == 1)\n    return float(hapax / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique lowercased word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r\"\\b[\\w'-]+\\b\", text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return unique / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of past-perfect constructions (\"had\" + past participle-like token ending in -ed) to tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    matches = re.findall(r'\\bhad\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average vowel-group count per word (rough syllable proxy)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+\", text.lower())\n    if not words:\n        return 0.0\n    totals = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        totals += max(1, len(groups))\n    return float(totals / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Personal pronoun token density (first/second/third person pronouns)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','you','your','yours','he','him','his','she','her','hers','they','them','their','theirs','it','its'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are quote characters (single/double/curly quotes)'\n    if not text:\n        return float(0.0)\n    QUOTES = set('\\'\"\u2018\u2019\u201c\u201d')\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    q = sum(1 for c in text if c in QUOTES)\n    return float(q / total)\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing emoji/high-plane unicode characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def has_emoji(tok):\n        for c in tok:\n            o = ord(c)\n            # common emoji/unicode pictograph ranges\n            if 0x1F300 <= o <= 0x1FAFF or 0x1F600 <= o <= 0x1F64F or 0x1F680 <= o <= 0x1F6FF:\n                return True\n        return False\n    count = sum(1 for t in tokens if has_emoji(t))\n    result = count / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: auxiliaries (was/were/is/are/been/being) followed by a past-participle-like token (/ed|en|wn|t endings)'\n    import re\n    if not text:\n        return 0.0\n    # look for patterns like \"was created\", \"were established\", \"is driven\", \"has been cited\"\n    pattern = re.compile(r'\\b(?:was|were|is|are|be|been|being|has|had|have)(?:\\s+been)?\\s+[A-Za-z-]+(?:ed|en|wn|t)\\b', re.I)\n    matches = pattern.findall(text)\n    words = re.findall(r'\\b\\w+\\b', text)\n    return float(len(matches)) / len(words) if words else 0.0\n\n",
    "def feature(text: str) -> float:\n    'Density of adjacent capitalized-word bigrams (e.g., \"Jeremiah Smith\", \"American colonies\") \u2014 heuristic for named entities'\n    import re\n    if not text:\n        return 0.0\n    matches = list(re.finditer(r'\\b\\w+\\b', text))\n    if len(matches) < 2:\n        return 0.0\n    bigram_count = 0\n    for i in range(len(matches) - 1):\n        w1 = matches[i].group(0)\n        w2 = matches[i+1].group(0)\n        # require both start with uppercase letter and look like proper names (not all-caps)\n        if w1 and w2 and w1[0].isupper() and w2[0].isupper() and not w1.isupper() and not w2.isupper():\n            # ensure the first of the pair isn't clearly at a sentence start\n            start = matches[i].start()\n            j = start - 1\n            while j >= 0 and text[j].isspace():\n                j -= 1\n            if j >= 0 and text[j] in '.!?':\n                # likely sentence start -> still could be names, but deprioritize\n                continue\n            bigram_count += 1\n    total_words = len(matches)\n    return float(bigram_count) / float(total_words)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are hyphenated and contain at least one capitalized component (e.g., Shapiro-Stiglitz)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b[\\w-]+\\b', text)\n    if not tokens:\n        return 0.0\n    hyphen_caps = 0\n    for t in tokens:\n        if '-' in t:\n            parts = [p for p in t.split('-') if p]\n            if any(p[0].isupper() for p in parts if p):\n                hyphen_caps += 1\n    return float(hyphen_caps) / float(len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like list items (bullets or numbered markers)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    bullet_re = re.compile(r'^\\s*(?:[-\\*\\u2022]|\\d+\\.|[a-zA-Z]\\))\\s+')\n    count = sum(1 for l in lines if bullet_re.match(l))\n    return float(count / len(lines))\n\n\n",
    "def feature(text: str) -> float:\n    'Density of mid-sentence capitalized words (heuristic proper-name density) = caps not immediately after .!?'\n    import re\n    if not text:\n        return 0.0\n    # find capitalized words like \"John\" or \"Mars\"\n    caps = list(re.finditer(r'\\b[A-Z][a-z]+\\b', text))\n    if not caps:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_tokens = len(words) if words else 1\n    count = 0\n    for m in caps:\n        start = m.start()\n        # look back to find last non-space character before this word\n        i = start - 1\n        while i >= 0 and text[i].isspace():\n            i -= 1\n        if i < 0:\n            # likely start of text -> treat as sentence start -> skip\n            continue\n        if text[i] in '.!?':\n            # likely sentence start -> skip\n            continue\n        count += 1\n    return float(count) / total_tokens\n\n\n",
    "def feature(text: str) -> float:\n    'Density of URL-like tokens per non-space token (urls / tokens)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    urls = re.findall(r'https?://\\S+|www\\.\\S+|[a-z0-9.-]+\\.(?:com|net|org|io|gov|edu)\\b', text, flags=re.IGNORECASE)\n    return float(len(urls) / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens containing an elongated letter run (same letter repeated 3+ times)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'([A-Za-z])\\1{2,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average naive syllable count per word (vowel groups, at least 1)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n    if not words:\n        return 0.0\n    sy_counts = []\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        sy = max(1, len(groups))\n        sy_counts.append(sy)\n    return float(sum(sy_counts) / len(sy_counts))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(count / len(sentences))\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    qcount = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(qcount / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Simple lexicon sentiment score normalized by token count: (pos_count - neg_count) / tokens'\n    import re\n    if not text:\n        return float(0.0)\n    pos = {'good','great','excellent','happy','love','nice','fortunate','pleasant','enjoy','awesome','best'}\n    neg = {'bad','poor','sad','hate','terrible','awful','worst','unfortunate','angry','problem','fail'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    pos_count = sum(1 for w in words if w in pos)\n    neg_count = sum(1 for w in words if w in neg)\n    return float((pos_count - neg_count) / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain quoted spans (paired double or paired single quotes or typographic quotes)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    quote_pairs = 0\n    for s in sentences:\n        dq = s.count('\"')\n        sq = s.count(\"'\")\n        lq = s.count('\u201c') + s.count('\u201d') + s.count('\u2018') + s.count('\u2019')\n        if dq >= 2 or lq >= 2 or sq >= 2:\n            quote_pairs += 1\n    return float(quote_pairs / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are common English stopwords (approximate stopword density)'\n    import re\n    if not text:\n        return 0.0\n    STOP = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on',\n        'with','he','as','you','do','at','this','but','his','by','from','they','we',\n        'say','her','she','or','an','will','my','one','all','would','there','their',\n        'what','so','up','out','if','about','who','get','which','go','me'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    stop_count = sum(1 for t in tokens if t in STOP)\n    return float(stop_count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of adjacent duplicate word pairs (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if len(words) < 2:\n        return 0.0\n    dup = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    result = dup / (len(words) - 1)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Normalized character-level Shannon entropy (0-1)'\n    import math\n    if not text:\n        return 0.0\n    freqs = {}\n    for c in text:\n        freqs[c] = freqs.get(c, 0) + 1\n    n = sum(freqs.values())\n    if n == 0:\n        return 0.0\n    probs = [v / n for v in freqs.values()]\n    import math as _m\n    H = -sum(p * _m.log2(p) for p in probs if p > 0)\n    types = len(probs)\n    if types <= 1:\n        return 0.0\n    return float(H / _m.log2(types))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation-to-word ratio (punctuation chars divided by number of words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = max(1, len(words))\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    result = punct / word_count\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of uppercase alphabetic characters to all alphabetic characters'\n    if not text:\n        return 0.0\n    letters = sum(1 for c in text if c.isalpha())\n    if letters == 0:\n        return 0.0\n    upper = sum(1 for c in text if c.isalpha() and c.isupper())\n    return float(upper / letters)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that look like URLs'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)\\S+', flags=re.I)\n    count = sum(1 for t in tokens if url_re.match(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Normalized character Shannon entropy (0-1)'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    cnt = Counter(text)\n    total = sum(cnt.values())\n    if total == 0:\n        return 0.0\n    entropy = 0.0\n    for v in cnt.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    # normalize by log2 of alphabet size (at least 1 to avoid div by zero)\n    alph = max(1, len(cnt))\n    norm = entropy / math.log2(alph) if alph > 1 else 0.0\n    return float(min(1.0, max(0.0, norm)))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of punctuation characters that are part of repeated punctuation runs (e.g., \"!!!\" or \"??\")'\n    import re\n    if not text:\n        return float(0.0)\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return float(0.0)\n    repeated_len = 0\n    for m in re.finditer(r'([^\\w\\s])\\1+', text):\n        repeated_len += len(m.group(0))\n    return float(repeated_len / total_punct)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word forms divided by total words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Punctuation characters per word (total punctuation chars divided by number of word tokens)'\n    import re, string\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    punct_count = sum(1 for c in text if c in string.punctuation)\n    return float(punct_count / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are all-caps (contain at least one letter and all letters uppercase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(t):\n        letters = [c for c in t if c.isalpha()]\n        return bool(letters) and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are hapax legomena (occur exactly once) relative to total tokens'\n    import re\n    from collections import Counter\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    freqs = Counter(tokens)\n    hapax = sum(1 for t in tokens if freqs[t] == 1)\n    return float(hapax / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric (pure numbers, possibly with commas/dots/signs)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:[.-]\\d+)?$')\n    count = sum(1 for t in tokens if num_re.match(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are adverbs ending in \"-ly\" (proxy for descriptive/flowery style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w'-]+\\b\", text, flags=re.IGNORECASE)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 2 and t.lower().endswith('ly'))\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'All-caps word density: proportion of words (len>=2) that are entirely uppercase'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) >= 2 and w.isupper())\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of 4-digit year-like numbers (1000-2100) per sentence to capture historical/academic references'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(1[0-9]{3}|20[0-9]{2}|2100)\\b', text)\n    sent_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    return float(len(years)) / float(sent_count)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are hedging/modal/qualifying words common in academic writing (may, might, could, suggest, indicate, likely, possibly, appears, tends)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    hedges = {'may','might','could','can','suggest','suggests','suggested','indicate','indicates','indicated','likely','possibly','potentially','appear','appears','appeared','tend','tends','tended','suggesting','indicating'}\n    count = sum(1 for w in words if w in hedges)\n    return float(count) / float(len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Ratio of ampersand tokens (&) to total word tokens (captures \"X & Y\" citation style)'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    amp_count = text.count('&')\n    return float(amp_count) / float(max(1, len(words)))\n\n",
    "def feature(text: str) -> float:\n    'Count of URLs or email-like tokens per 100 words (normalized density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    urls = re.findall(r'https?://\\S+|www\\.\\S+|\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', text, flags=re.I)\n    # normalize to per-100-words\n    result = (len(urls) / len(words)) * 100.0\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that end with a punctuation character'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if t and not t[-1].isalnum():\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Normalized entropy of token length distribution (0-1)'\n    import re, math\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    lengths = {}\n    for w in words:\n        l = len(w)\n        lengths[l] = lengths.get(l, 0) + 1\n    total = len(words)\n    probs = [v / total for v in lengths.values()]\n    entropy = -sum(p * math.log(p) for p in probs if p > 0)\n    k = len(probs)\n    if k <= 1:\n        return float(0.0)\n    max_entropy = math.log(k)\n    result = entropy / max_entropy\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Relative imbalance of bracket/parenthesis characters (abs(opens-closes) / total bracket chars)'\n    if not text:\n        return 0.0\n    opens = sum(1 for c in text if c in '([{')\n    closes = sum(1 for c in text if c in ')]}')\n    total = opens + closes\n    if total == 0:\n        return 0.0\n    imbalance = abs(opens - closes)\n    return float(imbalance / total)\n",
    "def feature(text: str) -> float:\n    'Density of emoji-like characters and common emoticons per character'\n    import re, unicodedata\n    if not text:\n        return 0.0\n    # common ASCII emoticons\n    emoticons = re.findall(r'(?:(?:[:;=8][\\-~]?[)D\\(\\]/\\\\OpP]))', text)\n    emot_count = len(emoticons)\n    # count symbols in Unicode category \"So\" (Symbol, other) as emoji-like\n    emoji_count = 0\n    for c in text:\n        try:\n            if unicodedata.category(c) == 'So':\n                emoji_count += 1\n        except Exception:\n            continue\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float((emot_count + emoji_count) / total_chars)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences ending with a question mark'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    q = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens whose alphabetic characters are all uppercase (word-level shouting)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_word_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        return len(letters) >= 2 and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_word_all_caps(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation chars divided by total punctuation count'\n    import string\n    if not text or not text.strip():\n        return 0.0\n    puncts = [c for c in text if c in string.punctuation]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    result = distinct / total\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens longer than 12 characters'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 12)\n    result = long_count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 1.0 if text.strip().endswith('?') else 0.0\n    q_count = sum(1 for s in sentences if s.strip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that lie inside quoted spans (double quotes and standalone single-quote pairs)'\n    import re\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    inside = 0\n    # double-quoted spans\n    for m in re.findall(r'\"(.*?)\"', text, flags=re.S):\n        inside += len(m)\n    # single-quoted spans but avoid contractions/embedded apostrophes by requiring non-word boundaries\n    for m in re.findall(r\"(?<!\\w)'(.*?)'(?!\\w)\", text, flags=re.S):\n        inside += len(m)\n    return float(min(1.0, inside / total))\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are inside single or double quotes (quoted content density)'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    # find double-quoted and single-quoted content (simple pairwise)\n    dq = re.findall(r'\"([^\"]+)\"', text)\n    sq = re.findall(r\"'([^']+)'\", text)\n    quoted_chars = sum(len(m) for m in dq) + sum(len(m) for m in sq)\n    return float(quoted_chars) / float(total_len) if total_len > 0 else 0.0\n\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        total_words += len(words)\n    return float(total_words / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of long words (length >= 8) among all words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    longw = sum(1 for w in words if len(w) >= 8)\n    result = longw / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are ALL CAPS (>=2 letters and contain a letter)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_shouting(t):\n        return any(c.isalpha() for c in t) and len([c for c in t if c.isalpha()]) >= 2 and t.upper() == t\n    count = sum(1 for t in tokens if is_shouting(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    total_words = sum(word_counts)\n    if total_words == 0:\n        return 0.0\n    result = total_words / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens matching a small sensory vocabulary (see, hear, feel, smell, taste, glow, etc.)'\n    import re\n    sensory = {'see','saw','seen','look','looked','look','hear','heard','listen','feel','felt','smell','smelled','taste',\n               'touch','glow','glowing','bright','dark','shimmer','shimmering','whisper','hiss','silent','silence',\n               'shine','shining','shadow','shadowed','glitter'}\n    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in sensory)\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average number of punctuation characters per word (punctuation-to-word ratio)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    punct_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct_chars / len(words))\n",
    "def feature(text: str) -> float:\n    'Titlecase word density: fraction of words that are Titlecase (First upper, rest lower) or single uppercase letter'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if w[0].isupper() and (len(w) == 1 or w[1:].islower()):\n            count += 1\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are email-like (contain \"@\" and a domain part)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if '@' in t:\n            parts = t.split('@', 1)\n            if len(parts) == 2 and '.' in parts[1] and parts[0]:\n                count += 1\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Normalized count of explicit simile markers (e.g., \"like a\", \"as if\", \"as though\", \"as ... as\") per token'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    patterns = [r'\\blike\\s+(a|an|the)\\b', r'\\bas if\\b', r'\\bas though\\b', r'\\bas\\s+\\w+\\s+as\\b']\n    tokens = re.findall(r'\\b\\w+\\b', lower)\n    if not tokens:\n        return 0.0\n    matches = 0\n    for p in patterns:\n        matches += len(re.findall(p, lower))\n    return float(matches) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are titlecase (First letter upper, rest lower)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if len(w) >= 1 and w[0].isupper() and (w[1:].islower() or len(w) == 1):\n            count += 1\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average vowel fraction per alphabetic word (vowels/word length)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'[A-Za-z]+', text)\n    if not words:\n        return 0.0\n    total_frac = 0.0\n    for w in words:\n        lw = w.lower()\n        vowels = sum(1 for c in lw if c in 'aeiou')\n        total_frac += (vowels / len(lw)) if len(lw) > 0 else 0.0\n    result = total_frac / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    STOPWORDS = {\n        'the','and','a','an','in','on','at','for','to','of','is','are','was','were',\n        'it','that','this','with','as','by','from','or','be','have','has','had','not',\n        'but','if','they','you','he','she','we','i','me','my','your','our','their'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in STOPWORDS)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length measured in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths) / len(lengths))\n\n",
    "def feature(text: str) -> float:\n    'Diversity of contractions: unique contraction forms divided by total contraction tokens (higher = varied contractions)'\n    import re\n    if not text:\n        return 0.0\n    # capture typical contractions like don't, I\\'m, he\\'ll, we\\'ve etc.\n    contractions = re.findall(r\"\\b[a-zA-Z]+\\'[a-zA-Z]{1,4}\\b\", text)\n    total = len(contractions)\n    if total == 0:\n        return 0.0\n    unique = len(set([c.lower() for c in contractions]))\n    return float(unique) / float(total)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (typographic quotes, em-dash, accented chars)'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / total if total else 0.0\n\n",
    "def feature(text: str) -> float:\n    'Comma density: number of commas per word'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    num_words = len(words)\n    if num_words == 0:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count) / float(num_words)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are long (length > 7)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long = sum(1 for w in words if len(w) > 7)\n    return float(long / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Density of emoticons and common emoji characters (per character)'\n    import re\n    if not text:\n        return 0.0\n    emoticon_pattern = re.compile(r'(:-\\)|:\\)|:-\\(|:\\(|:D|:-D|;-\\)|;\\)|:\\'\\(|<3|:\\*)')\n    emoticons = len(emoticon_pattern.findall(text))\n    emoji_ranges = [\n        (0x1F300, 0x1F5FF), (0x1F600, 0x1F64F), (0x1F680, 0x1F6FF),\n        (0x2600, 0x26FF), (0x2700, 0x27BF)\n    ]\n    emoji_chars = 0\n    for c in text:\n        o = ord(c)\n        if any(start <= o <= end for start, end in emoji_ranges):\n            emoji_chars += 1\n    total_symbols = emoticons + emoji_chars\n    if not text:\n        return 0.0\n    return float(total_symbols / max(1, len(text)))\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are purely numeric'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    nums = sum(1 for t in tokens if t.isdigit())\n    result = nums / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are titlecase words (Initial capital then lowercase)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    all_tokens = re.findall(r'\\w+', text)\n    if not all_tokens:\n        return 0.0\n    title_tokens = re.findall(r'\\b[A-Z][a-z]+\\b', text)\n    return float(len(title_tokens) / len(all_tokens))\n\n",
    "def feature(text: str) -> float:\n    'Hapax ratio: fraction of word types that occur exactly once in the text'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    counts = Counter(words)\n    hapax = sum(1 for v in counts.values() if v == 1)\n    return float(hapax / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace tokens that contain digits (numeric tokens)'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Comma density per sentence: average number of commas in each sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback to punctuation density if no clear sentences\n        return float(text.count(',')) if text else 0.0\n    return float(text.count(',')) / float(len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of sentence lengths (std/mean of words per sentence)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    result = std / mean\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that look like URLs (http/https/www)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    urls = re.findall(r'(?:https?://|www\\.)\\S+', text, flags=re.I)\n    return float(len(urls) / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    types = len(set(words))\n    result = types / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of common ASCII emoticons per token (smile/frown/wink/heart etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    # common ASCII emoticon patterns and heart <3\n    emot_re = re.compile(r'(:-\\)|:\\)|:-\\(|:\\(|:D|:d|:P|:p|;-?\\)|<3|:\\||:/|:\\\\|:O|:o|=\\)|=\\()')\n    count = 0\n    for t in tokens:\n        if emot_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are emojis or common emoticons'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticons = {':)', ':-)', ':(', ':-(', ':D', ':-D', ';)', ';-)', ':P', ':-P', ':/', ':-/', \":'(\", ':-|', ':o', ':-o'}\n    def is_emoji_char(c):\n        try:\n            cp = ord(c)\n        except Exception:\n            return False\n        return (\n            0x1F300 <= cp <= 0x1F5FF or\n            0x1F600 <= cp <= 0x1F64F or\n            0x1F680 <= cp <= 0x1F6FF or\n            0x2600  <= cp <= 0x26FF  or\n            0x2700  <= cp <= 0x27BF  or\n            0x1F900 <= cp <= 0x1F9FF\n        )\n    count = 0\n    for t in tokens:\n        if t in emoticons:\n            count += 1\n            continue\n        if any(is_emoji_char(ch) for ch in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with \"?\")'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a pronoun (I, he, she, they, we, it, you)'\n    import re\n    if not text:\n        return 0.0\n    # Split on sentence terminators; keep non-empty trimmed segments\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    pronouns = {'i', 'he', 'she', 'they', 'we', 'it', 'you'}\n    starts = 0\n    for s in sents:\n        w = re.findall(r'\\b\\w+\\b', s)\n        if not w:\n            continue\n        if w[0].lower() in pronouns:\n            starts += 1\n    return float(starts) / float(len(sents))\n\n\n",
    "def feature(text: str) -> float:\n    \"Smoothed ratio of contraction-like apostrophes to possessive \\\"'s\\\" occurrences\"\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    poss = len(re.findall(r\"\\b\\w+'s\\b\", lower))\n    # contractions: n't, 're, 've, 'll, 'm, 'd (exclude 's)\n    contr = len(re.findall(r\"\\b\\w+(?:n't|'re|'ve|'ll|'m|'d)\\b\", lower))\n    # smoothing to avoid division by zero\n    return float(contr + 1) / float(poss + 1)\n\n",
    "def feature(text: str) -> float:\n    'Density of repeated punctuation runs (total run chars / text length) for runs of length>=2'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    if total_len == 0:\n        return 0.0\n    runs = re.findall(r'([^\\w\\s]{2,})', text)\n    if not runs:\n        return 0.0\n    run_chars = sum(len(r) for r in runs)\n    return float(run_chars / total_len)\n\n\n",
    "def feature(text: str) -> float:\n    'Punctuation characters per word (overall punctuation density normalized by word count)'\n    import re\n    if not text:\n        return 0.0\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words = re.findall(r'\\w+', text)\n    return float(punct_count) / float(len(words) + 1)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of average alphabetic-token length to average token length (alpha-token length bias)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    alpha_tokens = [t for t in tokens if t.isalpha()]\n    if not alpha_tokens:\n        return 0.0\n    avg_alpha = sum(len(t) for t in alpha_tokens) / len(alpha_tokens)\n    avg_all = sum(len(t) for t in tokens) / len(tokens)\n    if avg_all == 0:\n        return 0.0\n    result = avg_alpha / avg_all\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that contain at least one digit (numeric-token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = num_count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a gerund/participle word (first token ends with \"ing\")'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    count = 0\n    for s in sents:\n        parts = s.split()\n        if parts:\n            first = re.sub(r'^[^\\w]+|[^\\w]+$', '', parts[0])  # strip non-word chars around\n            if first.lower().endswith('ing') and len(first) > 3:\n                count += 1\n    return float(count) / float(len(sents))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of paragraphs to sentences (paragraph density)'\n    import re\n    if not text:\n        return float(0.0)\n    paragraphs = [p for p in re.split(r'\\n{2,}', text.strip()) if p.strip()]\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    return float(len(paragraphs) / max(1, len(sentences)))\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain hyphens or dashes (hyphenated compounds are common in some academic styles)'\n    import re\n    if not text:\n        return 0.0\n    # consider ASCII hyphen and en/em dashes\n    tokens = re.findall(r\"\\b[\\w\\-\\\u2013\\\u2014]+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if '-' in t or '\u2013' in t or '\u2014' in t)\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas divided by sentence count; sentences fallback to 1 if none)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # split on sentence end punctuation; fallback to whole text if none\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    return comma_count / sent_count\n\n",
    "def feature(text: str) -> float:\n    'Normalized sentence-length variance (variance of words per sentence, divided by mean+1)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if len(sentences) <= 1:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    # normalize by mean+1 to keep scale reasonable\n    result = var / (mean + 1.0)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of sentence lengths (stdev/mean of words per sentence)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if len(sentences) <= 1:\n        return 0.0\n    lens = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lens or sum(lens) == 0:\n        return 0.0\n    mean = sum(lens) / len(lens)\n    var = sum((l - mean) ** 2 for l in lens) / len(lens)\n    stdev = math.sqrt(var)\n    cov = stdev / mean if mean > 0 else 0.0\n    return float(cov)\n\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    comma_count = text.count(',')\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    denom = max(1, len(sentences))\n    result = comma_count / denom\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    q = sum(1 for s in sents if s.rstrip().endswith('?'))\n    result = q / len(sents)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    result = unique / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average estimated syllable count per word using vowel-group heuristic'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[a-zA-Z]+\", text.lower())\n    if not words:\n        return 0.0\n    syll_counts = []\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        count = len(groups)\n        # ensure at least one syllable\n        syll_counts.append(max(1, count))\n    result = sum(syll_counts) / len(syll_counts)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Normalized Shannon entropy of non-space characters (0-1)'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    freqs = Counter(chars)\n    total = len(chars)\n    probs = [v / total for v in freqs.values()]\n    entropy = -sum(p * math.log2(p) for p in probs if p > 0)\n    if len(freqs) <= 1:\n        return 0.0\n    max_entropy = math.log2(len(freqs))\n    result = entropy / max_entropy\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Diversity of apostrophe-bearing tokens: unique apostrophe tokens divided by total apostrophe tokens (higher means many different contractions/possessives)'\n    import re\n    if not text:\n        return 0.0\n    apos_tokens = re.findall(r\"\\b[\\w']*'[^\\s]+\\b\", text)\n    total = len(apos_tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(t.lower() for t in apos_tokens))\n    return float(unique) / float(total)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: fraction of words that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    counts = Counter(words)\n    hapax = sum(1 for w, c in counts.items() if c == 1)\n    result = hapax / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of alphabetic words (length>=2) that are all uppercase'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b[A-Za-z]{2,}\\b', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.isupper())\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that contain at least one digit'\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Emoticon density: frequency of common ASCII emoticons per character'\n    import re\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    # common emoticon patterns like :-) :) :D :( :P ; ) XD etc.\n    emoticons = re.findall(r'(?:(?:[:;=8Xx][-^]?[)DdpP\\/\\\\]|[)DdpP\\/\\\\][-^]?[:;=8Xx]|:\\'\\(|:\\-\\||:O|:o))', text)\n    count = len(emoticons)\n    result = count / total\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with an uppercase letter (ignoring leading non-letters)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    starts_upper = 0\n    for s in sentences:\n        first_alpha = None\n        for c in s:\n            if c.isalpha():\n                first_alpha = c\n                break\n        if first_alpha is None:\n            # no alphabetic char in sentence; treat as not starting with uppercase\n            continue\n        if first_alpha.isupper():\n            starts_upper += 1\n    result = starts_upper / len(sentences)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Normalized count of ellipses (\"...\") occurrences per 100 words (captures poetic/hesitant style)'\n    if not text:\n        return 0.0\n    ellipses = text.count('...')\n    # word count robustly\n    import re\n    words = re.findall(r\"\\w+(?:'\\w+)?\", text)\n    wc = max(1, len(words))\n    return float(ellipses) / float(wc) * 100.0\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas/semicolons per sentence (punctuation complexity)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    punct_count = sum(1 for c in text if c == ',' or c == ';')\n    return float(punct_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Average length of repeated punctuation runs (e.g., \"!!!\", \"??\") considering runs length>1'\n    import re\n    if not text:\n        return float(0.0)\n    runs = [m.group(0) for m in re.finditer(r'([^\\w\\s])\\1*', text)]\n    long_runs = [len(r) for r in runs if len(r) > 1]\n    if not long_runs:\n        return float(0.0)\n    return float(sum(long_runs) / len(long_runs))\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (mean words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        words = re.findall(r'\\w+', s)\n        return float(len(words)) if words else 0.0\n    lens = [len(re.findall(r'\\w+', sent)) for sent in sentences]\n    if not lens:\n        return 0.0\n    return float(sum(lens) / len(lens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of adjacent token pairs that are exact repeats (e.g. \"the the\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    return float(repeats / (len(words) - 1))\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena rate: fraction of word types that occur exactly once (lowercased)'\n    import re\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\b\\w+\\b', text)]\n    if not words:\n        return 0.0\n    from collections import Counter\n    freqs = Counter(words)\n    hapaxes = sum(1 for v in freqs.values() if v == 1)\n    types = len(freqs)\n    return float(hapaxes) / max(types, 1)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word forms / total words)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    questions = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(questions / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing any non-ASCII character'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    non_ascii = 0\n    for t in tokens:\n        if any(ord(c) > 127 for c in t):\n            non_ascii += 1\n    return float(non_ascii / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Contraction density: fraction of whitespace-separated tokens containing an apostrophe'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if \"'\" in t)\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens considered \"long\" (length >= 7), a proxy for lexical complexity'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 7)\n    return float(long_count / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens containing non-ASCII characters (ord>127)'\n    if not text:\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    count = 0\n    for t in tokens:\n        if any(ord(c) > 127 for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Number of punctuation characters per word (punctuation-to-word ratio)'\n    import re\n    if not text:\n        return float(0.0)\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words = len(re.findall(r'\\w+', text))\n    if words == 0:\n        return float(0.0)\n    return float(punct / words)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths) / len(lengths))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are modal verbs (can/could/may/might/must/shall/should/will/would)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for t in tokens if t in modals)\n    result = count / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that contain at least one digit'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with an uppercase letter'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m and s[m.start()].isupper():\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio (words that occur exactly once divided by total tokens)'\n    import re\n    from collections import Counter\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    cnt = Counter(tokens)\n    hapax = sum(1 for w, c in cnt.items() if c == 1)\n    return float(hapax / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length measured in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not word_counts:\n        return 0.0\n    result = sum(word_counts) / len(word_counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens longer than 12 characters (long-word density)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    long_count = sum(1 for w in words if len(w) > 12)\n    return float(long_count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Normalized variance of sentence lengths (variance divided by mean, 0.0 if insufficient data)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences if re.findall(r'\\w+', s)]\n    if len(lengths) < 2:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((L - mean) ** 2 for L in lengths) / len(lengths)\n    return float(var / mean)\n",
    "def feature(text: str) -> float:\n    'Ratio of explicit past auxiliary verbs (was/were) to combined past+present auxiliaries (was/were vs is/are); 0 if none'\n    import re\n    if not text:\n        return 0.0\n    lower = text.lower()\n    past = len(re.findall(r'\\bwas\\b', lower)) + len(re.findall(r'\\bwere\\b', lower))\n    present = len(re.findall(r'\\bis\\b', lower)) + len(re.findall(r'\\bare\\b', lower))\n    denom = past + present\n    if denom == 0:\n        return 0.0\n    return float(past) / denom\n\n",
    "def feature(text: str) -> float:\n    'Density of bracket/parenthesis characters: fraction of characters that are ()[]{}'\n    if not text:\n        return float(0.0)\n    br = sum(1 for c in text if c in '()[]{}')\n    return float(br / len(text))\n",
    "def feature(text: str) -> float:\n    'Average sentence length in tokens (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # sentences split at punctuation followed by whitespace\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip())\n    token_re = re.compile(r'\\w+')\n    lengths = []\n    for s in sentences:\n        tokens = token_re.findall(s)\n        if tokens:\n            lengths.append(len(tokens))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths)) / float(len(lengths))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are surrounded by or contain double-quote characters, approximating dialog density'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    dq = 0\n    for t in tokens:\n        if '\"' in t or t.startswith('\"') or t.endswith('\"'):\n            dq += 1\n    return float(dq) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters present in the text'\n    if not text:\n        return 0.0\n    puncts = {c for c in text if not c.isalnum() and not c.isspace()}\n    return float(len(puncts))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain both letters and digits (alphanumeric-token rate)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    mixed = 0\n    for t in tokens:\n        has_digit = any(c.isdigit() for c in t)\n        has_alpha = any(c.isalpha() for c in t)\n        if has_digit and has_alpha:\n            mixed += 1\n    return float(mixed / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, us, our) as a proxy for personal vs. impersonal tone'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / len(words)\n\n\n",
    "def feature(text: str) -> float:\n    'Contraction ratio: fraction of tokens that are contractions (contain apostrophes like \"don\\'t\", \"I\\'m\", \"it\\'s\")'\n    import re\n    if not text:\n        return 0.0\n    # count contraction patterns with apostrophe\n    contr = re.findall(r\"\\b\\w+'[a-z]{1,4}\\b\", text.lower())\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(len(contr))\n    return len(contr) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    if not text or not text.strip():\n        return 0.0\n    sentences_count = text.count('.') + text.count('!') + text.count('?')\n    if sentences_count == 0:\n        return 0.0\n    result = text.count('?') / sentences_count\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters in the text that are digits'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    digits = sum(1 for c in text if c.isdigit())\n    result = digits / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average normalized longest repeated-character run per token (max_run/len(token))'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    total = 0.0\n    for t in tokens:\n        if not t:\n            continue\n        prev = ''\n        run = 0\n        max_run = 0\n        for c in t:\n            if c == prev:\n                run += 1\n            else:\n                run = 1\n                prev = c\n            if run > max_run:\n                max_run = run\n        total += (max_run / len(t))\n    return float(total / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Lexical diversity: ratio of unique word tokens to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\w+', text.lower())\n    if not toks:\n        return 0.0\n    uniq = len(set(toks))\n    return float(uniq / len(toks))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord(c) > 127) \u2014 picks up typographic quotes, em-dashes, and some copy-paste artifacts'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / total\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (may, might, could, would, should, must, can)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'may', 'might', 'could', 'would', 'should', 'must', 'can', 'shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord>127)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
    "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") measured as ellipsis occurrences per sentence (0 if no sentences)'\n    try:\n        import re\n        if not text:\n            return 0.0\n        ellipses = len(re.findall(r'\\.\\.\\.+', text))\n        # count sentences as occurrences of .!? (fallback minimum 1)\n        sentences = max(1, len(re.findall(r'[.!?]', text)))\n        return float(ellipses) / sentences\n    except Exception:\n        return 0.0\n\n",
    "def feature(text: str) -> float:\n    'Byte-level compression ratio using zlib (compressed_size / original_size)'\n    import zlib\n    if not text:\n        return float(0.0)\n    try:\n        data = text.encode('utf-8')\n    except Exception:\n        data = str(text).encode('utf-8', errors='ignore')\n    orig = len(data)\n    if orig == 0:\n        return float(0.0)\n    comp = zlib.compress(data)\n    result = len(comp) / orig\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    uniq = len(set(words))\n    return float(uniq / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Normalized entropy of token length distribution (0..1 approx)'\n    import re, math\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    lengths = [len(t) for t in tokens]\n    counts = Counter(lengths)\n    total = sum(counts.values())\n    entropy = 0.0\n    for c in counts.values():\n        p = c / total\n        entropy -= p * math.log(p, 2)\n    maxlen = max(lengths)\n    denom = math.log(maxlen + 1, 2) if maxlen > 0 else 1.0\n    return float(entropy / denom)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (small heuristic list)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','is','in','it','to','of','a','for','on','that','this','with','as','are','was','be','by','an','or','at','from','but','not'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stop)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord(c) > 127) \u2014 proxies emojis/foreign scripts'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing a character repeated three or more times consecutively'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    repeat_re = re.compile(r'(.)\\1{2,}')\n    count = sum(1 for t in tokens if repeat_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (approximate stopword density)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','to','of','a','that','it','for','on','with','as','are','was','be','by','this','an','or','from','at','not','but','have','has','had','were','they','you','i','he','she','we','their','them','his','her','its','which','who','what','when','where','why','how'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace tokens containing an apostrophe (contraction/possessive density)'\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if \"'\" in t)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of non-whitespace tokens that are all-caps (>=2 letters)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    def is_all_caps(t):\n        letters = ''.join([c for c in t if c.isalpha()])\n        return len(letters) >= 2 and letters.upper() == letters\n    count = sum(1 for t in tokens if is_all_caps(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of short-to-medium acronyms (all-caps tokens of length 2\u20136) as a fraction of word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    acr = re.findall(r'\\b[A-Z]{2,6}\\b', text)\n    return float(len(acr)) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that occur exactly once (hapax legomena ratio)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    cnt = Counter(words)\n    hapax = sum(1 for w, c in cnt.items() if c == 1)\n    return float(hapax / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$')\n    count = sum(1 for t in tokens if email_re.match(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence (words defined by \\\\w+)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid zero-division if all sentences empty\n    nonzero = [c for c in word_counts if c > 0]\n    if not nonzero:\n        return 0.0\n    result = sum(word_counts) / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words that are common English stopwords (small list)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {\n        'the','a','an','and','or','but','if','in','on','at','by','for','with',\n        'to','of','is','it','this','that','these','those','he','she','they',\n        'we','you','i','was','were','be','been','has','have','had','not','as'\n    }\n    words = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain a character repeated three or more times in a row (elongation)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1')\n    count = sum(1 for t in tokens if pattern.search(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are hedging/epistemic modal words (may, might, could, seem, appear, suggest, possibly, likely)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    hedges = {'may', 'might', 'could', 'would', 'seem', 'seems', 'seemed', 'appear', 'appears', 'appeared', 'suggest', 'suggests', 'suggested', 'possibly', 'perhaps', 'likely', 'probable', 'probabilistic'}\n    count = sum(1 for t in tokens if t in hedges)\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that start with a digit (e.g., \"3rd\", \"2020\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t and t[0].isdigit())\n    return float(count / len(tokens))\n",
    "def feature(text: str) -> float:\n    'Normalized character entropy (0-1) of non-space characters'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    counts = Counter(chars)\n    total = len(chars)\n    entropy = -sum((v/total) * math.log(v/total, 2) for v in counts.values() if v > 0)\n    k = len(counts)\n    if k <= 1:\n        return 0.0\n    norm = entropy / math.log(k, 2)\n    return float(norm)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain hyphens (compound/technical terms or ranges)'\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphens = sum(1 for t in tokens if '-' in t and any(ch.isalnum() for ch in t))\n    return float(hyphens) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    types = set(tokens)\n    return float(len(types) / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Citation year density: fraction of tokens that are 4-digit years in the 1900-2099 range'\n    import re\n    if not text:\n        return 0.0\n    years = re.findall(r'\\b(?:19|20)\\d{2}\\b', text)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    return float(len(years)) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average number of semicolons per sentence (indicative of complex, academic clause-joining)'\n    if not text:\n        return 0.0\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    if sentences <= 0:\n        sentences = 1\n    return float(text.count(';')) / float(sentences)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of alphabetic characters that are uppercase (signal of emphasis or shouting)'\n    if not text:\n        return float(0.0)\n    letters = [c for c in text if c.isalpha()]\n    if not letters:\n        return float(0.0)\n    upper = sum(1 for c in letters if c.isupper())\n    return float(upper / len(letters))\n",
    "def feature(text: str) -> float:\n    'Fraction of word types that occur only once (hapax legomena density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    freq = Counter(tokens)\n    hapax = sum(1 for w, c in freq.items() if c == 1)\n    return float(hapax / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (std dev / mean of lengths)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    sd = math.sqrt(var)\n    result = sd / mean\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        if s.rstrip().endswith('?'):\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of long words (length > 7) as a marker of lexical sophistication'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 7)\n    return float(long_count) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Function-word ratio: fraction of tokens that are common English function words (small stopword set)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','of','to','a','in','is','it','that','for','on','as','are','with','was','by','be','this','an','or','from','at','which','but','have','has','not','they','their','its','we','can','may','such','will'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stop)\n    return count / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of distinct word types divided by total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    types = set(words)\n    result = len(types) / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words that are titlecase (initial uppercase, rest lowercase) or single-letter uppercase'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    def is_title(w):\n        return w[0].isupper() and (len(w) == 1 or w[1:].islower())\n    title_count = sum(1 for w in words if is_title(w))\n    return float(title_count / len(words))\n",
    "def feature(text: str) -> float:\n    'Density of transition/adverbial connectors (however, moreover, therefore, furthermore, additionally, in addition) per token'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    transitions = {'however','moreover','therefore','furthermore','additionally','consequently','nevertheless','nonetheless','in addition','thus','hence'}\n    count = 0\n    for t in tokens:\n        if t in transitions:\n            count += 1\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (useful for language/emoji signals)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are hyphenated (contain a hyphen)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    hy = sum(1 for t in tokens if '-' in t)\n    result = hy / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs (scheme or common TLD pattern)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re1 = re.compile(r'^(https?://|www\\.)', re.IGNORECASE)\n    url_re2 = re.compile(r'\\.[a-z]{2,3}([/:]|$)', re.IGNORECASE)\n    def is_url(t):\n        if url_re1.search(t):\n            return True\n        if '@' in t:\n            return False\n        return bool(url_re2.search(t))\n    urls = sum(1 for t in tokens if is_url(t))\n    return float(urls / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are contractions/contain internal apostrophes'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    ct = 0\n    for t in tokens:\n        if re.search(r\"[A-Za-z]+'[A-Za-z]+\", t):\n            ct += 1\n    result = ct / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized character entropy (Shannon entropy divided by log2(unique_chars)), 0 for short texts'\n    import math\n    if not text:\n        return 0.0\n    counts = {}\n    for c in text:\n        counts[c] = counts.get(c, 0) + 1\n    n = sum(counts.values())\n    if n == 0 or len(counts) <= 1:\n        return 0.0\n    entropy = 0.0\n    for v in counts.values():\n        p = v / n\n        entropy -= p * math.log2(p)\n    # normalize by log2(k) where k = number of unique symbols\n    k = len(counts)\n    norm = math.log2(k) if k > 1 else 1.0\n    return float(entropy / norm)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of words longer than 12 characters (long-word density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 12)\n    result = long_count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if s.strip().endswith('?'))\n    return float(qcount / len(sentences)) if sentences else 0.0\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentence_count), sentences min 1'\n    if not text:\n        return 0.0\n    sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n    comma_count = text.count(',')\n    return float(comma_count) / sentence_count\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-empty tokens that are hyphenated (contain a hyphen between alnum parts)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t for t in re.findall(r'\\S+', text) if t.strip()]\n    if not tokens:\n        return 0.0\n    hyph_re = re.compile(r'[A-Za-z0-9]+-(?:[A-Za-z0-9]+)(?:-[A-Za-z0-9]+)*')\n    hyph_count = sum(1 for t in tokens if hyph_re.search(t))\n    result = hyph_count / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of token lengths (std / mean) using word tokens'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    variance = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(variance)\n    result = std / mean\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are long (12 or more characters)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) >= 12)\n    result = long_count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of punctuation characters that appear in long repeated runs (>=3)'\n    import re\n    if not text:\n        return 0.0\n    # Count total punctuation (non-alnum, non-space)\n    total_punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if total_punct == 0:\n        return 0.0\n    repeated = 0\n    for m in re.finditer(r'([^\\w\\s])\\1{2,}', text):\n        repeated += len(m.group(0))\n    result = repeated / total_punct\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of hyphen and dash characters (-, \u2014, \u2013) per word, capturing compound/descriptive style'\n    import re\n    if not text:\n        return 0.0\n    hyphen_count = text.count('-') + text.count('\\u2014') + text.count('\\u2013')\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(hyphen_count) / float(words)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_link(tok):\n        t = tok.lower()\n        if t.startswith('http://') or t.startswith('https://') or t.startswith('www.'):\n            return True\n        if '@' in tok and '.' in tok:\n            return True\n        return False\n    count = sum(1 for t in tokens if is_link(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of explicit quotation marks (double quotes + single quotes used as quotation marks, excluding apostrophes inside words), normalized by token count.'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    token_count = max(1, len(words))\n    double_q = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    # Count single quotes that are likely quotation marks (not embedded apostrophes inside words)\n    single_q_matches = re.findall(r\"(?<![A-Za-z0-9])'|'(?![A-Za-z0-9])\", text)\n    single_q = len(single_q_matches)\n    total_q = double_q + single_q\n    return float(total_q) / token_count\n\n",
    "def feature(text: str) -> float:\n    'Fraction of quotation marks that are curly/typographic quotes (e.g., \u2018 \u2019 \u201c \u201d) vs plain ASCII quotes, indicating published/edited text'\n    import re\n    if not text:\n        return 0.0\n    # count curly quote characters and total quote-like characters\n    curly = sum(text.count(ch) for ch in ('\u2018','\u2019','\u201a','\u201c','\u201d','\u201e'))\n    plain = sum(text.count(ch) for ch in (\"'\", '\"'))\n    total = curly + plain\n    if total == 0:\n        return 0.0\n    return float(curly) / total\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain quotation marks (dialogue indicator)'\n    try:\n        import re\n        # split into sentence-like segments\n        sentences = re.split(r'(?<=[.!?])\\s+', text)\n        if not sentences or (len(sentences) == 1 and sentences[0].strip() == ''):\n            return 0.0\n        quote_chars = re.compile(r'[\"\\u201c\\u201d]')\n        qcount = sum(1 for s in sentences if quote_chars.search(s))\n        return float(qcount) / float(len(sentences))\n    except Exception:\n        return 0.0\n\n",
    "def feature(text: str) -> float:\n    'Variance of sentence lengths (words per sentence) \u2014 narratives often have higher variance than formal prose'\n    import re, math\n    if not text:\n        return 0.0\n    # split on sentence terminators but keep non-empty\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sentences) <= 1:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences if re.findall(r'\\w+', s)]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / float(len(lengths))\n    var = sum((l - mean) ** 2 for l in lengths) / float(len(lengths))\n    return float(var)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of raw tokens composed only of punctuation characters'\n    import re, string\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    punct = set(string.punctuation)\n    count = 0\n    for t in tokens:\n        if t and all(c in punct for c in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing a character repeated three or more times in a row (elongation)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'(.)\\1\\1', t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are all-caps (shouting), requiring at least 2 letters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t) and t.isupper() and sum(1 for c in t if c.isalpha()) >= 2:\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Balance between past-tense markers and present-tense markers: (past-present)/(past+present) in [-1,1]'\n    if not text:\n        return 0.0\n    try:\n        words = re.findall(r'\\w+', text.lower())\n    except Exception:\n        words = []\n    if not words:\n        return 0.0\n    past_markers = {'was', 'were', 'had', 'did'}\n    present_markers = {'is', 'am', 'are', 'have', 'has', 'do', 'does'}\n    past_ed = sum(1 for w in words if w.endswith('ed') and len(w) > 2)\n    past_count = past_ed + sum(1 for w in words if w in past_markers)\n    present_ing = sum(1 for w in words if w.endswith('ing') and len(w) > 3)\n    present_count = present_ing + sum(1 for w in words if w in present_markers)\n    denom = past_count + present_count\n    if denom == 0:\n        return 0.0\n    return float(past_count - present_count) / float(denom)\n",
    "def feature(text: str) -> float:\n    'Ratio of unique word tokens to total word tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Exclamation sentence ratio: fraction of sentences that end with an exclamation mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if s.strip().endswith('!'))\n    result = count / len(sentences) if sentences else 0.0\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens that appear to be past-tense verbs (heuristic: alphabetic tokens ending with \"ed\" and length>3)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    past_ed = sum(1 for t in tokens if t.endswith('ed') and len(t) > 3)\n    return past_ed / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a lowercase letter (informal starts)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m and m.group(0).islower():\n            count += 1\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain any digit (captures explicit durations, numbers, or measurements)'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of relative/connecting pronouns (which, that, who, whom, whose) per token - hints at explanatory/relative-clause style'\n    import re\n    tokens = re.findall(r\"[A-Za-z]+\", text)\n    if not tokens:\n        return 0.0\n    rel = {'which', 'that', 'who', 'whom', 'whose'}\n    count = sum(1 for t in tokens if t.lower() in rel)\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that contain double or typographic quotation marks'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    quote_chars = set('\"\u201c\u201d')\n    count = sum(1 for s in sentences if any(c in s for c in quote_chars))\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are emoticons or emoji-like'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    emoticon_re = re.compile(r'^(?:[:;=8][\\-^]?[)DdpP\\]\\(\\/\\\\]|[)DdpP\\]\\(\\/\\\\][\\-^]?[:;=8])$')\n    count = 0\n    for tok in tokens:\n        t = tok.strip('.,;:!?)(\"\\'')\n        if not t:\n            continue\n        if emoticon_re.match(t):\n            count += 1\n            continue\n        # treat characters in common emoji/unicode ranges as emoji\n        if any(ord(c) >= 0x1F300 for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are written in all-caps (at least two letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        if len(letters) < 2:\n            return False\n        return all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of modal verbs (can,could,may,might,must,should,would,shall,will) among words'\n    import re\n    if not text:\n        return float(0.0)\n    MODALS = {'can','could','may','might','must','should','would','shall','will'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if w in MODALS)\n    result = count / len(words)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Shannon entropy of the character distribution (bits per character)'\n    import math, collections\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    freq = collections.Counter(text)\n    entropy = 0.0\n    for cnt in freq.values():\n        p = cnt / total\n        entropy -= p * math.log2(p)\n    return float(entropy)\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that mix letters and digits (alphanumeric mixed tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    mixed = 0\n    for t in tokens:\n        has_digit = any(c.isdigit() for c in t)\n        has_alpha = any(c.isalpha() for c in t)\n        if has_digit and has_alpha:\n            mixed += 1\n    return float(mixed / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Punctuation characters per word (punctuation-to-word ratio)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    word_count = len(words)\n    if word_count == 0:\n        return 0.0\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct_count / word_count)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit (numbers, IDs, dates)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are common English stopwords (simple list)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {\n        'the','and','a','to','of','in','that','is','it','for','on','with','as','are','was','be','by','this','have','or','an','at','from','not','but','they','you','I','we'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation symbols used divided by total punctuation occurrences (0-1)'\n    import string\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / float(total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens in ALL CAPS (at least two letters) indicating emphasis or acronyms'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    caps = 0\n    for t in tokens:\n        alpha_count = sum(1 for c in t if c.isalpha())\n        if alpha_count >= 2 and all((not c.isalpha()) or c.isupper() for c in t):\n            caps += 1\n    return float(caps / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are inside HTML/XML-like tags'\n    import re\n    if not text:\n        return 0.0\n    tags = re.findall(r'<[^>]+>', text)\n    if not tags:\n        return 0.0\n    total_len = len(text)\n    if total_len == 0:\n        return 0.0\n    inside_len = sum(len(m) for m in tags)\n    return float(min(1.0, inside_len / total_len))\n\n\n",
    "def feature(text: str) -> float:\n    'Token repetition rate: 1 - (unique_tokens / total_tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    total = len(tokens)\n    return float(1.0 - (unique / total))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens that contain at least one digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of sentence lengths (stddev/mean of words per sentence)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    total = len(words)\n    if total == 0:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / float(total)\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating conjunction (and, but, or, so, because, yet)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    start_re = re.compile(r'^\\s*(and|but|or|so|because|yet)\\b', re.I)\n    count = sum(1 for s in sentences if start_re.search(s))\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit (numbers, codes, dates)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-empty tokens that are all-uppercase words (length>1, contains letters)'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\S+', text)\n    if not toks:\n        return 0.0\n    def is_all_caps(t):\n        has_alpha = any(c.isalpha() for c in t)\n        return has_alpha and len([c for c in t if c.isalpha()]) > 1 and t.isupper()\n    count = sum(1 for t in toks if is_all_caps(t))\n    return float(count / len(toks))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are emoticons or contain emoji codepoints'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticon_re = re.compile(r'^(?:[:;=8][\\-^]?[)DdpP(/\\\\]|<3|:-?[\\]|\\\\]|:\\'-?\\(|:\\||;-\\))$')\n    def has_emoji(token):\n        for c in token:\n            o = ord(c)\n            if 0x1F300 <= o <= 0x1FAFF or 0x1F600 <= o <= 0x1F64F or 0x2600 <= o <= 0x26FF:\n                return True\n        return False\n    count = 0\n    for t in tokens:\n        if emoticon_re.search(t) or has_emoji(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average length of alphabetic tokens (letters only)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    alpha_tokens = re.findall(r'[A-Za-z]+', text)\n    if not alpha_tokens:\n        return 0.0\n    avg = sum(len(t) for t in alpha_tokens) / len(alpha_tokens)\n    return float(avg)\n\n",
    "def feature(text: str) -> float:\n    'Parenthesis density: number of parentheses characters \"(\" or \")\" per word (indicator of citations/parentheticals)'\n    import re\n    if not text:\n        return 0.0\n    paren_count = text.count('(') + text.count(')')\n    words = max(1, len(re.findall(r'\\w+', text)))\n    return float(paren_count) / float(words)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are in ALL CAPS (signal of emphasis/shouting)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # require at least two alphabetic chars to avoid counting single-letter tokens like 'I'\n        if any(c.isalpha() for c in t) and t.isupper() and sum(1 for c in t if c.isalpha()) >= 2:\n            count += 1\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of alphabetic characters that are vowels (a,e,i,o,u)'\n    if not text:\n        return 0.0\n    letters = [c.lower() for c in text if c.isalpha()]\n    if not letters:\n        return 0.0\n    vowels = sum(1 for c in letters if c in 'aeiou')\n    result = vowels / len(letters)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that end with \"ing\" (present participles / progressive aspect)'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    ing_count = 0\n    for t in tokens:\n        tl = t.lower()\n        if len(tl) > 3 and tl.endswith('ing'):\n            ing_count += 1\n    return float(ing_count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are common prepositions (of, in, on, with, by, for, to, from, about, as, at)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    preps = {'of', 'in', 'on', 'with', 'by', 'for', 'to', 'from', 'about', 'as', 'at', 'into', 'through', 'between', 'among', 'over', 'against', 'during', 'without', 'within', 'toward', 'towards'}\n    count = sum(1 for t in tokens if t in preps)\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stop = {'a','an','the','and','or','but','if','while','for','to','of','in','on','at','by','with','from','that','this','these','those','is','are','was','were','be','been','being','it','its','as','he','she','they','we','you','I','me','him','her','them','us'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stop)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words that occur only once (hapax legomena ratio)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    freq = {}\n    for w in words:\n        freq[w] = freq.get(w, 0) + 1\n    hapax = sum(1 for v in freq.values() if v == 1)\n    return float(hapax / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are common English stopwords (simple stopword density)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there','their'}\n    toks = re.findall(r'\\w+', text.lower())\n    if not toks:\n        return 0.0\n    count = sum(1 for t in toks if t in stopwords)\n    return float(count / len(toks))\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (approx.), using split on .!? to capture narrative short sentences vs long formal sentences'\n    if not text:\n        return 0.0\n    import re\n    # split into sentences by punctuation, ignore empty fragments\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(s.split()) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts)) / float(len(word_counts))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-empty lines that begin with a quotation mark or a dash (dialogue/line-oriented style)'\n    if not text:\n        return 0.0\n    try:\n        lines = [ln for ln in text.splitlines() if ln.strip()]\n        if not lines:\n            return 0.0\n        starts = 0\n        for ln in lines:\n            s = ln.lstrip()\n            if s.startswith('\"') or s.startswith(\"'\") or s.startswith('-') or s.startswith('\u2014') or s.startswith('\u2013'):\n                starts += 1\n        return float(starts) / len(lines)\n    except Exception:\n        return 0.0\n\n",
    "def feature(text: str) -> float:\n    'Paragraph length coefficient of variation: std/mean of paragraph word counts (0 if fewer than 2 paragraphs or no words)'\n    import re, math\n    if not text:\n        return 0.0\n    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n    if len(paragraphs) < 2:\n        return 0.0\n    counts = []\n    for p in paragraphs:\n        words = re.findall(r\"\\b[\\w'\u2019]+\\b\", p)\n        counts.append(len(words))\n    # filter zero-length paragraphs\n    counts = [c for c in counts if c > 0]\n    if len(counts) < 2:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    variance = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(variance)\n    return std / mean\n\n",
    "def feature(text: str) -> float:\n    'Approximate Flesch Reading Ease score (higher = easier); 0 if not computable'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not words or not sentences:\n        return 0.0\n    # syllable heuristic similar to other feature\n    total_syll = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        syl = max(1, len(groups))\n        if w.endswith('e') and len(groups) > 1:\n            syl = max(1, syl - 1)\n        total_syll += syl\n    words_count = len(words)\n    sent_count = len(sentences)\n    # Flesch Reading Ease\n    score = 206.835 - 1.015 * (words_count / sent_count) - 84.6 * (total_syll / words_count)\n    return float(score)\n\n",
    "def feature(text: str) -> float:\n    'Density of Unicode symbol-like characters (currency/math/other symbols)'\n    import unicodedata\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    sym = 0\n    for c in text:\n        try:\n            cat = unicodedata.category(c)\n        except Exception:\n            cat = ''\n        if cat.startswith('So') or cat.startswith('Sc') or cat.startswith('Sm'):\n            sym += 1\n    result = sym / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of quotation characters (double and curly quotes) as fraction of characters'\n    if not text:\n        return 0.0\n    quote_chars = ['\"', '\u201c', '\u201d', '\u00ab', '\u00bb']\n    count = sum(text.count(q) for q in quote_chars)\n    return float(count) / max(1.0, len(text))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of titlecase tokens that appear NOT at the start of sentences (proxy for proper-name density)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence fragments then tokens to avoid counting sentence-initial capitals\n    sentences = re.split(r'[.!?]+', text)\n    total_tokens = 0\n    title_non_initial = 0\n    for s in sentences:\n        s = s.strip()\n        if not s:\n            continue\n        parts = s.split()\n        if len(parts) <= 1:\n            continue\n        # skip the first token as sentence-initial\n        for tok in parts[1:]:\n            total_tokens += 1\n            # consider tokens with internal alphabetic characters and titlecase form\n            if tok.istitle() and any(c.isalpha() for c in tok):\n                title_non_initial += 1\n    if total_tokens == 0:\n        return 0.0\n    return float(title_non_initial) / float(total_tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that consist of a single word'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    single = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) == 1:\n            single += 1\n    return float(single / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain a run of the same punctuation character of length >=3 (e.g., \"!!!\", \"...\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    rep_re = re.compile(r'([!?\\.,\"\\'-])\\1{2,}')\n    count = sum(1 for t in tokens if rep_re.search(t))\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Normalized character entropy (0-1) measuring char distribution uniformity'\n    import math\n    if not text:\n        return float(0.0)\n    freq = {}\n    for c in text:\n        freq[c] = freq.get(c, 0) + 1\n    n = sum(freq.values())\n    if n == 0 or len(freq) <= 1:\n        return float(0.0)\n    entropy = 0.0\n    for v in freq.values():\n        p = v / n\n        entropy -= p * math.log2(p)\n    # normalize by max entropy log2(|alphabet|)\n    max_e = math.log2(len(freq))\n    if max_e <= 0:\n        return float(0.0)\n    return float(entropy / max_e)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a common preposition (in, on, at, during, within, under, above, across, into, among, between, along, beneath, inside, outside, over)'\n    import re\n    preps = {'in','on','at','during','within','under','above','across','into','among','between','along','beneath','inside','outside','over'}\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s and s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.match(r'\\s*([A-Za-z]+)', s)\n        if m:\n            if m.group(1).lower() in preps:\n                count += 1\n    return float(count) / float(len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain any numeric digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    result = num / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that contain at least one exclamation mark'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sents:\n        return 0.0\n    ex = sum(1 for s in sents if '!' in s)\n    return float(ex / len(sents))\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (simple fixed list)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    STOPWORDS = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not',\n        'on','with','he','as','you','do','at','this','but','his','by','from',\n        'they','we','say','her','she','or','an','will','my','one','all','would',\n        'there','their','what','so','up','out','if','about','who','get','which',\n        'go','me'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in STOPWORDS)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Approximate passive-voice indicator: occurrences of forms like \"was|were|is|are|been|being\" followed by an -ed word per word'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r'\\b(?:was|were|is|are|been|being|be)\\s+[A-Za-z0-9\\-]+ed\\b', text, flags=re.IGNORECASE)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Approximate average syllables per word using vowel-group heuristic'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[ A-Za-z0-9\\'-]*[A-Za-z][A-Za-z0-9\\'-]*\\b', text)\n    # fallback to simpler word extraction if above fails\n    if not words:\n        words = re.findall(r'\\w+', text)\n    words = [w for w in words if any(ch.isalpha() for ch in w)]\n    if not words:\n        return 0.0\n    def syllable_count(w: str) -> int:\n        s = w.lower()\n        groups = re.findall(r'[aeiouy]+', s)\n        count = len(groups)\n        # common heuristic: silent trailing 'e' reduces one syllable if more than one group\n        if s.endswith('e') and count > 1:\n            count -= 1\n        return max(1, count)\n    total = sum(syllable_count(w) for w in words)\n    result = total / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    s = text.strip()\n    sentences = [p for p in re.split(r'(?<=[.!?])\\s+', s) if p.strip()]\n    if not sentences:\n        sentences = [s]\n    qcount = sum(1 for sent in sentences if sent.rstrip().endswith('?'))\n    return float(qcount / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Average syllable count per alphabetic word (simple vowel-group heuristic)'\n    import re\n    if not text:\n        return 0.0\n    words = [w for w in re.findall(r\"[A-Za-z']+\", text) if any(c.isalpha() for c in w)]\n    if not words:\n        return 0.0\n    def syllables(w):\n        w = w.lower()\n        groups = re.findall(r'[aeiouy]+', w)\n        # heuristics: at least one syllable\n        return max(1, len(groups))\n    avg = sum(syllables(w) for w in words) / len(words)\n    return float(avg)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are titlecase (First letter uppercase, rest lowercase) including single-letter \"I\"'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    def is_title(w):\n        if not w:\n            return False\n        if len(w) == 1:\n            return w.isupper()\n        return w[0].isupper() and w[1:].islower()\n    count = sum(1 for w in words if is_title(w))\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of non-empty tokens whose alphabetic letters are all uppercase (shouting tokens)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_shout(t):\n        letters = [c for c in t if c.isalpha()]\n        if len(letters) < 2:\n            return False\n        return all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_shout(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are modal verbs (can, could, may, might, should, would, must) indicating hedging or instruction'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'should', 'would', 'must', 'shall'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / len(tokens)\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing any digit (numbers, dates, codes)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average commas/semicolons per sentence (proxy for sentence complexity)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(s.count(',') + s.count(';') for s in sentences)\n    result = count / len(sentences) if sentences else 0.0\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence (mean sentence length in words)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    word_counts = [c for c in word_counts if c > 0]\n    if not word_counts:\n        return 0.0\n    result = sum(word_counts) / len(word_counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of ASCII emoticons and simple heart markers per token (e.g. :), :(, :D, <3)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticon_patterns = [\n        r'[:;=8][\\-~]?[)\\]\\(DdpP/\\\\]',  # smiles/frowns\n        r'<3',                          # heart\n        r'\\^\\_^',                       # cute face\n        r'[xX][-~]?[D)]'                # laughing variants\n    ]\n    combined = re.compile('|'.join('(?:%s)' % p for p in emoticon_patterns))\n    # count occurrences across text (not just token match) but normalize by token count\n    matches = combined.findall(text)\n    count = len(matches)\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of quoted/dialogue segments per sentence (0 if no sentences)'\n    import re\n    if not text:\n        return 0.0\n    # find quoted segments using straight and curly double quotes\n    quoted = re.findall(r'[\u201c\"\u201d](.+?)[\u201c\"\u201d]', text, flags=re.DOTALL)\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    return float(len(quoted)) / float(len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that contain a quoted segment (double quotes or typographic quotes)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    quote_chars = set(['\"', '\u201c', '\u201d', '\u00ab', '\u00bb'])\n    def has_quote(s):\n        return any(c in quote_chars for c in s)\n    count = sum(1 for s in sentences if has_quote(s))\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Density of ellipses (ASCII \"...\" or unicode \"\u2026\") as a fraction of characters, capturing excerpting or trailing ellipses'\n    if not text:\n        return 0.0\n    ellipses = text.count('...') + text.count('\u2026')\n    return float(ellipses) / max(1, len(text))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent word pairs that are identical (immediate repeated words)'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i - 1])\n    return float(repeats / (len(words) - 1))\n",
    "def feature(text: str) -> float:\n    'Proportion of characters that are non-ASCII (likely non-Latin or special chars)'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
    "def feature(text: str) -> float:\n    'Density of common dialogue-reporting verbs (said, asked, replied, shouted, whispered, etc.) per word'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    reporting_verbs = r'\\b(said|asked|replied|muttered|whispered|shouted|exclaimed|answered|sighed|laughed|grinned|smiled)\\b'\n    matches = re.findall(reporting_verbs, text, flags=re.IGNORECASE)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that consist only of punctuation characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if all(not c.isalnum() for c in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Diversity of punctuation characters used (distinct punctuation types normalized)'\n    import string\n    if not text:\n        return 0.0\n    punc = set(c for c in text if c in string.punctuation)\n    if not punc:\n        return 0.0\n    result = len(punc) / len(string.punctuation)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized character Shannon entropy (over non-space characters, 0..1)'\n    import math\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return float(0.0)\n    cnt = Counter(chars)\n    total = sum(cnt.values())\n    ent = 0.0\n    for f in cnt.values():\n        p = f / total\n        ent -= p * math.log2(p)\n    # normalize by log2(number of distinct chars) to bring to 0..1\n    distinct = len(cnt)\n    if distinct <= 1:\n        result = 0.0\n    else:\n        result = ent / math.log2(distinct)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average commas+semicolons per sentence'\n    import re\n    if not text:\n        return 0.0\n    count = text.count(',') + text.count(';')\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    return float(count / max(1, len(sentences)))\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas / sentence count)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    total_commas = sum(s.count(',') for s in sentences)\n    return float(total_commas / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Internal-punctuation token ratio: fraction of tokens that contain punctuation and also alphanumeric chars (e.g., e-mail, contractions)'\n    import re, string\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def has_internal_punct(tok):\n        has_p = any(c in string.punctuation for c in tok)\n        has_alnum = any(c.isalnum() for c in tok)\n        all_punct = all((not c.isalnum()) for c in tok)\n        return has_p and has_alnum and (not all_punct)\n    count = sum(1 for t in tokens if has_internal_punct(t))\n    result = count / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Fraction of words that are capitalized mid-sentence (approximate proper noun density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = []\n    mid_cap = 0\n    total = 0\n    for s in sentences:\n        toks = re.findall(r'\\w+', s)\n        for i, t in enumerate(toks):\n            total += 1\n            if i > 0 and len(t) > 0 and t[0].isupper():\n                # treat as mid-sentence capitalized\n                mid_cap += 1\n    if total == 0:\n        return 0.0\n    return float(mid_cap / total)\n",
    "def feature(text: str) -> float:\n    'Average vowel-group count per word (proxy for syllable-like units)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return float(0.0)\n    groups = [len(re.findall(r'[aeiouy]+', w.lower())) or 1 for w in words]\n    return float(sum(groups) / len(groups))\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not sentences:\n        return float(len(words)) if words else 0.0\n    total_words = 0\n    for s in sentences:\n        total_words += len(re.findall(r'\\w+', s))\n    return float(total_words / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (proxy for clause density and complex sentence structure)'\n    import re\n    if not text:\n        return 0.0\n    # Rough sentence split on .!? and newlines\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        # fallback: treat whole text as one sentence\n        sentences = [text.strip()]\n    comma_count = text.count(',')\n    return float(comma_count) / float(len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are in all-uppercase (length>=2, contains letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    up = 0\n    for t in tokens:\n        # consider tokens that have at least one letter and are all upper\n        if any(c.isalpha() for c in t) and len(t) > 1 and t.isupper():\n            up += 1\n    result = up / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Citation-like token density: occurrences of bracket citations [1], parenthetical author-year citations (Smith, 2020), or \"et al.\" per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    bracket_cites = len(re.findall(r'\\[\\s*\\d+\\s*\\]', text))\n    author_year = len(re.findall(r'\\([A-Z][A-Za-z\\-\\']{1,20},\\s*\\d{4}\\)', text))\n    et_al = len(re.findall(r'\\bet\\s+al\\.?', text, re.IGNORECASE))\n    total = bracket_cites + author_year + et_al\n    return float(total) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are long (7+ characters) \u2014 higher values suggest more descriptive or formal vocabulary'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    long_count = sum(1 for t in tokens if len(t) >= 7)\n    return float(long_count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (possible emojis/foreign chars)'\n    if not text:\n        return 0.0\n    n = len(text)\n    if n == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / n)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of capitalized tokens appearing mid-sentence (proxy for proper nouns / named entities)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences, then tokens per sentence\n    sentences = [s.strip() for s in re.split(r'([.!?])', text)]\n    # build real sentence chunks (join fragments)\n    chunks = []\n    cur = ''\n    for part in sentences:\n        if not part:\n            continue\n        cur += part\n        if re.search(r'[.!?]\\s*$', part):\n            chunks.append(cur.strip())\n            cur = ''\n        elif len(part) > 0 and part[-1] in '.!?':\n            chunks.append(cur.strip())\n            cur = ''\n    if cur:\n        chunks.append(cur.strip())\n    if not chunks:\n        chunks = [text]\n    mid_caps = 0\n    total_tokens = 0\n    for s in chunks:\n        tokens = re.findall(r'\\b\\w+\\b', s)\n        for i, t in enumerate(tokens):\n            total_tokens += 1\n            if i == 0:\n                continue\n            # capitalized but not all-caps (to avoid acronyms)\n            if t[0].isupper() and not t.isupper():\n                mid_caps += 1\n    if total_tokens == 0:\n        return 0.0\n    return mid_caps / total_tokens\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.strip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are common English stopwords (approximate)'\n    import re\n    if not text:\n        return float(0.0)\n    stopwords = {\n        'the','and','is','in','it','you','that','he','was','for','on','are','with',\n        'as','I','his','they','be','at','one','have','this','from','or','had','by',\n        'not','word','but','what','some','we','can','out','other','were','all','there',\n        'when','up','use','your','how','said','an','each','she'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if t in stopwords)\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence), robust to missing sentence punctuation'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_words = len(words)\n    # Count non-empty sentence-like segments\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sentences = len(sentences) if sentences else 1\n    return total_words / float(num_sentences)\n\n",
    "def feature(text: str) -> float:\n    'Average vowel-group count per word (simple syllable proxy using vowel runs)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    counts = []\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        cnt = len(groups)\n        if cnt < 1:\n            cnt = 1\n        counts.append(cnt)\n    result = sum(counts) / len(counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens ending with \"ing\" (proxy for progressive/descriptive phrasing)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return float(count) / len(tokens)\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit (numeric token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\S+\\b', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    result = num_count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are numeric (integers or decimals)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d+(\\.\\d+)?$')\n    count = sum(1 for t in tokens if num_re.match(t.strip('(),%')))  # strip common wrappers\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens containing an apostrophe (contractions/possessives)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if \"'\" in t and any(c.isalpha() for c in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email-like tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'https?://|www\\.|@|\\.\\w{2,4}', flags=re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Normalized character-level Shannon entropy (0-1), measuring character diversity'\n    from collections import Counter\n    import math\n    if not text:\n        return 0.0\n    counts = Counter(text)\n    total = sum(counts.values())\n    if total == 0:\n        return 0.0\n    entropy = 0.0\n    for ccount in counts.values():\n        p = ccount / total\n        entropy -= p * math.log2(p)\n    unique = len(counts)\n    if unique <= 1:\n        return 0.0\n    max_entropy = math.log2(unique)\n    result = entropy / max_entropy\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with ?)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q_count / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens containing an apostrophe'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if \"'\" in t)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are ALL CAPS and length>=2 (shouting tokens)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    caps = sum(1 for w in words if len(w) >= 2 and w.isupper())\n    return float(caps / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average fraction of uppercase letters among letter characters within tokens'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    letter_tokens = []\n    for t in tokens:\n        letters = [c for c in t if c.isalpha()]\n        if letters:\n            upp = sum(1 for c in letters if c.isupper())\n            letter_tokens.append(upp / len(letters))\n    if not letter_tokens:\n        return float(0.0)\n    return float(sum(letter_tokens) / len(letter_tokens))\n",
    "def feature(text: str) -> float:\n    'Approximate lexical density: fraction of tokens likely to be content words'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','a','an','of','to','in','is','it','that','this','for','on','with','as','are','was','were','be','by','or','from','at','not','but','you','i'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    content = sum(1 for t in tokens if len(t) > 3 and t not in stopwords)\n    return float(content / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation-to-word ratio: total punctuation characters divided by word count'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    num_words = len(words)\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if num_words == 0:\n        return float(punct_count)\n    return punct_count / float(num_words)\n",
    "def feature(text: str) -> float:\n    'Loose passive-voice indicator: count of auxiliary verbs followed by an -ed participle (is/was/were + Xed) normalized by words'\n    import re\n    if not text:\n        return 0.0\n    text_l = text.lower()\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|has|have|had)\\b\\s+\\w+ed\\b', text_l)\n    words = re.findall(r'\\w+', text_l)\n    if not words:\n        return 0.0\n    return float(len(matches)) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: ratio of unique word tokens to total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    return float(len(set(tokens)) / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\w+['\u2019]?\\w*|\\w+\", text.lower())\n    if not words:\n        return 0.0\n    first_person = {'i', \"i'm\", 'im', \"i've\", \"i'd\", 'me', 'my', 'mine', 'we', \"we're\", 'us', 'our', 'ours'}\n    count = sum(1 for w in words if w in first_person)\n    return float(count) / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a subordinating conjunction (as, when, while, although, because, since, if, though, after, before)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences using punctuation\n    raw_sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in raw_sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    starters = {'as', 'when', 'while', 'although', 'because', 'since', 'if', 'though', 'after', 'before', 'once', 'unless'}\n    count = 0\n    for s in sentences:\n        m = re.match(r\"\\s*([A-Za-z']+)\", s)\n        if m and m.group(1).lower() in starters:\n            count += 1\n    return float(count) / len(sentences)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are third-person pronouns (he/she/they and variants) as a narrative-person indicator'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    third = {'he','she','they','him','her','them','his','their','hers','theirs'}\n    count = sum(1 for t in tokens if t in third)\n    return float(count) / total\n\n",
    "def feature(text: str) -> float:\n    'Proportion of raw tokens that appear to be URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'://|^www\\.|https?://', re.IGNORECASE)\n    email_re = re.compile(r'\\b[\\w.+-]+@[\\w-]+\\.[\\w.-]+\\b')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of unique adjacent word bigrams to total bigrams (bigram diversity)'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if len(words) < 2:\n        return 0.0\n    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n    unique = len(set(bigrams))\n    return float(unique / len(bigrams))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with an uppercase letter (ignoring leading quotes/parentheses)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        return float(0.0)\n    def starts_upper(s):\n        for ch in s:\n            if ch.isalpha():\n                return ch.isupper()\n            if ch.isdigit():\n                return False\n        return False\n    count = sum(1 for s in sents if starts_upper(s))\n    return float(count / len(sents))\n\n",
    "def feature(text: str) -> float:\n    'Average number of punctuation characters per token'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    punct_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct_chars / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are quotation marks or quote-like symbols'\n    if not text:\n        return float(0.0)\n    quote_chars = set('\"\\'`\u201c\u201d\u2018\u2019\u00ab\u00bb')\n    total_len = len(text)\n    if total_len == 0:\n        return float(0.0)\n    qcount = sum(1 for c in text if c in quote_chars)\n    return float(qcount / total_len)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens that look like internal-abbreviations with embedded dots (e.g., \"S.S.\", \"U.S.\")'\n    if not text:\n        return 0.0\n    import re\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if re.search(r'[A-Za-z]\\.[A-Za-z]', t):\n            # avoid counting pure ellipses or trailing punctuation-only tokens\n            count += 1\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent word pairs that are exact repeats (e.g., \"the the\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = [w.lower() for w in re.findall(r'\\b\\w+\\b', text)]\n    if len(tokens) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(tokens)) if tokens[i] == tokens[i-1])\n    return float(repeats / max(1, len(tokens) - 1))\n\n",
    "def feature(text: str) -> float:\n    'URL/email token density: fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def looks_like_link(tok):\n        if 'http' in tok.lower() or 'www.' in tok.lower() or '@' in tok:\n            return True\n        if re.search(r'\\.(com|net|org|io|gov|edu|co)(?:\\W|$)', tok.lower()):\n            return True\n        return False\n    count = sum(1 for t in tokens if looks_like_link(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Uppercase-token ratio: fraction of tokens that are all uppercase and length>=2 (shouting tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) >= 2 and t.isupper())\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    return float(len(set(words)) / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating conjunction (and, but, or, so, for, nor, yet)'\n    import re\n    if not text:\n        return 0.0\n    conj = {'and', 'but', 'or', 'so', 'for', 'nor', 'yet'}\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        m = re.search(r'\\b\\w+\\b', s)\n        if m and m.group(0).lower() in conj:\n            count += 1\n    return float(count / len(sentences))\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or common domain references'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    url_pattern = re.compile(r'https?://|www\\.|\\.com\\b|\\.net\\b|\\.org\\b|\\.io\\b|\\.gov\\b|\\.edu\\b', re.IGNORECASE)\n    urls = sum(1 for t in tokens if url_pattern.search(t))\n    return float(urls / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that start with an uppercase letter (capitalized-word ratio)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    cap_count = sum(1 for t in tokens if t[0].isupper())\n    result = cap_count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num_tokens / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of common English stopwords to total words (approximate)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','it','of','to','a','that','i','you','for','on','with','as','are','was','be','this','by','an','or','not'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of hedging/epistemic verbs (seem, appear, suggest, indicate variants) to total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    hedges = {'seem','seems','seemed','appear','appears','appeared','suggest','suggests','suggested','indicate','indicates','indicated','imply','implies','implied'}\n    count = sum(1 for w in words if w in hedges)\n    return float(count) / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating conjunction (and,but,or,so,yet,for,nor)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    conjunctions = {'and', 'but', 'or', 'so', 'yet', 'for', 'nor'}\n    count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s.lower())\n        if words and words[0] in conjunctions:\n            count += 1\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like dates (dd/mm, mm-dd-yyyy, yyyy-mm-dd, month names)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    date_patterns = [\n        re.compile(r'^\\d{1,2}[/-]\\d{1,2}(?:[/-]\\d{2,4})?$'),  # 1/2, 01-02-2000\n        re.compile(r'^\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}$'),        # 2000-01-02\n        re.compile(r'^(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[a-z]*\\.?$', re.I)\n    ]\n    count = 0\n    for t in tokens:\n        t2 = t.strip('.,;()[]{}\"\\'')\n        for p in date_patterns:\n            if p.match(t2):\n                count += 1\n                break\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words that occur only once in the text (hapax legomena ratio)'\n    import re\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    counts = Counter(words)\n    hapax = sum(1 for w, c in counts.items() if c == 1)\n    return float(hapax / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average number of tokens per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    token_counts = [len(re.findall(r'\\S+', s)) for s in sentences]\n    if not token_counts:\n        return 0.0\n    result = sum(token_counts) / len(token_counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of modal verbs (can/could/may/might/must/should/would/will/shall) to total words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total = len(words)\n    if total == 0:\n        return 0.0\n    modals = re.findall(r'\\b(?:can|could|may|might|must|should|would|will|shall)\\b', text, flags=re.I)\n    return float(len(modals)) / float(total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are common English stopwords (function-word density)'\n    import re\n    if not text:\n        return float(0.0)\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with',\n        'he','as','you','do','at','this','but','his','by','from','they','we','say','her',\n        'she','or','an','will','my','one','all','would','there','their','what','so','up',\n        'out','if','about','who','get','which','go','me'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    stop_count = sum(1 for w in words if w in stopwords)\n    return float(stop_count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Smoothed ratio of common irregular past-tense verbs to total tokens (captures non-\"ed\" past usage)'\n    import re\n    if not text:\n        return 0.0\n    irregulars = {'was','were','had','saw','went','came','got','made','became','began','brought',\n                  'broke','chose','did','drank','drove','flew','found','gave','kept','knew','left',\n                  'lost','put','read','ran','said','sent','slept','spoke','took','told','thought',\n                  'won','felt','built','fell','held','heard','lost','paid','rode','rose','shot','shut'}\n    tokens = re.findall(r\"\\b\\w+(?:['\u2019]\\w+)?\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in irregulars)\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences containing adjacent duplicate words (e.g., \"the the\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    dup_sentences = 0\n    for s in sentences:\n        tokens = re.findall(r'\\w+', s.lower())\n        if len(tokens) < 2:\n            continue\n        for i in range(1, len(tokens)):\n            if tokens[i] == tokens[i-1]:\n                dup_sentences += 1\n                break\n    result = dup_sentences / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', re.I)\n    email_re = re.compile(r'^[\\w.+-]+@[\\w.-]+\\.\\w+$')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(https?://\\S+|www\\.\\S+|\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b)', re.I)\n    matches = sum(1 for t in tokens if pattern.search(t))\n    return float(matches / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', re.I)\n    email_re = re.compile(r'^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$')\n    domain_like_re = re.compile(r'\\.\\w{2,}(?:/|$)')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t) or domain_like_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences containing quotation marks or quotes'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    quote_chars = set(['\"', '\u201c', '\u201d', \"\u00ab\", \"\u00bb\"])\n    count = 0\n    for s in sentences:\n        if any(c in s for c in quote_chars):\n            count += 1\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating/discourse word (And, But, So, Also, However) \u2014 signals conversational or rhetorical structure'\n    import re\n    if not text:\n        return 0.0\n    parts = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not parts:\n        return 0.0\n    starts = 0\n    for s in parts:\n        m = re.match(r'^\\W*([A-Za-z\\'\u2019]+)', s)\n        if m:\n            w = m.group(1).lower()\n            if w in {'and', 'but', 'so', 'also', 'however', 'thus', 'then', 'therefore', 'meanwhile'}:\n                starts += 1\n    return float(starts) / float(len(parts))\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence (sentences split on .!? boundaries)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    result = sum(word_counts) / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a coordinating conjunction (and, but, or, so, because, then)'\n    import re\n    if not text:\n        return 0.0\n    conj = {'and', 'but', 'or', 'so', 'because', 'then', 'also', 'yet'}\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        return 0.0\n    count = 0\n    for s in sents:\n        m = re.match(r'^[\\'\"(\\[]*\\s*([A-Za-z]+)', s)\n        if m:\n            if m.group(1).lower() in conj:\n                count += 1\n    return float(count / len(sents))\n\n",
    "def feature(text: str) -> float:\n    'Normalized Shannon entropy of character distribution (0-1)'\n    from collections import Counter\n    import math\n    if not text:\n        return 0.0\n    counts = Counter(text)\n    total = sum(counts.values())\n    if total == 0 or len(counts) <= 1:\n        return 0.0\n    entropy = 0.0\n    for v in counts.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    denom = math.log2(len(counts))\n    return float(entropy / denom) if denom > 0 else 0.0\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Pronoun density: fraction of word tokens that are common pronouns'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','you','he','she','it','we','they','me','him','her','us','them','my','your','his','her','our','their','mine','yours','hers'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in pronouns)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are capitalized but not sentence-initial (captures named-entity mid-sentence density)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n    mid_caps = 0\n    total = 0\n    for s in sentences:\n        words = re.findall(r\"\\b[\\w\u2019'-]+\\b\", s, re.UNICODE)\n        if not words:\n            continue\n        for w in words[1:]:\n            total += 1\n            # exclude lone \"I\"\n            if len(w) > 1 and w[0].isupper():\n                mid_caps += 1\n    if total == 0:\n        return 0.0\n    return float(mid_caps) / total\n\n",
    "def feature(text: str) -> float:\n    'Normalized Shannon entropy of word distribution (0-1), measures lexical diversity'\n    import re, math\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    total = len(words)\n    probs = [freq / total for freq in freqs.values()]\n    entropy = -sum(p * math.log2(p) for p in probs if p > 0)\n    # normalize by log2(number of types) to keep in [0,1] when more than one type exists\n    types = len(freqs)\n    if types <= 1:\n        return 0.0\n    norm = entropy / math.log2(types)\n    return float(norm)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q_count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are Titlecase (start with uppercase then lowercase)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.istitle())\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average token surprisal (mean -log2(token frequency) in this text)'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    n = len(tokens)\n    freq = {}\n    for t in tokens:\n        freq[t] = freq.get(t, 0) + 1\n    total = 0.0\n    for t in tokens:\n        p = freq.get(t, 0) / n\n        if p > 0:\n            total += -math.log2(p)\n    return float(total / n)\n\n",
    "def feature(text: str) -> float:\n    'Average number of colon characters (:) per sentence, 0.0 if no sentences'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    num_sent = len(sentences)\n    colons = text.count(':')\n    if num_sent == 0:\n        return 0.0\n    return float(colons) / float(num_sent)\n\n",
    "def feature(text: str) -> float:\n    'Average number of clause separators (commas/semicolons/colons) per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    sep_count = text.count(',') + text.count(';') + text.count(':')\n    return float(sep_count / max(1, len(sentences)))\n",
    "def feature(text: str) -> float:\n    'Variance of word lengths (higher means more variation in word size)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    n = len(words)\n    if n <= 1:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    return float(var)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique words divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(total)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    total_words = 0\n    for s in sentences:\n        total_words += len(re.findall(r'\\w+', s))\n    if not sentences:\n        return 0.0\n    return float(total_words / max(1, len(sentences)))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are -ing gerunds/continuous verbs (heuristic: tokens ending with \"ing\")'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    ing_count = sum(1 for t in tokens if len(t) > 3 and t.endswith('ing'))\n    return float(ing_count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that are URLs or email addresses'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    url_re = re.compile(r'^(https?://|www\\.)', re.I)\n    email_re = re.compile(r'^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Estimated average syllables per word using vowel-group heuristic'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words:\n        return 0.0\n    total = 0\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        syl = len(groups)\n        # adjust for silent e endings (simple heuristic)\n        if syl > 1 and w.lower().endswith('e'):\n            syl -= 1\n        if syl < 1:\n            syl = 1\n        total += syl\n    return float(total / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with a question mark)'\n    import re\n    if not text:\n        return float(0.0)\n    # split into sentence-like segments\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    questions = sum(1 for s in sentences if s.endswith('?'))\n    return float(questions / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens that contain at least one digit (numeric token density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    numeric = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(numeric / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        sentences = [s]\n    count = 0\n    for sent in sentences:\n        s2 = sent.lstrip()\n        if s2 and s2[0].isalpha() and s2[0].islower():\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (mean words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    return float(sum(word_counts) / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.endswith('?'))\n    return float(q_count / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are in ALL CAPS (words of length>=2)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return float(0.0)\n    caps = sum(1 for t in tokens if any(c.isalpha() for c in t) and len([c for c in t if c.isalpha()])>=2 and t.isupper())\n    result = caps / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters in the text that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of alphabetic tokens ending in \"ed\" (simple proxy for past-tense verb usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    words = max(1, len(tokens))\n    ed_count = 0\n    for t in tokens:\n        tl = t.lower()\n        if len(tl) > 3 and tl.endswith('ed'):\n            ed_count += 1\n    return float(ed_count) / words\n\n",
    "def feature(text: str) -> float:\n    'Approximate passive/past constructions: occurrences of \"was|were <word>ed\" per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    matches = re.findall(r'\\b(?:was|were)\\s+\\w+ed\\b', text, flags=re.IGNORECASE)\n    return len(matches) / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Normalized lexical entropy of word frequency distribution (0..1), approximates lexical variety vs repetition'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    cnt = Counter(tokens)\n    N = float(len(tokens))\n    probs = [v / N for v in cnt.values()]\n    # entropy\n    H = -sum(p * math.log(p + 1e-12) for p in probs)\n    V = len(cnt)\n    if V <= 1:\n        return 0.0\n    # normalize by log(V)\n    return float(H / (math.log(V) + 1e-12))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or domain mentions'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    url_re = re.compile(r'https?://|www\\.|[a-z0-9.-]+\\.(com|org|net|edu|gov|io|co)\\b', re.IGNORECASE)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is lowercase'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        m = re.search(r'[A-Za-z]', s)\n        return float(1.0 if (m and m.group(0).islower()) else 0.0)\n    count = 0\n    for seg in sentences:\n        m = re.search(r'[A-Za-z]', seg)\n        if m and m.group(0).islower():\n            count += 1\n    return float(count / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that match common ASCII emoticons or heart (<3)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    emoticon_re = re.compile(r'^(?:[:;=8Xx][-~]?[)\\]D\\(\\]/\\\\OpP]|<3)$', flags=re.I)\n    count = 0\n    for t in tokens:\n        if emoticon_re.search(t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing a character repeated three or more times in a row (elongation)'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\b\\S+\\b', text)\n    if not toks:\n        return 0.0\n    rep_re = re.compile(r'(.)\\1\\1+')\n    count = sum(1 for t in toks if rep_re.search(t))\n    return float(count / len(toks))\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a lowercase letter (informal/lax capitalization)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    low_start = 0\n    for s in sentences:\n        for ch in s:\n            if ch.isspace():\n                continue\n            if ch.isalpha():\n                if ch.islower():\n                    low_start += 1\n                break\n            else:\n                break\n    result = low_start / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Contraction density: fraction of tokens that match common contractions (e.g., don\\'t, I\\'m)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    contractions = re.findall(r\"\\b[a-zA-Z]+['\u2019][a-zA-Z]+\\b\", text)\n    result = len(contractions) / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are common English stopwords (simple stopword density)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'if', 'is', 'are', 'was', 'were', 'in', 'on', 'at', 'for', 'to', 'of', 'with', 'by', 'from', 'that', 'this', 'these', 'those', 'it', 'i', 'you', 'he', 'she', 'they', 'we'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Longest consecutive run of non-alphanumeric (punctuation/symbol) characters normalized by text length'\n    if not text:\n        return float(0.0)\n    max_run = 0\n    run = 0\n    for c in text:\n        if not c.isalnum() and not c.isspace():\n            run += 1\n            if run > max_run:\n                max_run = run\n        else:\n            run = 0\n    result = max_run / max(1, len(text))\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'URL/token density: fraction of word-like tokens that look like URLs/domains'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    urls = re.findall(r'https?://\\S+|www\\.\\S+|\\b[\\w-]+\\.(?:com|net|org|io|gov|edu|co)\\b', text, flags=re.I)\n    result = len(urls) / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of non-whitespace tokens that look like URLs, emails, or domain-like tokens'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\b\\S+@\\S+\\b|\\b\\w+\\.(?:com|org|net|edu|gov|io|co|me|info)\\b', re.I)\n    matches = sum(1 for t in tokens if pattern.search(t))\n    result = matches / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing a character repeated three or more times consecutively'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    repeated = 0\n    for t in tokens:\n        if re.search(r'(.)\\1\\1', t):\n            repeated += 1\n    result = repeated / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (stddev / mean)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    stdev = math.sqrt(var)\n    result = stdev / mean\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that are all-uppercase (length>=2)'\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    def is_shout(t):\n        letters = [c for c in t if c.isalpha()]\n        return len(letters) >= 2 and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_shout(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Lexical density measured as fraction of content words (non-function words) among tokens'\n    import re\n    if not text:\n        return 0.0\n    function_words = {'the','a','an','of','and','to','in','for','on','with','as','by','is','are','was','were','be','that','which','who','whom','this','these','those','it','its','their','they','them','at','from','or','but','if','than','so','then','such'}\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    function_count = sum(1 for t in tokens if t in function_words)\n    content_count = len(tokens) - function_count\n    return float(content_count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    result = num_tokens / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a coordinating conjunction (and,but,or,so,yet,for,nor)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = {'and','but','or','so','yet','for','nor'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'^\\s*([A-Za-z]+)', s)\n        if m and m.group(1).lower() in conj:\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average vowel-to-consonant ratio per word (letters only)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    vowels = set('aeiou')\n    ratios = []\n    for w in words:\n        letters = [c for c in w if c.isalpha()]\n        if not letters:\n            continue\n        v = sum(1 for c in letters if c in vowels)\n        c = sum(1 for c in letters if c.isalpha() and c not in vowels)\n        # avoid division by zero: if no consonants, treat ratio as v (large)\n        if c == 0:\n            ratios.append(float(v))\n        else:\n            ratios.append(v / c)\n    if not ratios:\n        return 0.0\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain any digit (numeric token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = num / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like list items (bulleted or numbered)'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    item_re = re.compile(r'^\\s*(?:[-*\u2022]|(?:\\d+[\\.\\)]|\\([a-zA-Z0-9]+\\))|[a-zA-Z]\\))\\s+')\n    count = sum(1 for l in lines if item_re.match(l))\n    return float(count / len(lines))\n",
    "def feature(text: str) -> float:\n    'Longest consecutive run of punctuation characters normalized by text length'\n    import re\n    if not text:\n        return 0.0\n    # punctuation = any non-alphanumeric, non-space\n    runs = re.findall(r'[^0-9A-Za-z\\s]+', text)\n    if not runs:\n        return 0.0\n    longest = max(len(r) for r in runs)\n    denom = max(1, len(text))\n    return float(longest / denom)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are predominantly ALL CAPS (>=2 letters uppercase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        if len(letters) < 2:\n            return False\n        return all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (lexical diversity): unique word forms / total words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like simple past-tense verbs (end with \"ed\"), as a loose proxy for narrative past tense usage'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w'-]*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    count_ed = sum(1 for t in tokens if len(t) > 3 and t.endswith('ed'))\n    return count_ed / len(tokens)\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or web addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'https?://|www\\.|[a-z0-9\\-]+\\.(com|net|org|edu|gov|io)(/|$)', re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating/subordinating conjunction (approx.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = {'and','but','or','so','because','although','though','however','yet','then','also','since','when','while'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        # remove leading non-letter characters like quotes or parentheses\n        s2 = re.sub(r'^[^A-Za-z0-9]+', '', s).strip()\n        m = re.match(r'([A-Za-z]+)', s2)\n        if m and m.group(1).lower() in conj:\n            starts += 1\n    return float(starts / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of titlecase words (words starting with an uppercase followed by lowercase letters) among all tokens'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    titlecase = sum(1 for t in tokens if len(t) > 1 and t[0].isupper() and t[1:].islower())\n    return titlecase / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence (words defined by \\\\w+)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if sentences:\n        counts = []\n        for s in sentences:\n            counts.append(len(re.findall(r'\\w+', s)))\n        if not counts:\n            return 0.0\n        return float(sum(counts) / len(counts))\n    # fallback: if no clear sentences, return average words per 1\n    return float(len(words) / max(1, len(sentences) if sentences else 1))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of non-whitespace tokens that contain a hyphen'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if '-' in t and len(t.strip('-')) > 0:\n            count += 1\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Relative variation of sentence lengths (coefficient of variation of words per sentence)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        return 0.0\n    lens = [len(re.findall(r'\\w+', sent)) for sent in sentences]\n    if not lens:\n        return 0.0\n    n = len(lens)\n    mean = sum(lens) / n\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lens) / n\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (stddev / mean), 0 if undefined'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
    "def feature(text: str) -> float:\n    'Flesch\u2013Kincaid grade level (approximate; 0.0 when not computable)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n    if not words or not sentences:\n        return 0.0\n    def syllables(w):\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        return max(1, len(groups))\n    total_syllables = sum(syllables(w) for w in words)\n    words_per_sentence = len(words) / max(1, len(sentences))\n    syllables_per_word = total_syllables / len(words)\n    score = 0.39 * words_per_sentence + 11.8 * syllables_per_word - 15.59\n    return float(score)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (lexical diversity): unique word tokens divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = set(tokens)\n    return float(len(unique) / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are written in all uppercase (acronyms, emphasis)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        # consider uppercase words with at least two characters to avoid counting \"I\"\n        if len(w) >= 2 and any(c.isalpha() for c in w) and w.isupper():\n            count += 1\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of ellipses (\"...\") per word (captures trailing/pausing stylistic markers)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = max(1, len(words))\n    ellipses = text.count('...')\n    return float(ellipses) / float(word_count)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are common ASCII emoticons (e.g., :) :-D ;))'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticon_re = re.compile(r'^(?:[:;=8X][-~]?[)DdpP/(\\\\|\\]|[\\]{}]|<3|:\\'[\\)])$|^<3$|^:-?O$|^:-?\\($', re.I)\n    count = 0\n    for t in tokens:\n        if emoticon_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'\\b(?:https?://|www\\.)', re.I)\n    email_re = re.compile(r'\\b[\\w.+-]+@[\\w-]+\\.[\\w.-]+\\b')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Comma density per sentence: average number of commas per sentence (comma_count / sentence_count)'\n    import re\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    sentence_count = max(1, sentence_count)\n    commas = text.count(',')\n    return float(commas) / float(sentence_count)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are pure numeric values (integers or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d[\\d,]*\\.?\\d*$')\n    nums = sum(1 for t in tokens if num_re.match(t.replace(' ', '')))\n    return float(nums / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation characters'\n    if not text:\n        return 0.0\n    punc = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(punc)\n    if total == 0:\n        return 0.0\n    uniq = len(set(punc))\n    result = uniq / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (words per sentence) to capture structural variability'\n    if not text:\n        return 0.0\n    import re, math\n    sentences = [s.strip() for s in re.findall(r'[^.!?]+[.!?]?', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        counts.append(len(words))\n    if len(counts) <= 1:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / (len(counts) - 1)\n    return float(math.sqrt(var))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of adjacent token pairs that are identical (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(len(tokens) - 1) if tokens[i] == tokens[i + 1])\n    return float(repeats / (len(tokens) - 1))\n",
    "def feature(text: str) -> float:\n    'Density of blank/empty lines (number of blank lines divided by total lines)'\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    if not lines:\n        return 0.0\n    blank = sum(1 for ln in lines if not ln.strip())\n    return float(blank) / float(len(lines))\n\n",
    "def feature(text: str) -> float:\n    'Uppercase-letter ratio: uppercase alphabetic chars divided by total alphabetic chars'\n    if not text:\n        return 0.0\n    alpha_total = sum(1 for c in text if c.isalpha())\n    if alpha_total == 0:\n        return 0.0\n    upper = sum(1 for c in text if c.isalpha() and c.isupper())\n    return float(upper / alpha_total)\n\n",
    "def feature(text: str) -> float:\n    'Density of the auxiliary \"had\" (common in past-perfect narrative) as tokens per word'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    return words.count('had') / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)\\S+|^\\S+\\.(com|net|org|io|gov|edu)(/.*)?$', re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average proportion of uppercase letters within word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    ratios = []\n    for w in words:\n        if len(w) == 0:\n            continue\n        up = sum(1 for c in w if c.isupper())\n        ratios.append(up / len(w))\n    if not ratios:\n        return 0.0\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are short (length <= 2)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    short = sum(1 for t in tokens if len(t) <= 2)\n    return float(short / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of tokens that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    hapaxes = sum(1 for t, c in counts.items() if c == 1)\n    return float(hapaxes / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain quotation marks (dialogue or quotes)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    quotes = set('\"\\'\u201c\u201d\u2018\u2019')\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        if any(q in s for q in quotes):\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return 0.0\n    # small common stopword set for efficiency\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not',\n        'on','with','he','as','you','do','at','this','but','his','by','from',\n        'they','we','say','her','she','or','an','will','my','one','all','would',\n        'there','their','what','so','up','out','if','about','who','get','which',\n        'go','me'\n    }\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average estimated syllable count per word (vowel-group heuristic)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    total = 0\n    for w in words:\n        groups = re.findall(r'[aeiouyAEIOUY]+', w)\n        s = len(groups)\n        # simple heuristic: words with no vowel groups count as 1\n        if s <= 0:\n            s = 1\n        # reduce by one for trailing silent \"e\" in longer words\n        if len(w) > 2 and w.lower().endswith('e') and s > 1:\n            s -= 1\n        total += max(1, s)\n    return float(total / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Density of common English function words (approximate stopword density)'\n    import re\n    if not text:\n        return 0.0\n    STOP = {'the','be','to','of','and','a','in','that','it','is','for','on','with','as','are','was','at','by','an','this','from','or','but'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in STOP)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of punctuation characters to alphabetic letters (punctuation_count / max(1, letter_count))'\n    if not text:\n        return 0.0\n    letters = sum(1 for c in text if c.isalpha())\n    punctuation = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punctuation / max(1, letters))\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'[\\w.+-]+@[\\w-]+\\.\\w+', re.I)\n    url_re = re.compile(r'https?://|www\\.', re.I)\n    count = 0\n    for t in tokens:\n        if email_re.search(t) or url_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word-tokens that are URLs or email addresses'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\S+', text)\n    if not words:\n        return 0.0\n    url_pattern = re.compile(r'(?i)\\b(?:https?://|www\\.)\\S+')\n    email_pattern = re.compile(r'\\b\\S+@\\S+\\.\\S+\\b')\n    count = 0\n    for w in words:\n        if url_pattern.search(w) or email_pattern.search(w):\n            count += 1\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating conjunction (and, but, or, so, yet, for, nor)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = {'and','but','or','so','yet','for','nor'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    total = 0\n    for s in sentences:\n        m = re.search(r'\\b(\\w+)\\b', s.strip())\n        if not m:\n            continue\n        total += 1\n        if m.group(1).lower() in conj:\n            count += 1\n    if total == 0:\n        return 0.0\n    result = count / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are emoticons or contain emoji-like characters'\n    import re\n    if not text:\n        return 0.0\n    emoticons = {':)', ':-)', ':(', ':-(', ':D', ':-D', ':P', ':-P', ';)', ';-)', ':/', ':-/', \":'(\", ':|'}\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if t in emoticons:\n            count += 1\n            continue\n        for ch in t:\n            oc = ord(ch)\n            if oc >= 0x1F300 and oc <= 0x1FAFF:  # Emoji / pictograph ranges (approx)\n                count += 1\n                break\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like headings (short lines with title-cased words or that end with a question/exclamation)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    heading_count = 0\n    for ln in lines:\n        words = ln.split()\n        if not words:\n            continue\n        # candidate heading: short and many title-cased initials, or ends with ?/!\n        if len(words) <= 8:\n            title_initials = sum(1 for w in words if w[0].isupper())\n            if title_initials >= max(1, len(words) // 2):\n                heading_count += 1\n                continue\n        if ln.endswith('?') or ln.endswith('!'):\n            heading_count += 1\n    return float(heading_count) / len(lines)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are all-uppercase tokens of length>=2 (ACRONYMS/SHOUT)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    caps = sum(1 for t in tokens if re.fullmatch(r'[A-Z]{2,}', t))\n    return float(caps / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Diversity of contraction forms: unique contraction tokens divided by total contraction-like tokens (higher = more varied contraction usage)'\n    import re\n    if not text:\n        return 0.0\n    contractions = re.findall(r\"\\b\\w+['\u2019]\\w+\\b\", text)\n    total = len(contractions)\n    if total == 0:\n        return 0.0\n    unique = len(set(c.lower() for c in contractions))\n    return unique / float(total)\n\n\n",
    "def feature(text: str) -> float:\n    'Average number of vowel groups per alphabetic word (simple syllable proxy)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'[A-Za-z]+', text)\n    if not words:\n        return 0.0\n    vg = sum(len(re.findall(r'[aeiouyAEIOUY]+', w)) for w in words)\n    return float(vg / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Density of discourse markers (however, therefore, moreover, thus, consequently, furthermore) per sentence'\n    import re\n    if not text:\n        return 0.0\n    markers = ['however', 'therefore', 'moreover', 'furthermore', 'thus', 'consequently', 'in addition', 'on the other hand']\n    lower = text.lower()\n    matches = 0\n    for m in markers:\n        # count whole-word occurrences; for multiword markers allow a simple substring search anchored by word boundaries\n        matches += len(re.findall(r'\\b' + re.escape(m) + r'\\b', lower))\n    # sentence count fallback\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(matches) / float(sentences)\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    stops = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there','their'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if w in stops)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that begin with common discourse markers (However, Moreover, In addition, Therefore, When, While, Although, Because, Thus, Meanwhile)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # Split into sentence-like chunks\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sents:\n        return 0.0\n    markers = {'however','moreover','furthermore','therefore','additionally','consequently','meanwhile','thus','hence','in','when','while','although','because','there','nonetheless','nevertheless'}\n    count = 0\n    for s in sents:\n        m = re.match(r\"^\\s*['\\\"\\(\\[]*([A-Za-z\\-]+)\", s)\n        if m:\n            first = m.group(1).lower()\n            if first in markers:\n                count += 1\n    return float(count) / float(len(sents))\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of word tokens that occur exactly once in the text'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freq = {}\n    for w in words:\n        freq[w] = freq.get(w, 0) + 1\n    hapax = sum(1 for w in words if freq.get(w, 0) == 1)\n    return float(hapax / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of token lengths (stdev/mean) for word tokens'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    lengths = [len(t) for t in tokens]\n    if len(lengths) <= 1:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    stdev = math.sqrt(var)\n    return float(stdev / mean)\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are in ALL CAPS (length >= 2)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    caps = sum(1 for t in tokens if len(t) >= 2 and t.isupper())\n    return float(caps / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are all-caps (length>=2)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.isupper() and len(w) >= 2)\n    return float(count / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are in ALL CAPS (and contain at least one letter)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t) and t.isupper():\n            count += 1\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of characters that lie inside parentheses (measures parenthetical/planned content like \"(50 words)\")'\n    import re\n    if not text:\n        return 0.0\n    parens = re.findall(r'\\([^)]*\\)', text)\n    if not parens:\n        return 0.0\n    inside_chars = sum(len(p) - 2 for p in parens)  # exclude parentheses chars themselves\n    total_chars = len(text)\n    return float(inside_chars) / total_chars if total_chars > 0 else 0.0\n\n",
    "def feature(text: str) -> float:\n    'Ratio of the definite article \"the\" to total tokens (lowercased)'\n    if not text:\n        return 0.0\n    toks = text.split()\n    if not toks:\n        return 0.0\n    the_count = sum(1 for t in toks if t.lower().strip(\".,;:!?\\\"'()[]{}\") == 'the')\n    return the_count / len(toks)\n\n",
    "def feature(text: str) -> float:\n    'Average count of colons and semicolons per sentence (proxy for clause-complex punctuation use)'\n    import re\n    if not text:\n        return 0.0\n    colons_semis = text.count(':') + text.count(';')\n    # sentence count fallback to at least 1\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(colons_semis) / float(sentence_count)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'(?i)^(https?://|www\\.|ftp://)')\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}$')\n    domain_re = re.compile(r'\\.\\w{2,4}(/|$)')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.match(t) or domain_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating conjunction (and, but, or, so, yet, for, nor)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = {'and','but','or','so','yet','for','nor'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'\\b([A-Za-z]+)\\b', s)\n        if m and m.group(1).lower() in conj:\n            count += 1\n    return float(count / len(sentences))\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total word tokens'\n    import re\n    words = re.findall(r'\\w+', (text or '').lower())\n    if not words:\n        return 0.0\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with ?)'\n    import re\n    if not text:\n        return 0.0\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [p for p in re.split(r'(?<=[.!?])\\s+', s) if p.strip()]\n    if not sentences:\n        return 0.0\n    q = sum(1 for sent in sentences if sent.rstrip().endswith('?'))\n    return float(q / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of hapax legomena (words that occur exactly once)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    hapax = sum(1 for w,c in freqs.items() if c == 1)\n    return float(hapax / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (comma density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        # treat entire text as one sentence\n        sentences = [text.strip()]\n    commas = sum(s.count(',') for s in sentences)\n    return float(commas / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing a repeated character sequence of length >=3'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1{2,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Longest run of the same character divided by text length (captures repeated characters)'\n    import re\n    if not text:\n        return 0.0\n    longest = 0\n    for m in re.finditer(r'(.)\\1*', text, flags=re.S):\n        ln = len(m.group(0))\n        if ln > longest:\n            longest = ln\n    denom = max(1, len(text))\n    result = longest / denom\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proxy for passive voice: fraction of \"be\" auxiliaries followed by likely past participles (be + .*ed or *en)'\n    import re\n    if not text:\n        return 0.0\n    lowered = text.lower()\n    # Match common 'be' forms followed by a token that ends with ed or en (approximate past participle)\n    matches = re.findall(r'\\b(?:is|are|was|were|am|be|been|being)\\s+\\w+(?:ed|en)\\b', lowered)\n    # Normalise by token count to avoid bias with very long/short texts\n    tokens = re.findall(r'\\b\\w+\\b', lowered)\n    denom = max(1, len(tokens))\n    return float(len(matches)) / denom\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words with elongated letters (e.g., \"sooo\", \"loooove\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if re.search(r'(.)\\1{2,}', w))\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Density of Latinate/technical suffixes (words ending with -ize/-ise/-ic/-al/-ary/-ous/-ive) as a proxy for formal/academic tone'\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w[\\w'-]*\", text.lower())\n    if not tokens:\n        return 0.0\n    suffixes = ('ize','ise','ic','al','ary','ous','ive','ent','ant')\n    count = 0\n    for t in tokens:\n        for suf in suffixes:\n            if len(t) > len(suf) + 1 and t.endswith(suf):\n                count += 1\n                break\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total word tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per (detected) sentence: comma_count / (sentence_count+epsilon) to estimate clause density'\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    sentences = text.count('.') + text.count('!') + text.count('?')\n    sentences = max(1, sentences)\n    return float(commas) / float(sentences)\n\n\n",
    "def feature(text: str) -> float:\n    'Character-level Shannon entropy (bits) normalized by log2(unique_chars+1), 0 for empty text'\n    import math, collections\n    if not text:\n        return 0.0\n    counts = collections.Counter(text)\n    total = float(len(text))\n    entropy = 0.0\n    for cnt in counts.values():\n        p = cnt / total\n        entropy -= p * math.log2(p)\n    # normalize by maximum possible log2(len(unique_chars)+1) to keep in [0,1]\n    unique = len(counts)\n    if unique <= 1:\n        return 0.0\n    norm = math.log2(unique)\n    if norm <= 0:\n        return 0.0\n    return float(entropy) / float(norm)\n",
    "def feature(text: str) -> float:\n    'Density of immediate repeated word bigrams (e.g., \"very very\") as fraction of all bigrams'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = len(tokens) - 1\n    repeats = sum(1 for i in range(len(tokens) - 1) if tokens[i] == tokens[i + 1])\n    return float(repeats / bigrams)\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of vocabulary types that occur only once (an indicator of lexical richness vs repetition)'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    c = Counter(tokens)\n    vocab = len(c)\n    if vocab == 0:\n        return 0.0\n    hapax = sum(1 for k,v in c.items() if v == 1)\n    return float(hapax) / float(vocab)\n",
    "def feature(text: str) -> float:\n    'Fraction of word-tokens that are first-person pronouns (I/me/we/us/my/our...)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','we','us','my','mine','our','ours'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in pronouns)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (lexical diversity): unique word forms divided by total words'\n    import re\n    if not text:\n        return 0.0\n    tokens = [w.lower() for w in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\") normalized by adjacent pairs'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    n = len(words)\n    if n < 2:\n        return 0.0\n    repeats = sum(1 for a, b in zip(words, words[1:]) if a == b)\n    result = repeats / max(1, n - 1)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens appearing inside double-quoted spans to total tokens (approximate dialogue proportion)'\n    import re\n    if not text:\n        return 0.0\n    total_tokens = len(re.findall(r'\\b\\w+\\b', text))\n    if total_tokens == 0:\n        return 0.0\n    parts = re.split(r'\"', text)\n    inside_tokens = 0\n    # odd-indexed parts are inside quotes when quotes are paired\n    for i in range(1, len(parts), 2):\n        inside_tokens += len(re.findall(r'\\b\\w+\\b', parts[i]))\n    return float(inside_tokens) / float(total_tokens)\n\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (distinct word types divided by total tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    return float(len(set(tokens))) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text:\n        return 0.0\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [p for p in re.split(r'(?<=[.!?])\\s+', s) if p.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for sent in sentences:\n        total_words += len(re.findall(r'\\w+', sent))\n    return float(total_words / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that mix letters and digits (alphanumeric tokens)'\n    import re\n    tokens = re.findall(r'\\w+', text or '')\n    if not tokens:\n        return 0.0\n    mixed = 0\n    for t in tokens:\n        has_alpha = any(c.isalpha() for c in t)\n        has_digit = any(c.isdigit() for c in t)\n        if has_alpha and has_digit:\n            mixed += 1\n    return float(mixed / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of unique word types to total word tokens (type-token ratio)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = set(words)\n    return float(len(unique) / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that look like email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}$')\n    count = 0\n    for t in tokens:\n        if email_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (stddev / mean) to capture lexical length variability'\n    import re, math\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0 or len(lengths) < 2:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that consist only of punctuation characters'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if all(not c.isalnum() for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    result = unique / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Semicolon density: fraction of characters that are semicolons'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    semis = text.count(';')\n    return float(semis) / float(total_chars)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total words'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ed\" (simple past-tense heuristic)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[A-Za-z']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    ed_count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ed'))\n    return float(ed_count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average approximate syllable count per word (vowel-group heuristic)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z']+\", text)\n    if not words:\n        return 0.0\n    def syls(w):\n        groups = re.findall(r'[aeiouy]+', w.lower())\n        return max(1, len(groups))\n    totals = sum(syls(w) for w in words)\n    return float(totals / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Density of first-person pronouns (I, me, my, we, our, etc.) relative to token count'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours',\"i'm\",\"i've\",\"i'd\",\"i'll\"}\n    count = 0\n    for w in tokens:\n        if w.lower() in first_person:\n            count += 1\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(num / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that contain three or more identical characters in a row (elongation)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1')\n    count = sum(1 for w in words if pattern.search(w))\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are written in ALL CAPS (length>=2)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if len(w) >= 2 and any(c.isalpha() for c in w) and w.upper() == w:\n            count += 1\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', flags=re.I)\n    email_re = re.compile(r'^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$')\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total tokens (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    total = len(tokens)\n    if total == 0:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique) / float(total)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length measured in words'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid dividing by zero if a sentence has zero words; count it as zero length\n    result = sum(word_counts) / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords (simple small stopword list) among tokens'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','a','of','to','in','is','it','that','for','on','with','as','are','was','be','by','an','this','which','or','from'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stop)\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Em-dash or double-dash frequency normalized by word count (heuristic for stylistic punctuation like \"\u2014\" or \"--\")'\n    import re\n    if not text:\n        return 0.0\n    dash_count = text.count('\u2014') + text.count('--')\n    words = re.findall(r'\\w+', text)\n    return float(dash_count) / max(1.0, len(words))\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: words that appear exactly once divided by total words'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    cnt = Counter(words)\n    hapax = sum(1 for w, c in cnt.items() if c == 1)\n    result = hapax / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of punctuation characters per word'\n    import re, string\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    punct = sum(1 for c in text if (not c.isalnum()) and (not c.isspace()))\n    result = punct / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of hapax legomena (tokens that occur exactly once) among all word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if not tokens:\n        return 0.0\n    from collections import Counter\n    freqs = Counter(tokens)\n    hapaxes = sum(1 for v in freqs.values() if v == 1)\n    return float(hapaxes) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length measured in word tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    result = sum(lengths) / len(lengths)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with the definite article \"the\" (case-insensitive)'\n    sentences = re.split(r'[.!?]+', text)\n    sent_tokens = []\n    for s in sentences:\n        s = s.strip()\n        if s:\n            tokens = re.findall(r'\\w+', s, flags=re.UNICODE)\n            sent_tokens.append(tokens)\n    if not sent_tokens:\n        return 0.0\n    starts = 0\n    for tokens in sent_tokens:\n        if tokens and tokens[0].lower() == 'the':\n            starts += 1\n    return float(starts) / float(len(sent_tokens))\n\n",
    "def feature(text: str) -> float:\n    'Adverb density: fraction of tokens that end with \"ly\" (heuristic for adverb use)'\n    import re\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of words that look like nominalizations (common formal suffixes like -tion, -ment, -ity, -ness)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    suffixes = ('tion', 'sion', 'ment', 'ity', 'ness', 'ence', 'ance', 'alism', 'ization')\n    count = 0\n    for w in words:\n        for s in suffixes:\n            if w.endswith(s) and len(w) > len(s) + 2:\n                count += 1\n                break\n    return float(count) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average commas per sentence (commas indicate clause complexity), normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    sentence_splits = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    # if no sentence punctuation, treat as one sentence\n    if not sentence_splits:\n        sentence_count = 1\n        comma_count = text.count(',')\n    else:\n        sentence_count = len(sentence_splits)\n        comma_count = sum(s.count(',') for s in sentence_splits)\n    return float(comma_count) / float(sentence_count) if sentence_count else 0.0\n\n",
    "def feature(text: str) -> float:\n    'Average Shannon entropy per token (mean of token-level char entropies)'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    total = 0.0\n    for t in tokens:\n        freq = {}\n        for c in t:\n            freq[c] = freq.get(c, 0) + 1\n        n = float(len(t))\n        ent = 0.0\n        for v in freq.values():\n            p = v / n\n            ent -= p * math.log2(p)\n        total += ent\n    return float(total / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word tokens divided by total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with the second-person pronoun \"you\" (robust to quotes/capitalization)'\n    import re\n    if not text:\n        return 0.0\n    # Split into sentence-like chunks using punctuation or newlines\n    parts = [p.strip() for p in re.split(r'[.!?]+\\s+|\\n+', text) if p.strip()]\n    if not parts:\n        return 0.0\n    starts_you = 0\n    for p in parts:\n        if re.match(r'^[\\'\"\\(\\[]*\\s*you\\b', p, re.I):\n            starts_you += 1\n    return float(starts_you) / len(parts)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are long (length >= 12 characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if len(w) >= 12)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that consist only of punctuation characters'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    punct_only = 0\n    for tok in tokens:\n        if tok and all(not c.isalnum() for c in tok):\n            punct_only += 1\n    return float(punct_only / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that are all-uppercase words (>=2 letters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    up_re = re.compile(r'^[A-Z]{2,}$')\n    count = sum(1 for t in tokens if up_re.match(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Normalized variance of sentence lengths (words per sentence)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    # normalize by mean+1 to reduce scale sensitivity\n    return float(var / (mean + 1.0))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens (length>1) that are fully uppercase (ALL CAPS words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    caps = sum(1 for w in words if len(w) > 1 and w.isupper())\n    result = caps / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Standard deviation of word lengths'\n    import re, math\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n",
    "def feature(text: str) -> float:\n    'Estimated fraction of sentences showing a passive-voice-like pattern (was/were/... + past-participle ending in -ed)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    passive_pattern = re.compile(r'\\b(?:was|were|is|are|been|be|had been|has been|have been|was being|were being)\\s+\\w+ed\\b', re.I)\n    passive_count = sum(1 for s in sentences if passive_pattern.search(s))\n    return float(passive_count) / len(sentences)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of punctuation characters to alphabetic letters (punctuation_count / letter_count)'\n    if not text:\n        return 0.0\n    letters = sum(1 for c in text if c.isalpha())\n    if letters == 0:\n        return 0.0\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct / letters)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that contain one or more non-ASCII characters'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        for c in w:\n            if ord(c) > 127:\n                count += 1\n                break\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Repeated-punctuation-run density: number of repeated punctuation runs (e.g., \"!!!\",\"...\") per character'\n    import re\n    if not text:\n        return 0.0\n    runs = re.findall(r'([!?\\.])\\1{1,}', text)\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    result = len(runs) / total_chars\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens longer than 7 characters (long-word density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_words = sum(1 for w in words if len(w) > 7)\n    result = long_words / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are long (>=12 characters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) >= 12)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like abbreviations/initialisms (e.g., U.S., E.G.)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'^[A-Za-z](?:\\.[A-Za-z])+\\.?$')\n    short_dot = re.compile(r'^[A-Za-z]{1,3}\\.$')\n    count = 0\n    for t in tokens:\n        if pattern.match(t) or short_dot.match(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter (informal style)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    def starts_lower(s):\n        for c in s:\n            if c.isalpha():\n                return c.islower()\n        return False\n    count = sum(1 for s in sentences if starts_lower(s))\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Parenthetical density: count of parentheses characters per token (captures citations or aside notes)'\n    if not text:\n        return 0.0\n    import re\n    words = re.findall(r'\\w+', text)\n    paren_count = text.count('(') + text.count(')')\n    denom = max(1, len(words))\n    return float(paren_count) / float(denom)\n\n",
    "def feature(text: str) -> float:\n    'Stopword density: fraction of word tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'a','an','the','and','or','but','if','while','with','without','to','of','in','on','for',\n        'is','are','was','were','be','been','being','at','by','from','as','that','this','these',\n        'those','it','its','he','she','they','them','we','us','you','your','I','me','my','mine'\n    }\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing internal non-alphanumeric characters'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # consider token containing any char that is not alnum as internal punctuation/symbol\n        if any((not c.isalnum()) for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-empty lines that are short (<= 5 words) \u2014 captures poetic or clipped/dialogue style'\n    import re\n    if not text:\n        return 0.0\n    lines = text.splitlines()\n    nonempty = [ln for ln in lines if ln.strip() != '']\n    if not nonempty:\n        return 0.0\n    short_lines = 0\n    for ln in nonempty:\n        wc = len(re.findall(r'\\w+', ln))\n        if 0 < wc <= 5:\n            short_lines += 1\n    return short_lines / len(nonempty)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among all words'\n    import re\n    if not text:\n        return float(0.0)\n    STOP = {'the','and','is','in','it','of','to','a','that','for','on','with','as','are','was','be','by','this','an','or','from','at','but','not','have'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if w in STOP)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, we, our, etc.)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\w+\", text)\n    if not tokens:\n        return 0.0\n    first_person = {'i','me','my','mine','we','us','our','ours'}\n    count = sum(1 for t in tokens if t.lower() in first_person)\n    return count / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Lexical diversity measured as unique word tokens divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of digit characters among all alphanumeric characters (digits / (digits+letters))'\n    if not text:\n        return 0.0\n    digits = sum(1 for c in text if c.isdigit())\n    letters = sum(1 for c in text if c.isalpha())\n    denom = digits + letters\n    if denom == 0:\n        return 0.0\n    return float(digits / denom)\n\n",
    "def feature(text: str) -> float:\n    'Semicolon density: number of semicolons per sentence (captures clause-complexity style)'\n    import re\n    semis = text.count(';')\n    sentence_count = max(1, len(re.findall(r'[.!?]+', text)))\n    return float(semis) / float(sentence_count)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens in titlecase (initial capital followed by lowercase)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return float(0.0)\n    count = 0\n    for t in tokens:\n        if len(t) >= 2 and t[0].isupper() and t[1:].islower():\n            count += 1\n        elif len(t) == 1 and t.isupper():\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens (small stopword set)'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','it','to','of','a','that','for','on','with','as','are','was','be','by','this','an','or','from','at','which'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    stop_count = sum(1 for w in words if w in stopwords)\n    result = stop_count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Population variance of word lengths (variance of token lengths)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    n = len(lengths)\n    if n < 2:\n        return 0.0\n    mean = sum(lengths) / n\n    var = sum((l - mean) ** 2 for l in lengths) / n\n    return float(var)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain double or angled quotation marks (dialogue/quotes)'\n    import re\n    if not text:\n        return 0.0\n    quotes = {'\"', '\u201c', '\u201d', '\u00ab', '\u00bb'}\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = 0\n    for s in sentences:\n        if any(q in s for q in quotes):\n            count += 1\n    return float(count / len(sentences))\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL-CAPS words (length>=2)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # strip surrounding punctuation\n        clean = re.sub(r'^[^\\w]+|[^\\w]+$', '', t)\n        if len(clean) >= 2 and any(c.isalpha() for c in clean) and clean.isupper():\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are measurement units or numeric tokens immediately followed by units (ml, mg, \u00b0C, %, etc.)'\n    import re\n    if not text:\n        return 0.0\n    units = {'ml','l','mg','g','kg','\u00b5l','ul','mm','cm','m','mol','mM','M','kda','kb','bp','kbp','%','\u00b0c','\u00b0f'}\n    # count explicit unit tokens\n    tokens = re.findall(r\"\\b[\\w%\u00b0\u00b5\u03bc]+(?:\\b|$)\", text.lower())\n    if not tokens:\n        return 0.0\n    unit_count = 0\n    # detect patterns like '10ml' or '10 ml' and standalone unit tokens\n    unit_pattern = re.compile(r'\\d+(?:[\\.,]\\d+)?\\s*(%|\u00b0c|\u00b0f|ml|l|mg|g|kg|\u00b5l|ul|mm|cm|m|mol|mmol|mM|M|kda|kb|bp|kbp)\\b', re.I)\n    for m in unit_pattern.finditer(text):\n        unit_count += 1\n    for t in tokens:\n        if t.strip('.,;:()[]') in units:\n            unit_count += 1\n    # normalize by token count\n    return float(unit_count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are questions (end with ?)'\n    import re\n    if not text:\n        return 0.0\n    # count sentence boundaries by punctuation\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        return 0.0\n    q_count = sum(1 for s in sents if s.rstrip().endswith('?'))\n    return float(q_count / len(sents))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing long repeated punctuation sequences (e.g., !!! or ???)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    rep = 0\n    pat = re.compile(r'([^\\w\\s])\\1{2,}')\n    for t in tokens:\n        if pat.search(t):\n            rep += 1\n    return float(rep / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Lexical diversity: fraction of unique word tokens (lowercased)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that end with an exclamation mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    ex_count = sum(1 for s in sentences if s.rstrip().endswith('!'))\n    return float(ex_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of hapax legomena (words that occur only once) to total tokens'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    hapax = sum(1 for _, c in counts.items() if c == 1)\n    result = hapax / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are questions'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    q_count = sum(1 for s in sentences if s.strip().endswith('?'))\n    result = q_count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (captures clause density and listing style)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_commas = sum(s.count(',') for s in sentences)\n    return float(total_commas / len(sentences))\n",
    "def feature(text: str) -> float:\n    'Paragraph break density: number of double newlines (\\\\n\\\\n) per 100 words'\n    import re\n    if not text:\n        return 0.0\n    words = max(1, len(re.findall(r'\\w+', text)))\n    pbreaks = text.count('\\n\\n')\n    return float(pbreaks) / float(words) * 100.0\n\n",
    "def feature(text: str) -> float:\n    'Ratio of numeric tokens (numbers, integers, decimals, formatted with commas/dots) to all tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d+(?:[.,]\\d+)*$')\n    numeric = sum(1 for t in tokens if num_re.match(t))\n    return float(numeric / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Heuristic passive-voice indicator: proportion of \"be\" forms followed by past-participle-like tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    token_count = max(1, len(tokens))\n    # look for common \"be + past-participle\" patterns (e.g., \"was produced\", \"is shown\", \"were written\", \"has been done\")\n    matches = re.findall(r'\\b(?:is|are|was|were|be|been|being|am)\\s+\\w+(?:ed|en)\\b', text, flags=re.IGNORECASE)\n    # also include \"has been <verb>\" or \"have been <verb>\"\n    matches += re.findall(r'\\b(?:has|have|had)\\s+been\\s+\\w+(?:ed|en)\\b', text, flags=re.IGNORECASE)\n    return float(len(matches)) / token_count\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are written in ALL CAPS (contains a letter and is uppercase)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(0.0)\n    caps = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t) and t.upper() == t:\n            caps += 1\n    return float(caps / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas indicate clause/subordinate density)'\n    import re\n    if not text:\n        return 0.0\n    commas = text.count(',')\n    # Count sentence separators (., !, ?). If none, treat as one sentence.\n    sentences = max(1, len(re.findall(r'[.!?]', text)))\n    return float(commas) / float(sentences)\n\n",
    "def feature(text: str) -> float:\n    'Parenthesis/bracket density: fraction of characters that are parentheses or square brackets'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    count = sum(1 for c in text if c in '()[]{}')\n    return float(count) / float(total_chars)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Standard deviation of word lengths (0.0 for empty or single-token texts)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if len(tokens) <= 1:\n        return 0.0\n    lengths = [len(t) for t in tokens]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of unique word types to total word tokens (lexical uniqueness)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    result = len(set(words)) / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of vocabulary that occurs exactly once'\n    import re, collections\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    freq = collections.Counter(words)\n    vocab = len(freq)\n    if vocab == 0:\n        return 0.0\n    hapax = sum(1 for _, c in freq.items() if c == 1)\n    return float(hapax / vocab)\n\n\n",
    "def feature(text: str) -> float:\n    'Density of em-dash/long-dash usage per word (\u2014, \u2013, or --), normalized by word count'\n    import re\n    if not text:\n        return 0.0\n    word_count = max(1, len(re.findall(r'\\w+', text)))\n    dash_count = text.count('\u2014') + text.count('\u2013') + text.count('--')\n    return dash_count / float(word_count)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain a character repeated three or more times consecutively'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that match a short list of common irregular past-tense verbs (approx. irregular past usage)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b[\\w']+\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    irregular_past = {'was','were','had','went','came','said','took','made','got','saw','knew','thought','told','left','felt','kept','found','gave','began','brought','ran','fell','flew','chose','spoke','slept','stood','wrote','read','shook','rode','built','brought','hid','held'}\n    count = sum(1 for t in tokens if t in irregular_past)\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that end with a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q_count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    result = len(words) / max(1, len(sents))\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Discourse marker density: frequency of common discourse markers (however, moreover, therefore, etc.) per word'\n    import re\n    if not text:\n        return 0.0\n    markers = re.findall(r'\\b(?:however|moreover|therefore|furthermore|additionally|consequently|nevertheless|thus|meanwhile|alternatively)\\b', text, re.I)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(markers)) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q_count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are hyphenated internal-word forms (e.g., well-known)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if re.search(r'\\w-\\w', t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized standard deviation of paragraph lengths (words per paragraph): stddev / (mean+1) to avoid blowup'\n    import re\n    if not text:\n        return 0.0\n    # paragraphs separated by empty lines\n    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n    if not paragraphs:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', p)) for p in paragraphs]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = var ** 0.5\n    return float(std) / (mean + 1.0)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of consecutive duplicate word pairs (e.g., \"the the\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    dup_pairs = sum(1 for a, b in zip(tokens, tokens[1:]) if a == b)\n    return float(dup_pairs / (len(tokens) - 1))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','it','of','to','a','that','i','you','for','on','with','as','was','are','be','at','by','an','this','have','or','not','they','from','but','we','he','she','his','her','them','which','their','were','has','had'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average number of punctuation characters per sentence'\n    import re, string\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    punct_counts = [sum(1 for c in s if c in string.punctuation) for s in sentences]\n    if not punct_counts:\n        return 0.0\n    return float(sum(punct_counts) / len(punct_counts))\n",
    "def feature(text: str) -> float:\n    'Average sentence length measured in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if words:\n            word_counts.append(len(words))\n        else:\n            word_counts.append(0)\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts) / len(word_counts))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word-like tokens that are all-caps (two or more letters uppercase)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(t):\n        letters = [c for c in t if c.isalpha()]\n        return len(letters) >= 2 and all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Punctuation variety: distinct punctuation types divided by total punctuation chars'\n    import string\n    if not text:\n        return 0.0\n    puncs = [c for c in text if c in string.punctuation]\n    if not puncs:\n        return 0.0\n    distinct = len(set(puncs))\n    return float(distinct / len(puncs))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of alphabetic word tokens that are fully uppercase (acronym density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    upper = 0\n    alpha_total = 0\n    for w in words:\n        if any(c.isalpha() for c in w):\n            alpha_total += 1\n            if w.isupper() and len([c for c in w if c.isalpha()]) >= 2:\n                upper += 1\n    if alpha_total == 0:\n        return 0.0\n    return float(upper / alpha_total)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio: proportion of words that occur exactly once'\n    import re\n    from collections import Counter\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    counts = Counter(words)\n    hapax = sum(1 for w, c in counts.items() if c == 1)\n    return float(hapax / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of words that are titlecased (start with uppercase followed by lowercase)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        # treat single-letter uppercase (e.g., \"A\") as titlecased as well\n        if w.istitle():\n            count += 1\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of first-person pronouns (I, me, my, we, us, our, mine, ours)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    pronouns = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n    count = sum(1 for w in words if w in pronouns)\n    result = count / len(words)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences whose first word begins with an uppercase letter'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    count = 0\n    for s in sentences:\n        m = re.search(r'\\b\\w+', s)\n        if m:\n            w = m.group(0)\n            if w and w[0].isupper():\n                count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena ratio (tokens that occur exactly once / total tokens)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    counts = Counter(tokens)\n    hapax = sum(1 for t, c in counts.items() if c == 1)\n    result = hapax / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain any non-ASCII character (unicode token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    def has_non_ascii(t):\n        for c in t:\n            if ord(c) > 127:\n                return True\n        return False\n    count = sum(1 for t in tokens if has_non_ascii(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that end with a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(qcount / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    \"Fraction of tokens that look like simple past-tense verbs (tokens ending with 'ed')\"\n    import re\n    try:\n        tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n        if not tokens:\n            return 0.0\n        ed_count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ed'))\n        return float(ed_count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n",
    "def feature(text: str) -> float:\n    'Average number of exclamation marks per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    exclam = text.count('!')\n    result = exclam / len(sentences) if sentences else 0.0\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (words per sentence); 0 for <2 sentences'\n    import re, math\n    if not text:\n        return 0.0\n    # split on sentence-ending punctuation, keep non-empty\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if len(sentences) < 2:\n        return 0.0\n    lengths = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lengths.append(len(words))\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(math.sqrt(var))\n\n",
    "def feature(text: str) -> float:\n    'Citation-year ratio: fraction of tokens that look like 4-digit years (1000-2099), common in scholarly text'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"[A-Za-z0-9'-]+\", text)\n    if not words:\n        return 0.0\n    years = re.findall(r'\\b(?:1[0-9]{3}|20[0-9]{2})\\b', text)\n    return float(len(years)) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\") among total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    repeats = len(re.findall(r'\\b(\\w+)\\s+\\1\\b', text.lower()))\n    result = repeats / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    words = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not words:\n        return 0.0\n    result = sum(words) / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of emoticons and emoji characters measured per character length'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    if total_len == 0:\n        return 0.0\n    # common ASCII emoticons\n    ascii_emots = re.findall(r'(?:(?:[:;=8][\\-~]?[)DdpP\\(\\\\/])|<3)', text)\n    ascii_count = len(ascii_emots)\n    # count emoji-range characters (several common Unicode blocks)\n    emoji_count = 0\n    for c in text:\n        o = ord(c)\n        if (0x1F300 <= o <= 0x1F5FF) or (0x1F600 <= o <= 0x1F64F) or (0x1F680 <= o <= 0x1F6FF) or (0x2600 <= o <= 0x26FF) or (0x2700 <= o <= 0x27BF):\n            emoji_count += 1\n    result = (ascii_count + emoji_count) / total_len\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a quotation or opening quote char'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    open_quotes = set(['\"', '\u201c', '\u00ab', '\u201a', '\u2018', \"'\"])\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    starts = 0\n    for s in sentences:\n        s_strip = s.lstrip()\n        if s_strip and s_strip[0] in open_quotes:\n            starts += 1\n    return float(starts / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are written in ALL CAPS (len>=2 and contain letters)'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\b\\S+\\b', text)\n    if not toks:\n        return 0.0\n    def is_all_caps(t):\n        has_alpha = any(ch.isalpha() for ch in t)\n        return has_alpha and t.upper() == t and sum(ch.isalpha() for ch in t) >= 2\n    count = sum(1 for t in toks if is_all_caps(t))\n    return float(count / len(toks))\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of word lengths (stddev / mean)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    result = std / mean\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Highest frequency of a sentence-start word divided by sentence count (repetition measure)'\n    import re, collections\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = []\n    for s in sentences:\n        m = re.findall(r'\\b\\w+\\b', s)\n        if m:\n            starts.append(m[0].lower())\n    if not starts:\n        return 0.0\n    freq = collections.Counter(starts)\n    most = max(freq.values())\n    return float(most / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that contain non-ASCII characters (accents, symbols)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if any(ord(ch) > 127 for ch in w):\n            count += 1\n    return float(count / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Normalized variability of token lengths (stddev / mean token length)'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    result = std / mean\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of common English stopwords (heuristic set) among tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','of','and','to','a','in','that','is','for','on','with','as','by','an','be','are','was','were','this','these','those','it','its','from','at'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    stopcount = sum(1 for w in words if w in stopwords)\n    return stopcount / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens (heuristic list)'\n    import re\n    if not text:\n        return 0.0\n    stop = {'the','and','a','to','of','in','is','it','you','that','he','was','for','on','are','as','with','his','they','i','at','be','this','have','from','or','one','had','by','word','but','not','what','all','were','we','when','your','can','said','there','use','an','each','which','she','do','how','their'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stop)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are common personal pronouns'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','me','my','mine','we','us','our','ours','you','your','yours',\n                'he','him','his','she','her','hers','they','them','their','theirs',\n                'himself','herself','themselves','ourselves'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in pronouns)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like social media handles or hashtags (start with @ or #)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) > 1 and (t[0] in ('@', '#') and re.match(r'^[#@][\\w\\-_]+$', t)):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-delimited tokens that contain hyphens, slashes, or underscores'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    special = sum(1 for t in tokens if ('-' in t) or ('_' in t) or ('/' in t))\n    result = special / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens matching common ASCII emoticons'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    # common simple emoticon pattern (covers many ASCII emoticons)\n    pattern = re.compile(r'(?i)(?:[:;=8Xx][\\-~]?[)D\\]pP/\\\\\\(])|(?:[)D\\]pP/\\\\\\(][\\-~]?[:;=8Xx])|(?:[:;=]\\'\\()|(?:<3)')\n    matches = pattern.findall(text)\n    result = len(matches) / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are punctuation-only (no alphanumeric characters)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    punct_only = sum(1 for t in tokens if all(not c.isalnum() for c in t))\n    result = punct_only / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Lexical diversity: fraction of unique word tokens (type/token ratio)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    uniq = len(set(tokens))\n    result = uniq / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of token lengths (std/mean)'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    lengths = [len(t) for t in tokens]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    result = std / mean\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Distinct-token ratio: number of unique word tokens divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    distinct = len(set(tokens))\n    result = distinct / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric (integers or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_re = re.compile(r'^[+-]?\\d+(\\.\\d+)?$')\n    count = sum(1 for t in tokens if num_re.match(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of characters that are symbol/emoji-like (Unicode \"So\" category or common emoji ranges)'\n    import unicodedata\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    sym = 0\n    for c in text:\n        try:\n            cat = unicodedata.category(c)\n        except Exception:\n            cat = ''\n        o = ord(c)\n        if cat == 'So' or 0x1F300 <= o <= 0x1F6FF or 0x1F900 <= o <= 0x1F9FF:\n            sym += 1\n    return float(sym / total)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent word tokens that are immediate repeats (case-insensitive)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    result = repeats / (len(words) - 1)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Fraction of repeated adjacent bigrams (duplicate bigrams / total bigrams), proxy for formulaic repetition'\n    import re\n    tokens = [t.lower() for t in re.findall(r'\\w+', text)]\n    if len(tokens) < 2:\n        return 0.0\n    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n    total = len(bigrams)\n    unique = len(set(bigrams))\n    repeats = total - unique\n    return float(repeats) / total\n\n",
    "def feature(text: str) -> float:\n    'Semicolon density: semicolons per 1000 characters (captures clause-linking complexity)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    semis = text.count(';')\n    # scale to per-1000 characters for numeric stability across lengths\n    return float(semis) / float(total_chars) * 1000.0\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are questions'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    q = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    return float(q / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of characters that are non-ASCII'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are pure numeric tokens (digits only)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    nums = sum(1 for t in tokens if re.fullmatch(r'\\d+', t))\n    return float(nums / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Normalized difference between exclamation/question density inside quotes vs outside (inside - outside)'\n    import re\n    if not text:\n        return 0.0\n    # find double-quoted spans and rest\n    quote_spans = re.findall(r'\"([^\"]*)\"', text)\n    inside_chars = sum(len(s) for s in quote_spans)\n    inside_marks = sum(s.count('!') + s.count('?') for s in quote_spans)\n    outside_text = re.sub(r'\"[^\"]*\"', ' ', text)\n    outside_chars = len(outside_text)\n    outside_marks = outside_text.count('!') + outside_text.count('?')\n    # normalize densities; handle zero-length regions\n    inside_density = (inside_marks / inside_chars) if inside_chars > 0 else 0.0\n    outside_density = (outside_marks / outside_chars) if outside_chars > 0 else 0.0\n    return float(inside_density - outside_density)\n\n",
    "def feature(text: str) -> float:\n    'Density of tokens containing technical symbols (hyphens, slashes, percent, degree, micro, \u00b1 etc.)'\n    s = text or ''\n    toks = s.split()\n    if not toks:\n        return 0.0\n    symbols = set('-/%%\u00b5\u03bc\u00b0\u00b1\u2013\u2014')\n    # count tokens that include any of these special technical characters\n    cnt = 0\n    for t in toks:\n        if any((ch in symbols) for ch in t):\n            cnt += 1\n    result = float(cnt) / float(len(toks))\n    return result\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are punctuation-only (emoji-like or standalone punctuation)'\n    import re, string\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    punct_set = set(string.punctuation)\n    count = 0\n    for t in tokens:\n        if t and all((c in punct_set) for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are in ALL CAPS (length>=2), indicating emphasis/shouting'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        t2 = t.strip('.,:;!?()[]{}\"\\'')\n        if len(t2) >= 2 and any(c.isalpha() for c in t2) and t2 == t2.upper():\n            count += 1\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that match common email address patterns'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    email_re = re.compile(r'^[\\w.+-]+@[\\w-]+\\.[\\w.-]+$', re.IGNORECASE)\n    emails = sum(1 for t in tokens if email_re.match(t))\n    return float(emails / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL CAPS (length>=2 alphabetic characters)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    def is_all_caps(tok):\n        letters = [c for c in tok if c.isalpha()]\n        if len(letters) < 2:\n            return False\n        return all(c.isupper() for c in letters)\n    count = sum(1 for t in tokens if is_all_caps(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Diversity of punctuation: number of distinct punctuation characters divided by total punctuation count'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if not c.isalnum() and not c.isspace()]\n    total = len(puncts)\n    if total == 0:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct) / total\n\n",
    "def feature(text: str) -> float:\n    'Proportion of characters that are non-ASCII'\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not word_counts:\n        return 0.0\n    return float(sum(word_counts) / len(word_counts))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of non-ASCII characters in the text (typographic punctuation, dashes, ellipses, accented letters)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii) / float(total_chars)\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (commas density reflecting clause complexity)'\n    import re\n    if not text:\n        return 0.0\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sentences = [s for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    comma_total = text.count(',')\n    return float(comma_total) / float(len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Density of tokens that contain acronyms/initialisms (>=2 consecutive uppercase letters)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = 0\n    for t in tokens:\n        if re.search(r'[A-Z]{2,}', t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one hyphen (detects age ranges, compound adjectives, e.g., \"73-year-old\")'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    hyphen_count = sum(1 for t in tokens if '-' in t)\n    return float(hyphen_count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Density of common markdown/formatting symbols (*, #, `_`, `) relative to text length'\n    if not text:\n        return 0.0\n    symbols = set('*#_`>')\n    count = sum(1 for c in text if c in symbols)\n    return count / max(1, len(text))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    if not text:\n        return 0.0\n    q = text.count('?')\n    total_sent = text.count('.') + text.count('!') + text.count('?')\n    if total_sent == 0:\n        return 0.0\n    return float(q / total_sent)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are numeric or contain digits (numeric density)'\n    if not text:\n        return 0.0\n    tokens = text.split()\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num) / max(1, len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens that contain any digit (years, chapter numbers, statistics) to total tokens'\n    tokens = text.split()\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(count) / float(n)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord>127), capturing emojis or foreign scripts'\n    if not text:\n        return 0.0\n    L = len(text)\n    if L == 0:\n        return 0.0\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / L)\n\n\n",
    "def feature(text: str) -> float:\n    'Average vowel-to-consonant ratio across words (letters only)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'[A-Za-z]+', text)\n    if not words:\n        return float(0.0)\n    vowels = set('aeiouAEIOU')\n    ratios = []\n    for w in words:\n        v = sum(1 for c in w if c in vowels)\n        c = sum(1 for c in w if c.isalpha()) - v\n        ratios.append((v / c) if c > 0 else float(v))\n    if not ratios:\n        return float(0.0)\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (simple list)'\n    import re\n    if not text:\n        return 0.0\n    STOPWORDS = {\n        'the','and','is','in','it','of','to','a','an','that','this','for','on',\n        'with','as','are','was','were','be','by','at','from','or','but','not',\n        'he','she','they','we','you','i','me','my','our','your','his','her','their'\n    }\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in STOPWORDS)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like common emoticons (e.g. :-) :D <3)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'^(?:[:;=8][\\-~]?[)D\\(Pp3/\\\\]|<3)$')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    result = unique / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Longest run of the same character normalized by text length'\n    if not text:\n        return 0.0\n    max_run = 1\n    cur = 1\n    prev = text[0]\n    for c in text[1:]:\n        if c == prev:\n            cur += 1\n            if cur > max_run:\n                max_run = cur\n        else:\n            cur = 1\n            prev = c\n    return float(max_run / len(text))\n\n",
    "def feature(text: str) -> float:\n    'Density of numeric tokens (tokens containing digits) to capture dates, addresses, years'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    num_tokens = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_tokens) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are first-person pronouns (I, me, my, mine, we, us, our, ours)'\n    import re\n    words = re.findall(r'\\w+', text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n    first_person = re.findall(r'\\b(?:i|me|my|mine|we|us|our|ours)\\b', text, flags=re.IGNORECASE)\n    return float(len(first_person)) / float(wc)\n\n",
    "def feature(text: str) -> float:\n    'Pronoun density: fraction of word tokens that are common personal pronouns (I, you, he, she, we, they, etc.)'\n    import re\n    if not text:\n        return 0.0\n    pronouns = {'i','you','he','she','we','they','me','him','her','us','them','it','its','my','your','our','their','mine','yours','hers','his'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in pronouns)\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (ord > 127)'\n    if not text:\n        return 0.0\n    total = len(text)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    return float(non_ascii / total)\n\n",
    "def feature(text: str) -> float:\n    'Approximate Flesch-Kincaid grade level (0 if insufficient data)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    sentence_count = text.count('.') + text.count('!') + text.count('?')\n    if sentence_count == 0:\n        return 0.0\n    # syllable heuristic similar to common simple rule\n    syllables = 0\n    for w in words:\n        s = len(re.findall(r'[aeiouy]+', w))\n        if w.endswith('e') and s > 1:\n            s -= 1\n        if s < 1:\n            s = 1\n        syllables += s\n    words_count = len(words)\n    # Flesch-Kincaid Grade Level\n    fk = 0.39 * (words_count / sentence_count) + 11.8 * (syllables / words_count) - 15.59\n    return float(fk)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are purely numeric (digits-only tokens)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    num = sum(1 for t in tokens if t.isdigit())\n    result = num / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing a repeated character sequence of length >=3 (elongation)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    rep_re = re.compile(r'(.)\\1{2,}', re.I)\n    count = sum(1 for t in tokens if rep_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Heading-style fraction: fraction of non-empty lines that are all-uppercase or end with a colon (likely headers)'\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    def is_header(ln):\n        s = ln.strip()\n        # treat as header if ends with ':' or is uppercase with at least one letter\n        return s.endswith(':') or (any(ch.isalpha() for ch in s) and s == s.upper())\n    headers = sum(1 for ln in lines if is_header(ln))\n    return headers / len(lines)\n",
    "def feature(text: str) -> float:\n    'Average commas per sentence (commas divided by sentence count, sentences>=1)'\n    import re\n    if not text:\n        return float(0.0)\n    commas = text.count(',')\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    count_sents = max(1, len(sents))\n    return float(commas / count_sents)\n\n",
    "def feature(text: str) -> float:\n    'Lexical diversity: number of unique lowercased word tokens divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b[a-zA-Z']+\\b\", text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique) / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Variance of token lengths (average squared deviation of word lengths)'\n    import re, math\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    lengths = [len(t) for t in tokens]\n    mean = sum(lengths) / len(lengths)\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    return float(var)\n\n\n",
    "def feature(text: str) -> float:\n    'Average estimated syllables per word (simple vowel-group heuristic)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    totals = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        syll = max(1, len(groups))\n        # simple adjustment for trailing silent e\n        if w.endswith('e') and syll > 1 and not w.endswith(('le', 'ye')):\n            syll -= 1\n        totals += syll\n    result = totals / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Hapax ratio: fraction of word types that occur only once (vocabulary richness indicator)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = [w.lower() for w in re.findall(r'\\b\\w+\\b', text)]\n    if not words:\n        return 0.0\n    freqs = Counter(words)\n    hapaxes = sum(1 for w, c in freqs.items() if c == 1)\n    return float(hapaxes) / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Normalized standard deviation of word lengths (stdev / mean length)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    stdev = math.sqrt(var)\n    return float(stdev / mean)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like a short title (short line, initial caps on words)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    title_lines = 0\n    for ln in lines:\n        words = ln.split()\n        if 1 <= len(words) <= 6:\n            all_initial_caps = True\n            for w in words:\n                cleaned = re.sub(r'^[^A-Za-z]+|[^A-Za-z]+$', '', w)\n                if not cleaned:\n                    all_initial_caps = False\n                    break\n                if not cleaned[0].isupper():\n                    all_initial_caps = False\n                    break\n            if all_initial_caps:\n                title_lines += 1\n    return float(title_lines) / len(lines)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of words that contain at least one digit'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    digit_words = sum(1 for w in words if any(ch.isdigit() for ch in w))\n    return float(digit_words / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of longest consecutive repeated word run to total words (detects stuttering/repetition)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    longest = 1\n    current = 1\n    for i in range(1, len(tokens)):\n        if tokens[i] == tokens[i-1]:\n            current += 1\n            if current > longest:\n                longest = current\n        else:\n            current = 1\n    result = longest / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with \"?\")'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    q_count = sum(1 for s in sentences if s.strip().endswith('?'))\n    result = q_count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words longer than 12 characters (very long-word density)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 12)\n    return float(long_count / len(words))\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like code/identifiers (contain _ or \\\\ or / or camelCase or ::)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    code_like = 0\n    camel_re = re.compile(r'[a-z][A-Z]')\n    for t in tokens:\n        if '_' in t or '\\\\' in t or '/' in t or '::' in t or camel_re.search(t):\n            code_like += 1\n    return float(code_like / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if words:\n            word_counts.append(len(words))\n    if not word_counts:\n        return 0.0\n    result = sum(word_counts) / len(word_counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of adjacent repeated tokens (e.g., \"the the\") indicating stuttering/repetition'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if len(tokens) < 2:\n        return 0.0\n    repeats = 0\n    pairs = 0\n    for i in range(len(tokens) - 1):\n        pairs += 1\n        if tokens[i] == tokens[i + 1]:\n            repeats += 1\n    return float(repeats / pairs)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences whose first alphabetic character is uppercase (start-cap consistency)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    def starts_upper(s):\n        for ch in s:\n            if ch.isalpha():\n                return ch.isupper()\n        return False\n    count = sum(1 for s in sentences if starts_upper(s))\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter (first alphabetic char)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sents = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sents:\n        sents = [text.strip()]\n    count = 0\n    for s in sents:\n        m = re.search(r'[A-Za-z]', s)\n        if m and m.group(0).islower():\n            count += 1\n    result = count / len(sents)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word tokens divided by total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    result = unique / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain a hyphen (hyphenated words)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if '-' in t and any(c.isalpha() for c in t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of short words (length <= 2 characters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    short = sum(1 for w in words if len(w) <= 2)\n    result = short / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs, emails, mentions, or hashtags'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pat = re.compile(r'https?://|www\\.|@[A-Za-z0-9_]+|#[A-Za-z0-9_]+|[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}', flags=re.I)\n    count = sum(1 for t in tokens if pat.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average number of transitional/adversative adverbs (however, moreover, therefore, etc.) per sentence'\n    import re\n    if not text:\n        return 0.0\n    trans = {'however','moreover','furthermore','therefore','consequently','nevertheless','additionally','subsequently','meanwhile','thus','hence'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = max(1, len(sentences))\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    trans_count = sum(1 for t in tokens if t in trans)\n    return float(trans_count) / float(sent_count)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique words / total words)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    return float(len(set(words)) / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Ratio of first-person pronouns (I, me, my, mine, we, us, our, ours) to total tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    first_person = re.findall(r'\\b(?:i|me|my|mine|we|us|our|ours)\\b', text, flags=re.IGNORECASE)\n    return float(len(first_person)) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average estimated syllables per word (simple vowel-group heuristic)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    total = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w)\n        syl = max(1, len(groups))\n        # heuristic: trailing silent e often reduces syllable count\n        if w.endswith('e') and len(groups) > 1:\n            syl = max(1, syl - 1)\n        total += syl\n    result = total / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Comma density per sentence: average number of commas per sentence (proxy for clause-complexity)'\n    import re\n    if not text:\n        return 0.0\n    comma_count = text.count(',')\n    # count non-empty sentence fragments\n    sents = [s for s in re.split(r'[.!?]', text) if s.strip()]\n    num_sents = len(sents) if len(sents) > 0 else 1\n    return float(comma_count) / float(num_sents)\n\n",
    "def feature(text: str) -> float:\n    'Average count of coordinating conjunctions (and, but, or, so, for, nor, yet) per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj_re = re.compile(r'\\b(?:and|but|or|so|for|nor|yet)\\b', flags=re.I)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = [len(conj_re.findall(s)) for s in sentences]\n    result = sum(counts) / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words written in all caps (length>=2)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    allcaps = sum(1 for w in words if len(w) >= 2 and w.isalpha() and w.isupper())\n    return float(allcaps / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation-to-word ratio (punctuation characters / number of word tokens)'\n    import re\n    if not text:\n        return 0.0\n    punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    words = re.findall(r'\\w+', text)\n    denom = max(1, len(words))\n    return float(punct_count / denom)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total word tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    types = len(set(words))\n    result = types / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of the definite article \"the\" per token (may capture generic/over-descriptive phrasing)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    the_count = sum(1 for w in words if w == 'the')\n    return float(the_count) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that end with \"ing\" (proxy for progressive aspect / gerunds)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z']+\", text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) >= 4 and t.endswith('ing'))\n    return float(count) / len(tokens)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of alphabetic words that are in ALL CAPS (length >=2)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    alpha_words = re.findall(r'\\b[A-Za-z]{2,}\\b', text)\n    if not alpha_words:\n        return 0.0\n    all_caps = sum(1 for w in alpha_words if w.isupper())\n    result = all_caps / len(alpha_words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Stopword ratio: fraction of tokens that are common English stopwords (function-word density)'\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you',\n        'do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one',\n        'all','would','there','their','what','so','up','out','if','about','who','get','which','go','me'\n    }\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    return float(count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of all \\\"\\'s\\\" occurrences that attach to a capitalized word (proxy for proper-noun possessives like \\\"John\\'s\\\")'\n    import re\n    if not text:\n        return 0.0\n    poss = re.findall(r\"\\b([A-Za-z]\\w*)'s\\b\", text)\n    total_s = len(re.findall(r\"\\b\\w+'s\\b\", text))\n    if total_s == 0:\n        return 0.0\n    cap_count = sum(1 for w in poss if w and w[0].isupper())\n    return float(cap_count) / float(total_s)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of nonempty lines that look like list items or bullets'\n    import re\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return 0.0\n    pattern = re.compile(r'^\\s*(?:[-*\u2022]|[0-9]+[.\\)]|\\u2022|>\\s+)\\s+')\n    bullets = sum(1 for l in lines if pattern.match(l))\n    return float(bullets / len(lines))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total words'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation-to-word ratio: punctuation characters per whitespace-separated word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\S+', text)\n    if not words:\n        return 0.0\n    punct = sum(1 for c in text if (not c.isalnum()) and (not c.isspace()))\n    return float(punct / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens whose alphabetic characters are all uppercase (shouting tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        letters = [c for c in t if c.isalpha()]\n        if letters and all(c.isupper() for c in letters):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation-to-word ratio (punctuation characters per word)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    word_count = len(words)\n    if word_count == 0:\n        return float(0.0)\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    return float(punct / word_count)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a lowercase letter (informal/fragment style)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    count = 0\n    for s in sentences:\n        s2 = s.lstrip(' \"\\'\u201c\u201d\u2018\u2019(')\n        if s2 and s2[0].islower():\n            count += 1\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words longer than 6 characters (long-word density)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    long_count = sum(1 for w in words if len(w) > 6)\n    result = long_count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of colons and semicolons per character (formal/complex punctuation usage)'\n    count = text.count(':') + text.count(';')\n    denom = max(1, len(text))\n    return float(count) / float(denom)\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of sentence lengths (std / mean of words per sentence)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not counts:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    if mean == 0:\n        return 0.0\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain any non-ASCII character (accents, emoji, foreign script)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if any(ord(c) > 127 for c in t):\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Colon density: fraction of characters that are colons (\":\"), common in formal lists/clauses'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    return float(text.count(':')) / total_chars\n\n",
    "def feature(text: str) -> float:\n    'Proportion of text lines that begin with a dialogue marker (quote, dash, or quote-like characters)'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    count = 0\n    for l in lines:\n        stripped = l.lstrip()\n        if not stripped:\n            continue\n        first = stripped[0]\n        if first in {'\"', \"'\", '\u201c', '\u201d', '\u2014', '-', '\u2013'}:\n            count += 1\n    return float(count) / float(len(lines))\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are all-uppercase words (length>=2, indicates \"shouting\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) >= 2 and any(c.isalpha() for c in t) and t.upper() == t:\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-whitespace tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    digit_tokens = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(digit_tokens / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric-like (dates, numbers, amounts)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'^[\\d\\.,:/+-]+$')\n    num = sum(1 for t in tokens if pattern.match(t))\n    return float(num / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Non-ASCII punctuation density: fraction of characters that are punctuation and non-ASCII (captures smart quotes, em-dashes, ellipses)'\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    count = 0\n    for c in text:\n        if not c.isalnum() and not c.isspace() and ord(c) > 127:\n            count += 1\n    return float(count) / total_chars\n\n",
    "def feature(text: str) -> float:\n    'Ellipsis density: number of \"...\" occurrences normalized by token count'\n    import re\n    ellipses = text.count('...')\n    tokens = re.findall(r'\\w+', text)\n    return ellipses / max(1, len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of non-empty lines that begin with a list marker (bullets or numbered)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    marker_re = re.compile(r'^\\s*(?:[-*\u2022]|\\d+[\\.\\)])\\s+')\n    count = sum(1 for ln in lines if marker_re.match(ln))\n    result = count / len(lines)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if re.search(r'\\w', s)]\n    if not sentences:\n        # fallback to words-based average if no sentence punctuation\n        words = re.findall(r'\\w+', text)\n        return float(len(words) / max(1, 1)) if words else 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    return float(sum(lengths) / len(lengths))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing an apostrophe (contractions or possessives)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if (\"'\" in t) or (\"\\u2019\" in t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(len(words) - 1) if words[i] == words[i + 1])\n    return float(repeats / (len(words) - 1))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ly\" (proxy for adverb/adjectival style) per word'\n    try:\n        import re\n        tokens = re.findall(r'\\w+', text.lower())\n        if not tokens:\n            return 0.0\n        ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 2)\n        return float(ly_count) / float(len(tokens))\n    except Exception:\n        return 0.0\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing at least one digit'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of capitalized tokens that are likely proper nouns (capitalized not at sentence start)'\n    import re\n    if not text:\n        return 0.0\n    raw_tokens = re.findall(r\"\\b[^\\s]+\\b\", text)\n    if not raw_tokens:\n        return 0.0\n    proper_like = 0\n    capitalized_total = 0\n    sentence_boundaries = set()\n    for m in re.finditer(r'[.!?]+', text):\n        # mark index after punctuation as potential sentence start\n        sentence_boundaries.add(m.end())\n    for i, tok in enumerate(raw_tokens):\n        stripped = tok.strip('()[]{}\"\\'\u201c\u201d')  # remove surrounding punctuation\n        if not stripped:\n            continue\n        if stripped[0].isupper():\n            capitalized_total += 1\n            # compute approximate start index of token\n            try:\n                start_idx = text.index(tok)\n            except ValueError:\n                start_idx = None\n            # if not at very start and not immediately after sentence punctuation, count as proper-like\n            if start_idx is None:\n                proper_like += 1\n            else:\n                if start_idx != 0 and start_idx not in sentence_boundaries:\n                    proper_like += 1\n    if capitalized_total == 0:\n        return 0.0\n    return float(proper_like) / capitalized_total\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens ending with \"ly\" (common adverb suffix) \u2014 may capture stylistic adverb use'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if len(t) > 2 and t.endswith('ly'))\n    return float(count) / len(tokens)\n\n\n",
    "def feature(text: str) -> float:\n    'Density of common ASCII emoticons among whitespace-separated tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    emoticons = {':)', ':-)', ':(', ':-(', ':D', ':-D', ':P', ':-P', ';)', ';-)', ':/', ':-/', ':-|', ':|', 'XD', 'xD'}\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    def norm(t):\n        return t.strip('.,!?;:').replace('\"','').replace(\"'\", \"\").strip()\n    count = 0\n    for t in tokens:\n        if norm(t) in emoticons or norm(t).upper() in emoticons:\n            count += 1\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Short-word ratio: fraction of word tokens with length <= 3'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    short = sum(1 for w in words if len(w) <= 3)\n    return float(short / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    return float(len(words) / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(https?://|www\\.|@)')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Density of emoticons and emoji (matches common ascii emoticons and basic emoji chars)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticon_re = re.compile(r'^(?:[:;=8][-^]?[)D(Pp/\\|\\\\]|<3|:\\||:\\(|:\\))$')\n    emoticon_count = sum(1 for t in tokens if emoticon_re.search(t))\n    # heuristic emoji count by codepoint ranges\n    emoji_count = 0\n    for ch in text:\n        try:\n            oc = ord(ch)\n        except Exception:\n            continue\n        if 0x1F300 <= oc <= 0x1F9FF:\n            emoji_count += 1\n    total = emoticon_count + emoji_count\n    result = total / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of punctuation characters to word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    num_words = len(words)\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if num_words == 0:\n        return 0.0\n    return float(punct / num_words)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that belong to consecutive repeated-token runs (e.g., \"no no no\")'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    n = len(tokens)\n    if n == 0:\n        return 0.0\n    run_tokens = 0\n    i = 0\n    while i < n:\n        j = i + 1\n        while j < n and tokens[j].lower() == tokens[i].lower():\n            j += 1\n        if j - i >= 2:\n            run_tokens += (j - i)\n        i = j\n    return float(run_tokens / n)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are capitalized but not at the start of a sentence'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [seg.strip() for seg in re.split(r'(?<=[.!?])\\s+', text.strip()) if seg.strip()]\n    if not sentences:\n        return float(0.0)\n    cap_count = 0\n    mid_tokens = 0\n    for sent in sentences:\n        tokens = re.findall(r'\\S+', sent)\n        if len(tokens) <= 1:\n            continue\n        for t in tokens[1:]:\n            mid_tokens += 1\n            first = t[0]\n            if first.isalpha() and first.isupper():\n                cap_count += 1\n    if mid_tokens == 0:\n        return float(0.0)\n    return float(cap_count / mid_tokens)\n\n",
    "def feature(text: str) -> float:\n    'Coefficient of variation of token lengths (std/mean) using word tokens'\n    import re, math\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    lengths = [len(w) for w in words]\n    mean = sum(lengths) / len(lengths)\n    if mean == 0:\n        return 0.0\n    var = sum((l - mean) ** 2 for l in lengths) / len(lengths)\n    std = math.sqrt(var)\n    result = std / mean\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with a coordinating/subordinating conjunction (e.g., and, but, or, so)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    conj = {'and', 'but', 'or', 'so', 'because', 'however', 'then', 'also', 'although', 'though'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'\\b([A-Za-z]+)\\b', s)\n        if m and m.group(1).lower() in conj:\n            count += 1\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence (approximate clause density)'\n    import re\n    if not text:\n        return 0.0\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    comma_count = s.count(',')\n    if not sentences:\n        # if no clear sentences, normalize by 1 to give overall density\n        return float(comma_count / 1)\n    return float(comma_count / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are fully uppercase (shouting indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    upper_count = 0\n    for t in tokens:\n        if any(c.isalpha() for c in t) and t.isupper() and len(t) > 1:\n            upper_count += 1\n    return float(upper_count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that contain at least one digit'\n    import re\n    words = re.findall(r'\\b\\w+\\b', text or '')\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if any(c.isdigit() for c in w))\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens containing a run of the same letter three or more times (elongation)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = 0\n    for w in words:\n        if re.search(r'(.)\\1\\1', w, flags=re.IGNORECASE):\n            count += 1\n    result = count / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'List-item line density: fraction of non-empty lines that appear to be list items (bullets or numbered)'\n    import re\n    if not text:\n        return float(0.0)\n    lines = [l for l in text.splitlines() if l.strip()]\n    if not lines:\n        return float(0.0)\n    pattern = re.compile(r'^\\s*(?:[-*+]|(?:\\d+[\\.\\)]))\\s+')\n    list_lines = sum(1 for l in lines if pattern.match(l))\n    return float(list_lines / len(lines))\n",
    "def feature(text: str) -> float:\n    'Average syllables per word using a simple vowel-group heuristic'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    total_syll = 0\n    for w in words:\n        syll = len(re.findall(r'[aeiouy]+', w))\n        if w.endswith('e') and syll > 1:\n            syll -= 1\n        if syll < 1:\n            syll = 1\n        total_syll += syll\n    return float(total_syll / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are non-ASCII (useful for detecting foreign scripts/emojis)'\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    non_ascii = sum(1 for c in text if ord(c) > 127)\n    result = non_ascii / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Hapax legomena proportion: words that occur exactly once over total words'\n    import re, collections\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    freq = collections.Counter(tokens)\n    hapax = sum(1 for w, c in freq.items() if c == 1)\n    return float(hapax / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average number of commas per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n    sent_count = len(sentences)\n    if sent_count == 0:\n        return 0.0\n    comma_count = text.count(',')\n    return float(comma_count / sent_count)\n",
    "def feature(text: str) -> float:\n    'Density of common emoticons/emoji-like tokens (matches per character)'\n    import re\n    if not text:\n        return 0.0\n    # ASCII emoticons and simple heart token\n    ascii_emot_re = re.compile(r'(?:(?:[:;=8][\\-^]?[)D\\(Pp/\\\\])|<3)')\n    matches = ascii_emot_re.findall(text)\n    count = len(matches)\n    # Attempt to count a few emoji ranges; ignore errors if regex engine differs\n    try:\n        emoji_re = re.compile(r'[\\U0001F300-\\U0001F6FF\\U0001F900-\\U0001F9FF\\U00002600-\\U000027BF]')\n        count += len(emoji_re.findall(text))\n    except re.error:\n        # Fallback: count uncommon non-ascii symbols that are neither alnum nor whitespace\n        count += sum(1 for ch in text if ord(ch) > 10000 and not ch.isalnum() and not ch.isspace())\n    total_chars = max(1, len(text))\n    result = count / total_chars\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of common ASCII emoticons per token (e.g., :-) :D ;))'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    emot_re = re.compile(r'(?:(?:[:;=8][-^]?[)DpP\\(/\\\\]|<3|:-?\\|))', flags=re.I)\n    matches = emot_re.findall(text)\n    return float(len(matches) / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are very short (3 words or fewer)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    short = 0\n    for s in sentences:\n        wc = len(re.findall(r'\\w+', s))\n        if wc <= 3:\n            short += 1\n    result = short / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens with elongated letter sequences (three or more repeats)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    elongated_re = re.compile(r'(.)\\1\\1', re.IGNORECASE)\n    count = sum(1 for w in words if elongated_re.search(w))\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if any(c.isdigit() for c in w))\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Density of explicit line breaks: number of newline characters normalized by text length'\n    if not text:\n        return 0.0\n    # Normalize by length to avoid division by zero\n    return float(text.count('\\n')) / float(len(text) + 1)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (end with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    q = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = q / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized Shannon entropy of character distribution (0-1)'\n    import math\n    if not text:\n        return float(0.0)\n    freq = {}\n    for ch in text:\n        freq[ch] = freq.get(ch, 0) + 1\n    total = sum(freq.values())\n    if total == 0:\n        return float(0.0)\n    entropy = 0.0\n    for v in freq.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    unique = len(freq)\n    if unique <= 1:\n        return float(0.0)\n    denom = math.log2(unique)\n    if denom <= 0:\n        return float(0.0)\n    return float(entropy / denom)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that appear inside single or double quotation marks'\n    import re\n    if not text:\n        return 0.0\n    total_len = len(text)\n    if total_len == 0:\n        return 0.0\n    inside = 0\n    for pat in (r'\"(.*?)\"', r\"'(.*?)'\"):\n        for m in re.findall(pat, text, flags=re.S):\n            inside += len(m)\n    result = min(1.0, inside / total_len)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain explicit quotes (double/curly/angle quotes)'\n    import re\n    if not text:\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', '\u00ab', '\u00bb')\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    qsent = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            qsent += 1\n    result = qsent / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (simple list)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    stopwords = {'the','and','is','in','it','of','to','a','that','i','you','was','for','on','with','as','are','this','be','at','or','by','an','have','not'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing elongated character sequences (3+ repeated chars)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    elong_re = re.compile(r'(.)\\1{2,}')\n    count = sum(1 for t in tokens if elong_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Digit character density: fraction of characters that are digits'\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    digits = sum(1 for c in text if c.isdigit())\n    return float(digits / total)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of words in ALL CAPS (length >=2) to total words'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return float(0.0)\n    caps = sum(1 for w in words if len(w) >= 2 and w.isalpha() and w.isupper())\n    result = caps / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (small built-in list)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stop = {'the','and','of','to','a','in','is','it','you','that','he','she','they','on','for','with','as','i','we','be','was','are','this','by','an','or'}\n    words = [w.lower() for w in re.findall(r'\\b\\w+\\b', text)]\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stop)\n    return float(count / len(words))\n",
    "def feature(text: str) -> float:\n    'Std deviation of sentence lengths (in words) divided by mean sentence length'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lens = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lens:\n        return 0.0\n    mean = sum(lens) / len(lens)\n    if mean == 0 or len(lens) == 1:\n        return 0.0\n    var = sum((L - mean) ** 2 for L in lens) / len(lens)\n    std = math.sqrt(var)\n    return float(std / mean)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence (words measured with \\\\w+), 0 for empty'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    counts = []\n    for s in sentences:\n        w = re.findall(r'\\w+', s)\n        counts.append(len(w))\n    if not counts:\n        return 0.0\n    return float(sum(counts) / len(counts))\n\n",
    "def feature(text: str) -> float:\n    \"Indicator for explicit translation tags like '[translated]' (1.0 if present, else 0.0)\"\n    if not text:\n        return 0.0\n    return 1.0 if '[translated]' in text.lower() else 0.0\n\n",
    "def feature(text: str) -> float:\n    'Density of capitalized words that look like names or proper nouns (not sentence-start \"I\")'\n    try:\n        words = re.findall(r'\\b[A-Za-z][a-z]+\\b', text)\n    except Exception:\n        words = []\n    try:\n        total_words = re.findall(r'\\w+', text)\n    except Exception:\n        total_words = []\n    if not total_words:\n        return 0.0\n    # count capitalized words (first letter uppercase) excluding single-letter \"I\"\n    cap_count = sum(1 for w in words if w[0].isupper() and w.lower() != 'i')\n    return float(cap_count) / float(len(total_words))\n\n",
    "def feature(text: str) -> float:\n    'Average per-token punctuation proportion (punctuation chars / token length)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    ratios = []\n    for t in tokens:\n        L = len(t)\n        if L == 0:\n            ratios.append(0.0)\n        else:\n            punct = sum(1 for c in t if not c.isalnum())\n            ratios.append(punct / L)\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized character entropy (Shannon) across distinct characters, in [0,1]'\n    import math\n    from collections import Counter\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    freqs = Counter(text)\n    ntypes = len(freqs)\n    if ntypes <= 1:\n        return 0.0\n    entropy = 0.0\n    for v in freqs.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    # normalize by max entropy = log2(ntypes)\n    norm = entropy / math.log2(ntypes) if ntypes > 1 else 0.0\n    return float(norm)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not counts:\n        return 0.0\n    return float(sum(counts) / len(counts))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain at least one digit (numeric token density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if re.search(r'\\d', t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Standard deviation of sentence lengths (words per sentence)'\n    import re, math\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    counts = [c for c in counts if c > 0]\n    if len(counts) < 2:\n        return 0.0\n    mean = sum(counts) / len(counts)\n    var = sum((c - mean) ** 2 for c in counts) / len(counts)\n    result = math.sqrt(var)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Quoted content fraction: fraction of characters contained inside single or double quotes'\n    import re\n    if not text:\n        return 0.0\n    total = len(text)\n    if total == 0:\n        return 0.0\n    inside = 0\n    # matches either \"...\" or '...'\n    for a, b in re.findall(r'\"(.*?)\"|\\'(.*?)\\'', text, flags=re.S):\n        if a:\n            inside += len(a)\n        if b:\n            inside += len(b)\n    return float(min(1.0, inside / total))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a conjunction or discourse marker'\n    import re\n    if not text:\n        return 0.0\n    CONJ = {'and', 'but', 'or', 'so', 'because', 'however', 'then', 'also', 'yet', 'thus', 'therefore'}\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if words and words[0].lower() in CONJ:\n            starts += 1\n    return float(starts / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences longer than 20 words (long-sentence density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    long_count = 0\n    for s in sentences:\n        if len(re.findall(r'\\w+', s)) > 20:\n            long_count += 1\n    return float(long_count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation variety: number of distinct punctuation characters used divided by total characters (captures stylistic richness)'\n    import re\n    if not text:\n        return 0.0\n    punct_chars = set('. , ; : ! ? - \u2014 ( ) [ ] { } \" \\' / \\\\ @ # $ % ^ & * _ ` ~ < > = +'.split())\n    # flatten to single-char set\n    punct_chars = set(''.join(punct_chars))\n    found = set(c for c in text if c in punct_chars)\n    # normalize by length to avoid bias from long texts\n    denom = max(1, len(text))\n    return float(len(found)) / denom\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are hyphenated words (letter-hyphen-letter)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = 0\n    for t in tokens:\n        if re.search(r'[A-Za-z]-[A-Za-z]', t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words longer than 6 characters'\n    import re\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    long_count = sum(1 for w in words if len(w) > 6)\n    return float(long_count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain a hyphen (compound adjective/noun usage indicator)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z0-9\u2019'-]+\", text)\n    if not tokens:\n        return 0.0\n    hyphenated = sum(1 for t in tokens if '-' in t)\n    return float(hyphenated) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are personal pronouns (common English pronouns)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    pronouns = {'i','me','you','he','him','she','her','we','us','they','them','my','your','yours','his','hers','our','their','theirs','mine','its','it'}\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in pronouns)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are very short (<=4 words) \u2014 often dialogue fragments'\n    import re\n    if not text:\n        return 0.0\n    # split on sentence terminators but keep fragments\n    sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    if not sentences:\n        return 0.0\n    short_count = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if len(words) <= 4 and len(words) > 0:\n            short_count += 1\n    return float(short_count) / float(len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Density of simple emoticons (counts of common ASCII emoticons per character)'\n    if not text:\n        return 0.0\n    emoticons = [':)', ':-)', ':(', ':-(', ':D', ':-D', ';)', ';-)', ':P', ':-P', ':/', ':-/', ':|', '<3', ':o', ':O', ':-O', '^-^']\n    lower = text\n    total = 0\n    for e in emoticons:\n        total += lower.count(e)\n    if len(text) == 0:\n        return 0.0\n    result = total / len(text)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain a question mark'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    qcount = sum(1 for s in sentences if '?' in s)\n    result = qcount / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of detected Latin binomial-like phrases (Capitalized lowercase + lowercase, e.g., \"Homo sapiens\") to token count'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    word_count = len(words)\n    if word_count == 0:\n        return 0.0\n    matches = re.findall(r'\\b[A-Z][a-z]{2,}\\s+[a-z]{2,}\\b', text)\n    # each match covers two words; normalize by token count\n    return float(len(matches) * 2) / word_count\n\n",
    "def feature(text: str) -> float:\n    'Punctuation character variety: distinct punctuation types divided by total punctuation characters'\n    import string\n    if not text:\n        return 0.0\n    puncs = [c for c in text if c in string.punctuation]\n    if not puncs:\n        return 0.0\n    distinct = len(set(puncs))\n    return float(distinct / len(puncs))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio (unique word types divided by total word tokens)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    types = set(words)\n    result = len(types) / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of punctuation characters per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # split into sentences but keep fallback to whole text\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    punct_counts = []\n    for s in sentences:\n        punct = sum(1 for c in s if not c.isalnum() and not c.isspace())\n        punct_counts.append(punct)\n    result = sum(punct_counts) / max(1, len(sentences))\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that contain at least one numeric token'\n    import re\n    if not text:\n        return float(0.0)\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    with_num = 0\n    for s in sentences:\n        if re.search(r'\\d', s):\n            with_num += 1\n    return float(with_num / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing letter elongations (a letter repeated 3 or more times)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'([A-Za-z])\\1{2,}')\n    count = sum(1 for t in tokens if pattern.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text:\n        return 0.0\n    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    if not sents:\n        # treat the whole text as one sentence\n        return float(len(words))\n    lengths = []\n    for s in sents:\n        wl = re.findall(r'\\w+', s)\n        if wl:\n            lengths.append(len(wl))\n    if not lengths:\n        return 0.0\n    return float(sum(lengths) / len(lengths))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of repeated adjacent word bigrams (1 - unique_bigrams/total_bigrams), 0 if no bigrams'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    total = max(0, len(words) - 1)\n    if total == 0:\n        return 0.0\n    bigrams = [' '.join((words[i], words[i+1])) for i in range(len(words)-1)]\n    unique = len(set(bigrams))\n    return 1.0 - (unique / float(total))\n\n",
    "def feature(text: str) -> float:\n    'Average number of modal verbs per sentence (can/could/may/might/will/would/shall/should/must/ought)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    modal_re = re.compile(r'\\b(?:can|could|may|might|will|would|shall|should|must|ought)\\b', flags=re.I)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if re.search(r'\\w', s)]\n    if not sentences:\n        # if no sentences, compute per entire text as one sentence\n        return float(len(modal_re.findall(text)) / 1.0)\n    counts = [len(modal_re.findall(s)) for s in sentences]\n    return float(sum(counts) / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens that contain any digit (numeric token density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num_count = sum(1 for t in tokens if re.search(r'\\d', t))\n    return float(num_count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of unique word types that occur exactly once (hapax legomena ratio)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    ctr = Counter(words)\n    hapax = sum(1 for w, c in ctr.items() if c == 1)\n    return float(hapax / len(ctr))\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric-like (contain digits and no letters)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    def is_numeric_token(t):\n        if re.search(r'[A-Za-z]', t):\n            return False\n        return bool(re.search(r'\\d', t))\n    count = sum(1 for t in tokens if is_numeric_token(t))\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Normalized variance of sentence lengths (word counts) = var(lengths)/(mean+1)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lengths = [len(re.findall(r'\\w+', s)) for s in sentences]\n    if not lengths:\n        return 0.0\n    mean = sum(lengths) / len(lengths)\n    var = sum((L - mean) ** 2 for L in lengths) / len(lengths)\n    return float(var / (mean + 1.0))\n\n",
    "def feature(text: str) -> float:\n    'Average fraction of punctuation characters inside tokens (per-token punctuation ratio)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    ratios = []\n    for t in tokens:\n        if not t:\n            continue\n        punct = sum(1 for c in t if (not c.isalnum()))\n        ratios.append(punct / len(t))\n    if not ratios:\n        return 0.0\n    result = sum(ratios) / len(ratios)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a capitalized (title-like) first word'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    valid = 0\n    cap = 0\n    for s in sentences:\n        m = re.search(r'\\w+', s)\n        if not m:\n            continue\n        valid += 1\n        word = m.group(0)\n        if word[0].isupper() and (len(word) == 1 or word[1:].islower()):\n            cap += 1\n    if valid == 0:\n        return float(0.0)\n    result = cap / valid\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with an uppercase letter (formal-capitalization signal)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = 0\n    for s in sentences:\n        m = re.search(r'[A-Za-z]', s)\n        if m and s[m.start()].isupper():\n            count += 1\n    return float(count / len(sentences))\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are numeric (integers or decimals)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    nums = re.findall(r'\\b\\d+(?:\\.\\d+)?\\b', text)\n    result = len(nums) / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Estimated average syllable count per word (vowel-group heuristic)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'[A-Za-z]+', text)\n    if not words:\n        return float(0.0)\n    vowel_group_re = re.compile(r'[aeiouyAEIOUY]+')\n    counts = []\n    for w in words:\n        groups = vowel_group_re.findall(w)\n        counts.append(max(1, len(groups)))\n    result = sum(counts) / len(counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Normalized character-level Shannon entropy over letters a-z (0-1)'\n    import re, math\n    if not text or not text.strip():\n        return 0.0\n    letters = re.findall(r'[a-z]', text.lower())\n    if not letters:\n        return 0.0\n    from collections import Counter\n    freq = Counter(letters)\n    total = len(letters)\n    ent = 0.0\n    for v in freq.values():\n        p = v / total\n        if p > 0:\n            ent -= p * math.log2(p)\n    # normalize by log2(26)\n    max_ent = math.log2(26)\n    result = ent / max_ent if max_ent > 0 else 0.0\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain any parentheses or bracket characters'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if any(c in s for c in '()[]{}'))\n    return float(count / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are emoticons or likely emoji'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    emoticon_re = re.compile(r'(:-\\)|:\\)|:-\\(|:\\(|:D|:-D|;-\\)|;\\)|:P|:-P|:\\'\\(|:\\*|<3|:-O|:O)', re.I)\n    count = 0\n    for t in tokens:\n        if emoticon_re.search(t):\n            count += 1\n            continue\n        for ch in t:\n            oc = ord(ch)\n            # common emoji/unicode pictograph ranges heuristic\n            if oc >= 0x1F300 and oc <= 0x1F6FF:\n                count += 1\n                break\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of explicit dialogue markers: count of quotation marks and em-dashes per sentence'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    num_sent = max(1, len(sentences))\n    quote_marks = text.count('\"') + text.count('\u201c') + text.count('\u201d') + text.count(\"''\")\n    emdash_marks = text.count('\u2014') + text.count('--') + text.count('\u2013')\n    markers = quote_marks + emdash_marks\n    return float(markers) / num_sent\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that contain at least one digit'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    has_digit = sum(1 for t in tokens if any(ch.isdigit() for ch in t))\n    return float(has_digit / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that contain any non-ASCII character'\n    if not text or not text.strip():\n        return float(0.0)\n    tokens = text.split()\n    if not tokens:\n        return float(0.0)\n    def has_non_ascii(t):\n        return any(ord(c) > 127 for c in t)\n    count = sum(1 for t in tokens if has_non_ascii(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    uniq = len(set(words))\n    return float(uniq / len(words))\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are emoji/unicode pictographs'\n    import re\n    if not text:\n        return 0.0\n    try:\n        pattern = re.compile(\n            r'['\n            r'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n            r'\\U0001F600-\\U0001F64F'  # emoticons\n            r'\\U0001F680-\\U0001F6FF'  # transport & map\n            r'\\U0001F700-\\U0001F77F'  # alchemical\n            r'\\U0001F780-\\U0001F7FF'\n            r'\\U0001F800-\\U0001F8FF'\n            r'\\U0001F900-\\U0001F9FF'\n            r'\\U0001FA00-\\U0001FA6F'\n            r'\\u2600-\\u26FF'          # misc symbols\n            r'\\u2700-\\u27BF'          # dingbats\n            r']', flags=re.UNICODE)\n        emojis = pattern.findall(text)\n        total = len(text)\n        return float(len(emojis) / total) if total > 0 else 0.0\n    except re.error:\n        # fallback: simple heuristic for surrogate ranges\n        count = sum(1 for c in text if ord(c) > 0x2600)\n        return float(count / len(text)) if text else 0.0\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are likely emoji or miscellaneous symbol characters'\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    ranges = [\n        (0x1F600, 0x1F64F),  # emoticons\n        (0x1F300, 0x1F5FF),  # symbols & pictographs\n        (0x1F680, 0x1F6FF),  # transport & map\n        (0x2600, 0x26FF),    # miscellaneous symbols\n        (0x2700, 0x27BF),    # dingbats\n        (0x1F900, 0x1F9FF),  # supplemental symbols and pictographs\n    ]\n    def is_emoji(cp):\n        return any(start <= cp <= end for start, end in ranges)\n    count = 0\n    for ch in text:\n        if is_emoji(ord(ch)):\n            count += 1\n    return float(count / total)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain double-quote style characters (dialogue indicator)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    quote_chars = set(['\"', '\u201c', '\u201d', '\u00ab', '\u00bb'])\n    count = 0\n    for s in sentences:\n        if any(q in s for q in quote_chars):\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens containing three or more repeated letters in a row (elongation)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    repeat_re = re.compile(r'([a-zA-Z])\\1\\1+')\n    count = sum(1 for w in words if repeat_re.search(w))\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are all-uppercase words (shouting tokens)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        # require at least one alpha and more than one character to avoid single-letter capitals\n        if any(c.isalpha() for c in t) and len([c for c in t if c.isalpha()]) >= 2 and t.isupper():\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of short words (<=3 characters) among all words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    short = sum(1 for w in words if len(w) <= 3)\n    result = short / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of short words (<=3 letters) to long words (>7 letters) approximated as short_count / (1 + long_count)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    if not tokens:\n        return 0.0\n    short_count = sum(1 for t in tokens if len(t) <= 3)\n    long_count = sum(1 for t in tokens if len(t) > 7)\n    return float(short_count) / (1.0 + float(long_count))\n\n",
    "def feature(text: str) -> float:\n    'Median sentence length in words (robust to outliers)'\n    import re, math\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+|\\n+', s)\n    lens = []\n    for sent in sentences:\n        sent = sent.strip()\n        if not sent:\n            continue\n        words = re.findall(r'\\w+', sent)\n        lens.append(len(words))\n    if not lens:\n        return 0.0\n    lens.sort()\n    n = len(lens)\n    if n % 2 == 1:\n        return float(lens[n//2])\n    else:\n        return (lens[n//2 - 1] + lens[n//2]) / 2.0\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that contain any digit (numeric content density)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if any(c.isdigit() for c in t))\n    return float(num / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word types divided by total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    unique = len(set(words))\n    return float(unique / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.|mailto:|[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,})', flags=re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are ALL CAPS words (>=2 letters)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    caps = 0\n    for w in words:\n        # consider alpha-only tokens where letters are uppercase and length>=2\n        if any(c.isalpha() for c in w) and all((not c.isalpha()) or c.isupper() for c in w) and sum(1 for c in w if c.isalpha()) >= 2:\n            caps += 1\n    return float(caps / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are common stopwords'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {\n        'the','and','a','an','of','to','in','is','it','that','this','for','on','with','as','are','was','were','be','by',\n        'or','from','at','not','but','you','i','he','she','they','we','his','her','their','our','your','us','them','so','if'\n    }\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    count = sum(1 for t in tokens if t in stopwords)\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens containing an apostrophe (contractions/possessives)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    count = sum(1 for t in tokens if \"'\" in t)\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return 0.0\n    stopwords = {'the','and','is','in','it','to','of','a','that','for','on','with',\n                 'as','are','was','were','be','by','this','an','or','from','at',\n                 'not','have','has','but','we','they','you','i','he','she','them'}\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    sw = sum(1 for w in words if w in stopwords)\n    return float(sw / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of short tokens (word tokens shorter than 3 characters)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return float(0.0)\n    short = sum(1 for t in tokens if len(t) < 3)\n    return float(short / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    stopwords = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that contain at least one digit (numbers, codes, alphanumerics)'\n    import re\n    if not text:\n        return 0.0\n    toks = re.findall(r'\\S+', text)\n    if not toks:\n        return 0.0\n    count = sum(1 for t in toks if any(c.isdigit() for c in t))\n    return float(count / len(toks))\n\n",
    "def feature(text: str) -> float:\n    'Emoticon density: fraction of tokens that are common emoticons (e.g., :-) :( :D)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+|\\S', text)\n    if not tokens:\n        return 0.0\n    # common emoticon patterns (simple)\n    emoticons = re.findall(r'(?:(?:[:;=8][\\-^]?[)\\]DdpP(/\\|\\\\])|[)\\]DdpP(/\\|\\\\][\\-^]?[:;=8])', text)\n    result = len(emoticons) / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Ratio of punctuation characters to alphabetic characters'\n    if not text:\n        return 0.0\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    letters = sum(1 for c in text if c.isalpha())\n    if letters == 0:\n        return 0.0\n    result = punct / letters\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    qcount = sum(1 for s in sentences if s.endswith('?'))\n    result = qcount / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average vowel proportion per token (vowels / alphabetic chars), averaged across tokens with letters'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return float(0.0)\n    totals = 0.0\n    count = 0\n    vowels = set('aeiouy')\n    for t in tokens:\n        letters = [c for c in t if c.isalpha()]\n        if not letters:\n            continue\n        v = sum(1 for c in letters if c in vowels)\n        totals += (v / len(letters))\n        count += 1\n    if count == 0:\n        return float(0.0)\n    return float(totals / count)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens ending in -ly (a proxy for adverb/adverbial style)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    ly_count = sum(1 for t in tokens if t.endswith('ly') and len(t) > 3)\n    return float(ly_count) / len(tokens)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like contractions (letters apostrophe letters)'\n    import re\n    if not text:\n        return 0.0\n    matches = re.findall(r\"\\b[a-zA-Z]+['\u2019][a-zA-Z]+\\b\", text)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    return float(len(matches) / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of whitespace-separated tokens that look like URLs or email addresses'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|ftp://|www\\.|mailto:|[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,})', flags=re.I)\n    count = sum(1 for t in tokens if url_re.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of whitespace-separated tokens that are predominantly numeric (dates, currencies, numbers)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    num = 0\n    num_re = re.compile(r'^[\\d\\-,./%\u20ac$\u00a3\u00a5]+$')\n    for t in tokens:\n        t2 = t.strip('.,:;!?()[]{}\"\\'')\n        if t2 and num_re.match(t2) and re.search(r'\\d', t2):\n            num += 1\n    return float(num / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Average number of punctuation characters per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    punct_counts = []\n    for s in sentences:\n        punct_counts.append(sum(1 for c in s if not c.isalnum() and not c.isspace()))\n    result = sum(punct_counts) / len(sentences) if sentences else 0.0\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing a character repeated three or more times consecutively (elongation)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    pattern = re.compile(r'(.)\\1\\1')\n    count = sum(1 for t in tokens if pattern.search(t))\n    result = count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (words per sentence)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    s = text.strip()\n    sentences = [seg for seg in re.split(r'(?<=[.!?])\\s+', s) if seg.strip()]\n    if not sentences:\n        # fallback: treat whole text as one sentence\n        sentences = [s]\n    total_words = 0\n    for seg in sentences:\n        total_words += len(re.findall(r'\\w+', seg))\n    if not sentences:\n        return 0.0\n    return float(total_words / len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Heuristic passive-voice sentence fraction (forms of \"be\" near past participles)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    # look for be-verb within a few tokens of a word ending with ed/en (simple heuristic)\n    pattern = re.compile(r'\\b(?:am|is|are|was|were|be|been|being)\\b(?:\\s+\\w+){0,3}\\s+\\w+(?:ed|en)\\b', flags=re.I)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        sentences = [text.strip()]\n    count = sum(1 for s in sentences if pattern.search(s))\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of tokens that look like URLs or email addresses'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(https?://|www\\.)', flags=re.I)\n    email_re = re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', flags=re.I)\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or email_re.search(t) or '://' in t:\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word forms divided by total word tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text.lower())\n    if not tokens:\n        return 0.0\n    unique = len(set(tokens))\n    return float(unique / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are all-caps words (shouting), length >=2'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    cap_count = 0\n    for t in tokens:\n        # strip common surrounding punctuation\n        t2 = re.sub(r'^[^\\w]+|[^\\w]+$', '', t)\n        if len(t2) >= 2 and any(c.isalpha() for c in t2) and t2.isupper():\n            cap_count += 1\n    result = cap_count / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Ratio of unique words to total words (lexical diversity)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    return float(len(set(words)) / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Normalized character entropy (Shannon) of the text excluding whitespace'\n    import math\n    from collections import Counter\n    if not text or not text.strip():\n        return 0.0\n    chars = [c for c in text if not c.isspace()]\n    if not chars:\n        return 0.0\n    cnt = Counter(chars)\n    total = len(chars)\n    entropy = 0.0\n    for v in cnt.values():\n        p = v / total\n        entropy -= p * math.log2(p)\n    unique = len(cnt)\n    if unique <= 1:\n        return 0.0\n    max_entropy = math.log2(unique)\n    result = entropy / max_entropy if max_entropy > 0 else 0.0\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of URLs or domain-like tokens among whitespace-separated tokens'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'https?://\\S+|www\\.\\S+|\\b[\\w.-]+\\.(?:com|org|net|io|gov|edu|me|info)\\b', flags=re.I)\n    url_count = sum(1 for t in tokens if url_re.search(t))\n    return float(url_count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters contained inside single or double quotes'\n    import re\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    # find both \"...\" and '...' occurrences (non-greedy)\n    matches = re.findall(r'\"(.*?)\"|\\'(.*?)\\'', text, flags=re.S)\n    inside = 0\n    for a, b in matches:\n        if a:\n            inside += len(a)\n        if b:\n            inside += len(b)\n    result = min(1.0, inside / total)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are sensory words (see, look, hear, feel, smell, taste, touch and variants)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    if not words:\n        return 0.0\n    senses = {'see','saw','seen','look','looked','gaze','gazed','watch','watched',\n              'hear','heard','listen','listened','feel','felt','touch','touched',\n              'smell','smelt','smelled','taste','tasted','observe','observed','perceive','perceived'}\n    count = sum(1 for w in words if w in senses)\n    return float(count) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are common English stopwords (small set)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {\n        'the','a','an','in','on','of','to','is','are','was','were','and','or','but',\n        'for','with','without','by','at','from','that','this','it','as','be','has','have'\n    }\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w.lower() in stopwords)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of purely numeric tokens to total tokens'\n    import re\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    num = sum(1 for t in tokens if t.isdigit())\n    return float(num / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Average word length (characters) \u2014 longer average can signal more formal or descriptive prose'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    total_len = sum(len(w) for w in words)\n    return float(total_len) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of multi-character tokens that are written in ALL CAPS (shouting indicator)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    caps = sum(1 for t in tokens if len(t) >= 2 and t.isupper())\n    result = caps / len(tokens)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that look like URLs (http/www or domain-like patterns)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    url_re = re.compile(r'^(?:https?://|www\\.)', flags=re.I)\n    domain_re = re.compile(r'\\w+\\.\\w{2,}', flags=re.I)\n    count = 0\n    for t in tokens:\n        if url_re.search(t) or domain_re.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that look like dialogue (start with quotes or dashes)'\n    import re\n    if not text:\n        return float(0.0)\n    markers = {'\"', '\u201c', '\u201d', '\u2018', '\u2019', \"'\", '-', '\u2014', '\u2013'}\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    count = 0\n    for s in sentences:\n        s_stripped = s.lstrip()\n        if not s_stripped:\n            continue\n        if s_stripped[0] in markers:\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of bigram tokens that are repeated beyond their first occurrence (repeat-density of bigrams)'\n    import re\n    from collections import Counter\n    if not text:\n        return 0.0\n    words = re.findall(r'\\b[a-zA-Z0-9]+\\b', text.lower())\n    if len(words) < 2:\n        return 0.0\n    bigrams = ['%s %s' % (words[i], words[i+1]) for i in range(len(words)-1)]\n    freq = Counter(bigrams)\n    total = len(bigrams)\n    repeated_tokens = sum((f - 1) for f in freq.values() if f > 1)\n    return float(repeated_tokens) / float(total) if total > 0 else 0.0\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word types divided by total word tokens'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    types = set(words)\n    return float(len(types) / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Density of emojis or ASCII emoticons (per character)'\n    import re\n    if not text:\n        return 0.0\n    total_chars = len(text)\n    if total_chars == 0:\n        return 0.0\n    # count Unicode emoji by common ranges\n    emoji_count = 0\n    for c in text:\n        o = ord(c)\n        if 0x1F300 <= o <= 0x1F5FF or 0x1F600 <= o <= 0x1F64F or 0x1F680 <= o <= 0x1F6FF or 0x2600 <= o <= 0x26FF:\n            emoji_count += 1\n    # ASCII emoticon patterns like :) :-) :D :P ;-)\n    emoticons = re.findall(r'(?:(?:[:;=8][\\-^]?[)DdpP\\(/\\\\|])|(?:[)DdpP\\(/\\\\|][\\-^]?[:;=8]))', text)\n    emoji_count += len(emoticons)\n    return float(emoji_count / total_chars)\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: number of unique word forms divided by total words (lexical diversity)'\n    import re\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    uniq = len(set(words))\n    return float(uniq) / float(len(words))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of alphabetic characters that are uppercase'\n    if not text:\n        return 0.0\n    letters = [c for c in text if c.isalpha()]\n    if not letters:\n        return 0.0\n    upper = sum(1 for c in letters if c.isupper())\n    return float(upper / len(letters))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation diversity: number of distinct punctuation characters divided by total punctuation characters (0 if none)'\n    import string\n    punct_chars = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    if not punct_chars:\n        return 0.0\n    distinct = set(punct_chars)\n    return float(len(distinct)) / float(len(punct_chars))\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    word_counts = [len(re.findall(r'\\w+', s)) for s in sentences]\n    # avoid division by zero if sentences exist but have no words\n    valid_counts = [c for c in word_counts if c > 0]\n    if not valid_counts:\n        return 0.0\n    result = sum(valid_counts) / len(valid_counts)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of modal verbs (can/could/may/might/must/shall/should/will/would) among tokens'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\b\\w[\\w']*\\b\", text.lower())\n    if not tokens:\n        return 0.0\n    modals = {'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would'}\n    count = sum(1 for t in tokens if t in modals)\n    return float(count) / float(len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word tokens that are entirely uppercase (shouting words)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    all_caps = sum(1 for w in words if w.isupper() and len(w) > 1)\n    return float(all_caps / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are ALL-CAPS words of length>=2 (shouting/acronym signal)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"[A-Za-z]+\", text)\n    if not tokens:\n        return 0.0\n    caps = sum(1 for t in tokens if t.isupper() and len(t) >= 2)\n    result = caps / len(tokens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of double-quote characters (indicator of direct speech/dialogue) per word'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    total_words = max(1, len(words))\n    quote_chars = text.count('\"') + text.count('\u201c') + text.count('\u201d')\n    return float(quote_chars) / total_words\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    total_words = 0\n    for s in sentences:\n        total_words += len(re.findall(r'\\w+', s))\n    return float(total_words / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens matching common ASCII emoticons or heart \"<3\"'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    patterns = [\n        r'[:;=8][\\-~]?[)DdpP]',  # :-) :D ;) etc\n        r'[\\)\\(][\\-~]?[:;=8]',  # reversed\n        r'<3', r':-?\\|', r':-?/', r':\\'\\(', r'XD', r'xd'\n    ]\n    combined = re.compile('(?:' + '|'.join(patterns) + r')$')\n    count = 0\n    for t in tokens:\n        if combined.search(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: unique word count divided by total word count (lexical diversity)'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    unique = len(set(words))\n    return float(unique / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of tokens that appear to be possessives (ending with \\'s, \u2019s, or s\\')'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r\"\\S+\", text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        tl = t.lower().rstrip('.,;:!?')\n        if tl.endswith(\"'s\") or tl.endswith(\"\u2019s\") or tl.endswith(\"s'\"):\n            count += 1\n    return float(count) / float(len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of characters that are likely emoji (common Unicode emoji ranges)'\n    import re\n    if not text:\n        return 0.0\n    try:\n        emoji_re = re.compile('[\\U0001F300-\\U0001F5FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F900-\\U0001F9FF\\U00002600-\\U000026FF\\U00002700-\\U000027BF]', flags=re.UNICODE)\n        matches = emoji_re.findall(text)\n    except re.error:\n        # fallback: treat high-codepoint chars as emoji-like\n        matches = [c for c in text if ord(c) >= 0x1F300]\n    total = len(text)\n    if total == 0:\n        return 0.0\n    return float(len(matches) / total)\n\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words (sentences split on .!?; fallback to 1 if no sentence boundary)'\n    import re\n    if not text:\n        return 0.0\n    # split into sentences using punctuation, keep non-empty\n    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    words = re.findall(r\"\\b[\\w']+\\b\", text)\n    if not words:\n        return 0.0\n    if not sents:\n        return float(len(words))\n    avg = float(len(words)) / float(len(sents))\n    return avg\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that are questions (ending with a question mark)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = sum(1 for s in sentences if s.rstrip().endswith('?'))\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of non-empty lines that look like list items (bullets or numbered)'\n    import re\n    if not text:\n        return 0.0\n    lines = [ln for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0\n    pat = re.compile(r'^\\s*(?:[-*+]|(?:\\d+\\.))\\s+')\n    count = sum(1 for ln in lines if pat.match(ln))\n    return float(count / len(lines))\n\n",
    "def feature(text: str) -> float:\n    'Average syllable-like count per word (approx: vowel groups)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r\"[A-Za-z]+\", text)\n    if not words:\n        return 0.0\n    tot = 0\n    for w in words:\n        groups = re.findall(r'[aeiouy]+', w.lower())\n        count = len(groups)\n        if count == 0:\n            count = 1\n        tot += count\n    result = tot / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of emoticons and emoji characters in the text'\n    import re\n    if not text:\n        return float(0.0)\n    # common ASCII emoticons\n    emoticons = re.findall(r'[:;=8][\\-~]?[)D\\]pP/(\\\\]|<3', text)\n    # count characters in common emoji ranges (simple heuristic)\n    emoji_count = sum(1 for c in text if ord(c) >= 0x1F300 and ord(c) <= 0x1FAFF)\n    total_chars = len(text)\n    if total_chars == 0:\n        return float(0.0)\n    return float((len(emoticons) + emoji_count) / total_chars)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of word-like tokens that contain any digit (numeric token density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    words = re.findall(r'\\b\\w+\\b', text)\n    if not words:\n        return 0.0\n    num = sum(1 for w in words if any(ch.isdigit() for ch in w))\n    return float(num / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that are very short (3 words or fewer)'\n    import re\n    if not text or not text.strip():\n        return float(0.0)\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return float(0.0)\n    short = 0\n    for s in sentences:\n        wc = len(re.findall(r'\\w+', s))\n        if wc <= 3:\n            short += 1\n    result = short / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of tokens that are numeric-like (digits, currency, percents)'\n    import re\n    if not text:\n        return float(0.0)\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return float(0.0)\n    num_re = re.compile(r'^[\\d\\.,%+\\-]+$')\n    cur_re = re.compile(r'^[\u00a3$\u20ac]\\s?[\\d\\.,]+$')\n    count = 0\n    for t in tokens:\n        if num_re.match(t) or cur_re.match(t):\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Newline density: fraction of characters that are newline characters (indicates paragraph/section structure)'\n    if not text:\n        return 0.0\n    return text.count('\\n') / float(len(text))\n\n",
    "def feature(text: str) -> float:\n    'Proportion of words that are capitalized but not sentence-initial (internal titlecase)'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    total_words = 0\n    cap_internal = 0\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        if not words:\n            continue\n        for i, w in enumerate(words):\n            total_words += 1\n            if i > 0 and w and w[0].isupper():\n                cap_internal += 1\n    if total_words == 0:\n        return 0.0\n    return float(cap_internal / total_words)\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that contain explicit quotation characters'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    quote_chars = ('\"', '\u201c', '\u201d', '\u00ab', '\u00bb')\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    count = sum(1 for s in sentences if any(q in s for q in quote_chars))\n    return float(count / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of short, capitalized header-like lines (e.g., \"Background\" on its own line)'\n    if not text:\n        return 0.0\n    lines = [l for l in text.splitlines() if l.strip() != '']\n    if not lines:\n        return 0.0\n    count = 0\n    for l in lines:\n        s = l.strip()\n        parts = s.split()\n        if 1 <= len(parts) <= 3 and parts[0][:1].isupper() and all(p.isalpha() for p in parts):\n            # treat single- or short-word title-like lines as headers\n            count += 1\n    return float(count) / float(len(lines))\n\n",
    "def feature(text: str) -> float:\n    'Variety of punctuation: distinct punctuation characters divided by total punctuation chars'\n    if not text:\n        return 0.0\n    puncts = [c for c in text if (not c.isalnum()) and (not c.isspace())]\n    if not puncts:\n        return 0.0\n    distinct = len(set(puncts))\n    return float(distinct / len(puncts))\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens that are second-person pronouns (you, your, yours) indicating promotional/personal tone'\n    if not text:\n        return 0.0\n    words = [w.strip('.,;:\"\\'()[]') .lower() for w in text.split()]\n    if not words:\n        return 0.0\n    second = sum(1 for w in words if w in {'you', 'your', 'yours', 'yourselves', 'yourself'})\n    return second / len(words)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a lowercase letter (indicative of informal/fragment style)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lower_start = 0\n    for s in sentences:\n        s_stripped = s.lstrip(' \\t\\n\"\\'' )\n        first_alpha = None\n        for ch in s_stripped:\n            if ch.isalpha():\n                first_alpha = ch\n                break\n            if ch.isalnum():\n                first_alpha = ch\n                break\n        if first_alpha is not None and first_alpha.isalpha() and first_alpha.islower():\n            lower_start += 1\n    return float(lower_start / len(sentences))\n\n\n",
    "def feature(text: str) -> float:\n    'Density of spaced hyphen/dash occurrences (\" - \", em-dash, en-dash) per sentence'\n    import re\n    if not text:\n        return 0.0\n    occurrences = len(re.findall(r'(?:\\s-\\s|\\u2014|\\u2013)', text))\n    sentences = max(1, len(re.findall(r'[.!?]+', text)))\n    return occurrences / float(sentences)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of sentences that start with a common coordinating/subordinating conjunction'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts = {'and', 'but', 'or', 'so', 'because', 'however', 'then', 'thus', 'yet', 'although', 'since'}\n    count = 0\n    for s in sentences:\n        m = re.findall(r'\\w+', s)\n        if m and m[0].lower() in starts:\n            count += 1\n    result = count / len(sentences)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Passive-voice proxy: matches of \"was/were/is/are/been/being/has/had\" followed within 0-3 words by an -ed token, normalized by sentence count'\n    import re\n    if not text:\n        return 0.0\n    # count sentences to normalize\n    sentence_count = max(1, len([s for s in re.split(r'[.!?]+', text) if s.strip()]))\n    # rough passive pattern\n    pattern = re.compile(r'\\b(?:was|were|is|are|been|being|be|has|had|have)\\b(?:\\s+\\w+){0,3}\\s+\\w+ed\\b', re.IGNORECASE)\n    matches = pattern.findall(text)\n    return float(len(matches)) / float(sentence_count)\n\n",
    "def feature(text: str) -> float:\n    'Proportion of adjacent repeated words (e.g., \"the the\")'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if len(words) < 2:\n        return 0.0\n    repeats = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n    result = repeats / (len(words) - 1)\n    return float(result)\n",
    "def feature(text: str) -> float:\n    'Proportion of common English stopwords among word tokens (simple stopword density)'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    stopwords = {'the','be','to','of','and','a','in','that','have','i','it','for','not','on','with','he','as','you','do','at','this','but','his','by','from','they','we','say','her','she','or','an','will','my','one','all','would','there','their'}\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in stopwords)\n    return float(count / len(words))\n",
    "def feature(text: str) -> float:\n    'Average sentence length in words'\n    import re\n    if not text:\n        return 0.0\n    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    lens = []\n    for s in sentences:\n        words = re.findall(r'\\w+', s)\n        lens.append(len(words))\n    if not lens:\n        return 0.0\n    result = sum(lens) / len(lens)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Average number of words per sentence'\n    import re\n    if not text or not text.strip():\n        return 0.0\n    sentences = [s for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n    if not sentences:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    avg = len(words) / max(1, len(sentences))\n    return float(avg)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are titlecase (First upper, rest lower)'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\w+', text)\n    if not tokens:\n        return 0.0\n    count = 0\n    for t in tokens:\n        if len(t) > 1 and t[0].isupper() and t[1:].islower():\n            count += 1\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Punctuation diversity: distinct punctuation characters divided by total punctuation characters'\n    import re\n    if not text:\n        return 0.0\n    punct = [c for c in text if not c.isalnum() and not c.isspace()]\n    if not punct:\n        return 0.0\n    distinct = len(set(punct))\n    total = len(punct)\n    return float(distinct) / float(total)\n\n",
    "def feature(text: str) -> float:\n    'Type-token ratio: distinct word forms divided by total word count'\n    import re\n    if not text:\n        return float(0.0)\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return float(0.0)\n    distinct = len(set(words))\n    result = distinct / len(words)\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Density of long repeated punctuation sequences (count of char repeats>=3 per character)'\n    import re\n    if not text:\n        return float(0.0)\n    total = len(text)\n    if total == 0:\n        return float(0.0)\n    # match any non-word non-space char repeated at least 3 times\n    repeats = re.findall(r'([^\\w\\s])\\1\\1+', text)\n    count = len(repeats)\n    result = count / total\n    return float(result)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of word tokens that are short (length <= 2)'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text)\n    if not words:\n        return 0.0\n    short = sum(1 for w in words if len(w) <= 2)\n    return float(short / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Sentence-end emphasis ratio: proportion of sentences ending in \"!\" or \"?\" (exclamation/question emphasis)'\n    if not text:\n        return 0.0\n    import re\n    # find sentence endings with their final punctuation\n    ends = re.findall(r'([^\\S\\r\\n]*)([^.!?]*)([.!?])', text)\n    if not ends:\n        return 0.0\n    total = len(ends)\n    emph = sum(1 for (_,_,p) in ends if p in ('!','?'))\n    return float(emph) / float(total)\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that begin with a subordinating or clause-introducing conjunction (because, although, since, while, if, when, though, unless, after, before, as, once)'\n    import re\n    subs = {'because','although','since','while','if','when','though','unless','after','before','as','once'}\n    s = text.strip()\n    if not s:\n        return 0.0\n    sentences = [sent.strip() for sent in re.split(r'(?<=[.!?])\\s+', s) if sent.strip()]\n    if not sentences:\n        return 0.0\n    starts = 0\n    for sent in sentences:\n        # remove leading non-word characters like quotes or dashes\n        sent_clean = re.sub(r'^[^\\w]+', '', sent).lower()\n        first_word_match = re.match(r'(\\w+)', sent_clean)\n        if first_word_match and first_word_match.group(1) in subs:\n            starts += 1\n    return float(starts) / float(len(sentences))\n\n",
    "def feature(text: str) -> float:\n    'Average number of punctuation characters per token'\n    import re\n    if not text:\n        return 0.0\n    tokens = re.findall(r'\\S+', text)\n    if not tokens:\n        return 0.0\n    total_punct = 0\n    for t in tokens:\n        total_punct += sum(1 for c in t if not c.isalnum())\n    return float(total_punct / len(tokens))\n\n\n",
    "def feature(text: str) -> float:\n    'Ratio of common English stopwords to total words'\n    import re\n    STOPWORDS = {\n        'the','and','is','in','it','of','to','a','an','that','this','for','on','with',\n        'as','are','was','were','be','by','or','from','at','not','but','have','has','had',\n        'you','i','he','she','they','we','their','them','his','her'\n    }\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    count = sum(1 for w in words if w in STOPWORDS)\n    return float(count / len(words))\n\n",
    "def feature(text: str) -> float:\n    'Average vowel fraction per word (vowels / letters) averaged across words'\n    import re\n    if not text:\n        return 0.0\n    words = re.findall(r'\\w+', text.lower())\n    if not words:\n        return 0.0\n    vowels = set('aeiouy')\n    total = 0.0\n    for w in words:\n        v = sum(1 for c in w if c.isalpha() and c in vowels)\n        cons = sum(1 for c in w if c.isalpha() and c not in vowels)\n        letters = v + cons\n        total += (v / letters) if letters > 0 else 0.0\n    return float(total / len(words))\n\n\n",
    "def feature(text: str) -> float:\n    'Fraction of tokens containing a character repeated three or more times consecutively'\n    import re\n    if not text:\n        return 0.0\n    tokens = text.split()\n    if not tokens:\n        return 0.0\n    pat = re.compile(r'(.)\\1{2,}', flags=re.I)\n    count = sum(1 for t in tokens if pat.search(t))\n    return float(count / len(tokens))\n\n",
    "def feature(text: str) -> float:\n    'Ratio of punctuation characters to alphabetic characters'\n    if not text:\n        return 0.0\n    alpha = sum(1 for c in text if c.isalpha())\n    punct = sum(1 for c in text if not c.isalnum() and not c.isspace())\n    if alpha == 0:\n        return 0.0\n    return float(punct / alpha)\n\n",
    "def feature(text: str) -> float:\n    'Fraction of sentences that start with the word \"as\" (case-insensitive)'\n    import re\n    if not text or text.strip() == '':\n        return 0.0\n    # Split into sentence-like chunks\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    if not sentences:\n        return 0.0\n    starts_with_as = 0\n    for s in sentences:\n        first_word_match = re.match(r\"^\\s*([A-Za-z']+)\", s)\n        if first_word_match and first_word_match.group(1).lower() == 'as':\n            starts_with_as += 1\n    return float(starts_with_as) / float(len(sentences))\n\n"
  ]
}